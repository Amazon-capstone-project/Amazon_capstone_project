{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/pipeline/1550_rows_46_columns_with_modelScores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date difference between 'CreationDate' and 'LastActivityDate' or 'LastEditDate'\n",
    "import re\n",
    "from datetime import datetime\n",
    "def last_date(row):\n",
    "    pattern = '^\\d{4}[-]\\d{2}[-]\\d{2}'\n",
    "    if not pd.isna(row['LastActivityDate']) :\n",
    "        matched = bool(re.match(pattern, row['LastActivityDate']))\n",
    "        if matched == True:\n",
    "            datetime_object = datetime.strptime(row['LastActivityDate'],  '%Y-%m-%dT%H:%M:%S.%f')\n",
    "            return (datetime_object-row['CreationDate']).days\n",
    "        else:\n",
    "            # Use LastEditDate\n",
    "            if not pd.isna(row['LastEditDate']):\n",
    "                matched_edit = bool(re.match(pattern, row['LastEditDate']))\n",
    "                if matched_edit == True:\n",
    "                    datetime_object = datetime.strptime(row['LastEditDate'],  '%Y-%m-%dT%H:%M:%S.%f')\n",
    "                    return (datetime_object-row['CreationDate']).days\n",
    "            \n",
    "    diff = (datetime.today()-row['CreationDate']).days\n",
    "    return diff\n",
    "\n",
    "df[['CreationDate']] = df[['CreationDate']].apply(pd.to_datetime)\n",
    "df['Date_diff'] = df.apply(lambda row : last_date(row), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1550 entries, 0 to 1549\n",
      "Data columns (total 49 columns):\n",
      "Unnamed: 0                     1550 non-null int64\n",
      "Unnamed: 0.1                   1550 non-null int64\n",
      "AnswerCount                    1298 non-null float64\n",
      "ClosedDate                     29 non-null object\n",
      "CommentCount                   955 non-null object\n",
      "CommentCount_ans               343 non-null object\n",
      "CommentCount_que               941 non-null float64\n",
      "ContentLicense                 953 non-null object\n",
      "CreationDate                   1550 non-null datetime64[ns]\n",
      "CreationDate_ans               343 non-null object\n",
      "CreationDate_que               1298 non-null object\n",
      "FavoriteCount                  462 non-null float64\n",
      "Id                             1550 non-null int64\n",
      "Id_ans                         343 non-null float64\n",
      "Id_que                         1298 non-null float64\n",
      "LastActivityDate               955 non-null object\n",
      "LastActivityDate_ans           343 non-null float64\n",
      "LastActivityDate_que           941 non-null object\n",
      "LastEditDate                   631 non-null object\n",
      "OwnerDisplayName               598 non-null object\n",
      "OwnerUserId                    598 non-null object\n",
      "ParentId                       941 non-null float64\n",
      "PostId                         952 non-null float64\n",
      "PostTypeId                     598 non-null float64\n",
      "Score                          1550 non-null int64\n",
      "Score_ans                      343 non-null float64\n",
      "Score_que                      1298 non-null float64\n",
      "Tags                           1298 non-null object\n",
      "Text                           952 non-null object\n",
      "Title                          1298 non-null object\n",
      "UserId                         952 non-null object\n",
      "ViewCount                      1298 non-null float64\n",
      "cnt_keywords                   1550 non-null int64\n",
      "from_dataset                   971 non-null object\n",
      "include_irrel                  1550 non-null bool\n",
      "negative_statement             1550 non-null int64\n",
      "of_answer                      952 non-null object\n",
      "punctuation                    1550 non-null object\n",
      "sentence                       1550 non-null object\n",
      "subj_irrel                     1550 non-null bool\n",
      "subject                        520 non-null object\n",
      "share_link                     1550 non-null int64\n",
      "aws_related_tags               1129 non-null object\n",
      "share_code                     1550 non-null int64\n",
      "outdated (manually checked)    187 non-null object\n",
      "is_training                    1550 non-null object\n",
      "keywords                       952 non-null object\n",
      "model_score                    1550 non-null float64\n",
      "Date_diff                      1550 non-null int64\n",
      "dtypes: bool(2), datetime64[ns](1), float64(13), int64(9), object(24)\n",
      "memory usage: 572.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 49)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_df = df[df['outdated (manually checked)'].isnull()==False]\n",
    "checked_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     130\n",
       "False     57\n",
       "Name: outdated (manually checked), dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_df['outdated (manually checked)'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Unnamed: 0.1',\n",
       " 'AnswerCount',\n",
       " 'ClosedDate',\n",
       " 'CommentCount',\n",
       " 'CommentCount_ans',\n",
       " 'CommentCount_que',\n",
       " 'ContentLicense',\n",
       " 'CreationDate',\n",
       " 'CreationDate_ans',\n",
       " 'CreationDate_que',\n",
       " 'FavoriteCount',\n",
       " 'Id',\n",
       " 'Id_ans',\n",
       " 'Id_que',\n",
       " 'LastActivityDate',\n",
       " 'LastActivityDate_ans',\n",
       " 'LastActivityDate_que',\n",
       " 'LastEditDate',\n",
       " 'OwnerDisplayName',\n",
       " 'OwnerUserId',\n",
       " 'ParentId',\n",
       " 'PostId',\n",
       " 'PostTypeId',\n",
       " 'Score',\n",
       " 'Score_ans',\n",
       " 'Score_que',\n",
       " 'Tags',\n",
       " 'Text',\n",
       " 'Title',\n",
       " 'UserId',\n",
       " 'ViewCount',\n",
       " 'cnt_keywords',\n",
       " 'from_dataset',\n",
       " 'include_irrel',\n",
       " 'negative_statement',\n",
       " 'of_answer',\n",
       " 'punctuation',\n",
       " 'sentence',\n",
       " 'subj_irrel',\n",
       " 'subject',\n",
       " 'share_link',\n",
       " 'aws_related_tags',\n",
       " 'share_code',\n",
       " 'outdated (manually checked)',\n",
       " 'is_training',\n",
       " 'keywords',\n",
       " 'model_score',\n",
       " 'Date_diff']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(checked_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only take useful features\n",
    "features = ['AnswerCount','CommentCount', 'CommentCount_que', 'FavoriteCount', 'Score', 'Score_que', 'ViewCount', 'cnt_keywords',\n",
    "       'include_irrel', 'negative_statement', 'of_answer', 'subj_irrel', 'share_link', 'share_code', 'model_score', 'Date_diff']\n",
    "X_train = checked_df[features]\n",
    "y = checked_df['outdated (manually checked)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y.astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.8353801169590642\n",
      "f1 score:  0.8820226937813146\n",
      "precision score:  0.8685626873126873\n",
      "recall score:  0.9\n",
      "roc_auc score:  0.8047435897435898\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "cont = make_pipeline(SimpleImputer(strategy='mean'), StandardScaler())\n",
    "\n",
    "categorical = df[features].dtypes == bool\n",
    "\n",
    "preprocess = make_column_transformer(\n",
    "    (cont, ~categorical), remainder = 'passthrough')\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scoring = ['accuracy', 'precision', 'recall','f1','roc_auc']\n",
    "\n",
    "scores_lr = cross_validate(make_pipeline(preprocess, LogisticRegression()), X_train, y_train, cv=10,\n",
    "                        scoring=scoring)\n",
    "\n",
    "print('accuracy score: ', np.mean(scores_lr['test_accuracy']))\n",
    "print('f1 score: ', np.mean(scores_lr['test_f1']))\n",
    "print('precision score: ', np.mean(scores_lr['test_precision']))\n",
    "print('recall score: ', np.mean(scores_lr['test_recall']))\n",
    "print('roc_auc score: ', np.mean(scores_lr['test_roc_auc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.8453216374269005\n",
      "f1 score:  0.8907283342455757\n",
      "precision score:  0.865551948051948\n",
      "recall score:  0.9230769230769231\n",
      "roc_auc score:  0.7937179487179488\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = Pipeline([(\"preprocessor\", preprocess),\n",
    "                 (\"clf\", RandomForestClassifier())])\n",
    "\n",
    "param_grid={'clf__max_features': ['sqrt','log2'],\n",
    "            'clf__n_estimators': (10, 50, 100)}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring='accuracy', cv=5,\n",
    "                    n_jobs=-1, return_train_score=True)\n",
    "\n",
    "grid_result =grid.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_result.best_params_\n",
    "\n",
    "rfc = RandomForestClassifier(max_features=best_params['clf__max_features'], \n",
    "                            n_estimators=best_params['clf__n_estimators'], \n",
    "                            random_state=False, verbose=False)\n",
    "\n",
    "scores_rfc = cross_validate(make_pipeline(preprocess, rfc), X_train, y_train, cv=10, scoring=scoring)\n",
    "                        \n",
    "print('accuracy score: ', np.mean(scores_rfc['test_accuracy']))\n",
    "print('f1 score: ', np.mean(scores_rfc['test_f1']))\n",
    "print('precision score: ', np.mean(scores_rfc['test_precision']))\n",
    "print('recall score: ', np.mean(scores_rfc['test_recall']))\n",
    "print('roc_auc score: ', np.mean(scores_rfc['test_roc_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.8827485380116957\n",
      "f1 score:  0.9208173690932313\n",
      "precision score:  0.8723214285714285\n",
      "recall score:  0.976923076923077\n",
      "roc_auc score:  0.8314102564102563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe = Pipeline([(\"preprocessor\", preprocess),\n",
    "                 (\"clf\", SVC())])\n",
    "\n",
    "param_grid={'clf__C': (0.001, 0.01, 0.1, 1, 10),\n",
    "            'clf__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'clf__gamma': (0.001, 0.01, 0.1, 1)}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring='accuracy', cv=5,\n",
    "                    n_jobs=-1, return_train_score=True)\n",
    "\n",
    "grid_result =grid.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_result.best_params_\n",
    "\n",
    "svc = SVC(C=best_params['clf__C'],  \n",
    "          kernel = best_params['clf__kernel'],\n",
    "          gamma = best_params['clf__gamma'],\n",
    "          random_state=False, verbose=False)\n",
    "\n",
    "scores_svc = cross_validate(make_pipeline(preprocess, svc), X_train, y_train, cv=10, scoring=scoring)\n",
    "                        \n",
    "print('accuracy score: ', np.mean(scores_svc['test_accuracy']))\n",
    "print('f1 score: ', np.mean(scores_svc['test_f1']))\n",
    "print('precision score: ', np.mean(scores_svc['test_precision']))\n",
    "print('recall score: ', np.mean(scores_svc['test_recall']))\n",
    "print('roc_auc score: ', np.mean(scores_svc['test_roc_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.7710526315789473\n",
      "f1 score:  0.8299246591600415\n",
      "precision score:  0.8429395604395603\n",
      "recall score:  0.8307692307692308\n",
      "roc_auc score:  0.7673076923076924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "pipe = Pipeline([(\"preprocessor\", preprocess),\n",
    "                 (\"clf\", SGDClassifier())])\n",
    "\n",
    "param_grid={'clf__penalty': ['l1', 'l2'],\n",
    "            'clf__alpha': np.arange(0.0001, 0.01)}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring='accuracy', cv=5,\n",
    "                    n_jobs=-1, return_train_score=True)\n",
    "\n",
    "grid_result =grid.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_result.best_params_\n",
    "\n",
    "sdg = SGDClassifier(penalty=best_params['clf__penalty'],  \n",
    "          alpha = best_params['clf__alpha'],\n",
    "          random_state=False, verbose=False)\n",
    "\n",
    "scores_sdg = cross_validate(make_pipeline(preprocess, sdg), X_train, y_train, cv=10, scoring=scoring)\n",
    "                        \n",
    "print('accuracy score: ', np.mean(scores_sdg['test_accuracy']))\n",
    "print('f1 score: ', np.mean(scores_sdg['test_f1']))\n",
    "print('precision score: ', np.mean(scores_sdg['test_precision']))\n",
    "print('recall score: ', np.mean(scores_sdg['test_recall']))\n",
    "print('roc_auc score: ', np.mean(scores_sdg['test_roc_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.850877192982456\n",
      "f1 score:  0.9028808611567232\n",
      "precision score:  0.8295483193277311\n",
      "recall score:  0.9923076923076923\n",
      "roc_auc score:  0.7971794871794872\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe = Pipeline([(\"preprocessor\", preprocess),\n",
    "                 (\"clf\", KNeighborsClassifier())])\n",
    "\n",
    "param_grid={'clf__n_neighbors': (3, 5, 10),\n",
    "            'clf__weights': ['uniform', 'distance']}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring='accuracy', cv=5,\n",
    "                    n_jobs=-1, return_train_score=True)\n",
    "\n",
    "grid_result =grid.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_result.best_params_\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = best_params['clf__n_neighbors'],  \n",
    "          weights = best_params['clf__weights'])\n",
    "\n",
    "scores_knn = cross_validate(make_pipeline(preprocess, knn), X_train, y_train, cv=10, scoring=scoring)\n",
    "                        \n",
    "print('accuracy score: ', np.mean(scores_knn['test_accuracy']))\n",
    "print('f1 score: ', np.mean(scores_knn['test_f1']))\n",
    "print('precision score: ', np.mean(scores_knn['test_precision']))\n",
    "print('recall score: ', np.mean(scores_knn['test_recall']))\n",
    "print('roc_auc score: ', np.mean(scores_knn['test_roc_auc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC has the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_n = preprocess.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "sfm = SelectFromModel(rfc).fit(X_train_n,y_train)\n",
    "print(sfm.transform(X_train_n).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = np.array(X_train.select_dtypes(exclude='bool').columns)\n",
    "bool_cols = np.array(X_train.select_dtypes(include='bool').columns)\n",
    "all_columns = np.hstack([numeric_cols, bool_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ViewCount', 'model_score', 'Date_diff'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns[sfm.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_useful = X_train_n[:, sfm.get_support()]\n",
    "X_train_useful.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.8230994152046783\n",
      "f1 score:  0.8679855585027999\n",
      "precision score:  0.8724775224775225\n",
      "recall score:  0.8769230769230768\n",
      "roc_auc score:  0.8525641025641028\n"
     ]
    }
   ],
   "source": [
    "scores_rfc_2 = cross_validate(rfc, X_train_useful, y_train, cv=10, scoring=scoring)\n",
    "                        \n",
    "print('accuracy score: ', np.mean(scores_rfc_2['test_accuracy']))\n",
    "print('f1 score: ', np.mean(scores_rfc_2['test_f1']))\n",
    "print('precision score: ', np.mean(scores_rfc_2['test_precision']))\n",
    "print('recall score: ', np.mean(scores_rfc_2['test_recall']))\n",
    "print('roc_auc score: ', np.mean(scores_rfc_2['test_roc_auc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the accuracy score was improved after select features, we noticed that the recall is 1 now, so we chose not to use feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1550, 16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pre = df[features]\n",
    "X_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_CommentCount(row):\n",
    "    if not pd.isna(row['CommentCount']):\n",
    "        matched = bool(re.match(r\"\\b[A-Z]\", row['CommentCount']))\n",
    "        pattern = '^\\d{4}[-]\\d{2}[-]\\d{2}'\n",
    "        matched2 = bool(re.match(pattern, row['CommentCount']))\n",
    "        if matched == True or matched2==True:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['CommentCount']\n",
    "    return 0\n",
    "\n",
    "df_new = X_pre.copy()  \n",
    "df_new['CommentCount_new'] = df_new.apply(lambda row : clean_CommentCount(row), axis = 1) \n",
    "X_pre = X_pre.assign(CommentCount=df_new['CommentCount_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_SVC = Pipeline([(\"preprocessor\", preprocess),\n",
    "                 (\"clf\", svc)])\n",
    "pipe_SVC.fit(X_train,y_train)\n",
    "res = pipe_SVC.predict(X_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rfc = Pipeline([(\"preprocessor\", preprocess),\n",
    "                 (\"clf\", rfc)])\n",
    "pipe_rfc.fit(X_train,y_train)\n",
    "res_rfc = pipe_rfc.predict(X_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predict'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/pipeline/1550_with_predict_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
