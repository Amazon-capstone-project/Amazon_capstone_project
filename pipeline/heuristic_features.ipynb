{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse false positives using linguistic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important constants\n",
    "\n",
    "'out of date' would be replced by 'outdated' during linguistic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = ['outdated', 'obsolete', 'deprecated', 'discouraged']\n",
    "IRREL_SUBS = ['data', 'disk', 'sensor', 'browser', 'actor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods about linguistic analysis\n",
    "\n",
    "We are using package 'spaCy'. We need to run `python -m spacy download en_core_web_sm` to get the language resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens  import Token, Doc, Span\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "KEYWORDS = ['outdated', 'obsolete', 'deprecated', 'discouraged']\n",
    "\n",
    "def analyse_sentence(sentence: spacy.tokens.Span) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Analyse a sentence from a doc.\n",
    "    punct: ? . !\n",
    "    negative statement: not\n",
    "    dep of keyword: ROOT or amod\n",
    "    https://spacy.io/usage/linguistic-features#pos-tagging\n",
    "    \"\"\"\n",
    "    ref_doc: List[Token] = sentence.doc\n",
    "    text = str(sentence)\n",
    "    if any([kw in text for kw in KEYWORDS]) is False:\n",
    "        return None\n",
    "    start, end = sentence.start, sentence.end - 1\n",
    "    punctuation = ref_doc[end]\n",
    "    target = \"\"\n",
    "    negation = False\n",
    "    for token in ref_doc[start:end]:\n",
    "        t_dep = token.dep_\n",
    "        if t_dep == 'neg': negation = True\n",
    "        if token.text not in KEYWORDS: continue\n",
    "        if t_dep == 'conj':\n",
    "            for child in token.children:\n",
    "                if child.dep_ in ['amod', 'acomp']:\n",
    "                    t_dep = child.dep_\n",
    "        if t_dep == 'ROOT':\n",
    "            for child in token.children:\n",
    "                if child.dep_ == 'nsubjpass':\n",
    "                    target = child.text\n",
    "        elif t_dep == 'amod':\n",
    "            for child in token.children:\n",
    "                if child.dep_ == 'attr':\n",
    "                    target = child.text\n",
    "        elif t_dep == 'acomp': # dep_ can be 'acomp'\n",
    "            for t in ref_doc[start:end]:\n",
    "                if t.dep_ == 'nsubj':\n",
    "                    target = t.text\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"{token.text}, {token.dep_}, {list(token.subtree)}\\n{token.doc}\\n\")\n",
    "            continue\n",
    "    return {\n",
    "        \"modified_noun\": target,\n",
    "        \"punctuation\": punctuation,\n",
    "        \"negative_statement\": negation,\n",
    "        \"text\": sentence.text\n",
    "    }\n",
    "\n",
    "def analyse_content(text: str, nlp) -> Dict:\n",
    "    \"\"\"Analyse a text i.e. the comment.\"\"\"\n",
    "    clean_text = text.replace('out of date', 'outdated')\n",
    "    document: Doc = nlp(clean_text)\n",
    "    res = [analyse_sentence(s) for s in document.sents]\n",
    "    analysis: List[dict] = [r for r in res if r is not None]\n",
    "    neg_cnt = sum([r.get('negative_statement', 0) for r in analysis])\n",
    "    if len(analysis) == 0:\n",
    "        analysis.append(dict())\n",
    "    output = {\n",
    "        \"cnt_keywords\": len(analysis),\n",
    "        \"subject\": analysis[0].get('modified_noun', \"\"),\n",
    "        \"punctuation\": analysis[0].get('punctuation', \"\"),\n",
    "        \"negative_statement\": neg_cnt,\n",
    "        \"sentence\": analysis[0].get('text', '')\n",
    "    }\n",
    "    return output\n",
    "\n",
    "def print_tokens(tokens: List[Token]):\n",
    "    \"\"\"Print a sequence of tokens.\"\"\"\n",
    "    print(\"Idx\\tText\\tdep\\ttag\\tpos\\thead\\tancestor\\tchildren\")\n",
    "    for t in tokens:\n",
    "        print(f\"{t.i}\\t{t.text}\\t{t.dep_}\"\n",
    "              f\"\\t{t.tag_}\\t{t.pos_}\\t{t.head}\"\n",
    "              f\"\\t{list(t.ancestors)}\\t{list(t.children)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods about data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def append_heuristic_features(raw: pd.DataFrame, nlp) -> pd.DataFrame:\n",
    "    \"\"\"Add columns about linguistic features\"\"\"\n",
    "    text_col: pd.Series = raw['Text']\n",
    "    res_series: pd.Series = text_col.apply(analyse_content, args=(nlp,))\n",
    "    feat_df = pd.DataFrame(res_series.to_list())\n",
    "    desired_cols = ['Id', 'PostId', 'Score', 'Text', 'CreationDate',\n",
    "                    'UserId', 'ContentLicense', 'of_answer', 'cnt_keywords', 'subject',\n",
    "                    'punctuation', 'negative_statement', 'sentence']\n",
    "    concat = pd.concat([raw.reset_index(), feat_df], axis=1)\n",
    "    return concat[desired_cols]\n",
    "\n",
    "def check_subject_relevance(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Check the column 'subject' and tell whether it may be irrelevant.\"\"\"\n",
    "    is_irrele = raw['subject'].apply(lambda subj: any([kw in subj for kw in IRREL_SUBS]))\n",
    "    bf_irrele = raw['sentence'].apply(lambda text: any([kw in text for kw in IRREL_SUBS]))\n",
    "    return raw.assign(subj_irrel=is_irrele, include_irrel=bf_irrele)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_filtered_comments = r\"data\\pipeline\\outdated_comments_by_keywords.csv\"\n",
    "PATH_comments_feated = r\"data\\pipeline\\obsol_com_keywords_heuri.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_comments = pd.read_csv(PATH_filtered_comments)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    comments_df1 = append_heuristic_features(data_comments, nlp)\n",
    "    comments_df2 = check_subject_relevance(comments_df1)\n",
    "    comments_df2.to_csv(PATH_comments_feated, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-DS",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
