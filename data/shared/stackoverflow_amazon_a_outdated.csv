,Id,PostTypeId,ParentId,CreationDate,Score,Body,OwnerUserId,LastEditorUserId,LastEditDate,LastActivityDate,CommentCount,ContentLicense,OwnerDisplayName,CommunityOwnedDate,,link
0,2012755,2,2012754,2010-01-06T11:31:26.400,0,"<p>The word 'service' is only in the xml 3 times, so I'm assuming that what you're looking for is:</p><pre><code>&lt;service name=""AmazonSearchService""&gt;   &lt;!-- Endpoint for Amazon Web APIs --&gt;   &lt;port name=""AmazonSearchPort"" binding=""typens:AmazonSearchBinding""&gt;   &lt;soap:address location=""http://soap.amazon.com/onca/soap2""/&gt;   &lt;/port&gt; &lt;/service&gt;</code></pre><p>Edit: Visiting <a href=""http://soap.amazon.com/onca/soap2"" rel=""nofollow noreferrer"">that url</a> in a browser shows a page with this message:</p><blockquote>  <p>Amazon Ecommerce Web Service 3.0 has  been deprecated after many years of  useful service on March 31st 2008.  Please upgrade to the Amazon  Associates Web Service 4.0 as detailed  in the <a href=""http://developer.amazonwebservices.com/connect/entry.jspa?categoryID=12&amp;externalID=627"" rel=""nofollow noreferrer"">migration guide</a>. Please visit  <a href=""http://developer.amazonwebservices.com/connect/forum.jspa?forumID=9&amp;start=0"" rel=""nofollow noreferrer"">Amazon Associates Web Service  Developer Forum</a> for more information.  If you came to this page from an RSS  feed, visit <a href=""http://www.amazon.com/b/?ie=UTF8&amp;node=390052011"" rel=""nofollow noreferrer"">Amazon's Product RSS Feeds</a>  page for an upgrade.</p></blockquote><p>The migration guide has many WSDL locations, depending on which national site you want; the US site's wsdl is at <a href=""http://webservices.amazon.com/AWSECommerceService/AWSECommerceService.wsdl"" rel=""nofollow noreferrer"">http://webservices.amazon.com/AWSECommerceService/AWSECommerceService.wsdl</a> and the schema is at <a href=""http://webservices.amazon.com/AWSECommerceService/AWSECommerceService.xsd"" rel=""nofollow noreferrer"">http://webservices.amazon.com/AWSECommerceService/AWSECommerceService.xsd</a></p>",128625,128625,2010-01-07T07:34:43.077,2010-01-07T07:34:43.077,3,CC BY-SA 2.5,,,,https://stackoverflow.com/questions/2012755
1,3096855,2,3096631,2010-06-22T20:25:06.187,16,"<p>Dirk's comments are spot on w.r.t multicore/foreach/doMC. </p><p>If you are doing thousands of simulations you may want to consider Amazon's Elastic Map Reduce (EMR) service. When I wanted to scale my simulations in R I started with huge EC2 instances and the multicore package (just like you!). It went well but I ran up a hell of an EC2 bill. I didn't really need all that RAM yet I was paying for it. And my jobs would finish at 3 AM then I would not get into the office until 8 AM so I paid for 5 hours I didn't need. </p><p>Then I discovered that I could use the EMR service to fire up 50 cheap small Hadoop instances, run my simulations, and then have them automatically shut down! I've totally abandoned running my sims on EC2 and now use EMR almost exclusively. This worked so well that my firm is beginning to test ways to migrate more of our periodic simulation activity to EMR. </p><p>Here's a <a href=""http://www.cerebralmastication.com/2010/02/using-the-r-multicore-package-in-linux-with-wild-and-passionate-abandon/"" rel=""nofollow noreferrer"">blog post</a> I wrote when I first started using multicore on EC2. Then when I discovered I could do this with Amazon EMR I wrote a <a href=""http://www.cerebralmastication.com/2010/02/you-can-hadoop-it-its-elastic-boogie-woogie-woog-ie/"" rel=""nofollow noreferrer"">follow up post</a>. </p><p><strong>EDIT:</strong>  since this post I've been working on a package for making it easier to use EMR with R for parallel apply functions. I've named the project <a href=""http://code.google.com/p/segue/"" rel=""nofollow noreferrer"">Segue and it's on Google Code</a>. </p><p><strong>Further Update:</strong> I've since deprecated Segue because there are much better and more mature offerings for accessing Amazon's services from R. </p>",37751,37751,2017-06-06T14:53:11.723,2017-06-06T14:53:11.723,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/3096855
2,3895343,2,3893211,2010-10-09T01:37:35.720,0,"<p>This is probably not the best time to criticize your code but:</p><p>All your script should be included externally, not inline.Do not use , this is a deprecated, ancient way of including inline script.  If you must, use //I see in your script you are using document.write().  DON'T!</p><p>You cannot cheat and include a script in such a way and expect it to be executed.</p><p>Use <a href=""http://api.jquery.com/jQuery.getScript/"" rel=""nofollow"">jQuery.getScript()</a></p>",454533,,,2010-10-09T01:37:35.720,0,CC BY-SA 2.5,,,,https://stackoverflow.com/questions/3895343
3,3991880,2,3931881,2010-10-21T20:59:47.593,0,"<p>It stopped working on my site. I assumed deprecated meant it won't work in the future, so don't build around it. I didn't think they'd remove it completely.</p>",462933,,,2010-10-21T20:59:47.593,0,CC BY-SA 2.5,,,,https://stackoverflow.com/questions/3991880
4,4150436,2,4149973,2010-11-11T00:31:39.633,3,"<p>A while back I had wrote a client library for communicating with Amazon's ECS (A2S) services.  Seems that Amazon hates their developers and keeps changing their service name and framework.  Well, my client is now completely broken because of <a href=""http://developer.amazonwebservices.com/connect/ann.jspa?annID=483"" rel=""nofollow"">Recent API Changes</a>.</p><p>Your best bet would be to look at their samples <a href=""http://aws.amazon.com/code/Product-Advertising-API/3941"" rel=""nofollow"">here</a>.</p><p>Good luck with this. Their API is huge and outdated from the experience I had, but that was two years ago. I get the impression that they really cater to developers for their enterprise services, not the free A2S.</p>",217858,217858,2010-11-11T00:39:00.347,2010-11-11T00:39:00.347,1,CC BY-SA 2.5,,,,https://stackoverflow.com/questions/4150436
5,4337337,2,4336842,2010-12-02T16:32:59.243,5,"<p>The answer is yes. You can test your bootstrap script like this:</p><pre><code>elastic_mapreduce --create --alive --ssh</code></pre><p>This will create a node and give you a ssh connection to it, from which you can test your bootstrap script.</p><p>UPDATE: For reference here is what I'm running:</p><pre><code>#!/bin/bashsudo apt-get -y -V install irb1.8 libreadline-ruby1.8 libruby libruby1.8 rdoc1.8 ruby ruby1.8 ruby1.8-devwget http://production.cf.rubygems.org/rubygems/rubygems-1.8.11.zipunzip rubygems-1.8.11.zipcd rubygems-1.8.11sudo ruby setup.rbsudo gem1.8 install bson bson_ext json tzinfo i18n activesupport --no-rdoc --no-ri</code></pre><p>UPDATE2: to install aws-sdk</p><pre><code>#!/bin/bash# ruby developer packagessudo apt-get -y -V  install ruby1.8-dev ruby1.8 ri1.8 rdoc1.8 irb1.8sudo apt-get -y -V  install libreadline-ruby1.8 libruby1.8 libopenssl-ruby# nokogiri requirementssudo apt-get -y -V  install libxslt-dev libxml2-devwget http://production.cf.rubygems.org/rubygems/rubygems-1.8.11.zipunzip rubygems-1.8.11.zipcd rubygems-1.8.11sudo ruby setup.rbsudo gem1.8 install aws-sdk --no-rdoc --no-ri</code></pre><p>-y on apt-get makes it not prompt you</p><p>I wget rubygems because the version you get with apt-get is way out of date, and some gems won't build using an old version.</p>",18818,125617,2012-02-22T14:56:11.633,2012-02-22T14:56:11.633,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/4337337
6,4979599,2,4976451,2011-02-12T17:52:28.150,2,"<p>The AMI's are kinda outdated now, but if you follow <a href=""http://groups.drupal.org/node/70268"" rel=""nofollow"" title=""these instructions"">these instructions from the pantheon group on g.d.o</a> using one of the Alestic Ubuntu 10.4 ami's as a base you should be fine to roll your own.</p>",149390,,,2011-02-12T17:52:28.150,0,CC BY-SA 2.5,,,,https://stackoverflow.com/questions/4979599
7,5029447,2,5027097,2011-02-17T13:19:11.590,3,"<p>The difference is that the mapred packages are deprecated. You should use since 0.20.x the new stuff inside mapreduce package. <br>For example the new way to implement a mapper, with mapred-package you have to implement the mapper interface. With the mapreduce-package you simply extend from a basic mapper class and override just the method you need.</p>",540873,,,2011-02-17T13:19:11.590,0,CC BY-SA 2.5,,,,https://stackoverflow.com/questions/5029447
8,5265964,2,4498192,2011-03-10T21:11:42.767,5,"<p>The ""query"" interface was the original search interface for SimpleDB.  It was set-based, non-standard and quite lovely, I thought.  However, over time AWS introduced a SQL-like query language (accessed via the Select request) and then deprecated and eventually removed the original query interface.</p><p>So, the reason it doesn't work in boto is because it is no longer supported by SimpleDB.  For more up-to-date boto documentation, look <a href=""http://boto.cloudhackers.com/"" rel=""noreferrer"">here</a>.</p>",653643,,,2011-03-10T21:11:42.767,0,CC BY-SA 2.5,,,,https://stackoverflow.com/questions/5265964
9,5654982,2,5639803,2011-04-13T20:07:19.853,3,"<p>For some reason, your OpenSSL does not get the intermediate certificate. (Outdated software?)</p><p>Install the intermediate certificate with the subject hash 0xeb99629b, available at <a href=""https://knowledge.verisign.com/support/ssl-certificates-support/index?page=content&amp;actp=CROSSLINK&amp;id=AR1513"" rel=""nofollow"">https://knowledge.verisign.com/support/ssl-certificates-support/index?page=content&amp;actp=CROSSLINK&amp;id=AR1513</a>.</p><p>You can use it with the <code>-CAfile</code> parameter in OpenSSL tools and with the environment variable <code>HTTPS_CA_FILE</code> for the Perl HTTPS stack. To use it system-wide, place it in the appropriate ca-certificates directory, e.g. <code>/etc/ssl/certs</code>, and <a href=""http://www.manpagez.com/man/1/c_rehash/"" rel=""nofollow""><code>c_rehash</code></a> the directory.</p>",46395,,,2011-04-13T20:07:19.853,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/5654982
10,5820746,2,5820120,2011-04-28T15:05:14.377,0,"<p>Man, that sounds incredibly frustrating. Looks like you're doing all of the things I would recommend, and trying lots of variations/tests. I've got one idea, and two maybe not so good ideas:</p><p>1) I ran into encoding issues when I used Miro, and ended up having files that would play fine locally... but ended up behaving differently when I viewed them online. To ensure that it's not an encoding issue, try subbing in an existing .ogv file that you know works - possibly from some other site/tutorial.</p><p>Standby reference: <a href=""http://www.bigbuckbunny.org/index.php/download/"" rel=""nofollow"">Big Buck Bunny</a></p><p>2) This is a longshot, and it's me showing how little I know about these formats... but maybe try using .ogg as an extension, instead of .ogv? I looked and see that .ogv is meant for video, with .ogg being deprecated... but changing the file extension might reveal something about your encoding process.</p><p>3) Another longshot: kill the codecs attribute for your mp4 source.</p>",671759,,,2011-04-28T15:05:14.377,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/5820746
11,5935495,2,4811761,2011-05-09T10:21:15.320,2,"<p>Just for completeness, I ended up using the SDK provided by Amazon for Java. It can be found here:</p><p><a href=""http://aws.amazon.com/sdkforjava/"" rel=""nofollow"">http://aws.amazon.com/sdkforjava/</a></p><p>All other client libraries were outdated or had missing functionality.</p>",292145,,,2011-05-09T10:21:15.320,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/5935495
12,7129403,2,4521846,2011-08-20T03:21:33.600,1,"<p>As msha points out, the sending of a workerID parameter to an ExternalQuestion page seems to be deprecated, or at least taken out of the latest version of the documentation.</p><p>However, a fellow researcher who's been using MTurk a lot says: ""People seem to be using it in the forums. I would go ahead with it...if it ever actually disappears, I'm sure that the developer community will yell very loudly. :) ""</p><p>I tried it empirically today (2011-08-19), and indeed a workerID is being sent to the ExternalQuestion page I set up on my own server, after the HIT has been accepted. This was in the sandbox. My ExternalQuestion page contained a Java Web Start button (as described here: <a href=""http://download.oracle.com/javase/tutorial/deployment/deploymentInDepth/createWebStartLaunchButtonFunction.html"" rel=""nofollow"">http://download.oracle.com/javase/tutorial/deployment/deploymentInDepth/createWebStartLaunchButtonFunction.html</a> ); I don't know if that made any difference.</p>",772978,,,2011-08-20T03:21:33.600,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/7129403
13,7322249,2,7259601,2011-09-06T15:20:43.983,2,"<p>We found the issue. We noticed that some instances in our cluster always produced the problem and some never did. Apparently the issue is unique to instances that run more recent CPU versions combined with slightly outdated kernels.</p><p>The issue is explained in full here : <a href=""https://bugs.launchpad.net/ubuntu/+source/linux/+bug/727459"" rel=""nofollow noreferrer"">https://bugs.launchpad.net/ubuntu/+source/linux/+bug/727459</a></p><p>As the running time increases the spikes shown in the graph below become longer and longer, possible due to time shift and at some point it'll spin the one core for a long time.</p><p>Cpu usage graphs. Affected instance versus unaffected:</p><p><img src=""https://i.stack.imgur.com/sM8r6.png"" alt=""Cpu usage graphs. Affected instance versus unaffected""></p><p>The issue has been fixed recently so a kernel update fixed this issue for us.</p>",813957,,,2011-09-06T15:20:43.983,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/7322249
14,7543698,2,6645026,2011-09-25T04:40:23.570,1,<p>i think you are facing this problem because you are using RAILS_ROOT which has been deprecated in Rails 3. try using Rails.root instead and see if you face the same problem. Please check the correct usage for Rails.root before making the correction. </p>,963290,,,2011-09-25T04:40:23.570,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/7543698
15,7642625,2,7638646,2011-10-04T02:11:20.433,0,"<p>As described in the question both Azure and EC2 will do the job very well. This is the kind of task both systems are designed for.</p><p>So the question becomes really: which is <em>best</em>? That depends on two things: what the application needs to do and your own experience and preference.</p><p>As it's a Windows application there should probably be a leaning towards Azure. While EC2 supports Windows, the tooling and support resources for Azure are probably deeper at this point. </p><p>If cost is a factor then a (somewhat outdated) resource is here: <a href=""http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost-comparison/"" rel=""nofollow"">http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost-comparison/</a> -- the conclusion is that, by and large, Azure and Amazon are roughly similar for compute charges.</p>",3546,,,2011-10-04T02:11:20.433,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/7642625
16,7651730,2,6675018,2011-10-04T17:25:34.107,4,"<p>What you really want is the equivalent of the <code>OpenListings</code> report from the <code>Product Advertising API</code> in MWS and that is the <code>RequestReport</code> call with a report type of <code>_GET_MERCHANT_LISTINGS_DATA_</code>. This returns you all the inventory a seller has listed on Amazon and from here to getting your ASIN from that list it's close.</p><p>You can find out more details in their <a href=""https://developer.amazonservices.com/gp/mws/docs.html"" rel=""nofollow"">documentation</a></p><p>Also, I advise you to not use the Product Advertising API anymore as Amazon deprecated it and it will be out of use this time next year.</p>",49032,,,2011-10-04T17:25:34.107,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/7651730
17,7718816,2,7704946,2011-10-10T21:19:34.097,4,"<p>Changing URLs is a safe way to invalidate outdated assets.</p><p>It is also a necessity if you want to allow users storing private images. Using a path deductible from the users account name/id/path would render privacy settings useless as soon as you store assets on a CDN.</p>",589017,,,2011-10-10T21:19:34.097,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/7718816
18,7865273,2,7864988,2011-10-23T09:05:42.187,-1,"<p>Judging from your post, the error lies in </p><pre><code>[[UIDevice currentDevice] uniqueIdentifier]</code></pre><p>The method UIDevice <code>uniqueIdentifier</code> is deprecated in iOS 5 and should not be called. From your code I cant really see what exactly you are trying to do but this post<a href=""https://stackoverflow.com/questions/6993325/uidevice-uniqueidentifier-deprecated-what-to-do-now"">UIDevice uniqueIdentifier Deprecated - What To Do Now?</a></p><p>should be helpful in overcoming the deprecated method. You should change the deprecated calls and also use the ones listed in the post above. That should do the trick.</p>",699863,-1,2017-05-23T11:55:40.253,2011-10-23T09:05:42.187,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/7865273
19,8106724,2,8106239,2011-11-12T18:51:24.333,2,"<p>There are lots of guides out there, some out of date, and some recent ones, below are some links. You might also be interested in <a href=""http://aws.amazon.com/elasticbeanstalk/"" rel=""nofollow"">Amazon Elastic Beanstalk</a>. Here are some links</p><ul><li><a href=""http://blog.peterdelahunty.com/2009/06/deploying-grails-app-on-ec2-in-from.html"" rel=""nofollow"">Deploying Grails App on Amazon EC2 form scratch</a></li><li><a href=""http://grails.1312388.n4.nabble.com/Amazon-EC2-td3055649.html"" rel=""nofollow"">Question about EC2 and Grails on the mailing list</a></li></ul><p>To answer the question about the credit card, I think you can't get away without one, since if you go over the free usage tier you need to have some sort of payment method to pay for the extra usage.</p>",372561,,,2011-11-12T18:51:24.333,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/8106724
20,8109806,2,8109792,2011-11-13T05:27:43.483,11,"<p>The constant <code>RAILS_ROOT</code> is deprecated and is not available in 3.1 (IMHO)</p><p>Try using :<code>:s3_credentials =&gt; ""#{Rails.root}/config/s3.yml""</code></p><p>Instead of <code>:s3_credentials =&gt; ""#{RAILS_ROOT}/config/s3.yml""</code></p>",619036,163203,2011-11-30T00:13:24.677,2011-11-30T00:13:24.677,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/8109806
21,8125958,2,8056962,2011-11-14T18:00:40.387,1,"<p>It used to be be possible by doing a brute force search with the Seller* API calls. However these calls have been deprecated since Nov 1 2011, so you're out of luck. If you happen to be working for this particular seller (instead of being simply a customer or a competitor), you'll want to use the APIs available to the seller (MWS) to download inventory reports.</p>",50812,,,2011-11-14T18:00:40.387,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/8125958
22,8127487,2,7800800,2011-11-14T20:15:19.397,1,"<p>I'm working on the same issue and I know how to execute the GA code you want to execute, especially since you are working with the older version of the code. I am working on the same issue with an Amazon webstore -- if you are able to upload a js file, and it looks like you are able to do this in files, I would suggest using the init() function which is deprecated but it still works - here is an example where I am doing something similar. I am setting the contents of a custom variable and creating the tracking object -- but I'm not firing the page tracker until later in the code.</p><p>Using the code below, replace the Custom Variable setting code with the linker and set Domain code you need for cross domain tracking. Since the GA code added by amazon doesn't try to set the domain the linker functions should prevent a second cookie from being created, and use the cookie data passed in the URL.</p><pre><code>&lt;script type=""text/javascript""&gt; var gaJsHost = ((""https:"" ==   document.location.protocol) ? ""https://ssl."" : ""http://www."");document.write(unescape(""%3Cscript src='"" + gaJsHost + ""google-analytics.com/ga.js'   type='text/javascript'%3E%3C/script%3E""));&lt;/script&gt;&lt;script type=""text/javascript""&gt;try {var pageTracker = _gat._getTracker(""UA-xxxxxxx-x""); pageTracker._initData();pageTracker._setCustomVar(1, ""GWO"", utmx('combination').toString(), 1);} catch(err) {}&lt;/script&gt;</code></pre>",1046301,,,2011-11-14T20:15:19.397,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/8127487
23,8173888,2,8173561,2011-11-17T20:33:04.200,6,"<p>Zach, the simple answer is that there's not a simple path to there from here :)</p><p>When I wrote Segue I hoped that someone would soon come out with something that would make Segue obsolete. Cloudnumbers may be it one day, but probably not yet. I have toyed with making Segue a foreach backend, but since I don't use it that way, my motivation has been pretty low to take the time to learn how to build the backend. </p><p>One of the things that is very promising, in my opinion, is using the <code>doRedis()</code> package with workers on Amazon EC2. doRedis uses a Redis server as the job controller and then lets workers connect to the Redis server and get/return jobs and results. I've been thinking for a while that it would be nice to have a dead simple way to deploy a doRedis cluster on EC2. But nobody has written one yet that I know of. </p>",37751,,,2011-11-17T20:33:04.200,11,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/8173888
24,8176018,2,8151783,2011-11-17T23:48:06.353,0,"<p>On November 8, 2010 the Reviews response group of the Product Advertising API will no longer return customer reviews content and instead will return a link to customer reviews content hosted on Amazon.com.So you won't be able to fetch back the AverageRating and Totalreviews.</p><p>Amazon API latest documentation:</p><p><a href=""http://docs.amazonwebservices.com/AWSECommerceService/latest/DG/"" rel=""nofollow"">http://docs.amazonwebservices.com/AWSECommerceService/latest/DG/</a></p><p>Looks like you can't get sellerfeedback from the new API:</p><p>Product Advertising API Change DetailsThe following changes will take effect on 11/1/2011:</p><p>Seller Operations: The SellerLookup, SellerListingLookup and SellerListingSearch operations will be deprecated. All requests for these operations will be rejected with a corresponding error message. </p>",943161,943161,2011-11-18T17:37:25.867,2011-11-18T17:37:25.867,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/8176018
25,8677786,2,8677696,2011-12-30T08:58:36.780,2,"<p>Yes this is very much possible. You will only have to deal with how would like to send the URL back after creating the pre signed URL. Maybe as XML or as plain text.. </p><p>I would recommend using the jets3t  library to create the URL. You could call this API:</p><p>org.jets3t.service.S3Service.createSignedPutUrl(String, String, Map, Date, boolean)</p><p>You could also an IAM policy instead. But your way is easier.</p><p>Edit: Added more information..</p><p>That's the beauty of it, when you are using the jets3t library you dont need to any of that. Just call the method : </p><pre><code> S3Service s3Service = new RestS3Service(awsCredentials);// Create a signed HTTP PUT URL valid for 5 minutes.  String putUrl = s3Service.createSignedPutUrl(bucket.getName(), object.getKey(),  object.getMetadataMap(), expiryDate, false);</code></pre><p>And use the URL to upload your image like below:</p><pre><code>SignedUrlHandler signedUrlHandler = new RestS3Service(null);S3Object object = new S3Object(bucket,&lt;your file object goes here&gt;); S3Object putObject = signedUrlHandler.putObjectWithSignedUrl(putUrl, object);</code></pre><p><strong>Edit for signedGetURL():</strong></p><p>Only the ones with ProviderCredential being passed in are deprecated.. This one is not : </p><p><a href=""http://jets3t.s3.amazonaws.com/api/org/jets3t/service/S3Service.html#createSignedGetUrl(java.lang.String"" rel=""nofollow"">http://jets3t.s3.amazonaws.com/api/org/jets3t/service/S3Service.html#createSignedGetUrl(java.lang.String</a>, java.lang.String, java.util.Date)</p><p>You wouldnt need to pass the credentials into this method anyway. Its only the s3service object that needs the credentials.. like this...</p><pre><code>S3Service s3service=new RestS3Service(credentials); // pass your credentials hereString createSignedGetUrl = s3service.createSignedGetUrl(storageObject.getBucketName(),  storageObject.getKey(),new DateTime().plusMinutes(5).toDate());</code></pre>",836487,836487,2012-01-03T07:42:52.123,2012-01-03T07:42:52.123,11,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/8677786
26,8718053,2,8691975,2012-01-03T20:14:59.603,5,"<p>I'm sorry to say that there is no way to get this info using the MWS API. The MWS API doesn't have any relative information, that is, it doesn't know anything about <em>other sellers' items</em> so there is no way to see what your items are priced at relative to others (which is the info you would need to determine if you own the Buy Box).</p><p>As you already know, you can get this info through the Product Advertising API but with the new limitations in place this may not be practical for the size of your inventory (would take <strike>three days</strike> two and a half hours at 20,000 items per <strike>day</strike> hour). The ""re-pricing"" service that you've used in the past was most likely affected by the new limitations. However, any existing accounts were given a grace period to change their software. The new limitations will go into effect for these accounts on February 12, 2012.</p><p>The only other option open to you is to get the info from the site (screen scrape). This isn't a very attractive alternative due to the latency issues but if you've got the infrastructure to do massive amounts of parallel calls then go for it. In certain situations I prefer to get this type of info from the site since this is what the buyers are seeing (most current info). In the past I've seen data coming from the Product Advertising API that was out of date or just plain wrong.</p><hr><p>The limits are defined in the documentation under the obscure subtitle of ""Efficiency Guidelines"" and is located <a href=""http://docs.amazonwebservices.com/AWSECommerceService/latest/DG/TroubleshootingApplications.html?r=2536"" rel=""nofollow"">here</a> (at the bottom of the page). </p><p>I must admit that it's been a while since I had worked with the PA-API and had forgotten the limits. I thought it was something like 2,000 a day but it's actually 2,000 per hour at one call per second. If you're making calls too quickly they'll return a 503 response which is documented <a href=""http://docs.amazonwebservices.com/AWSECommerceService/latest/DG/ErrorNumbers.html"" rel=""nofollow"">here</a>.</p><hr><p><strong>Update:</strong> Amazon has added a <a href=""https://developer.amazonservices.com/gp/mws/api.html?ie=UTF8&amp;section=products&amp;group=products&amp;version=latest"" rel=""nofollow"">Products API</a> to the MWS APIs. The <code>GetCompetitivePricingForSKU</code> gives you pricing information similar to the Product Advertising API's ItemLookup function. With this information you should be able to determine the price of the Buy Box owner. </p>",843318,843318,2012-02-12T00:14:06.683,2012-02-12T00:14:06.683,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/8718053
27,8927333,2,8407750,2012-01-19T14:01:34.183,2,"<p>Apparently I was looking at some badly out of date documentation.</p><p>in <code>AmazonS3Client</code> see:</p><pre><code>- (S3MultipartUpload * AmazonS3Client)initiateMultipartUploadWithKey:(NSString *)theKey withBucket:(NSString *)theBucket  </code></pre><p>Which will give you a <code>S3MultipartUpload</code> which will contain an <strong>uploadId</strong>.</p><p>You can then put together an S3UploadPartRequest using <code>initWithMultipartUpload:  (S3MultipartUpload *)   multipartUpload</code> and send that as you usually would.</p><p>S3UploadPartRequest contains an int property partNumber where you can specify the part # you're uploading.</p>",459082,,,2012-01-19T14:01:34.183,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/8927333
28,8995926,2,8995304,2012-01-24T23:38:31.250,12,"<h2>Preface</h2><p>First and foremost, you might want to reconsider whether you actually need these X.509 certificates - the tutorial is correct in principle:</p><blockquote>  <p>There are three types: access keys, X.509 certificates and key pairs.  The first and second type allow you to connect to the Amazon APIs.  Which type of credential depends on which API and tool you are using.  Some APIs and tools support both options, whereas others support just  one.</p></blockquote><p>However, nowadays most modern APIs and tools are interacting with AWS by means of access keys only rather than X.509 certificates.</p><p><strike>Unfortunately this is not the case for the EC2 API Tools the tutorial is based on though, which indeed require the use X.509 certificates due to being (mostly) based on the older EC2 SOAP API still.</strike></p><p><strong>Update</strong>: The EC2 API Tools meanwhile support AWS access keys as well and <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/setting_up_ec2_command_linux.html#set_aws_credentials_linux"">deprecated using X.509 certificates</a> accordingly:</p><blockquote>  <p>Although we donŠ—Èt encourage it, for a limited time you can still use  EC2_PRIVATE_KEY and EC2_CERT instead of AWS_ACCESS_KEY and  AWS_SECRET_KEY. For more information, see Deprecated Options in <a href=""http://docs.aws.amazon.com/AWSEC2/latest/CommandLineReference/CLTRG-common-args-api.html"">Common Options</a>   for API Tools in the Amazon Elastic Compute Cloud CLI  Reference. If you specify both sets of credentials, the command line  tools use the access key ID and secret access key.</p></blockquote><h3>Alternative</h3><p>You might want to check out an alternative first though: If you are comfortable in Python, I'd highly recommend the excellent <a href=""http://code.google.com/p/boto/"">boto</a> (<em>An integrated interface to current and future infrastructural services offered by Amazon Web Services</em>), which works just fine with access keys, offers almost the same feature set as the <em>EC2 API tools</em> (plus most other AWS APIs) and performs significantly faster due to targeting the newer AWS REST APIs only.</p><h2>Solution</h2><p><a href=""http://aws.amazon.com/iam/"">AWS Identity and Access Management (IAM)</a> does not support accessing the actual AWS account, it only covers the <a href=""http://aws.amazon.com/console/"">AWS Management Console</a>, and most AWS APIs of course. You'll need to sign in with the AWS account's login and password (i.e. those of the account owner) to access the <a href=""http://aws-portal.amazon.com/gp/aws/developer/account/index.html?action=access-key"">Security Credentials</a> page.</p><p>This is not recommended anymore though (see section <em>Security Credentials</em> within <a href=""http://docs.amazonwebservices.com/IAM/latest/UserGuide/IAM_Concepts.html#IAM_SecurityCredentials"">IAM Concepts</a>):</p><blockquote>  <p>[...] when you create an AWS account, AWS gives the  AWS account its own Secret Access Key and Access Key ID by default.  The AWS account can make API calls to AWS with them. <strong>We expect that  you won't use those credentials on a regular basis, but will use them  only to initially set up an administrators group for your  organization</strong>. We recommend that all further API interaction between  your AWS account and your AWS resources be at the user level (for  example, using users' security credentials). <em>[emphasis mine]</em></p></blockquote><p>However, <strong>you can still achieve your goal</strong> by using your own certificate as outlined further down in section <em>X.509 Certificates</em>:</p><blockquote>  <p>Although you can use IAM to create an access key, you can't use IAM to  create a signing certificate. However, you can use free third-party  tools such as OpenSSL to create the certificate. [...] After you have the  signing certificate, you must upload it to IAM; [...]</p></blockquote><p>How to actually do the latter is illustrated in <a href=""http://docs.amazonwebservices.com/IAM/latest/UserGuide/Using_UploadCertificate.html"">Uploading a Signing Certificate</a>.</p>",45773,45773,2013-04-12T14:10:19.463,2013-04-12T14:10:19.463,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/8995926
29,9064647,2,5302394,2012-01-30T13:31:21.533,1,"<p>Well, as stated by Geoff Appleford, Elasticfox is an outdated extension. I dont know why it's development stopped as it is a great tool.</p><p>If you wish to do things that are achieved through Elasticfox, I would suggest you to take a look at Hybridfox. Hybrdifox is a fork of Elasticfox which supports other private clouds as well. It is also a great tool too.</p><p>It is also open source and actively updated. Check it's project home(http://code.google.com/p/hybridfox/)</p><p>Sources:<a href=""http://code.google.com/p/hybridfox/"" rel=""nofollow"">Hybridfox home</a></p>",850018,,,2012-01-30T13:31:21.533,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/9064647
30,9076551,2,9069447,2012-01-31T08:33:18.883,9,"<p>Given your constraints, the desired functionality is unfortunately not covered by the two dedicated automation mechanisms available as <a href=""http://aws.amazon.com/products/"" rel=""nofollow noreferrer"">AWS Products &amp; Services</a> right now:</p><ul><li><a href=""http://docs.amazonwebservices.com/AutoScaling/latest/DeveloperGuide/Welcome.html?r=6645"" rel=""nofollow noreferrer"">Auto Scaling</a> - <em>is a web service designed to automatically <strong>launch or terminate</strong> Amazon Elastic Compute Cloud (Amazon EC2) instances based on user-defined policies, schedules, and health checks.</em></li><li><a href=""http://aws.amazon.com/cloudformation/"" rel=""nofollow noreferrer"">AWS CloudFormation</a> - <em>gives developers and systems administrators an easy way to <strong>create and manage</strong> a collection of related AWS resources, <strong>provisioning and updating</strong> them in an orderly and predictable fashion.</em></li></ul><p>While starting/stopping/rebooting an instance conceptually falls into the <em>manage</em> category of the latter, it is not available like so (which incidentally is the reason we provide a separate Task specifically for this functionality within the meanwhile deprecated <a href=""https://marketplace.atlassian.com/plugins/net.utoolity.bamboo.plugins.bamboo-aws-plugin"" rel=""nofollow noreferrer"">Bamboo AWS Plugin</a> and its successor <a href=""https://marketplace.atlassian.com/plugins/net.utoolity.atlassian.bamboo.tasks-for-aws"" rel=""nofollow noreferrer"">Tasks For AWS</a>).</p><p>Consequently the approaches outlined in my answer to <a href=""https://stackoverflow.com/a/9003134/45773"">How to turn on/off cloud instances during office hours</a> are still applicable, albeit with the additional constraint that you would need to find a provider hosting your script or continuous integration solution for free:</p><ul><li><p>Hosting scripts has e.g. been possible for quite a while already by means of those cron job providers.</p></li><li><p>Given the current explosion in <a href=""http://en.wikipedia.org/wiki/Platform_as_a_service"" rel=""nofollow noreferrer"">Platform as a Service (PaaS)</a> solutions there are quite some providers, that will allow you to do host scripts and/or continuous integration solutions one way or another as well.</p></li></ul><p>Obviously you'll need to verify, whether using the free tiers available for purposes like this is acceptable according to the respective <a href=""http://en.wikipedia.org/wiki/Terms_of_use"" rel=""nofollow noreferrer"">Terms of Use</a> of a provider in question.</p>",45773,-1,2017-05-23T11:53:17.133,2013-11-08T13:41:50.727,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/9076551
31,9179801,2,7487292,2012-02-07T16:20:03.357,4,"<p>Note that according to Amazon, at <a href=""http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/FileSystemConfig.html"" rel=""nofollow"">http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/FileSystemConfig.html</a> ""Amazon Elastic MapReduce - File System Configuration"", the S3 Block FileSystem is deprecated and its URI prefix is now s3bfs:// and they specifically discourage using it since ""it can trigger a race condition that might cause your job flow to fail"".</p><p>According to the same page, HDFS is now 'first-class' file system under S3 although it is ephemeral (goes away when the Hadoop jobs ends).</p>",98694,,,2012-02-07T16:20:03.357,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/9179801
32,9440295,2,9280539,2012-02-25T01:30:14.873,0,"<p>I'd like to note that AWS is deprecated when it comes to the store, you should be using MWS and here is the docs for the correct part of MWS you need V138592989.pdf"">https://images-na.ssl-images-amazon.com/images/G/01/mwsportal/doc/en_US/products/MWSProductsApiReference.<em>V138592989</em>.pdf and here is the link [hard to find] to the main MWS area. <a href=""https://developer.amazonservices.com/gp/mws/api.html/182-3268148-9799929?ie=UTF8&amp;section=products&amp;group=products&amp;version=latest"" rel=""nofollow"">https://developer.amazonservices.com/gp/mws/api.html/182-3268148-9799929?ie=UTF8&amp;section=products&amp;group=products&amp;version=latest</a></p><p>Keep in mind that without an active pro seller account you will not be able to get a developer key if you are trying to build apps that integrate with private parts of the amazon API. I just went through this and had to grab the free trial long enough to get a key, which still works after you cancel the trial. </p><p>Also, you can not receive any support from MWS without that pro seller account! I spent over an hour on the phone with my AWS rep and he didn't even know how to access the MWS site. He had to call them only to be told [by them] that they didn't know how to access their API either, this was all too new for them.</p><p>I did eventually get everything working for me after scouring through their docs and downloading the correct API for MWS.</p><p>Good Luck!</p>",526849,,,2012-02-25T01:30:14.873,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/9440295
33,9489329,2,9489036,2012-02-28T20:38:04.773,0,"<p>Although it may be a bit outdated, <a href=""http://freemarker.sourceforge.net/"" rel=""nofollow"">FreeMarker</a> is primarily a framework for rendering template files given some set of inputs.</p>",12604,,,2012-02-28T20:38:04.773,6,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/9489329
34,9560483,2,9559973,2012-03-05T01:06:55.170,1,"<blockquote>  <p>Is there a way to expand its size to the size of the volume without  losing my work?</p></blockquote><p>That depends on whether you can live with a few minutes downtime for the computation, i.e. whether stopping the instance (hence the computation process) is a problem or not - Eric Hammond has written a detailed article about <a href=""http://alestic.com/2010/02/ec2-resize-running-ebs-root"" rel=""nofollow"">Resizing the Root Disk on a Running EBS Boot EC2 Instance</a>, which addresses a different but pretty related problem:</p><blockquote>  <p>[...] what if you have an EC2 instance already running and you need to  increase the size of its root disk without running a different  instance?</p>   <p>As long as you are ok with a little down time on the EC2 instance (few  minutes), it is possible to change out the root EBS volume with a  larger copy, without needing to start a new instance.</p></blockquote><p>You have already done most of the steps he describes and created a new 300GB volume from the 180GB snapshot, but apparently you have missed the last required step indeed, namely resizing the file system on the volume - here are the instructions from Eric's article:</p><blockquote>  <p>Connect to the instance with ssh (not shown) and resize the root file  system to fill the new EBS volume. This step is done automatically at  boot time on modern Ubuntu AMIs:</p><pre><code># ext3 root file system (most common)sudo resize2fs /dev/sda1#(OR)sudo resize2fs /dev/xvda1# XFS root file system (less common):sudo apt-get update &amp;&amp; sudo apt-get install -y xfsprogssudo xfs_growfs /</code></pre></blockquote><p>So the details depend on the file system in use on that volume, but there should be a respective resize command available for all but the most esoteric or outdated ones, none of which I'd expect in a regular Ubuntu 10 installation.</p><p>Good luck!</p><hr><h3>Appendix</h3><blockquote>  <p>Is there a possibility that the snapshot is actually continuous with  another drive (e.g. /dev/sdb)?</p></blockquote><p>Not just like that, this would require a <a href=""http://en.wikipedia.org/wiki/RAID"" rel=""nofollow"">RAID</a> setup of sorts, which is unlikely to be available on a stock Ubuntu 10, except if somebody provided you with a respectively customized AMI. The size of <code>/dev/sdb</code> does actually hint towards this being your <a href=""http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/InstanceStorage.html"" rel=""nofollow"">Amazon EC2 Instance Storage</a>:</p><blockquote>  <p>When an instance is created from an Amazon Machine Image (AMI), in  most cases it comes with a preconfigured block of pre-attached disk  storage. Within this document, it is referred to as an instance store;  it is <strong>also known as an ephemeral store</strong>. An instance store provides  temporary block-level storage for Amazon EC2 instances. The data on  the instance store volumes <strong>persists only during the life of the  associated Amazon EC2 instance</strong>. The amount of this storage ranges from  160GiB to up to 3.3TiB and varies by Amazon EC2 instance type. [...] <em>[emphasis mine]</em></p></blockquote><p>Given this storage is not persisted on instance termination (in contrast to the <a href=""http://aws.amazon.com/ebs/"" rel=""nofollow"">EBS storage</a> we all got used to enjoy - the different behavior is detailed in <a href=""http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html"" rel=""nofollow"">Root Device Storage</a>), it should be treated with respective care (i.e. never store something on instance storage you couldn't afford to loose).</p>",45773,,,2012-03-05T01:06:55.170,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/9560483
35,9638670,2,9506791,2012-03-09T17:50:46.747,1,"<p>i would say API documentation is outdated:</p><p><a href=""http://aws.amazon.com/archives/Product%20Advertising%20API"" rel=""nofollow"">http://aws.amazon.com/archives/Product%20Advertising%20API</a> - says ""Last Modified: Jul 26, 2011 2:10 AM GMT""</p><p>this is CURRENT version of AWSECommerceService - it's from 2011-08-01 - and it does not have tagpage element: <a href=""http://ecs.amazonaws.com/AWSECommerceService/2011-08-01/AWSECommerceService.wsdl"" rel=""nofollow"">http://ecs.amazonaws.com/AWSECommerceService/2011-08-01/AWSECommerceService.wsdl</a></p><p>and here is OLDER version of AWSECommerceService - it's from 2011-04-01 - and it does have tagpage element: <a href=""http://ecs.amazonaws.com/AWSECommerceService/2011-04-01/AWSECommerceService.wsdl"" rel=""nofollow"">http://ecs.amazonaws.com/AWSECommerceService/2011-04-01/AWSECommerceService.wsdl</a></p>",1246870,,,2012-03-09T17:50:46.747,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/9638670
36,9723442,2,9723070,2012-03-15T15:56:37.887,5,"<p>According to your logfile, your version of fog is very very old. You're using 0.3.25, and the most recent tag is at 1.1.2. Try doing this:</p><pre><code>bundle update fog</code></pre><p>Your version of carrierwave is similarly out of date, so I'd <code>bundle update carrierwave</code> as well. That should help correct this issue.</p>",1224374,,,2012-03-15T15:56:37.887,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/9723442
37,9810696,2,9597500,2012-03-21T18:30:18.017,2,"<p>This has meanwhile been addressed in the AWS team response to the identical question asked in the AWS forum, see <a href=""https://forums.aws.amazon.com/message.jspa?messageID=325839#325839"" rel=""nofollow"">EC2 reports AMI: Unavailable</a>:</p><blockquote>  <p>This is an AWS owned AMI that is no longer publicly available as it is  deprecated. This will not affect your currently running instance.  Additionally, if you create an EBS AMI of your running instance you  will create a point in time backup of your current configuration --  which you can use to launch duplicate instances from. </p>   <p>The current AWS provided Windows Server 2008 32bit AMI is:  ami-541dcf3d</p></blockquote>",45773,,,2012-03-21T18:30:18.017,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/9810696
38,9857697,2,9857086,2012-03-25T03:23:14.873,4,"<p>You might want to start the script with:</p><pre><code>apt-get update</code></pre><p>as your apt cache might be out of date for the ""apt-get install"".</p><p>You can also debug the script by starting it with these two lines:</p><pre><code>#!/bin/bash -exexec &gt; &gt;(tee /var/log/rc.local.log|logger -t rc.local -s 2&gt;/dev/console) 2&gt;&amp;1</code></pre><p>This will echo each command and its output to /var/log/rc.local.log so you can find out what is failing and with what error.</p><p>Make sure the file is executable:</p><pre><code>sudo chmod 755 /etc/rc.local</code></pre><p>Note that rc.local is run on <em>every</em> boot, not just the first boot.  Make sure that you are ok with it being run again after the system has been up for a while.</p><p>Here's an article I wrote with more details about the ""exec"" command above: <a href=""http://alestic.com/2010/12/ec2-user-data-output"" rel=""nofollow"">http://alestic.com/2010/12/ec2-user-data-output</a></p>",111286,,,2012-03-25T03:23:14.873,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/9857697
39,10010745,2,10009638,2012-04-04T12:06:47.033,3,"<p>If you are about to register that domain anyway in case, you could simply try to call <a href=""http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/simpleworkflow/AmazonSimpleWorkflowClient.html#registerDomain%28com.amazonaws.services.simpleworkflow.model.RegisterDomainRequest%29"" rel=""nofollow"">registerDomain()</a> and catch the <a href=""http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/simpleworkflow/model/DomainAlreadyExistsException.html"" rel=""nofollow"">DomainAlreadyExistsException</a>:</p><blockquote>  <p>Returned if the specified domain already exists. You will get this  fault even if the existing domain is in deprecated status.</p></blockquote><p>Obviously this has the side effect of actually registering the domain, if it doesn't exist yet ;)</p><p>Otherwise <a href=""http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/simpleworkflow/AmazonSimpleWorkflowClient.html#describeDomain%28com.amazonaws.services.simpleworkflow.model.DescribeDomainRequest%29"" rel=""nofollow"">describeDomain()</a> should allow a similar approach, insofar it will throw an <a href=""http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/simpleworkflow/model/UnknownResourceException.html"" rel=""nofollow"">UnknownResourceException</a> in case of a non existing domain:</p><blockquote>  <p>Returned when the named resource cannot be found with in the scope of  this operation (region or <strong>domain</strong>). This could happen if the named  resource was never created or is no longer available for this  operation. <em>[emphasis mine]</em></p></blockquote>",45773,,,2012-04-04T12:06:47.033,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/10010745
40,10453972,2,10452834,2012-05-04T18:09:53.100,3,"<p>Its definitely possible. For webservices in Python, <a href=""https://bitbucket.org/jespern/django-piston/wiki/Home"" rel=""nofollow"">Piston</a> is my go-to. From there, you need to deploy it on a webserver of some kind. If you're deploying on EC2, you can use the <a href=""https://aws.amazon.com/amis/bitnami-djangostack-1-3-1-ubuntu-10-04"" rel=""nofollow"">bitami AMI preconfigured with Django and Apache.</a> Of course, you'll need to apt-get install the opencv ubuntu packages.</p><p>Now, given all that, it might or might not make sense to do this ""in the cloud"" with a webservice. You'll need to transfer the image you want to process, probably via an HTTP POST, so that will take some time and bandwidth. Depending on what you want to return, you might have a lot of data coming back too, which will have its own cost. Can you share with us what your application will do? I could provide better advice if I knew what you were trying to accomplish.</p><p>As you noted, the ubuntu OpenCV package is out of date. If you need something more recent, I've found its easy to install <a href=""http://www.ros.org/wiki/ROS/Installation"" rel=""nofollow"">ROS</a> which includes <a href=""http://www.ros.org/wiki/vision_opencv"" rel=""nofollow"">OpenCV 2.3</a> and its dependencies.</p>",104887,104887,2012-05-04T18:29:11.217,2012-05-04T18:29:11.217,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/10453972
41,10598297,2,10596456,2012-05-15T09:57:48.977,19,"<h1>Update</h1><p>Your rules are currently lacking an additional and likely relevant fragment related to the FAQ <a href=""http://aws.amazon.com/vpc/faqs/#S4"" rel=""noreferrer"">What are the differences between security groups in a VPC and network ACLs in a VPC?:</a></p><blockquote>  <p>Security groups in a VPC specify which traffic is allowed to or from  an Amazon EC2 instance. Network ACLs operate at the subnet level and  evaluate traffic entering and exiting a subnet. Network ACLs can be  used to set both Allow and Deny rules. Network ACLs do not filter  traffic between instances in the same subnet. In addition, <strong>network  ACLs perform stateless filtering while security groups perform  stateful filtering</strong>. <em>[emphasis mine]</em></p></blockquote><p>This is addressed further in <a href=""http://aws.amazon.com/vpc/faqs/#S5"" rel=""noreferrer"">What is the difference between stateful and stateless filtering?</a>:</p><blockquote>  <p><strong>Stateful filtering</strong> tracks the origin of a request and can  <strong>automatically allow the reply to the request</strong> to be returned to the  originating computer. [...]</p>   <p><strong>Stateless filtering, on the other hand</strong>, only examines the source or  destination IP address and the destination port, ignoring whether the  traffic is a new request or a reply to a request. In the above  example, <strong>two rules would need to be implemented on the filtering  device: one rule to allow traffic inbound to the web server on tcp  port 80, and another rule to allow outbound traffic from the webserver</strong>  (tcp port range 49,152 through 65,535). <em>[emphasis mine]</em></p></blockquote><p>Now, you allow all outbound traffic already, so this doesn't apply as per the example, but the same issue applies the other way round as well, so e.g. for HTTP requests originating from your EC2 instances you'll need to have a corresponding inbound rule as outlined, see section <a href=""http://docs.amazonwebservices.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html#VPC_ACLs_Ephemeral_Ports"" rel=""noreferrer"">Ephemeral Ports</a> within <a href=""http://docs.amazonwebservices.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html"" rel=""noreferrer"">Network ACLs</a> for more details on this:</p><blockquote>  <p>The client that initiates the request chooses the ephemeral port  range. The range varies depending on the client's operating system. [...]</p>   <p>If an instance in your VPC is the client initiating a request, your  network ACL must have an inbound rule to enable traffic destined for  the ephemeral ports specific to the type of instance (Amazon Linux,  Windows Server 2008, etc.).</p>   <p>In practice, to cover the different types of clients that might  initiate traffic to public-facing instances in your VPC, you need to  open ephemeral ports 1024-65535. [...]</p></blockquote><h2>Solution</h2><p>Accordingly, section <a href=""http://docs.amazonwebservices.com/AmazonVPC/latest/UserGuide/VPC_Appendix_NACLs.html#VPC_Appendix_NACLs_Scenario_2"" rel=""noreferrer"">Recommended Rules for Scenario 2</a> within <a href=""http://docs.amazonwebservices.com/AmazonVPC/latest/UserGuide/VPC_Appendix_NACLs.html"" rel=""noreferrer"">Appendix A: Recommended Network ACL Rules</a> suggests the following inbound rule (OS dependent example) for your scenario:</p><pre><code>Inbound:0.0.0.0/0 port 49152-65535 (TCP)</code></pre><p>To test whether this issue actually applies, you might simply include the entire ephemeral port range:</p><pre><code>Inbound:0.0.0.0/0 port 1024-65535 (TCP)</code></pre><hr><h3>Initial Answer (obsolete)</h3><blockquote>  <p>For the public network 10.0.0.0/24 in here I have an exposed load  balancer, which is redirecting trafic to the private network  10.0.1.0/24, where an app is responding over http</p></blockquote><p>Your setup suggests you intend to terminate SSL on the load balancer as usual; given your increased security requirements you might actually have setup the <a href=""http://aws.amazon.com/elasticloadbalancing/"" rel=""noreferrer"">Elastic Load Balancing</a> for back-end HTTPS communication as well (see <a href=""http://docs.amazonwebservices.com/ElasticLoadBalancing/latest/DeveloperGuide/SvcIntro_arch_workflow.html"" rel=""noreferrer"">Architectural Overview</a>) - you don't seem to have an ACL rule accommodating inbound HTTPS traffic to 10.0.1.0/24 though, so that would be the one missing in case:</p><pre><code>Inbound:10.0.0.0/24 port 80 (HTTP)10.0.0.0/24 port 443 (HTTPS) // &lt;= missing in your example currently!10.0.0.0/24 port 22 (SSH)10.0.2.0/24 port 3306 (MySql)10.0.3.0/24 port 3306 (MySql)OutboundALL ALL</code></pre>",45773,45773,2012-05-15T12:36:40.747,2012-05-15T12:36:40.747,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/10598297
42,10730428,2,8070186,2012-05-24T02:39:35.720,23,"<p>This answer was accurate at the time it was written but is now out of date. The AWS API's and Libraries (such as boto3) can now take a ""TagSpecification"" parameter that allows you to specify tags when running the ""create_instances"" call.</p><hr><p>Tags cannot be made until the instance has been created. Even though the function is called create_instance, what it's really doing is reserving and instance. Then that instance may or may not be launched. (Usually it is, but sometimes...)</p><p>So, you cannot add a tag until it's been launched. And there's no way to tell if it's been launched without polling for it. Like so:</p><pre><code>reservation = conn.run_instances( ... )# NOTE: this isn't ideal, and assumes you're reserving one instance. Use a for loop, ideally.instance = reservation.instances[0]# Check up on its status every so oftenstatus = instance.update()while status == 'pending':  time.sleep(10)  status = instance.update()if status == 'running':  instance.add_tag(""Name"",""{{INSERT NAME}}"")else:  print('Instance status: ' + status)  return None# Now that the status is running, it's not yet launched. The only way to tell if it's fully up is to try to SSH in.if status == ""running"":  retry = True  while retry:  try:  # SSH into the box here. I personally use fabric  retry = False  except:  time.sleep(10)# If we've reached this point, the instance is up and running, and we can SSH and do as we will with it. Or, there never was an instance to begin with.</code></pre>",1345461,212774,2020-02-07T23:31:10.450,2020-02-07T23:31:10.450,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/10730428
43,10772392,2,10235959,2012-05-27T07:36:42.873,0,"<p>The Amazon Documentation is incorrect/out of date - use <a href=""http://ecs.amazonaws.com"" rel=""nofollow"">ecs.amazonaws.com</a> instead of <a href=""http://webservices.amazon.com"" rel=""nofollow"">webservices.amazon.com</a></p><p>So sign this:</p><blockquote>  <p>GET\necs.amazonaws.com\n/onca/xml\nAWSAccessKeyId=AKIAIOSFODNN7EXAMPLE&amp;ItemId=0679722769&amp;Operation=ItemLookup&amp;ResponseGroup=ItemAttributes%2COffers%2CImages%2CReviews&amp;Service=AWSECommerceService&amp;Timestamp=2009-01-01T12%3A00%3A00Z&amp;Version=2009-01-06</p></blockquote><p>Sorry this is a really quick reply!</p>",1419880,1712248,2012-10-19T20:57:43.233,2012-10-19T20:57:43.233,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/10772392
44,10803486,2,10770971,2012-05-29T16:56:24.620,1,"<p>The Zend_Service_Amazon is out of date and uses old WSDL.see <a href=""http://zendframework.com/issues/browse/ZF-12046"" rel=""nofollow"">http://zendframework.com/issues/browse/ZF-12046</a></p><p>wait for 1.12 or use the provided batch file.</p>",151097,,,2012-05-29T16:56:24.620,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/10803486
45,10899935,2,9048756,2012-06-05T15:10:50.427,26,"<p><strong>UPDATE 2019/12/10</strong> Even though my answer was accepted as the best answer it's outdated. Please see this answer: <a href=""https://stackoverflow.com/a/17180929/799155"">https://stackoverflow.com/a/17180929/799155</a></p><hr><p>It's not possible Š—– I guess for a bunch of security reasons. Read through the thread you posted in your question ;)</p><p><em>Lorraine@AWS, Aug 13, 2010 2:02 PM</em></p><blockquote>  <p>Amazon does not offer APIs to place a customer order. Neither a  Corporate Account or the Associate Program will allow you to build the  type of solution you are describing.</p></blockquote><p><em>Lorraine@AWS, May 13, 2011 4:17 PM</em></p><blockquote>  <p>Amazon still does not offer this service, to the best of my knowledge,  and this isn't something I would ever expect to see included in the  Fulfillment Web Service which is specific to the FBA program.</p></blockquote>",799155,799155,2019-12-10T08:25:07.433,2019-12-10T08:25:07.433,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/10899935
46,11148178,2,8878736,2012-06-21T23:17:07.300,0,<p>If you have access to a Motorola Xoom running Honeycomb 3.2 you might want to test your app on that.  My app was rejected by Amazon because they experienced blank screens.  I've tested it thoroughly on Kindle Fire and have never seen that behavior.  It was only when I emphasized that point with the Amazon reviewer that I learned they were not testing on their own device but on Motorola's Xoom running an obsolete OS that I don't want to support.  I now have a Xoom and my app runs fine on it under ICS but is quite buggy with Honeycomb.</p>,1473527,,,2012-06-21T23:17:07.300,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/11148178
47,11196253,2,11194080,2012-06-25T19:44:24.853,49,"<p>So apparently EC2 has it's own limits. I assumed (incorrectly) that having production access to SES would also mean relaxed SMTP limitations from EC2, but as they are two completely separate products I guess that is not the case.</p><p>But as I stated in the last paragraph of my post, you can <a href=""https://aws-portal.amazon.com/gp/aws/html-forms-controller/contactus/ec2-email-limit-rdns-request"">Request to Remove Email Sending Limitations</a> to have these limits raised. I did that and the problem stopped (it took them around 5 hours to get my limits removed).</p><hr><h3>Update</h3><p>The EC2 throttling is documented in <a href=""http://docs.aws.amazon.com/ses/latest/DeveloperGuide/smtp-connect.html"">Connecting to the Amazon SES SMTP Endpoint</a> and actually constrained to port 25, so an alternative and immediate solution is simply using port 587 instead (it's a bit unfortunate that several official SES examples are using port 25 indeed):</p><blockquote>  <p><strong>Important</strong>  </p>   <p>Elastic Compute Cloud (EC2) throttles email traffic over port 25 by  default. To avoid timeouts when sending email through the SMTP  endpoint from EC2, use a different port (587 or 2587) or fill out a  <a href=""https://aws-portal.amazon.com/gp/aws/html-forms-controller/contactus/ec2-email-limit-rdns-request"">Request to Remove Email Sending Limitations</a> to remove the throttle.</p></blockquote><p>Beware that this might be slightly outdated as well, insofar both the <a href=""http://aws.amazon.com/console/#ses"">AWS Management Console</a> and section <a href=""http://docs.aws.amazon.com/ses/latest/DeveloperGuide/smtp-issues.html"">Amazon SES SMTP Issues</a> are referring to the more common alternative ports 465 and 587 only:</p><blockquote>  <p><strong>You are sending to Amazon SES from an Amazon EC2 instance via port 25 and you cannot reach your Amazon SES sending limits or you are  receiving time outs</strong> Š—” Amazon SES EC2 imposes default sending limits  on email sent via port 25 and throttles outbound connections if you  attempt to exceed those limits. To remove these limits, submit a  <a href=""https://aws-portal.amazon.com/gp/aws/html-forms-controller/contactus/ec2-email-limit-rdns-request"">Request to Remove Email Sending Limitations</a>. You can also connect  to Amazon SES via port 465 or port 587, neither of which is throttled.</p></blockquote>",1325454,45773,2013-01-29T09:42:20.920,2013-01-29T09:42:20.920,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/11196253
48,11620243,2,11617718,2012-07-23T20:45:08.983,4,"<p>You should move the volume to its own EBS volume anyway, this helps with write contention on the EBS volumes as well as other benefits. In addition, I have the logs writing to their own volume and back those up as well.</p><p>To answer the question, I do both. Having the EBS volume snapshotted and doing a dump of the database. This way if you want to sync your live data to a dev box (depending on the PII on the database) it is easy with a dump and restore, but you also can restore a new instance and attach a snapshot easily as well.If your database dump is less than 5gb you can sync it to S3 and forget about having to store the backups on their own volume, but if it isn't you will need to store it on its own EBS volume that is then also snapshotted on a regular basis.</p><p><a href=""https://github.com/chandlerh/EBS-Snapshot"" rel=""nofollow"">Here</a> is a script I wrote to do this, it might be outdated, but should work.</p>",667608,667608,2012-07-23T20:50:52.413,2012-07-23T20:50:52.413,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/11620243
49,11679847,2,11660441,2012-07-27T00:15:18.170,2,"<p>The ""real"" answer here, is that, dynamodb clients which don't match up with the latest or current versions can exhibit odd reflection / class loading error when we attempt to use them in a modern environment.  </p><ul><li>AWS jars exist on the class path of older EMR AMI instances can conflict with proper (latest) AWS jars used by hadoop job which invokes a non-EMR service (i.e. such as dynamodb, in our case). </li></ul><p>On my older AMI instance, I simply issued:</p><pre><code>mv $HOME/lib/aws-java-sdk-1.1.1.jar $HOME/lib/aws-java-sdk-1.1.1.jar.old </code></pre><p>To resolve the issue on a single node cluster. </p><hr><p>The ROOT cause of this error? was that I was using an older Ruby elastic-mapreduce client, which led to creation of older AMI versions in my EMR cloud, which had obsolete aws-sdk jars on the class path.  </p>",350542,350542,2012-08-01T05:55:07.680,2012-08-01T05:55:07.680,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/11679847
50,11781147,2,11764433,2012-08-02T15:44:54.120,2,"<p><strong>UPDATE: Specs are specs, but are only help if they match actual practice.</strong></p><p>Amazon's S3 specs say the signature should be formed as the following:</p><pre><code>Signature = URL-Encode( Base64( HMAC-SHA1( YourSecretAccessKeyID, UTF-8-Encoding-Of( StringToSign ) ) ) );StringToSign = HTTP-VERB + ""\n"" +  Content-MD5 + ""\n"" +  Content-Type + ""\n"" +  Expires + ""\n"" +  CanonicalizedAmzHeaders +  CanonicalizedResource;  </code></pre><p><strong>HOWEVER</strong> the actual request needed looks like this:</p><pre><code> StringToSign = HTTP-VERB + ""\n"" +  ""\n"" +   ""\n"" +  Expires + ""\n"" +  Bucket + CanonicalizedResource + ""?"" + CanonicalizedAmzHeaders;</code></pre><p>Strangely enough, in PHP you also can't seem to do this:</p><pre><code>$string = ""$type\n"".  ""\n"".  ""\n"".  ""$expiration\n"".  ""/$bucket$request?$headers"";</code></pre><p>It changes the signature and ends up rejected so must all be on a single line.  I haven't gone as far as checking whether this is a bug in our particular version of PHP or just a general PHP bug (or maybe intended functionality??!).  You must also include the name of your bucket even if you are using the vanity URLs available such as mybucket.s3.amazonaws.com or mybucket.mydomain.com.  The <a href=""http://docs.amazonwebservices.com/AmazonS3/latest/dev/RESTAuthentication.html#RESTAuthenticationQueryStringAuth"" rel=""nofollow noreferrer"">documentation</a> doesn't specify what you can or can't do and I made the assumption that since we are using an S3 based vanity URL it (S3) would pick up on the domain name and translate it to the bucket.</p><p>I ended up changing my function to be the following:</p><pre><code>function signRequest($bucket, $request, $expiration, $s3secret, $headers = '', $type = 'GET', $content_type = 'default'){  if ($expiration == 0 || $expiration == null)  {  $expiration = time() + 315576000; // 10 years (never)  }  if (strcmp($content_type, 'default') == 0)  {  $content_type = """";  }  $headers = trim($headers, '&amp;');  // This is the spec:  /*$string = ""$type\n"".  ""\n"".  ""$content_type\n"".  ""$expiration\n"".  ""$headers\n"".  ""$bucket$request"";*/  // but it will only work as this  $string = ""$type\n\n\n$expiration\n/$bucket$request?$headers"";  // this could be a single line of code but left otherwise for readability  // must be in UTF8 format  $string = utf8_encode(trim($string));  // encode to binary hash using sha1. require S3 bucket secret key  $hash = hash_hmac(""sha1"",$string, $s3secret,true);  // sha1 hash must be base64 encoded  $hash = base64_encode($hash);  // base64 encoded sha1 hash must be urlencoded  $signature = urlencode($hash);  return $signature;}</code></pre><p>Hopefully someone else finds this useful as well.</p><p><strong>UPDATE (20180109): Adding in the function that calls this (aka let's make this blatently simple).</strong></p><p>It helps understanding of what to pass to the signRequest() function.</p><pre><code>  private function genS3QueryString($bucketurl)  {  $file_type = 'application/force-download';  $expiry = '1831014000'; //Sun, Jan 09 2028 0700 UTC  $headers = '&amp;response-content-type='.$file_type.'&amp;response-expires='.$expiry;  $bucket = preg_replace(""/^.*?:\/\/(.*)\.s3\.amazonaws\.com\/.*/"", ""$1"", $bucketurl);  $request = preg_replace(""/^.*?:\/\/.*\//"", ""/"", $bucketurl);  $signature = $this-&gt;signRequest($bucket, $request, $expiry, S3_SECRET_KEY, $headers, 'GET', $file_type);  $signed_request = '?AWSAccessKeyId='.S3_KEY.'&amp;Expires='.$expiry.$headers.'&amp;Signature='.$signature;  return $signed_request;  }</code></pre><p>You'll note however the first function only generates the encrypted signature section required to attach to a GET request for AWS. As per the docs (see the link far above) more is required for the request, as is formed from the second function just above. </p><p>The expiry time I believe can be a rolling expiry time if you so desire but should be sufficiently in the future that the asset becomes outdated and replaced long before the actual expiry time. An arbitrary time was chosen to sufficiently outlast the current version of the website.</p><p>The second function only requires the bucket url of the protected asset. The desired response type could be added to the call or simply changed so that asset (in this case just non-displayable) documents are downloaded as a different type.</p><p>The <strong>signed_request</strong> that is then returned must then be appended back onto the bucketurl for a working URI to request from a protected S3 asset.</p>",1451959,1451959,2018-01-09T19:36:33.843,2018-01-09T19:36:33.843,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/11781147
51,11886716,2,11820166,2012-08-09T15:26:16.070,1,"<p>This <code>wasn't an issue with the AWSS3JavaClient code</code>, based on the fact that the this problem was happening both with S3 library and with other Java S3 libraries, and the fact that <code>SSL cert verification is done inside the JVM platform library code</code>, not inside our S3 library code. </p><p>The problem is that our JVM's keystore didn't have the most recent certificate authorities (CAs) that allow the JVM to form a chain of trust for whatever cert we're getting from the S3 SSL endpoint. This is a fairly common problem with Java and SSL, since the JVM maintains it's own keystore (i.e. it doesn't use certs from the OS). </p><p>If you face this problem,  try reproducing this issue with other JVMs. Whenever customers have seen this issue in the past, it's been because their local <code>JVM keystore</code> (the keystore ships with the JVM and contains the most recent certs and CAs) has been out of date. Upgrading to the latest JVM version has always fixed this in the past.</p><p>Try upgrading your <code>JVM version</code> to recent one, it should help because your keystore must have been expired! :)</p>",934796,,,2012-08-09T15:26:16.070,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/11886716
52,11943992,2,11940594,2012-08-14T00:08:20.640,2,"<p>If you're new to cloud hosting, rather than using EC2 directly consider using <a href=""https://aws.amazon.com/solution-providers/isv/enterprisedb"" rel=""nofollow"">EnterpriseDB's cloud options</a>. <a href=""http://www.enterprisedb.com/cloud-database/pricing-amazon"" rel=""nofollow"">Details here</a>.</p><p>If you want to use EC2 directly, sign up and create an instance. </p><p>Choose your preferred Linux distro image. I'm assuming you'll use Linux on EC2; if you want to use Windows that's because you probably already know how. Let the new VM provision and boot up, then SSH into it as per the documentation available on Amazon for EC2 and for that particular VM image. Perform any recommended setup for that VM image as per its documentation.</p><p>Once you've done the recommended setup for that instance, you can install PostgreSQL:</p><ul><li>For Ubuntu, <code>apt-get install postgresql</code></li><li>For Fedora, <code>yum install postgresql</code></li><li>For CentOS, use the <a href=""http://wiki.postgresql.org/wiki/RPM_Installation"" rel=""nofollow"">PGDG yum repository</a>, <em>not</em> the outdated version of PostgreSQL provided.</li></ul><p>You can now connect to Pg as the default <code>postgres</code> superuser:</p><pre><code>sudo -u postgres psql</code></pre><p>and are able to generally use PostgreSQL much the same way you do on any other computer. You'll probably want to make yourself a user ID and a new database to restore into:</p><pre><code>echo ""CREATE USER $USER;"" | sudo -u postgres psqlecho ""CREATE DATABASE thedatabase WITH OWNER $USER"" | sudo -u postgres psql</code></pre><p>Change ""thedatabase"" to whatever you want to call your db, of course.</p><p>The exact procedure for restoring the dump to your new DB depends on the dump format. </p><p>For <code>pg_dump -Fc</code> or PgAdmin-III custom-format dumps:</p><pre><code>sudo -u postgres pg_restore --dbname thedatabase thebackupfile</code></pre><p>See ""man pg_restore"" and the <a href=""http://www.postgresql.org/docs/9.1/static/app-pgrestore.html"" rel=""nofollow"">online documentation</a> for details on <code>pg_restore</code>.</p><p>For plain SQL format dumps you will want to stream the dump through a decompression program then to psql. Since you haven't said anything about the dump file name or format it's hard to know what to do. I'll assume it's gzip'ed ("".gz"" file extension), in which case you'd do something like:</p><pre><code>gzip -d thedumpfile.gz | sudo -u postgres psql thedatabase</code></pre><p>If its file extension is "".bz2"" change <code>gzip</code> to <code>bzip2</code>. If it's a <code>.zip</code> you'll want to <code>unzip</code> it then run <code>psql</code> on it using <code>sudo -u postgres psql -f thedumpfilename</code>.</p><p>Once restored you can connect to the db with <code>psql thedatabase</code>.</p>",398670,398670,2012-08-14T01:51:55.680,2012-08-14T01:51:55.680,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/11943992
53,11996761,2,10463360,2012-08-16T23:16:20.953,0,"<p>Was researching my own (now resolved problem), and found this question via <a href=""http://www.uploadify.com/forum/#/discussion/1416/uploading-to-amazon-s3/p1"" rel=""nofollow"">http://www.uploadify.com/forum/#/discussion/1416/uploading-to-amazon-s3/p1</a></p><p>That very thread got me running a little over a year ago, but I have recently started experiencing problems with Chrome 21/win.</p><p>Your code run through <a href=""http://jsbeautifier.org/"" rel=""nofollow"">http://jsbeautifier.org/</a></p><pre><code>$(document).ready(function () {  $('#file').uploadify({  ""formData"": {  ""AWSAccessKeyId"": ""AKIAJOVJ2J3JX5Z6AIKA"",  ""bucket"": ""my-stupid-bucket"",  ""acl"": ""private"",  ""key"": ""uploads/${filename}"",  ""signature"": ""2I0HPQ2SoZOmhAUhYEb4nANFSCQ%253D"",  ""policy"": ""[... snip ...]""  },  ""fileObjName"": ""file"",  ""uploader"": ""http://my-stupid-bucket.amazonaws.com"",  ""swf"": ""/static/uploadify/uploadify.swf""  });});</code></pre><p>I just upgraded to uploadify 3.1 and ran into similar invalid signature problems.  The answer seems to be that the code in that thread is simply outdated.  </p><p>IIRC, that thread states you should encodeURIComponent the signature/etc.  This is no longer the case, it seems.  My 2.x code is working with 3.1 without encoding the signature and policy.  Guessing this was some workaround to an uploadify/flash problem that has since been resolved.</p><p>Additionally, it looks like your signature is double URL encoded.</p><p>TL;DR;<br>Don't encodeURIComponent your form data with uploadify 3.1 (and possibly earlier versions).</p>",670023,,,2012-08-16T23:16:20.953,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/11996761
54,12269514,2,12268986,2012-09-04T18:42:35.643,4,"<p>After some error and trial, got the script working:</p><pre><code>#!/bin/bashhostname=`ec2-describe-tags --filter ""resource-type=instance"" \  --filter ""resource-id=$(ec2-metadata -i | cut -d ' ' -f2)"" \  --filter ""key=Name"" | grep Name`IFS=""|"" read -ra NAME &lt;&lt;&lt; ""$hostname""hostname=${NAME[4]}echo $hostname</code></pre><p>Used IFS to get the string parsed into arrays, and luckily I know the 4th element is always the hostname.</p><p>EDIT (20-DEC-2012): In the short time since this was posted, several of the relevant ec2 command line tools have been modified, and flags changed or deprecated (e.g., the -i flag from above no longer seems to work on the current version of ec2metadata).  Bearing that in mind, here is the command line script I used to get the current machine's ""Name"" tag (can't speak to the rest of the script):</p><pre><code>ec2-describe-tags --filter ""resource-type=instance"" --filter ""resource-id=$(ec2metadata --instance-id)"" | awk '{print $5}'</code></pre><p>On Debian/Ubuntu, you need to <code>apt-get install cloud-utils ec2-api-tools</code> to get these working (the later is only on Ubuntu Multiverse).</p>",957423,1002047,2012-12-21T05:02:30.783,2012-12-21T05:02:30.783,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/12269514
55,12358373,2,12356290,2012-09-10T19:41:03.517,0,"<p>There could be several root causes:</p><ul><li>Outdated java installation which doesn't contain a valid root certificate for some of the Amazon servers</li><li>Invalid server-side certificate that is not trusted by any valid issuer</li></ul><p>To debug to the console from the client side, add <code>-Djavax.net.debug=all</code> to the command line.  The page on <a href=""http://docs.oracle.com/javase/1.5.0/docs/guide/security/jsse/ReadDebug.html"" rel=""nofollow"">Debugging SSL/TLS connections</a> may provide insight into the underlying error.</p>",1588577,,,2012-09-10T19:41:03.517,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/12358373
56,12405620,2,12405433,2012-09-13T11:51:12.960,3,"<p>They dropped support for obsolete APIs recently, and the newest version requires a valid Associate Tag.</p><p><a href=""https://affiliate-program.amazon.com/gp/advertising/api/detail/api-changes.html"" rel=""nofollow"">https://affiliate-program.amazon.com/gp/advertising/api/detail/api-changes.html</a></p><blockquote>  <p>Associate Tag Parameter: Every request made to the API should include a valid Associate Tag. Any request that does not contain a valid Associate Tag will be rejected with an appropriate error message. </p></blockquote><p>ASSOC_TAG must be your real tag (one that matches the API key).</p>",989121,,,2012-09-13T11:51:12.960,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/12405620
57,12578500,2,11359990,2012-09-25T07:53:01.707,5,"<p>NOTE: In the three years since this response was posted, Vagrant has introduced the <code>facter</code> hash option. See @thomas' answer below for more details. I believe that this is the right way to go and makes my proposed kernel command line trick pretty obsolete. The rationale for using a fact hasn't changed, though, only strengthened (e.g. Vagrant currently supports AWS provider).</p><p>ORIGINAL REPLY: Be careful - you assume that you only use virtualbox for vagrant and vice versa, but Vagrant is working on support for other virtualization technologies (e.g. kvm), and you might use VirtualBox without vagrant one day (e.g. for production).</p><p>Instead, the trick I use is to pass the kernel a ""vagrant=yes"" parameter when I build the basebox, which is then accessible via /proc/cmdline. Then you can create a new fact based on that (e.g. /etc/vagrant file and check for it in subsequent facter runs).</p>",164137,164137,2015-12-27T03:22:07.100,2015-12-27T03:22:07.100,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/12578500
58,12590624,2,12588748,2012-09-25T20:35:32.993,0,"<p>The Flume NG HDFS sink doesn't implement anything special for S3 support. Hadoop has some built-in support for S3, but I don't know of anyone actively working on it. From what I have heard, it is somewhat out of date and may have some durability issues under failure.</p><p>That said, I know of people using it because it's ""good enough"".</p><p>Are you saying that ""//xyz"" (with multiple adjacent slashes) is a valid path name on S3? As you probably know, most Unixes collapse adjacent slashes.</p>",1220179,,,2012-09-25T20:35:32.993,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/12590624
59,12820595,2,5402013,2012-10-10T13:31:29.223,4,"<p>I think that is now outdated by ec2-bundle-vol and ec2-migrate-image, BTW you can also take a look at this Perl script by Lincoln D. Stein:<a href=""http://search.cpan.org/~lds/VM-EC2/bin/migrate-ebs-image.pl"" rel=""nofollow"">http://search.cpan.org/~lds/VM-EC2/bin/migrate-ebs-image.pl</a></p><p>Usage:</p><blockquote>  <p>$ migrate-ebs-image.pl --from us-east-1 --to ap-southeast-1 ami-123456</p></blockquote>",930720,930720,2012-10-12T11:09:33.617,2012-10-12T11:09:33.617,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/12820595
60,12922794,2,8304757,2012-10-16T20:21:51.373,2,"<p>Had I enough rep I'd have commented on an earlier solution but alas I'm limited to providing a new answer...</p><p>So, if you pass a MerchantId value of All, you will see this in your response:</p><pre><code>&lt;MerchantId&gt;Deprecated&lt;/MerchantId&gt;</code></pre><p>which indicates to me that perhaps you should not provide this parameter, counter to what the selected answer suggests.</p><p>More than that, I suspect there is something else going on.  I'll probably pose a whole new question about this here, but I am submitting a request for a product with a valid ASIN, getting back True, requesting the response groups Offers, OfferFull, and OfferSummary, and I am not getting back any price information.  Yet when I look at the very same product on amazom.com, I see see price information.</p><p>So something else could be wrong here.</p>",1224138,,,2012-10-16T20:21:51.373,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/12922794
61,12983725,2,12980745,2012-10-19T23:23:13.703,5,"<p>Though you can upload objects to S3 with sizes up to 5TB, S3 has a size limit of 5GB for an individual PUT operation.</p><p>In order to load files larger than 5GB (or even files larger than 100MB) you are going to want to use the multipart upload feature of S3.</p><blockquote><p><a href=""http://docs.amazonwebservices.com/AmazonS3/latest/dev/UploadingObjects.html"" rel=""noreferrer"">http://docs.amazonwebservices.com/AmazonS3/latest/dev/UploadingObjects.html</a></p><p><a href=""http://aws.typepad.com/aws/2010/11/amazon-s3-multipart-upload.html"" rel=""noreferrer"">http://aws.typepad.com/aws/2010/11/amazon-s3-multipart-upload.html</a></p></blockquote><p>(Ignore the outdated description of a 5GB object limit in the above blog post.  The current limit is 5TB.)</p><p>The boto library for Python supports multipart upload, and the latest boto software includes an &quot;s3multiput&quot; command line tool that takes care of the complexities for you and even parallelizes part uploads.</p><blockquote><p><a href=""https://github.com/boto/boto"" rel=""noreferrer"">https://github.com/boto/boto</a></p></blockquote>",111286,-1,2020-06-20T09:12:55.060,2012-10-19T23:23:13.703,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/12983725
62,13030279,2,13016797,2012-10-23T12:20:27.320,9,"<p>I've given up on this, as I think it's a bug in php - in particular the mysql_connect code, which is now deprecated. It could probably be solved by compiling php yourself with changes to the source using steps similar to those mentioned in the bug report that @eggyal mentioned: <a href=""https://bugs.php.net/bug.php?id=54158"">https://bugs.php.net/bug.php?id=54158</a></p><p>Instead, I'm going to work around it by doing a system() call and using the mysql command line:</p><pre><code>$sql = ""LOAD DATA LOCAL INFILE '$csvPathAndFile' INTO TABLE $tableName FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\""' ESCAPED BY '\\\\\\\\' LINES TERMINATED BY '\\\\r\\\\n';"";system(""mysql -u $dbUser -h $dbHost --password=$dbPass --local_infile=1 -e \""$sql\"" $dbName"");</code></pre><p>That's working for me.</p>",151503,,,2012-10-23T12:20:27.320,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/13030279
63,13357555,2,12345363,2012-11-13T08:30:10.960,0,"<p>I think this has been deprecated in favor of ""mon-put-metric-alarm""</p>",251936,,,2012-11-13T08:30:10.960,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/13357555
64,13871309,2,12923080,2012-12-14T01:18:40.523,15,"<h1>Edit:</h1><p>An item returned by the Amazon Product API can represent either a <strong>single variation item</strong> (a single size and/or a single color) or a <strong>variation parent</strong>. When a single variation item is returned, you just have to use the same approach as you initially did and you'll be able to fetch the price.</p><p>A <strong>parent variation item</strong> (your case), however, is not associated with any offer (price), because it is an abstraction of a product and acts as a container for the existing product variations (different sizes, colors).</p><p>In this case, every variation contained within the variation parent has its own price and you can simply iterate through the set of variations and fetch the price you need.</p><p>Adding the <strong>Variations</strong> response group to your search/look up request is crucial, so don't omit it. </p><p>The request body:</p><pre><code>&lt;env:Envelope xmlns:env='http://schemas.xmlsoap.org/soap/envelope/'&gt;  &lt;env:Header/&gt;  &lt;env:Body&gt;  &lt;ns1:ItemLookup xmlns='http://webservices.amazon.com/AWSECommerceService/2011-08-01' xmlns:ns1='http://webservices.amazon.com/AWSECommerceService/2011-08-01'&gt;  &lt;AWSAccessKeyId&gt;xxxxxxxxxxxxxxxxxxx&lt;/AWSAccessKeyId&gt;  &lt;AssociateTag&gt;xxxxxxxx&lt;/AssociateTag&gt;  &lt;Request&gt;  &lt;IdType&gt;ASIN&lt;/IdType&gt;  &lt;MerchantId&gt;All&lt;/MerchantId&gt;  &lt;ItemId&gt;B008M4TB9C&lt;/ItemId&gt;  &lt;ResponseGroup&gt;Variations&lt;/ResponseGroup&gt;  &lt;/Request&gt;  &lt;Signature&gt;xxxxxxxx&lt;/Signature&gt;  &lt;Timestamp&gt;2012-12-13T23:49:27Z&lt;/Timestamp&gt;  &lt;/ns1:ItemLookup&gt;  &lt;/env:Body&gt;&lt;/env:Envelope&gt;</code></pre><p>The response body.</p><p>A price for a single variation can be found under:</p><p><strong>Item->Variations->Item->Offer->OfferListing->Price</strong>.</p><pre><code>&lt;env:Envelope xmlns:env='http://schemas.xmlsoap.org/soap/envelope/'&gt;  &lt;env:Body&gt;  &lt;ItemLookupResponse xmlns='http://webservices.amazon.com/AWSECommerceService/2011-08-01'&gt;  &lt;OperationRequest&gt;  &lt;HTTPHeaders&gt;  &lt;Header Name='UserAgent' Value='JBossRemoting - 2.5.1 (Flounder)'/&gt;  &lt;/HTTPHeaders&gt;  &lt;RequestId&gt;05c3ecdd-60ae-4a87-8bcb-70f80a5f5d5b&lt;/RequestId&gt;  &lt;Arguments&gt;  &lt;Argument Name='Service' Value='AWSECommerceService'/&gt;  &lt;/Arguments&gt;  &lt;RequestProcessingTime&gt;0.1092920000000000&lt;/RequestProcessingTime&gt;  &lt;/OperationRequest&gt;  &lt;Items&gt;  &lt;Request&gt;  &lt;IsValid&gt;True&lt;/IsValid&gt;  &lt;ItemLookupRequest&gt;  &lt;IdType&gt;ASIN&lt;/IdType&gt;  &lt;MerchantId&gt;Deprecated&lt;/MerchantId&gt;  &lt;ItemId&gt;B008M4TB9C&lt;/ItemId&gt;  &lt;ResponseGroup&gt;Variations&lt;/ResponseGroup&gt;  &lt;VariationPage&gt;All&lt;/VariationPage&gt;  &lt;/ItemLookupRequest&gt;  &lt;/Request&gt;  &lt;Item&gt;  &lt;ASIN&gt;B008M4TB9C&lt;/ASIN&gt;  &lt;ParentASIN&gt;B008M4TB9C&lt;/ParentASIN&gt;    &lt;VariationSummary&gt;  &lt;LowestPrice&gt;  &lt;Amount&gt;49500&lt;/Amount&gt;  &lt;CurrencyCode&gt;USD&lt;/CurrencyCode&gt;  &lt;FormattedPrice&gt;$495.00&lt;/FormattedPrice&gt;  &lt;/LowestPrice&gt;  &lt;HighestPrice&gt;  &lt;Amount&gt;49500&lt;/Amount&gt;  &lt;CurrencyCode&gt;USD&lt;/CurrencyCode&gt;  &lt;FormattedPrice&gt;$495.00&lt;/FormattedPrice&gt;  &lt;/HighestPrice&gt;  &lt;/VariationSummary&gt;    &lt;Variations&gt;  ...  &lt;Item&gt;  &lt;ASIN&gt;B007HQYIBW&lt;/ASIN&gt;  &lt;ParentASIN&gt;B008M4TB9C&lt;/ParentASIN&gt;  ...  &lt;ImageSets&gt;  ...  &lt;/ImageSets&gt;  &lt;ItemAttributes&gt;  ....  &lt;/ItemAttributes&gt;  &lt;VariationAttributes&gt;  &lt;VariationAttribute&gt;  &lt;Name&gt;Color&lt;/Name&gt;  &lt;Value&gt;Black&lt;/Value&gt;  &lt;/VariationAttribute&gt;  &lt;VariationAttribute&gt;  &lt;Name&gt;Size&lt;/Name&gt;  &lt;Value&gt;6 B(M) US&lt;/Value&gt;  &lt;/VariationAttribute&gt;  &lt;/VariationAttributes&gt;  &lt;Offers&gt;  &lt;Offer&gt;  &lt;Merchant&gt;  &lt;Name&gt;Amazon.com&lt;/Name&gt;  &lt;/Merchant&gt;  &lt;OfferAttributes&gt;  &lt;Condition&gt;New&lt;/Condition&gt;  &lt;/OfferAttributes&gt;  &lt;OfferListing&gt;  &lt;OfferListingId&gt;xxxxxxxxxx&lt;/OfferListingId&gt;  &lt;Price&gt;  &lt;Amount&gt;49500&lt;/Amount&gt;  &lt;CurrencyCode&gt;USD&lt;/CurrencyCode&gt;  &lt;FormattedPrice&gt;$495.00&lt;/FormattedPrice&gt;  &lt;/Price&gt;  &lt;Availability&gt;Usually ships in 24 hours&lt;/Availability&gt;  &lt;AvailabilityAttributes&gt;  &lt;AvailabilityType&gt;now&lt;/AvailabilityType&gt;  &lt;MinimumHours&gt;0&lt;/MinimumHours&gt;  &lt;MaximumHours&gt;0&lt;/MaximumHours&gt;  &lt;/AvailabilityAttributes&gt;  &lt;IsEligibleForSuperSaverShipping&gt;1&lt;/IsEligibleForSuperSaverShipping&gt;  &lt;/OfferListing&gt;  &lt;/Offer&gt;  &lt;/Offers&gt;  &lt;/Item&gt;  &lt;/Variations&gt;  &lt;/Item&gt;  ...  &lt;/Items&gt;  &lt;/ItemLookupResponse&gt;  &lt;/env:Body&gt;  &lt;/env:Envelope&gt;</code></pre>",767499,767499,2016-08-03T02:26:35.630,2016-08-03T02:26:35.630,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/13871309
65,13895036,2,5949296,2012-12-15T18:41:25.380,2,"<p>This is code from S3SignURL, It uses no outside DLL's just pure core C#</p><p><a href=""https://github.com/DigitalBodyGuard/S3SignURL/"" rel=""nofollow"">https://github.com/DigitalBodyGuard/S3SignURL/</a></p><pre><code>using System;using System.Collections.Generic;using System.Text;namespace s3_polocySigning{  public static class Encode  {  //   static string thanks = ""http://stackoverflow.com/questions/6999648/signing-post-form-in-c-sharp-for-uploading-to-amazon-s3"";  public static string BuildURL(string AccessKey, string SecretKey, DateTime timeToExpire, string BucketName, string FileKey)  {  System.Security.Cryptography.HMAC hmacProvider = System.Security.Cryptography.HMAC.Create();  string returnString = string.Empty;  hmacProvider.Key = System.Text.ASCIIEncoding.ASCII.GetBytes(SecretKey);  string expirationString = ConvertToUnixTimestamp(timeToExpire).ToString();  //System.Uri.UriSchemeHttp &amp;/ System.Web.HttpUtility.UrlEncode  string assembledRequest = ""GET"" + ""\n"" + ""\n"" + ""\n"" + expirationString + ""\n"" + ""/"" + BucketName + ""/"" + UrlEncode(FileKey);  byte[] hashedSignature = hmacProvider.ComputeHash(System.Text.ASCIIEncoding.ASCII.GetBytes(assembledRequest));  returnString = Convert.ToBase64String(hashedSignature);  return ""https://"" + ""s3.amazonaws.com/"" + BucketName + ""/"" + FileKey + ""?AWSAccessKeyId="" + AccessKey + ""&amp;Expires="" + expirationString + ""&amp;Signature="" + UrlEncode(returnString);  }  private static double ConvertToUnixTimestamp(DateTime ExpDate)  {  if (DateTime.MinValue == ExpDate)  return  2133721337;  DateTime origin = new DateTime(1970, 1, 1, 0, 0, 0, 0);  TimeSpan diff = ExpDate - origin;  return Convert.ToDouble(Math.Floor(diff.TotalSeconds));  }  public static string GetSig(string policyStr, string secretKey)  {  policyStr = GetBase64_string(policyStr);  var signature = new System.Security.Cryptography.HMACSHA1(GetBase64(secretKey));  var bytes = GetBase64(policyStr);  var moreBytes = signature.ComputeHash(bytes);  var encodedCanonical = Convert.ToBase64String(moreBytes);  return encodedCanonical;  }  public static string GetBase64_string(string policyStr)  {  policyStr = policyStr.Replace(""/r"", """").Replace(""/n"", """").Replace(System.Environment.NewLine, ""\n"");  return Convert.ToBase64String(Encoding.ASCII.GetBytes(policyStr));  }  public static byte[] GetBase64(string policyStr)  {  return Encoding.ASCII.GetBytes(policyStr);  }  // ThanksTo = ""http://www.west-wind.com/weblog/posts/2009/Feb/05/Html-and-Uri-String-Encoding-without-SystemWeb"";  // avoid useing System.Web.HttpUtility.UrlEncode  /// &lt;summary&gt;  /// UrlEncodes a string without the requirement for System.Web  /// &lt;/summary&gt;  /// &lt;param name=""String""&gt;&lt;/param&gt;  /// &lt;returns&gt;&lt;/returns&gt;  // [Obsolete(""Use System.Uri.EscapeDataString instead"")]  public static string UrlEncode(string text)  {  // Sytem.Uri provides reliable parsing  return System.Uri.EscapeDataString(text);  }  /// &lt;summary&gt;  /// UrlDecodes a string without requiring System.Web  /// &lt;/summary&gt;  /// &lt;param name=""text""&gt;String to decode.&lt;/param&gt;  /// &lt;returns&gt;decoded string&lt;/returns&gt;  public static string UrlDecode(string text)  {  // pre-process for + sign space formatting since System.Uri doesn't handle it  // plus literals are encoded as %2b normally so this should be safe  text = text.Replace(""+"", "" "");  return System.Uri.UnescapeDataString(text);  }  /// &lt;summary&gt;  /// Retrieves a value by key from a UrlEncoded string.  /// &lt;/summary&gt;  /// &lt;param name=""urlEncoded""&gt;UrlEncoded String&lt;/param&gt;  /// &lt;param name=""key""&gt;Key to retrieve value for&lt;/param&gt;  /// &lt;returns&gt;returns the value or """" if the key is not found or the value is blank&lt;/returns&gt;  public static string GetUrlEncodedKey(string urlEncoded, string key)  {  urlEncoded = ""&amp;"" + urlEncoded + ""&amp;"";  int Index = urlEncoded.IndexOf(""&amp;"" + key + ""="", StringComparison.OrdinalIgnoreCase);  if (Index &lt; 0)  return """";  int lnStart = Index + 2 + key.Length;  int Index2 = urlEncoded.IndexOf(""&amp;"", lnStart);  if (Index2 &lt; 0)  return """";  return UrlDecode(urlEncoded.Substring(lnStart, Index2 - lnStart));  }  }}</code></pre>",1906709,,,2012-12-15T18:41:25.380,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/13895036
66,13935773,2,13935772,2012-12-18T15:05:18.773,4,"<p>Note: Some questions at stackoverflow are <a href=""https://stackoverflow.com/questions/2338758/zip-up-all-paperclip-attachments-stored-on-s3"">outdated</a>, some paperclip methods are gone.</p><p>Lets say we got a User and it <code>:has_many =&gt; user_attachments</code></p><pre><code>GC.disable@user = User.find(params[:user_id])zip_filename = ""User attachments - #{@user.id}.zip"" # the file nametmp_filename = ""#{Rails.root}/tmp/#{zip_filename}"" # the pathZip::ZipFile.open(tmp_filename, Zip::ZipFile::CREATE) do |zip|  @user.user_attachments.each { |e|   attachment = Paperclip.io_adapters.for(e.attachment) #has_attached_file :attachment (,...)  zip.add(""#{e.attachment.original_filename}"", attachment.path)  }endsend_data(File.open(tmp_filename, ""rb+"").read, :type =&gt; 'application/zip', :disposition =&gt; 'attachment', :filename =&gt; zip_filename)File.delete tmp_filenameGC.enableGC.start</code></pre><p>The trick is to disable the GC in order to avoid <code>Errno::ENOENT</code> exception. The GC will delete the downloaded attachment from S3 before it gets zipped.</p><p>Sources: <br><a href=""https://github.com/thoughtbot/paperclip/issues/833"" rel=""nofollow noreferrer"">to_file broken in master?</a> <br><a href=""https://github.com/thoughtbot/paperclip/issues/986"" rel=""nofollow noreferrer"">io_adapters.for(object.attachment).path failing randomly</a></p>",808398,-1,2017-05-23T12:15:57.480,2012-12-18T15:05:18.773,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/13935773
67,13992926,2,13860361,2012-12-21T15:36:09.507,1,"<p>The cron log shows that cron is executing the command, but it is having no apparent effect. You know that the ec2-user can run the command from the shell. So first make sure that root can run this command successfully from the shell:</p><pre><code>sudo bash/opt/aws/bin/ec2-create-snapshot --region us-east-1 -K /home/ec2-user/pk.pem -C /home/ec2-user/cert.pem -d ""vol-******** snapshot"" vol-********</code></pre><p>If this doesn't work then the problem isn't with cron, it's with differences between the ec2-user and root accounts. I suppose that could be permissions based, but root is very powerful. It may be due to their different environments.</p><p>When you run the command as ec2-user you may have the AWS_ACCESS_KEY and AWS_SECRET_KEY environment variables set to valid values. In that case the ec2 command line tool may be using them and ignoring the -K and -C options. The credentials specified using -K and -C may not be valid but you wouldn't know that because they are being ignored. The root account has a different environment, presumably without valid values for those variables.</p><p>This might happen because the use of certificates (the -K and -C options) instead of access keys has been deprecated by Amazon. See <a href=""http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/SettingUp_CommandLine.html#set-aws-credentials"" rel=""nofollow"">http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/SettingUp_CommandLine.html#set-aws-credentials</a>.</p><p>You can try to test this theory by seeing if those variables are set when you run the command from the console:</p><pre><code>echo $AWS_ACCESS_KEYecho $AWS_SECRET_KEY</code></pre><p>If they show values, this is probably the reason for the difference.</p><p>Another thing to try is to replace the command with one with valid access keys instead of a certificate (the -O and -W parameters instead of -K and -C). You can create a user via IAM that can only do the operation you want, and use its credentials for the command.</p>",1919054,,,2012-12-21T15:36:09.507,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/13992926
68,14119015,2,12345363,2013-01-02T08:37:18.777,0,"<p>that command is now deprecated and replaced by auto scaling policies and cloudwatch alarms.</p><p>There is some documentation on how to do this here:<a href=""http://docs.amazonwebservices.com/AutoScaling/latest/DeveloperGuide/US_SetUpASLBApp.html"" rel=""nofollow"">http://docs.amazonwebservices.com/AutoScaling/latest/DeveloperGuide/US_SetUpASLBApp.html</a></p>",294465,,,2013-01-02T08:37:18.777,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/14119015
69,14192909,2,14186477,2013-01-07T09:16:43.143,3,"<p>That's an 'interesting' (and worrisome) problem - section <em>DB Engine Version Management</em> within chapter <a href=""http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/RDSFAQ.MySQL.html"" rel=""nofollow"">MySQL Database Engine</a> in the <a href=""http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/RDSFAQ.html"" rel=""nofollow"">Amazon RDS Technical FAQ</a> seems to imply that restoring an unsupported MySQL snapshot might not be possible anymore in fact (even though it is nowhere stated explicitly):</p><blockquote>  <p><strong>Does Amazon RDS provide guidelines for supporting new MySQL version releases and/or deprecating MySQL versions that are currently  supported?</strong></p>   <p>[...]</p>   <p>We intend to support major MySQL version releases, including MySQL  5.1, for 3 years after they are initially supported by Amazon RDS.</p>   <p><strong>We intend to support minor MySQL version releases (e.g. MySQL 5.1.45)  for at least 1 year</strong> after they are initially supported by Amazon RDS.</p>   <p>After a MySQL major or minor version has been Š—“deprecatedŠ—, <strong>we expect  to provide a three month grace period for you to initiate an upgrade  to a supported version prior to an automatic upgrade being applied</strong>  during your scheduled maintenance window.</p>   <p><em>[emphasis mine]</em></p></blockquote><p>According to <a href=""https://forums.aws.amazon.com/thread.jspa?messageID=250048&amp;#250048"" rel=""nofollow"">Impossible to create a RDS instance in EU-west</a>, MySQL 5.1.42 has been deprecated as of May 24, 2011 the latest already, so this <em>three month grace period</em> has long passed.</p><p>Obviously the apparent effect of this deprecation you encountered (i.e. the inability to restore respectively outdated snapshots) will come to a surprise for many, so there might be options to deal with it eventually still, but I'm afraid you'll need to contact AWS for a solution, either directly or via the <a href=""https://forums.aws.amazon.com/forum.jspa?forumID=60&amp;start=0"" rel=""nofollow"">Amazon Relational Database Service Forum</a> - please post your findings as an answer here if possible, insofar I'd expect this problem to show up regularly as time goes by.</p>",45773,,,2013-01-07T09:16:43.143,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/14192909
70,14199262,2,6963740,2013-01-07T15:47:49.850,0,<p>Just create a snapshot of the volume you have your modified files contained and attach it your outdated instance after detaching the outdated volume.</p>,1507259,,,2013-01-07T15:47:49.850,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/14199262
71,14375613,2,14374095,2013-01-17T09:15:45.710,0,"<p>You are using mismatched version of jars. com.google.gwt.user.client.HTTPRequest was available in older version of GWT gwt-servlet.jar and gwt-user.jar . It got deprecated in GWT 1.5 and in latest GWT it is eliminated. You should be using GWT RequestBuilder.</p><pre><code>&lt;inherits name='com.google.gwt.http.HTTP'/&gt;</code></pre><p>In Java code.</p><pre><code>import com.google.gwt.http.client.RequestBuilder</code></pre><p>Reference - <a href=""http://google-web-toolkit.googlecode.com/svn/javadoc/1.5/com/google/gwt/user/client/HTTPRequest.html"" rel=""nofollow"">http://google-web-toolkit.googlecode.com/svn/javadoc/1.5/com/google/gwt/user/client/HTTPRequest.html</a></p><p>The third party jar you are using is very old and not updated for 3 years. </p><p><strong>In your case the <a href=""http://code.google.com/p/gwt-s3/source/browse/trunk/main/src/com/nubotech/gwt/oss/client/s3/MockS3OnlineStorageService.java"" rel=""nofollow"">MockS3OnlineStorageService.java</a> inside the gwt-s3-api-0.9.3.jar is using HTTPRequest class which is no longer supported in GWT 2.4</strong></p>",1155801,1155801,2013-01-17T14:59:20.670,2013-01-17T14:59:20.670,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/14375613
72,14517372,2,14517323,2013-01-25T07:33:47.267,7,"<p>From <a href=""http://dev.mysql.com/doc/refman/5.5/en/server-system-variables.html#sysvar_table_type"" rel=""noreferrer"">the documentation</a>:</p><blockquote>  <p>This variable was removed in MySQL 5.5.3. Use <a href=""http://dev.mysql.com/doc/refman/5.5/en/server-system-variables.html#sysvar_storage_engine"" rel=""noreferrer"">storage_engine</a> instead.</p></blockquote><p>Which in turn says:</p><blockquote>  <p>This variable is deprecated as of MySQL 5.5.3. Use <a href=""http://dev.mysql.com/doc/refman/5.5/en/server-system-variables.html#sysvar_default_storage_engine"" rel=""noreferrer"">default_storage_engine</a>  instead.  </p></blockquote><p>Therefore you should use <code>SET default_storage_engine=InnoDB</code>, which FWIW is the default since MySQL 5.5.5.</p>",44853,,,2013-01-25T07:33:47.267,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/14517372
73,14661530,2,14424116,2013-02-02T11:42:20.807,1,"<p>I am not sure for this, but I seem to have followed the same guide as you, and it must be outdated. The CarrierWave Uploader API seems to have changed. Now, uploaded images are public by default, which you can change via the config.fog_public configuration option. </p><p>Look <a href=""https://github.com/jnicklas/carrierwave#using-amazon-s3"" rel=""nofollow"" title=""CarrierWave Readme"">here</a> and <a href=""https://github.com/jnicklas/carrierwave/blob/master/lib/carrierwave/storage/fog.rb"" rel=""nofollow"" title=""CarierrWave fog reference"">here</a> for more info. </p><p>I ended up with only:</p><pre><code>...:provider  =&gt; 'AWS',:aws_access_key_id   =&gt; ENV['S3_KEY'],:aws_secret_access_key =&gt; ENV['S3_SECRET'],:region  =&gt; ENV['S3_REGION']...</code></pre><p>In my fog initializer. Nothing more was needed. </p>",190833,,,2013-02-02T11:42:20.807,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/14661530
74,14927018,2,14926906,2013-02-17T22:48:52.360,0,"<p>The blog post is a little bit outdated. In order to use the amazon-ec2 gem, you need to <code>require 'AWS'</code>, not <code>require 'EC2'</code>. This means you have to change the second line to:</p><pre><code>%w(optparse rubygems AWS resolv pp).each {|l| require l}</code></pre><p>You will also have to change this line:</p><pre><code>EC2::Base.new(options).describe_instances # etc...</code></pre><p>to</p><pre><code>AWS::EC2::Base.new(options).describe_instances # etc...</code></pre>",192702,,,2013-02-17T22:48:52.360,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/14927018
75,14930504,2,11014584,2013-02-18T06:15:42.870,69,"<p>Thank you Wilman your commands worked correctly, small improvement need to be considered if we are increasing EBSs into larger sizes</p><ol><li>Stop the instance</li><li>Create a snapshot from the volume</li><li>Create a new volume based on the snapshot increasing the size</li><li>Check and remember the current's volume mount point (i.e. <code>/dev/sda1</code>)</li><li>Detach current volume</li><li>Attach the recently created volume to the instance, setting the exact mount point</li><li>Restart the instance</li><li><p>Access via SSH to the instance and run <code>fdisk /dev/xvde</code></p><p><strong>WARNING: DOS-compatible mode is deprecated. It's strongly recommended to  switch off the mode (command 'c') and change display units to  sectors (command 'u')</strong></p></li><li><p>Hit <kbd>p</kbd> to show current partitions</p></li><li>Hit <kbd>d</kbd> to delete current partitions (if there are more than one, you have to delete one at a time) NOTE: Don't worry data is not lost</li><li>Hit <kbd>n</kbd> to create a new partition</li><li>Hit <kbd>p</kbd> to set it as primary</li><li>Hit <kbd>1</kbd> to set the first cylinder</li><li>Set the desired new space (if empty the whole space is reserved)</li><li>Hit <kbd>a</kbd> to make it bootable</li><li>Hit <kbd>1</kbd> and <kbd>w</kbd> to write changes</li><li>Reboot instance OR use <code>partprobe</code> (from the <code>parted</code> package) to tell the kernel about the new partition table</li><li>Log via SSH and run resize2fs /dev/xvde1</li><li>Finally check the new space running df -h</li></ol>",2082182,808723,2017-02-18T16:28:38.837,2017-02-18T16:28:38.837,16,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/14930504
76,15006137,2,15003022,2013-02-21T15:34:59.530,4,"<p>From your question, it is not clear whether you want to:<br>1-avoid sending messages to malformed email addresses; or<br>2-avoid sending messages to email addresses which are not verified under your AWS account.</p><p>The answer for 1 is spread in different forms accross forums, SO, etc. You either do it simple, i.e., craft a short and clear regular expression which validates roughly 80% of the cases, or you use a very complex regular expression (in order to validate against the full compliance -- good luck, check <a href=""http://ex-parrot.com/~pdw/Mail-RFC822-Address.html"" rel=""nofollow"">this example</a>), check whether the domain is not only valid but also up and running, and, last but not least, check if the account is valid under that domain. Up to you. I'd go with a simple regex.</p><p>The answer for 2 is available at <a href=""http://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-email-addresses.html"" rel=""nofollow"">Verifying Email Addresses in Amazon SES</a> -- the Amazon SES API and SDKs support the operations below, so you should be covered in any case:</p><blockquote>  <p>Using the Amazon SES API</p>   <p>You can also manage verified email addresses with the Amazon SES API. The following actions are available:</p>   <p>VerifyEmailIdentity<br>  ListIdentities<br>  DeleteIdentity<br>  GetIdentityVerificationAttributes  </p>   <p>Note<br>  The API actions above are preferable to the following older API actions, which are deprecated as of the May 15, 2012 release of Domain Verification.  </p>   <p>VerifyEmailAddress<br>  ListVerifiedEmailAddresses<br>  DeleteVerifiedEmailAddress  </p>   <p>You can use these API actions to write a customized front-end application for email address verification. For a complete description of the API actions related to email verification, go to the Amazon Simple Email Service API Reference.</p></blockquote>",347777,,,2013-02-21T15:34:59.530,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/15006137
77,15305590,2,15304667,2013-03-09T00:18:10.560,5,"<p>In my opinion, it's an understandable trade-off DynamoDB made.  To be highly available and redundant, they need to replicate data.  To get super-low latency, they allowed inconsistent reads.  I'm not sure of their internal implementation, but I would guess that the higher this 64KB cap is, the longer your inconsistent reads might be out of date with the actual current state of the item.  And in a super low-latency system, milliseconds may matter.</p><p>This pushes the problem of an inconsistent Query returning chunk 1 and 2 (but not 3, yet) to the client-side.</p><p>As per question comments, if you want to store larger data, I recommend storing in S3 and referring to the S3 location from an attribute on an item in DynamoDB.</p>",886749,,,2013-03-09T00:18:10.560,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/15305590
78,15370062,2,15202555,2013-03-12T19:11:35.443,0,"<p>Below is the link you can refer,</p><p><a href=""http://mpouttuclarke.wordpress.com/2011/06/24/how-to-run-an-elastic-mapreduce-job-using-the-java-sdk/"" rel=""nofollow"">http://mpouttuclarke.wordpress.com/2011/06/24/how-to-run-an-elastic-mapreduce-job-using-the-java-sdk/</a></p><p>Note: some of the methods used in the above method are deprecated. Refer to the aws <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/elasticmapreduce/AmazonElasticMapReduceClient.html"" rel=""nofollow"">reference</a> guide for an updated version.</p>",2162531,3285719,2014-12-14T17:36:38.837,2014-12-14T17:36:38.837,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/15370062
79,15535141,2,15535140,2013-03-20T21:32:14.087,64,"<p>Log into your brand new instance:</p><pre><code>[19:59:22] paul:~ $ ssh -i ~/.ssh/server.pem ec2-user@your.ip  __|  __|_  )  _|  (   /   Amazon Linux AMI  ___|\___|___|  https://aws.amazon.com/amazon-linux-ami/2012.09-release-notes/</code></pre><p>and impersonate the root:</p><pre><code>$ sudo su -</code></pre><p>You are logged in as root from here. Apply the server updates:</p><pre><code>[root@ip-xx ~]# yum update...Complete![root@ip-xx ~]# yum groupinstall ""Development Tools""...Install  72 Package(s)...Complete!</code></pre><p>Now, this is where it differs from the standard solution. Install RVM but without a distribution of ruby:</p><pre><code>[root@ip-xx ~]# \curl -L https://get.rvm.io | bash -s stable  % Total  % Received % Xferd  Average Speed   Time  Time   Time  Current  Dload  Upload   Total   Spent  Left  Speed100   184  100   184  0   0  135  0  0:00:01  0:00:01 --:--:--   183100 11861  100 11861  0   0   7180  0  0:00:01  0:00:01 --:--:-- 64113Downloading RVM from wayneeseguin branch stable  % Total  % Received % Xferd  Average Speed   Time  Time   Time  Current  Dload  Upload   Total   Spent  Left  Speed100   124  100   124  0   0  161  0 --:--:-- --:--:-- --:--:--   185100 1615k  100 1615k  0   0   258k  0  0:00:06  0:00:06 --:--:--  401kInstalling RVM to /usr/local/rvm/  Creating group 'rvm'# RVM:  Shell scripts enabling management of multiple ruby environments.# RTFM: https://rvm.io/# HELP: http://webchat.freenode.net/?channels=rvm (#rvm on irc.freenode.net)# Cheatsheet: http://cheat.errtheblog.com/s/rvm/# Screencast: http://screencasts.org/episodes/how-to-use-rvm# In case of any issues read output of 'rvm requirements' and/or 'rvm notes'Installation of RVM in /usr/local/rvm/ is almost complete:  * First you need to add all users that will be using rvm to 'rvm' group,  and logout - login again, anyone using rvm will be operating with `umask u=rwx,g=rwx,o=rx`.  * To start using RVM you need to run `source /etc/profile.d/rvm.sh`  in all your open shell windows, in rare cases you need to reopen all shell windows.# root,##   Thank you for using RVM!#   I sincerely hope that RVM helps to make your life easier and#   more enjoyable!!!## ~Wayne</code></pre><p>Let's check that no ruby version is installed:</p><pre><code>[root@ip-xx ~]# rvm listrvm rubies# No rvm rubies installed yet. Try 'rvm help install'.</code></pre><p>Now, openssl is already installed on the system but is incompatible with setup:</p><pre><code>[root@ip-xx ~]# openssl versionOpenSSL 1.0.0k-fips 5 Feb 2013[root@ip-xx ~]# openssl version -dOPENSSLDIR: ""/etc/pki/tls""</code></pre><p>Attempting to install ruby2 with this version will result in the following error:</p><pre><code>[root@ip-xx ~]# rvm install 2.0.0 -- --with-openssl-dir=/etc/pki/tlsFetching yaml-0.1.4.tar.gz to /usr/local/rvm/archives######################################################################## 100.0%Extracting yaml to /usr/local/rvm/src/yaml-0.1.4Prepare yaml in /usr/local/rvm/src/yaml-0.1.4.Configuring yaml in /usr/local/rvm/src/yaml-0.1.4.Compiling yaml in /usr/local/rvm/src/yaml-0.1.4.Installing yaml to /usr/local/rvm/usrInstalling Ruby from source to: /usr/local/rvm/rubies/ruby-2.0.0-p0, this may take a while depending on your cpu(s)...ruby-2.0.0-p0 - #downloading ruby-2.0.0-p0, this may take a while depending on your connection...######################################################################## 100.0%ruby-2.0.0-p0 - #extracting ruby-2.0.0-p0 to /usr/local/rvm/src/ruby-2.0.0-p0ruby-2.0.0-p0 - #extracted to /usr/local/rvm/src/ruby-2.0.0-p0ruby-2.0.0-p0 - #configuringruby-2.0.0-p0 - #compilingruby-2.0.0-p0 - #installing Retrieving rubygems-2.0.3######################################################################## 100.0%Extracting rubygems-2.0.3 ...Removing old Rubygems files...Installing rubygems-2.0.3 for ruby-2.0.0-p0 ...Error running 'env GEM_PATH=/usr/local/rvm/gems/ruby-2.0.0-p0:/usr/local/rvm/gems/ruby-2.0.0-p0@global:/usr/local/rvm/gems/ruby-2.0.0-p0:/usr/local/rvm/gems/ruby-2.0.0-p0@global GEM_HOME=/usr/local/rvm/gems/ruby-2.0.0-p0 /usr/local/rvm/rubies/ruby-2.0.0-p0/bin/ruby /usr/local/rvm/src/rubygems-2.0.3/setup.rb', please read /usr/local/rvm/log/ruby-2.0.0-p0/rubygems.install.logInstallation of rubygems did not complete successfully.Saving wrappers to '/usr/local/rvm/bin'.ruby-2.0.0-p0 - #adjusting #shebangs for (gem irb erb ri rdoc testrb rake).cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/gem'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/irb'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/erb'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/ri'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/rdoc'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/testrb'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/rake'? yruby-2.0.0-p0 - #importing default gemsets, this may take time ...Install of ruby-2.0.0-p0 - #complete</code></pre><p>Although you'll get ruby2, the ""Installation of rubygems did not complete successfully"" - notice the warning: ""Error running 'env GEM_PATH=...""</p><p>Instead, we'll get RVM install a copy of openssl for us (see <a href=""https://rvm.io/packages/openssl/"" rel=""noreferrer"">https://rvm.io/packages/openssl/</a>). The zlib-devel package is required:</p><pre><code>[root@ip-xx ~]# yum install zlib-devel...Installed:  zlib-devel.x86_64 0:1.2.5-7.11.amzn1  Complete![root@ip-xx ~]# rvm pkg install opensslFetching openssl-1.0.1c.tar.gz to /usr/local/rvm/archivesExtracting openssl to /usr/local/rvm/src/openssl-1.0.1cConfiguring openssl in /usr/local/rvm/src/openssl-1.0.1c.Compiling openssl in /usr/local/rvm/src/openssl-1.0.1c.Installing openssl to /usr/local/rvm/usrPlease note that it's required to reinstall all rubies:  rvm reinstall all --forceUpdating openssl certificates</code></pre><p>We can now install ruby2:</p><pre><code>[root@ip-xx ~]# rvm reinstall 2.0.0 --with-openssl-dir=/usr/local/rvm/usrRemoving /usr/local/rvm/src/ruby-2.0.0-p0...Removing /usr/local/rvm/rubies/ruby-2.0.0-p0...Installing Ruby from source to: /usr/local/rvm/rubies/ruby-2.0.0-p0, this may take a while depending on your cpu(s)...ruby-2.0.0-p0 - #downloading ruby-2.0.0-p0, this may take a while depending on your connection...ruby-2.0.0-p0 - #extracting ruby-2.0.0-p0 to /usr/local/rvm/src/ruby-2.0.0-p0ruby-2.0.0-p0 - #extracted to /usr/local/rvm/src/ruby-2.0.0-p0ruby-2.0.0-p0 - #configuringruby-2.0.0-p0 - #compilingruby-2.0.0-p0 - #installing Removing old Rubygems files...Installing rubygems-2.0.3 for ruby-2.0.0-p0 ...Installation of rubygems completed successfully.Saving wrappers to '/usr/local/rvm/bin'.ruby-2.0.0-p0 - #adjusting #shebangs for (gem irb erb ri rdoc testrb rake).cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/gem'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/irb'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/erb'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/ri'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/rdoc'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/testrb'? ycp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/rake'? yruby-2.0.0-p0 - #importing default gemsets, this may take time ...Install of ruby-2.0.0-p0 - #complete Making gemset ruby-2.0.0-p0 pristine.Making gemset ruby-2.0.0-p0@global pristine.[root@ip-xx ~]# ruby -vruby 2.0.0p0 (2013-02-24 revision 39474) [x86_64-linux]</code></pre><p>And rails4:</p><pre><code>[root@ip-xx ~]# gem install rails -v 4.0.0beta1...28 gems installed[root@ip-xx ~]# gem list*** LOCAL GEMS ***actionmailer (4.0.0.beta1)actionpack (4.0.0.beta1)activemodel (4.0.0.beta1)activerecord (4.0.0.beta1)activerecord-deprecated_finders (0.0.3)activesupport (4.0.0.beta1)arel (4.0.0.beta2)atomic (1.0.1)bigdecimal (1.2.0)builder (3.1.4)bundler (1.3.4)erubis (2.7.0)hike (1.2.1)i18n (0.6.4)io-console (0.4.2)json (1.7.7)mail (2.5.3)mime-types (1.21)minitest (4.3.2)multi_json (1.7.1)polyglot (0.3.3)psych (2.0.0)rack (1.5.2)rack-test (0.6.2)rails (4.0.0.beta1)railties (4.0.0.beta1)rake (10.0.3, 0.9.6)rdoc (4.0.0, 3.12.2)rubygems-bundler (1.1.1)rvm (1.11.3.6)sprockets (2.9.0)sprockets-rails (2.0.0.rc3)test-unit (2.0.0.0)thor (0.17.0)thread_safe (0.1.0)tilt (1.3.6)treetop (1.4.12)tzinfo (0.3.37)</code></pre><p>That's all folks!</p>",230606,,,2013-03-20T21:32:14.087,8,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/15535141
80,15563793,2,12449626,2013-03-22T06:07:37.363,79,"<p>This kind of situation is normal, it's not weird.<br>The message you got:  </p><blockquote>  <p>debug3: Incorrect RSA1 identifier<br>  debug3: Could not load ""/home/user_name/.ssh/id_rsa"" as a RSA1 public key  </p></blockquote><p>does not indicate an error, indeed.<br>RSA1 public key is only used in SSH protocol  1, which is already out of date. Nowadays, SSH protocol  2 is mostly used.  </p><p>During a normal SSH login process, you will most probably see that warning message with <code>ssh -vvv</code>.<br>You will probably feel surprised, but don't worry, it's normal.  </p><p>reference:<br><a href=""https://bbs.archlinux.org/viewtopic.php?id=122646"">https://bbs.archlinux.org/viewtopic.php?id=122646</a>, #9  </p>",1108071,1108071,2013-05-12T15:30:40.770,2013-05-12T15:30:40.770,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/15563793
81,15586927,2,5285085,2013-03-23T12:44:56.287,0,"<p>Some time ago I did my fork especially for S3 in mind. My fork work with official AWS-SDK, instead of old aws-s3 which is mostly outdated.</p><p>If anybody will search for S3 solution for paperclip this is one that work (today)</p><p><a href=""https://github.com/krzak/dm-paperclip-s3"" rel=""nofollow"">https://github.com/krzak/dm-paperclip-s3</a></p><p>take a look at readme to get how to configure paperclip for S3</p>",699944,699944,2013-05-12T11:27:08.930,2013-05-12T11:27:08.930,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/15586927
82,16145598,2,2573919,2013-04-22T11:04:55.100,15,"<p>As many have commented already, the <a href=""https://stackoverflow.com/a/2628984/45773"">initially accepted answer</a> doesn't apply to <a href=""http://aws.amazon.com/cloudfront/"" rel=""nofollow noreferrer"">Amazon CloudFront</a> in fact, insofar <a href=""http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html"" rel=""nofollow noreferrer"">Serving Private Content through CloudFront</a> requires the use of  dedicated <a href=""http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls-overview.html"" rel=""nofollow noreferrer"">CloudFront Signed URLs</a> - accordingly <a href=""https://stackoverflow.com/a/6624431/45773"">secretmike's answer</a> has been correct, but it is meanwhile outdated after he himself took the time and <a href=""https://github.com/boto/boto/pull/357"" rel=""nofollow noreferrer"">Added support for generating signed URLs for CloudFront</a> (thanks much for this!).</p><p><a href=""http://boto.readthedocs.org/en/latest/"" rel=""nofollow noreferrer"">boto</a> now supports a dedicated <a href=""http://boto.readthedocs.org/en/latest/ref/cloudfront.html#boto.cloudfront.distribution.Distribution.create_signed_url"" rel=""nofollow noreferrer"">create_signed_url</a> method and the former binary dependency M2Crypto has recently been replaced with a <a href=""http://stuvel.eu/rsa"" rel=""nofollow noreferrer"">pure-Python RSA implementation</a> as well, see <a href=""https://github.com/boto/boto/pull/1214"" rel=""nofollow noreferrer"">Don't use M2Crypto for cloudfront URL signing</a>.</p><p>As increasingly common, one can find one or more good usage examples within the related unit tests (see <a href=""https://github.com/boto/boto/blob/develop/tests/unit/cloudfront/test_signed_urls.py"" rel=""nofollow noreferrer"">test_signed_urls.py</a>), for example <a href=""https://github.com/boto/boto/blob/417de058ed3636a8088168c28d2672972ace284f/tests/unit/cloudfront/test_signed_urls.py#L343"" rel=""nofollow noreferrer"">test_canned_policy(self)</a> - see <a href=""https://github.com/boto/boto/blob/417de058ed3636a8088168c28d2672972ace284f/tests/unit/cloudfront/test_signed_urls.py#L16"" rel=""nofollow noreferrer"">setUp(self)</a> for the referenced variables <code>self.pk_id</code>and <code>self.pk_str</code> (obviously you'll need your own keys):</p><pre><code>def test_canned_policy(self):  """"""  Generate signed url from the Example Canned Policy in Amazon's  documentation.  """"""  url = ""http://d604721fxaaqy9.cloudfront.net/horizon.jpg?large=yes&amp;license=yes""  expire_time = 1258237200  expected_url = ""http://example.com/"" # replaced for brevity  signed_url = self.dist.create_signed_url(  url, self.pk_id, expire_time, private_key_string=self.pk_str)  # self.assertEqual(expected_url, signed_url)</code></pre>",45773,-1,2017-05-23T12:10:31.817,2014-07-21T07:22:32.513,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/16145598
83,16226191,2,534774,2013-04-25T23:06:38.133,9,"<p>As of Mar 8, 2012, Amazon EC2 supports 64-bit AMIs across all instance types.</p><p>This makes the previous answers in this listing outdated as they assume different instance types require different architecture choices.</p><p>I recommend always using 64-bit AMIs so that you have the most flexibility in changing the instance type of an instance and your custom AMIs created from the instance will work on any other instance types.</p><p>I've written more about this here: <a href=""http://alestic.com/2012/03/ec2-64-bit"" rel=""noreferrer"">http://alestic.com/2012/03/ec2-64-bit</a></p><p>There are some good discussion points in the reader comments on that article for specialized exceptions where 32-bit might perform better than 64-bit, but remember that this restricts what instance types you can run on.</p>",111286,,,2013-04-25T23:06:38.133,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/16226191
84,16285688,2,16284028,2013-04-29T18:29:07.647,1,"<p>Unfortunately the dedicated page <a href=""http://wiki.apache.org/hadoop/AmazonEC2"" rel=""nofollow"">Running Hadoop on Amazon EC2</a> (which doesn't facilitate <code>HADOOP_HOME</code> indeed) turns out to be fairly out of date in itself and doesn't seem to apply to the most recent stable version anymore (1.0.4 at the time of this writing). I'm not aware of an updated 'native' tutorial, but apparently users are quite happy with an approach via <a href=""http://whirr.apache.org/"" rel=""nofollow"">Apache Whirr</a> (which incidentally <em>started out in 2007 as some bash scripts in <a href=""https://issues.apache.org/jira/browse/HADOOP-884"" rel=""nofollow"">Apache Hadoop</a> for running Hadoop clusters on EC2</em>).</p><p>Accordingly there is a <a href=""http://whirr.apache.org/docs/0.8.1/quick-start-guide.html"" rel=""nofollow"">Getting Started with WhirrŠÜ¢</a> available, in addition there are also related 3rd party tutorials, e.g.:</p><ul><li><a href=""http://chimpler.wordpress.com/2013/01/20/deploying-hadoop-on-ec2-with-whirr/"" rel=""nofollow"">Deploying Hadoop on EC2 with Whirr</a></li><li><a href=""http://blog.cloudera.com/blog/2012/10/set-up-a-hadoophbase-cluster-on-ec2-in-about-an-hour/"" rel=""nofollow"">How-to: Set Up an Apache Hadoop/Apache HBase Cluster on EC2 in (About) an Hour</a></li></ul><p>I hope you'll be able to merge the information in the book about using <a href=""http://hadoop.apache.org/"" rel=""nofollow"">Apache Hadoop</a> with these about running a Hadoop cluster via Apache Whirr - good luck!</p>",45773,,,2013-04-29T18:29:07.647,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/16285688
85,16531304,2,16531252,2013-05-13T21:17:58.747,11,"<p><a href=""http://php.net/manual/en/function.mysql-connect.php"">The documentation for PHP says</a> </p><blockquote>  <p>This extension is deprecated as of PHP 5.5.0, and will be removed in the future. Instead, the <a href=""http://www.php.net/manual/en/book.mysqli.php"">MySQLi</a> or <a href=""http://www.php.net/manual/en/ref.pdo-mysql.php"">PDO_MySQL</a> extension should be used.</p></blockquote><pre><code>$mysqli = new mysqli('localhost', 'my_user', 'my_password', 'my_db');/* * This is the ""official"" OO way to do it, * BUT $connect_error was broken until PHP 5.2.9 and 5.3.0. */if ($mysqli-&gt;connect_error) {  die('Connect Error (' . $mysqli-&gt;connect_errno . ') '  . $mysqli-&gt;connect_error);}</code></pre>",227299,227299,2013-05-13T21:34:22.233,2013-05-13T21:34:22.233,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/16531304
86,16595224,2,16593687,2013-05-16T18:52:24.237,2,"<p>This is an interesting problem arising from the bad choice of method naming in AWS SDK. Note that the AWS SDK doesn't support ARC.</p><pre><code>[self.s3 copyObject:copyToNewFolder];</code></pre><p>When ARC sees this method name, it assumes that the method returns a value with outstanding retain count according to Obj-C naming conventions. <code>Methods in the alloc, copy, init, mutableCopy, and new families are implicitly marked __attribute__((ns_returns_retained))</code> (<a href=""http://clang.llvm.org/docs/AutomaticReferenceCounting.html#retained-return-values"" rel=""nofollow noreferrer"">Source</a>). But, surprise surprise, this <code>copyObject</code> method returns an autoreleased instance (See <a href=""https://github.com/aws/aws-sdk-ios/blob/master/src/Amazon.S3/AmazonS3Client.m#L474"" rel=""nofollow noreferrer"">AmazonS3Client.m:474</a> and <a href=""https://github.com/aws/aws-sdk-ios/blob/master/src/Amazon.S3/AmazonS3Client.m#L578"" rel=""nofollow noreferrer"">AmazonS3Client.m:578</a>) since AWS is using <code>copy</code> in a different context here. The result? An over-released object and the crash.</p><p>The solution is to mark the method explicitly with <code>__attribute__((ns_returns_not_retained))</code> or <code>NS_RETURNS_NON_RETAINED</code>. I see that Amazon has already deprecated this method (perhaps to rectify the problem you're facing), so you can simply use <code>objectCopy</code> method instead.</p><p>Further reading: <a href=""http://blog.mugunthkumar.com/articles/migrating-your-code-to-objective-c-arc/"" rel=""nofollow noreferrer"">1</a>, <a href=""https://stackoverflow.com/questions/11095855/how-arc-works-when-using-third-library-code"">2</a></p>",440060,-1,2017-05-23T12:29:22.727,2013-05-16T18:52:24.237,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/16595224
87,16797599,2,16797358,2013-05-28T17:01:50.630,51,"<p>When you say delete from Hadoop, you really mean delete from HDFS.</p><p>To delete something from HDFS do one of the two</p><p>From the command line:</p><ul><li>deprecated way:</li></ul><p><code>hadoop dfs -rmr hdfs://path/to/file</code></p><ul><li>new way (with hadoop 2.4.1) :</li></ul><p><code>hdfs dfs -rm -r hdfs://path/to/file</code></p><p>Or from java:</p><pre><code>FileSystem fs = FileSystem.get(getConf());fs.delete(new Path(""path/to/file""), true); // delete file, true for recursive </code></pre>",2312796,579750,2014-08-01T07:16:17.557,2014-08-01T07:16:17.557,13,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/16797599
88,17008875,2,16874294,2013-06-09T10:52:25.377,5,"<p>First off, you should not (never) use the <a href=""http://php.net/manual/en/book.mysql.php"">mysql_</a>* extension and its function anymore. They are outdated, deprecated and will be removed from php in a near version. Use <a href=""http://php.net/manual/en/book.mysqli.php"">mysqli_</a>, or <a href=""http://php.net/manual/en/book.pdo.php"">PDO</a>. See <a href=""http://www.php.net/manual/en/function.mysql-connect.php"">the huge red warning at the top of the mysql_connect page in the php docs</a> for more info.</p><ul><li>Check that you connect to the right server</li></ul><p>Connect to your webserver in ssh, and use the local mysql client to connect to the database server as root (you will be asked the password for the root user):</p><pre><code>$ mysql -uroot -p -hXXXXXXX.XXXXXXXXXX.eu-west-1.rds.amazonaws.comEnter password:Welcome to the MySQL monitor.  Commands end with ; or \g.Your MySQL connection id is 100530Server version: 5.1.66-0+squeeze1 (Debian)[...]Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt;</code></pre><p>If this work, you should end up at a mysql prompt. If this didn't work, your issue lies in your network configuration, your webserver cannot connect to the mysql server (check ports openings).</p><ul><li><p>Make sure the database in question exists on the server</p><pre><code>mysql&gt; show databases;+--------------------+| Database   |+--------------------+| information_schema || [... db names ...] |+--------------------+X rows in set (0.00 sec)mysql&gt;</code></pre></li></ul><p>If you cannot see your database name in the list, this means the server your see in phpmyadmin is not the correct one; check your phpmyadmin settings.</p><ul><li><p>Check if you can connect with this user using the cli mysql client. Note that I do not specify a database on the connection line.</p><pre><code>// close the root connectionmysql&gt; quitBye// connect using your specific user, replace USERNAME with your db user (and type its password when asked)$ mysql -uUSERNAME -pEnter password:[...]mysql&gt; </code></pre></li></ul><p>If this fails and you do not get to a prompt, this can be either because the permission does not exists, or because this user cannot connect from the host you specified. Look at the error message mysql will display; something like!</p><pre><code>ERROR 1045 (28000): Access denied for user 'USERNAME'@'HOST' (using password: NO)</code></pre><p>What is in the HOST is important and you have to make sure it is allowed in your privileges. If everything seems ok and it is still not working, try editing the privileges for that user and set the HOST to '%' (means anything is allowed). If it works with that, it means the HOST is wrongly configured in mysql's privileges.</p><ul><li><p>Check if the user can see the database in question. When you connect with a specific user, ""show databases"" will only list databases this user has privileges to see.</p><pre><code>mysql &gt; show databases;+--------------------+| Database   |+--------------------+| information_schema || [... db names ...] |+--------------------+X rows in set (0.00 sec)mysql&gt;</code></pre></li></ul><p>If you cannot see your database in the list, it means the privileges for the user are wrong / not set for this database.</p><p>If every steps here works fine, it probably means you have an error in your php code, double check the host, password, user name, ...</p><ul><li><p>If you really can't figure out what parts of the privileges are wrong, try creating a dummy user will all permissions from any host:</p><pre><code>$ mysql -uroot -p -hXXXXXXX.XXXXXXXXXX.eu-west-1.rds.amazonaws.comEnter password:mysql &gt; GRANT ALL ON yourdbname.* to 'testuser'@'%' IDENTIFIED BY 'dummypassword';mysql &gt; quitBye// note that I give the password in the prompt directly, to make sure this isn't an input error just in case$ mysql -utestuser -pdummypasswordmysql &gt;</code></pre></li></ul><p>Once you made this user and can connect to it, try to connect to it using your php code. Once this work, delete the user (using <code>DROP USER 'testuser'</code>), create it again with a more limited HOST privilege and try again, until you replicated the permissions you want. </p><p><strong>Remember to delete that user</strong> and you're done.</p>",105768,,,2013-06-09T10:52:25.377,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/17008875
89,17239899,2,17232232,2013-06-21T16:17:44.867,8,"<p>In both of your methods, you are just building a Request object, and you are never sending the request to AWS.Try the following in your second example:</p><pre><code>// Instantiating rdsClient directly is deprecated, use AmazonRDSClientBuilder.// AmazonRDSClient rdsClient = new AmazonRDSClient(/*add your credentials and the proper constructor overload*/);AmazonRDS rdsClient = AmazonRDSClientBuilder.defaultClient();DescribeDBInstancesRequest request = new DescribeDBInstancesRequest();DescribeDBInstancesResult result = rdsClient.describeDBInstances(request);List&lt;DBInstance&gt; list = result.getDBInstances();System.out.println(""list length = "" + list.size());</code></pre><p>An example for method 1 (for modifying your instance(s)) should be similar.</p>",347777,347777,2017-08-08T16:14:15.337,2017-08-08T16:14:15.337,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/17239899
90,17436026,2,17413598,2013-07-02T21:20:15.337,6,"<p>Most likely you are running into the known vagrant-aws issue <a href=""https://github.com/mitchellh/vagrant-aws/issues/72"" rel=""nofollow"">#72: Failing with EC2 Amazon Linux Images</a>.</p><p><strong>Edit 3 (Feb 2014):</strong> Vagrant 1.4.0 (released Dec 2013) and later versions now support the boolean configuration parameter <code>config.ssh.pty</code>.  Set the parameter to true to force Vagrant to use a PTY for provisioning.  Vagrant creator Mitchell Hashimoto <a href=""https://github.com/mitchellh/vagrant/issues/2731#issuecomment-31312336"" rel=""nofollow"">points out</a> that you must not set <code>config.ssh.pty</code> on the global config, you must set it on the node config directly.</p><p>This new setting should fix the problem, and you shouldn't need the workarounds listed below anymore. (But note that I haven't tested it myself yet.)  See <a href=""https://github.com/mitchellh/vagrant/blob/master/CHANGELOG.md"" rel=""nofollow"">Vagrant's CHANGELOG</a> for details -- unfortunately the <code>config.ssh.pty</code> option is not yet documented under <a href=""http://docs.vagrantup.com/v2/vagrantfile/ssh_settings.html"" rel=""nofollow"">SSH Settings</a> in the Vagrant docs.</p><p><strong>Edit 2:</strong> Bad news.  It looks as if even a <code>boothook</code> will not be ""faster"" to run (to update /etc/sudoers.d/ for <code>!requiretty</code>) than Vagrant is trying to rsync.  During my testing today I started seeing sporadic ""mkdir -p /vagrant"" errors again when running <code>vagrant up --no-provision</code>.  So we're back to the previous point where the most reliable fix seems to be a custom AMI image that already includes the applied patch to <code>/etc/sudoers.d</code>.</p><p><strong>Edit:</strong> Looks like I found a more reliable way to fix the problem.  Use a <a href=""https://cloudinit.readthedocs.org/en/latest/topics/format.html#cloud-boothook"" rel=""nofollow"">boothook</a> to perform the fix.  I manually confirmed that a script passed as a <code>boothook</code> is executed before Vagrant's rsync phase starts.  So far it has been working reliably for me, and I don't need to create a custom AMI image.</p><p>Extra tip: And if you are relying on <code>cloud-config</code>, too, you can create a <a href=""https://cloudinit.readthedocs.org/en/latest/topics/format.html#mime-multi-part-archive"" rel=""nofollow"">Mime Multi Part Archive</a> to combine the <code>boothook</code> and the <code>cloud-config</code>.  You can get the latest version of the <a href=""https://raw.github.com/lovelysystems/cloud-init/master/tools/write-mime-multipart"" rel=""nofollow"">write-mime-multipart</a> helper script from GitHub.</p><p>Usage sketch:</p><pre><code>$ cd /tmp$ wget https://raw.github.com/lovelysystems/cloud-init/master/tools/write-mime-multipart$ chmod +x write-mime-multipart$ cat boothook.sh#!/bin/bashSUDOERS_FILE=/etc/sudoers.d/999-vagrant-cloud-init-requirettyecho ""Defaults:ec2-user !requiretty"" &gt; $SUDOERS_FILEecho ""Defaults:root !requiretty"" &gt;&gt; $SUDOERS_FILEchmod 440 $SUDOERS_FILE$ cat cloud-config#cloud-configpackages:  - puppet  - git  - python-boto$ ./write-mime-multipart boothook.sh cloud-config &gt; combined.txt</code></pre><p>You can then pass the contents of 'combined.txt' to aws.user_data, for instance via:</p><pre><code>aws.user_data = File.read(""/tmp/combined.txt"")</code></pre><p>Sorry for not mentioning this earlier, but I am literally troubleshooting this right now myself. :)</p><h2>Original answer (see above for a better approach)</h2><p><strong>TL;DR</strong>: The most reliable fix is to ""patch"" a stock Amazon Linux AMI image, save it and then use the customized AMI image in your <code>Vagrantfile</code>.  See below for details.</p><h3>Background</h3><p>A potential workaround is described (and linked in the bug report above) at <a href=""https://github.com/mitchellh/vagrant-aws/pull/70/files"" rel=""nofollow"">https://github.com/mitchellh/vagrant-aws/pull/70/files</a>.  In a nutshell, add the following to your <code>Vagrantfile</code>:</p><pre><code>aws.user_data = ""#!/bin/bash\necho 'Defaults:ec2-user !requiretty' &gt; /etc/sudoers.d/999-vagrant-cloud-init-requiretty &amp;&amp; chmod 440 /etc/sudoers.d/999-vagrant-cloud-init-requiretty\nyum install -y puppet\n""</code></pre><p>Most importantly this will configure the OS to not require a tty for user <code>ec2-user</code>, which seems to be the root of the problem.  I /think/ that the additional installation of the <code>puppet</code> package is not required for the actual fix (although Vagrant may use Puppet for provisioning the machine later, depending on how you configured Vagrant).</p><h3>My experience with the described workaround</h3><p>I have tried this workaround but Vagrant still occasionally fails with the same error.  It might be a ""race condition"" where Vagrant happens to run its rsync phase faster than cloud-init (which is what <code>aws.user_data</code> is passing information to) can prepare the workaround for #72 on the machine for Vagrant.  If Vagrant is faster you will see the same error; if cloud-init is faster it works.</p><h3>What will work (but requires more effort on your side)</h3><p>What definitely works is to run the command on a stock Amazon Linux AMI image, and then save the modified image (= create an image snapshot) as a custom AMI image of yours.</p><pre><code># Start an EC2 instance with a stock Amazon Linux AMI image and ssh-connect to it$ sudo su - root$ echo 'Defaults:ec2-user !requiretty' &gt; /etc/sudoers.d/999-vagrant-cloud-init-requiretty$ chmod 440 /etc/sudoers.d/999-vagrant-cloud-init-requiretty# Note: Installing puppet is mentioned in the #72 bug report but I /think/ you do not need it#   to fix the described Vagrant problem.$ yum install -y puppet</code></pre><p>You must then use this custom AMI image in your <code>Vagrantfile</code> instead of the stock Amazon one.  The obvious drawback is that you are not using a stock Amazon AMI image anymore -- whether this is a concern for you or not depends on your requirements.</p><h3>What I tried but didn't work out</h3><p>For the record: I also tried to pass a <code>cloud-config</code> to <code>aws.user_data</code> that included a <a href=""https://cloudinit.readthedocs.org/en/latest/topics/examples.html"" rel=""nofollow"">bootcmd</a> to set <code>!requiretty</code> in the same way as the embedded shell script above.  According to the cloud-init docs <code>bootcmd</code> is run ""very early"" in the startup cycle for an EC2 instance -- the idea being that <code>bootcmd</code> instructions would be run earlier than Vagrant would try to run its rsync phase.  But unfortunately I discovered that the <code>bootcmd</code> feature is not implemented in the outdated <code>cloud-init</code> version of current Amazon's Linux AMIs (e.g. ami-05355a6c has cloud-init 0.5.15-69.amzn1 but <a href=""https://launchpad.net/ubuntu/+source/cloud-init/+changelog"" rel=""nofollow"">bootcmd was only introduced in 0.6.1</a>).</p>",1743580,1743580,2014-02-26T11:05:01.250,2014-02-26T11:05:01.250,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/17436026
91,17487502,2,13859906,2013-07-05T11:10:14.803,2,"<p>As of Riak 1.3, riak-admin reip is deprecated and the use of riak-admin cluster replace is the recomended way of replacing a cluster's name.</p><p>These are the commands I had to issue:</p><pre><code>riak stop # stop the noderiak-admin down riak@127.0.0.1 # take it downsudo rm -rf /var/lib/riak/ring/* # delete the riak ringsudo sed -i ""s/127.0.0.1/`hostname -i`/g"" /etc/riak/vm.args # Change the name in configriak-admin cluster force-replace riak@127.0.0.1 riak@""`hostname -i`"" # replace the nameriak start # start the node</code></pre><p>That should set the node's name to riak@[your EC2 internal IP address].</p>",618756,,,2013-07-05T11:10:14.803,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/17487502
92,17657951,2,17537852,2013-07-15T15:27:50.363,1,"<p>If Amazon changes the queue URL in a breaking way it will not be immediate and will be deprecated slowly, and will take effect moving up a version (i.e. when you upgrade your SDK). </p><p>While the documentation doesn't guarantee it, Amazon knows that it would be a massively breaking change for thousands of customers.   </p><p>Furthermore, lots of customers use hard coded queue URLs which they get from the console, so those customers would not get the updated queue URL format either.   </p><p>In the end, you will be safe either way.  If you have LOTs of queues, then you will be better off formatting them yourself.  If you have a small number of queues, then it shouldn't make much difference either way.</p>",175308,,,2013-07-15T15:27:50.363,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/17657951
93,17692217,2,17692132,2013-07-17T06:01:49.083,2,"<p>And immediately after posting this I found <a href=""https://bugzilla.novell.com/show_bug.cgi?id=597230"" rel=""nofollow"">this old bug</a> that lead me to the problem</p><p>The problem was from my including an old Mono.Security.dll that was a requirement for Thrift. By deleting that out of date dll I was able run without error</p>",217415,,,2013-07-17T06:01:49.083,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/17692217
94,17698279,2,17698216,2013-07-17T11:12:53.300,0,"<p><strong>Updated Answer</strong></p><p>You mentioned in a comment to my original answer that you'd tried CarrierWave. But...have you tried out <a href=""https://github.com/dwilkie/carrierwave_direct"" rel=""nofollow"">CarrierWaveDirect</a>? I haven't used it myself, but it appears to perform the same S3 direct-uploading, with the familiar API of CarrierWave.</p><hr><p><strong>Original Deprecated Answer</strong></p><p>You'll probably want to look into either <a href=""https://github.com/thoughtbot/paperclip"" rel=""nofollow"">Paperclip</a> or <a href=""https://github.com/markevans/dragonfly"" rel=""nofollow"">Dragonfly</a>. They each support uploading images to S3 (or other cloud storage provider), and also provide features for manipulating the images. The two solutions handle manipulations in different ways--Paperclip performs thumbnailing at upload-time, whereas Dragonfly performs thumbnailing on-the-fly--but either should meet your needs.</p>",5030,5030,2013-07-17T11:50:08.880,2013-07-17T11:50:08.880,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/17698279
95,17755189,2,17752183,2013-07-19T20:45:21.090,2,"<p>The documentation link you are pointing to is for V1 of the Amazon DynamoDB API, which is deprecated. The V2 version, which introduced local secondary indexes, is contained in the <code>com.amazonaws.services.dynamodbv2</code> package. Here is the V2 <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/model/ResourceInUseException.html"" rel=""nofollow"">documentation</a> for <code>ResourceNotFoundException</code>.</p>",645112,,,2013-07-19T20:45:21.090,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/17755189
96,17849197,2,17705840,2013-07-25T04:46:27.990,0,"<p>If you are using the older version of the api (in the java package <code>com.amazonaws.services.dynamodb.model</code>) then the syntax is a little different, and you have to use <code>withRangeKeyCondition</code>.  Here's the <code>QueryRequest</code> class with that older version, you can see how it's deprecated:<a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodb/model/QueryRequest.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodb/model/QueryRequest.html</a></p><p>If you are using the newer version of the API (which you should, in the java package <code>com.amazonaws.services.dynamodbv2.model</code>, then you will see that the <code>QueryRequest</code> has the <code>withKeyConditions</code> method, as shown here:<a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/model/QueryRequest.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/model/QueryRequest.html</a></p>",886749,,,2013-07-25T04:46:27.990,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/17849197
97,17915920,2,17608326,2013-07-29T04:10:45.097,0,"<p>This is a bug because opswork was using Chef 9 (very outdated Chef).</p><p>Currently they have already upgraded to Chef 11.4, so you can try again, because my script in the question now is working.</p>",696686,,,2013-07-29T04:10:45.097,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/17915920
98,18097147,2,17963734,2013-08-07T07:26:14.617,0,"<p>and it is not only deprecated, it is not supported by Amazon.To comply with Amazon requirements, please read this page:<a href=""https://developer.amazon.com/sdk/fire/specifications.html#AppFeature"" rel=""nofollow"">https://developer.amazon.com/sdk/fire/specifications.html#AppFeature</a></p>",1268593,,,2013-08-07T07:26:14.617,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/18097147
99,18454181,2,18446118,2013-08-26T22:38:06.307,1,"<p>The option <code>-K, --private-key KEY</code> is actually one of the <em>Deprecated Options</em>, see <a href=""http://docs.aws.amazon.com/AWSEC2/latest/CommandLineReference/CLTRG-common-args-api.html"" rel=""nofollow"">Common Options for CLI Tools</a>:</p><blockquote>  <p>For a limited time, you can still use the private key and X.509  certificate instead of your access key ID and secret access key.  However, we recommend that you start using your access key ID (-O,  --aws-access-key) and secret access key (-W, --aws-secret-key) now, as the private key (-K, --private-key) and X.509 certificate (-C, --cert)  won't be supported after the transition period elapses. For more  information, see <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/setting-up-your-tools.html#set-aws-credentials"" rel=""nofollow"">Tell the Tools Who You Are</a>.</p></blockquote><p>I highly recommend to follow the advise and only use <em>your access key ID (-O, --aws-access-key) and secret access key (-W, --aws-secret-key)</em> going forward - after all, you have them available already ;)</p><p>Accordingly, you might be able to use <code>-O</code> and <code>-W</code> explicitly, but the error actually indicates that you are simply using an outdated version of the <a href=""http://aws.amazon.com/developertools/351"" rel=""nofollow"">Amazon EC2 API tools</a> and might just need to update those in order to get them to automatically pick up the environment variables <code>AWS_ACCESS_KEY</code> and <code>AWS_SECRET_KEY</code> as explained and advertized in <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/SettingUp_CommandLine.html#set_aws_credentials_linux"" rel=""nofollow"">Tell the CLI Tools Who You Are</a>.</p><hr><h2>Alternative</h2><p>AWS is currently establishing the <a href=""http://aws.amazon.com/cli/"" rel=""nofollow"">AWS Command Line Interface</a> as <em>a unified tool to manage your AWS services</em>, which is presumably going to replace those various <em>Command Line Tools</em> per service still available on the <a href=""http://aws.amazon.com/tools/"" rel=""nofollow"">Tools for Amazon Web Services</a> hub. While it is still classified as a <em>Developer Preview</em>, it works quite well already and makes the entire AWS usage more streamlined and versatile, and also more discoverable, because it is supporting <a href=""http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-command-completion"" rel=""nofollow"">Command Completion</a> for example.</p>",45773,45773,2013-08-26T22:45:13.727,2013-08-26T22:45:13.727,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/18454181
100,18870334,2,18762983,2013-09-18T10:55:24.017,2,"<p>Posting this so it might be helpful for others. The problem was happening because when I installed Ubuntu I had installed the ec2-tools using the apt-get from terminal.</p><p>This version of ec2 which Ubuntu has is an outdated version (it was last updated in 2011).When I found this out, I removed it. And reconfigured the path to the current version of ec2 cli tools I had downloaded and it worked!!! :)</p><p>The way to install newer versions of the ec2-api-tools, as suggested by <a href=""https://help.ubuntu.com/community/EC2StartersGuide"" rel=""nofollow"">https://help.ubuntu.com/community/EC2StartersGuide</a>, is to simply add the aws-tools PPA:</p><pre><code>sudo apt-add-repository ppa:awstools-dev/awstoolssudo apt-get upgrade</code></pre><p>and then a simple <code>apt-get install ec2-api-tools</code> will install the correct version. :)</p>",2463341,1463507,2015-01-29T16:08:37.833,2015-01-29T16:08:37.833,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/18870334
101,19116121,2,10676753,2013-10-01T12:25:11.817,86,"<p>In your <code>/etc/my.cnf</code> file:</p><pre><code>performance_schema = 0</code></pre><p>And restart MySQL (<code>sudo /etc/init.d/mysql restart</code>). This should chop memory usage dramatically if you previously had it on.</p><hr><p><strong>Edit:</strong> For MySQL versions between 5.7.8 and 8.0.1 (<a href=""https://dev.mysql.com/doc/refman/8.0/en/added-deprecated-removed.html"" rel=""nofollow noreferrer"">not required from 8.0.1 onwards</a>), the above is not enough to free your memory of performance schema data:</p><blockquote>  <p>As of MySQL 5.7.8, even when the Performance Schema is disabled, it continues to populate the global_variables, session_variables, global_status, and session_status tables.</p></blockquote><p>(<a href=""http://dev.mysql.com/doc/refman/5.7/en/performance-schema-system-variables.html"" rel=""nofollow noreferrer"">source</a>)</p><p>To prevent this behaviour, set <code>show_compatibility_56</code> to 1 in addition to <code>performance_schema</code>. That is to say, your my.cnf changes should look like:</p><pre><code>performance_schema = 0show_compatibility_56 = 1</code></pre>",1329367,3089595,2020-05-01T16:05:14.257,2020-05-01T16:05:14.257,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/19116121
102,19266376,2,19264018,2013-10-09T08:07:42.550,0,"<p>The WebSocket implementation you are using seems to implement the deprecated Hixie-76 version of the WebSocket protocol, which is no longer supported by Firefox (and Chrome and others).</p><p>I suggest having a look at <a href=""http://socketo.me/"" rel=""nofollow"">Ratchet</a>. This is up to date and well tested.</p>",884770,,,2013-10-09T08:07:42.550,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/19266376
103,19393344,2,19165680,2013-10-16T00:26:31.640,1,"<p>Use the CloudWatch alarms - as-create-or-update-trigger has been deprecated and only there for backwards compatibility.</p><p>From<a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-trigger.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-trigger.html</a></p><blockquote>  <p><strong>Important</strong> <br/>  Triggers are a deprecated feature of Auto Scaling. We  recommend that you switch from using triggers to using Auto Scaling  policies and alarms. For more information, see Configuring Auto  Scaling in the Auto Scaling Developer Guide.</p></blockquote>",735268,,,2013-10-16T00:26:31.640,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/19393344
104,19569598,2,19568975,2013-10-24T15:03:29.730,0,"<p>Maybe you should try Web::Scraper (<a href=""http://metacpan.org/pod/Web::Scraper"" rel=""nofollow"">http://metacpan.org/pod/Web::Scraper</a>).It will get the job done in a much cleaner way.</p><p>[EDIT] Anyway, I checked the HTML code of a random review and it appears that your pattern is outdated. The reviewer's name, for instance, is introduced by 'By' and not by 'Reviewer'. </p>",1069567,11827,2014-04-05T13:41:47.623,2014-04-05T13:41:47.623,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/19569598
105,19601112,2,18263589,2013-10-25T23:46:54.903,1,"<p>The problem was that I was using a forked/obsolete version of django storages which did not properly convert content-type headers to strings from unicode before sending them to boto, which converts unicode strings to ascii strings (as required for HTTP headers) by using urllib's <code>quoteplus</code> escape mechanism. The problem was fixed by switching to the current version of django-storages. </p><p>For a more detailed analysis of the issue see: <a href=""https://github.com/boto/boto/issues/1669#issuecomment-27132112"" rel=""nofollow"">https://github.com/boto/boto/issues/1669#issuecomment-27132112</a></p>",652693,652693,2013-10-26T20:23:02.583,2013-10-26T20:23:02.583,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/19601112
106,19691660,2,4488793,2013-10-30T19:04:08.483,1,"<p>You can follow the official documentation of setting up Amazon ec2 instance: <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-instance_linux.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-instance_linux.html</a></p><p>You should start with an AMI that you are familiar with. For example, if you use Ubuntu, you can just use one of the Ubuntu AMI in the recommended page. I didn't use the BitNami server and my Django site is deployed smoothly. </p><p>If you are using Apache server, just follow the instructions on the official Django doc:<a href=""https://docs.djangoproject.com/en/1.5/howto/deployment/wsgi/modwsgi/"" rel=""nofollow"">https://docs.djangoproject.com/en/1.5/howto/deployment/wsgi/modwsgi/</a></p><p>I tried quite a few blogs but as you said, they are outdated. Just use the official docs and it will save you a lot of time. </p>",2905013,,,2013-10-30T19:04:08.483,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/19691660
107,19859648,2,19855409,2013-11-08T12:55:10.050,0,"<p>You need to use HTTPS, not HTTP.</p><blockquote>  <p>Note that SOAP requests, both authenticated and anonymous, must be sent to Amazon S3 using SSL. Amazon S3 returns an error when you send a SOAP request over HTTP.</p></blockquote><p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/APISoap.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonS3/latest/API/APISoap.html</a></p><p>Note also that the SOAP interface is deprecated.</p>",1695906,,,2013-11-08T12:55:10.050,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/19859648
108,19865359,2,11066598,2013-11-08T17:45:07.987,1,"<p>From personal expirence I can say that most documentation on elastic beanstalk customization is outdated. What about using of custom ami, you need to know the actual version of the elastic beanstalk anyway installs beanstalk scripts and performs configuration on new instance bootstrap, so if you use custom ami it also happens. Base on this I would recommend to use custom ami when you need to have some OS level customizations. </p><p>If you need to install some additional software or change something I would recommend to use approach described here: <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html"" rel=""nofollow"">http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p><p>Good example described here: <a href=""http://www.hudku.com/blog/innocuous-looking-evil-devil/"" rel=""nofollow"">http://www.hudku.com/blog/innocuous-looking-evil-devil/</a></p><p>Also you can find a lot of examples on github if you try to find keyword: .ebextentions</p>",179111,,,2013-11-08T17:45:07.987,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/19865359
109,20170105,2,20169816,2013-11-24T01:40:44.487,17,"<p>See next answer; this is <strong>out of date</strong>.</p><hr><p>Support for</p><pre><code>DROP TABLE IF EXISTS tablename;</code></pre><p>was added in PostgreSQL 8.2. Redshift is a very heavily modified fork of 8.1 by ParAccel, and as far as I know they've backported very few changes from newer versions. It's very unlikely that it supports <code>IF EXISTS</code>; you probably need to do a catalog query to determine if the table exists by looking up <code>information_schema</code>, then deciding whether you'll create it based on the result.</p>",398670,398670,2015-04-30T04:45:27.737,2015-04-30T04:45:27.737,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/20170105
110,20599958,2,13200258,2013-12-15T21:20:49.440,0,"<p>Could be because you have exceeded the maximum allowed number of versions (500).</p><p>Command to check the number of versions deployed:</p><pre><code>elastic-beanstalk-describe-application-versions |grep '| git-'|wc -l</code></pre><p>If the number of versions is exceeded, you can either ask Amazon Web Services team to increase limits for your application, or just delete obsolete versions.</p><p>Hope it helps.</p>",3079042,,,2013-12-15T21:20:49.440,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/20599958
111,20694541,2,20693898,2013-12-20T00:54:35.157,2,"<p>It appears there is also a cmdlet called <code>Read-S3Object</code> that ends up with the same result. Had to use that.</p><p>Didn't see anything about <code>Copy-S3object</code> being deprecated or having its functionality changed, so that's unfortunate.</p><p>Assuming you have:</p><ul><li>Powershell V3</li><li>Amazon Tools for Powershell v2.x</li><li>Appropriate Amazon Credentials</li></ul><p>Then the following script should work:</p><pre><code>Import-Module ""C:\Program Files (x86)\AWS Tools\PowerShell\AWSPowerShell\AWSPowerShell.psd1""### SET ONLY THE VARIABLES BELOW ###$accessKey = """"  # Amazon access key.  $secretKey = """"  # Amazon secret key.$fileContainingAmazonKeysSeparatedByNewLine = """" # Full path to a file, e.g. ""C:\users\killeens\desktop\myfile.txt""$existingFolderToPlaceDownloadedFilesIn = """"   # Path to a folder, including a trailing slash, such as ""C:\MyDownloadedFiles\"" NOTE: This folder must already exist.$amazonBucketName = """"   # the name of the Amazon bucket you'll be retrieving the keys for.### SET ONLY THE VARIABLES ABOVE ###$creds = New-AWSCredentials -AccessKey $accessKey -SecretKey $secretKeySet-AWSCredentials -Credentials $creds$amazonKeysToDownload = Get-Content $fileContainingAmazonKeysSeparatedByNewLine$uniqueAmazonKeys = $amazonKeysToDownload | Sort-Object | Get-Unique$startingpath = $existingFolderToPlaceDownloadedFilesIn$uniqueAmazonKeys | ForEach-Object {  $keyname = $_  $fullpath = $startingpath + $keyname  Read-S3Object -BucketName $amazonBucketName -Key $keyname -File $fullpath  }</code></pre><p>Obviously there would be better ways to produce this (as a function that accepts parameters, in a Powershell v4 workflow with parallel loops and a throttle count, better dealing with credentials, etc.) but this gets it done in its most basic form.</p>",316847,316847,2013-12-20T19:52:02.843,2013-12-20T19:52:02.843,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/20694541
112,21029597,2,21007947,2014-01-09T19:40:13.243,0,"<p>The code above was missing a call similar to </p><pre><code>DescribeJobFlowsResult describeJobFlowsResult =  client.describeJobFlows(describeJobFlowsRequest);</code></pre><p>This got me a solution that works, but unfortunately Amazon deprecated the method but didn't provide an alternative.  I wish I had a non deprecated solution so this is only a partial answer.</p>",2908865,,,2014-01-09T19:40:13.243,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/21029597
113,21087369,2,21083772,2014-01-13T09:10:23.250,5,"<p>The documentation quote you provide comes from the SQS tutorial.  The <a href=""http://docs.pythonboto.org/en/latest/ref/sqs.html#boto.sqs.queue.Queue.write_batch"" rel=""noreferrer"">SQS API docs</a> correctly describe the current return value.  The SQS tutorial is simply out of date and needs to be corrected.  I have created an <a href=""https://github.com/boto/boto/issues/1985"" rel=""noreferrer"">issue</a> to track this.</p><p>If the write fails for any reason, the service will return an HTTP error code which, in turn, will cause boto to raise an SQSError exception.  If no exception is raised, the write was successful.</p>",653643,,,2014-01-13T09:10:23.250,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/21087369
114,21267306,2,18413238,2014-01-21T19:27:26.553,0,"<p>The AWS SDK was out of date.Updating it solved the hole trouble :)Thanks Deefour</p>",800389,,,2014-01-21T19:27:26.553,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/21267306
115,21285751,2,21283214,2014-01-22T14:33:17.343,1,"<p>The doc strings are out of date in boto.  I have submitted a pull request (<a href=""https://github.com/boto/boto/pull/2017"" rel=""nofollow"">https://github.com/boto/boto/pull/2017</a>) to fix that.  However, you can still specify a value of <code>application</code> for the <code>protocol</code> parameter.  Boto is not trying to validate that parameter value.  Make sure you supply the appropriate value for the <code>endpoint</code> parameter.</p>",653643,,,2014-01-22T14:33:17.343,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/21285751
116,21359305,2,21141496,2014-01-26T03:29:47.913,1,"<p>I think for what you've described, you will probably want to avoid Elastic Bean Stalk, and deploy right onto EC2 instance that you control.</p><p>Front end will run web load, and mostly query from cache.  This can be behind an elastic load balancer, and you can use autoscaling rules to ensure you always have enough resources to handle the load. </p><p>I would probably look at solr for full text search, but I'm not an expert in this - I think solr will have some of the scalibility, replication, etc to make managing your search infrastructure a bit easier to manage. There are some good AWS Solr reference architectures that are designed to scale.</p><p>It sounds like you will need a couple of back end service layers - one to pull in the data, another to normalize it.  If you are going to commit to AWS, you can probably build these so that a central control process doles out work to instances you get via the spot market - that can help to reduce the overall costs.  If the spot market spikes, you can choose to either slow down importing/processing, or use on-demand instances and increase the costs a bit.</p><p>I'd probably design this to use a combination of mysql and a no-sql store.  Mysql for core functionality - accounts, user preferences, etc, but NoSQL for the product information.  You probably want to store that in a format that can be used directly by the UI with minimal processing.  Properly designed, this should allow sharding of the NoSQL store, which will help scalibility, although you'll need a way to reproduce data if a node goes down.  </p><p>To handle the relationship between products and related data (comments, posts, etc) you will need to associate them with whatever key is used to retrieve them from the NoSQL store. If you are going to be dealiing with millions and millions of product records, you will probably want to determine your data retention requirements - do you really need to keep details of a product that has been obsolete and/or unavailable for years?  </p><p>If search is going to be the primary interface to the data, however, you may not need a NoSQL solution - simply pull what you need back from solr.  </p><p>You can put caching in front of most of these layers.</p>",4782,4782,2014-01-26T15:57:59.453,2014-01-26T15:57:59.453,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/21359305
117,21419328,2,19259653,2014-01-28T23:45:49.200,9,"<p><em>Note: This answer refers to <a href=""https://github.com/aws/aws-sdk-ios-v1"" rel=""nofollow"">a now deprecated version of the AWS SDK for iOS</a>.</em></p><hr><p>I ran into the same problem when migrating a project to CocoaPods. Without modifying any of the AWS header files, I was able to avoid by changing my header import lines from:</p><pre><code>#import &lt;AWSS3/AWSS3.h&gt;#import &lt;AWSSNS/AWSSNS.h&gt;</code></pre><p>to:</p><pre><code>#import &lt;AmazonS3Client.h&gt;#import &lt;AmazonSNSClient.h&gt;</code></pre><p>This works because all that <code>AWSS3.h</code> does is <code>#define AWS_MULTI_FRAMEWORK</code> and then <code>#import ""AmazonSNSClient.h""</code>, and AWS_MULTI_FRAMEWORK is responsible for the other header files expecting a different directory structure than what CocoaPods sets up.</p><p>To find out what file names you need to include, just look inside the <code>AWS*.h</code> file you were importing and then import the files named inside directly.</p>",10947,10947,2015-11-02T15:37:13.040,2015-11-02T15:37:13.040,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/21419328
118,21706417,2,21690037,2014-02-11T15:45:39.100,3,"<p>v0.5.1 of dompdf did not yet implement SPL autoloading, it registered <code>__autoload()</code>. This outdated method of autoloading didn't work very well because it was difficult to register more than one autoload function at a time. Plus, when using writing PHP that <em>does</em> use SPL autoloading the v0.5.1 autoloader will not be called. PHP SPL autoloading disables calling <code>__autoload()</code> (<a href=""http://www.php.net/manual/en/function.spl-autoload-register.php"" rel=""nofollow"">spl_autoload_register</a>).</p><p>v0.6.0 (<a href=""https://github.com/dompdf/dompdf/releases"" rel=""nofollow"">just released</a>) uses SPL autoloading and should be compatible with AWS.phar. v0.6.0 is (for the most part) a drop-in replacement for v0.5.1 so you might want to try upgrading your copy of dompdf.</p><p>If that's not possible for whatever reason you could register the dompdf autoloader manually using SPL after including dompdf_config.inc.php. At a minimum you could try the following:</p><pre><code>&lt;?phprequire_once('dompdf/dompdf_config.inc.php');spl_autoload_register('DOMDPF_autoload');// ...?&gt;</code></pre><p>See the <a href=""https://github.com/dompdf/dompdf/blob/v0.6.0/include/autoload.inc.php"" rel=""nofollow"">dompdf v0.6.0 autoload include</a> for a more complete example.</p>",264628,,,2014-02-11T15:45:39.100,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/21706417
119,21715971,2,21714001,2014-02-12T00:09:34.940,0,"<p>The Hadoop implementation of S3 file system is out of date, so writing data to S3 from hive doesn't work well. We fix the issue with reading. Now DSE can read S3 files, but writing has issue. We will check it to see whether we could fix it soon</p>",3268070,,,2014-02-12T00:09:34.940,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/21715971
120,21789195,2,21730183,2014-02-14T20:44:32.017,7,"<p>Per an Amazonian, this is not possible: <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=146102&amp;tstart=0"" rel=""noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=146102&amp;tstart=0</a></p><p>A workaround that worked for my use case, though, was to just specify a <code>RangeKeyCondition</code> greater than the last retrieved object's timestamp.  Here's the idea:</p><pre><code>Condition hashKeyCondition = new Condition();hashKeyCondition.withComparisonOperator(ComparisonOperator.EQ).withAttributeValueList(new AttributeValue().withS(hashKeyAttributeValue));Condition rangeKeyCondition = new Condition();rangeKeyCondition.withComparisonOperator(ComparisonOperator.GT).withAttributeValueList(new AttributeValue().withN(timestamp.toString()));Map&lt;String, Condition&gt; keyConditions = new HashMap&lt;String, Condition&gt;();keyConditions.put(MappedItem.INDEXED_ATTRIBUTE_NAME, hashKeyCondition);keyConditions.put(MappedItem.TIMESTAMP, rangeKeyCondition);QueryRequest queryRequest = new QueryRequest();queryRequest.withTableName(tableName);queryRequest.withIndexName(MappedItem.INDEX_NAME);queryRequest.withKeyConditions(keyConditions);QueryResult result = amazonDynamoDBClient.query(queryRequest);List&lt;MappedItem&gt; mappedItems = new ArrayList&lt;MappedItem&gt;();for(Map&lt;String, AttributeValue&gt; item : result.getItems()) {  MappedItem mappedItem = dynamoDBMapper.marshallIntoObject(MappedItem.class, item);  mappedItems.add(mappedItem);}return mappedItems;</code></pre><p>Note that the <code>marshallIntoObject</code> method is deprecated in favor of a protected method in the <code>DynamoDBMapper</code> class, but it's easy enough to write a marshaller were a future upgrade to break the mapping.</p><p>Not as elegant as using the mapper but it accomplishes the same thing.</p>",1416224,,,2014-02-14T20:44:32.017,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/21789195
121,21935218,2,21933724,2014-02-21T13:01:53.433,1,"<p>I am not 100% sure, but after digging the <a href=""http://docs.aws.amazon.com/AWSAndroidSDK/latest/javadoc/"" rel=""nofollow"">documentation</a>, it seems like the classes that cannot be resolved are deprecated and are no longer included in the jar files...They do still exist in the source code folder, but seeing that since they are deprecared, I guess there is no use trying to follow the tutorial...</p>",2253918,,,2014-02-21T13:01:53.433,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/21935218
122,21964644,2,21873561,2014-02-23T05:35:05.953,2,"<p>The crux to the solution is to not use the (apparently deprecated) <code>bodyParser()</code>. I'm not certain what it does, but it screws up the form's parts that <code>multiparty</code> uses. So instead, if you have the same problem I had, instead of using <code>bodyParser()</code>, use the things you need explicitely (for example):</p><pre><code>app.use(express.urlencoded());app.use(express.json());</code></pre><p>And then for your multipart stuff, just use multiparty to parse the body yourself. The author of multiparty gives <a href=""http://andrewkelley.me/post/do-not-use-bodyparser-with-express-js.html"" rel=""nofollow"">more info</a> on the subject.</p>",971592,,,2014-02-23T05:35:05.953,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/21964644
123,22038122,2,11957811,2014-02-26T10:12:38.657,1,"<p>Note:</p><p>The trust_proxy_headers option is deprecated and will be removed in Symfony 2.3. </p><p>See <a href=""http://symfony.com/doc/2.0/reference/configuration/framework.html#trusted-proxies"" rel=""nofollow"">a trusted_proxies </a> and <a href=""http://symfony.com/doc/2.0/components/http_foundation/trusting_proxies.html"" rel=""nofollow"">a Trusting Proxies</a> for details on how to properly trust proxy data.</p>",3228106,,,2014-02-26T10:12:38.657,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/22038122
124,22640761,2,22614106,2014-03-25T16:30:03.680,0,"<p>I actually had two problems. The first was that my bundler was outdated. I fixed that by running</p><pre><code>gem update bundler</code></pre><p>and the second problem was that I needed to bundle install as the root user since bundler was trying to install gems in a root owned location. </p><p>Hope this helps someone in the future.</p>",3224822,,,2014-03-25T16:30:03.680,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/22640761
125,22650476,2,11254908,2014-03-26T02:38:09.320,6,"<p>You can also use the installation script from loggly itself. The setup below follows the instructions for the legacy setup on <a href=""https://www.loggly.com/docs/configure-syslog-script/"" rel=""nofollow"">https://www.loggly.com/docs/configure-syslog-script/</a> with minor changes (no confirmation prompts, sudo command replaced since no tty is available) </p><p>(edit: updated link, seems to be an outdated solution now in loggly docs)</p><p>Place the following script in .ebextensions/loggly.config</p><p>Replace TOKEN and ACCOUNT with your own.</p><pre><code>## Install loggly.com on AWS Elastic Beanstalk# Tested with node.js environment# Save this file as .ebextensions/loggly.config# Deploy per normal scripts or aws.push. To help debug the push, ssh &amp; tail /var/log/cfn-init.log# See Also /var/log/eb-tools.log#commands:  01_loggly_dl:  command: wget -q -O /tmp/loggly.py https://www.loggly.com/install/configure-syslog.py  02_loggly_config:  command: su --session-command=""python /tmp/loggly.py setup --auth TOKEN --account ACCOUNT --yes""</code></pre>",3140406,3140406,2015-06-12T23:35:00.183,2015-06-12T23:35:00.183,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/22650476
126,22665386,2,22653307,2014-03-26T15:16:19.973,13,"<p>You are correct indeed.  Windows AMI are deprecated when a new version is released (see <a href=""http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/Basics_WinAMI.html"" rel=""noreferrer"">http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/Basics_WinAMI.html</a>) </p><p>There is no ""point and click"" solution as of today, documentation says : ""AWS updates the AWS Windows AMIs several times a year. Updating involves deprecating the previous AMI and replacing it with a new AMI and AMI ID. To find an AMI after it's been updated, use the name instead of the ID. The basic structure of the AMI name is usually the same, with a new date added to the end. You can use a query or script to search for an AMI by name, confirm that you've found the correct AMI, and then launch your instance.""</p><p>One possible solution might be to develop a CloudFormation Custom Resource that would check for AMI availability before launching an EC2 instance.</p><p>See this documentation about CFN Custom Resources : <a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/crpg-walkthrough.html"" rel=""noreferrer"">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/crpg-walkthrough.html</a></p><p>And this talk from re:Invent : <a href=""https://www.youtube.com/watch?v=ZhGMaw67Yu0#t=945"" rel=""noreferrer"">https://www.youtube.com/watch?v=ZhGMaw67Yu0#t=945</a> (and <a href=""https://github.com/awslabs/aws-cfn-custom-resource-examples/tree/master/examples/ami-lookup"" rel=""noreferrer"">this sample code</a> for AMI lookup)</p><p>You also have the option to create your own custom AMI based on an Amazon provided one.Even if you do not modify anything.  Your custom AMI will be an exact copy of the one provided by Amazon but will stay available after Amazon AMI's deprecation.</p><p>Netflix has open sourced tools to help to manage AMIs, have a look at <a href=""https://github.com/Netflix/aminator"" rel=""noreferrer"">Aminator</a></p><p>Linux AMI are deprecated years after release (2003.11 is still available today !) but Windows AMI are deprecated as soon as a patched version is available.  This is for security reason.</p>",663360,663360,2014-03-30T18:04:03.843,2014-03-30T18:04:03.843,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/22665386
127,22667220,2,22666177,2014-03-26T16:23:56.330,0,"<ol><li><p>You really, really should not use mysql_real_escape_string() or mysql_query() or any of the mysql functions. They are deprecated, insecure, and will be removed from PHP in the future, breaking your entire app. Please save yourself and your users a lot of headache by using <a href=""http://us2.php.net/mysqli"" rel=""nofollow"">mysqli</a> or <a href=""http://www.php.net/manual/en/book.pdo.php"" rel=""nofollow"">PDO</a> instead.</p></li><li><p>Use a foreach loop to iterate over the array. </p><pre><code>foreach ($instanceIds as $id){  $sql = ""INSERT INTO server_tbl (userName, serverName, serverId, isRunning) VALUES ('$name', 'runtest', '$id', 'X')"";  //do your insert here with the above SQL statement, depending on whether you use mysqli or PDO</code></pre></li></ol>",3169502,,,2014-03-26T16:23:56.330,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/22667220
128,22690887,2,22690644,2014-03-27T14:38:20.140,5,"<p>Environment variables are not passed to <code>sudo</code>: <a href=""https://stackoverflow.com/questions/8633461/how-to-keep-environment-variables-when-using-sudo"">How to keep Environment Variables when Using SUDO</a></p><blockquote>  <p>the trick is to add enviroment variables to sudoers config:</p><pre><code>sudo visudo</code></pre>   <p>add these lines</p><pre><code>Defaults env_keep +=""http_proxy""Defaults env_keep +=""https_proxy""</code></pre>   <p>form ArchLinux wiki  <a href=""https://wiki.archlinux.org/index.php/Sudo#Environment_variables_.28Outdated.3F.29"" rel=""nofollow noreferrer"">https://wiki.archlinux.org/index.php/Sudo#Environment_variables_.28Outdated.3F.29</a></p></blockquote>",1120015,-1,2017-05-23T12:32:53.290,2014-03-27T14:38:20.140,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/22690887
129,22902003,2,22899486,2014-04-07T00:58:39.563,0,"<p>I looks like your <code>update</code> endpoints might be out of date for the packages that you are installing when you are bootstrapping the instance. I would check that your <code>[tomcat]</code> recipe has the following command (as sudo):</p><pre><code>apt-get update</code></pre><p>I would also make sure that your ubuntu user in your AMI has sudo passwordless access for installation (this is the default for Amazon ubuntu AMIs)</p>",2989261,,,2014-04-07T00:58:39.563,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/22902003
130,22902499,2,22900102,2014-04-07T02:08:04.157,-1,"<p>FTP is an obsolete and insecure protocol, you should not use it. Servers should only run SSH, and you can use SFTP or SCP to transfer files.</p><p>For S3, there are many tools (<a href=""http://s3tools.org/s3cmd"" rel=""nofollow"">Python</a>, <a href=""http://s3sync.net/wiki.html"" rel=""nofollow"">Ruby</a>, etc) that work well.  You could also use <code>fuse</code> to ""mount"" S3 like a filesystem, but I don't recommend these because when they have problems, they are hard to troubleshoot.</p><p>For GDrive, there are several (I tried out one a while back and it worked, but I don't remember which one.)</p>",1832301,,,2014-04-07T02:08:04.157,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/22902499
131,23012207,2,22802547,2014-04-11T12:11:09.397,1,"<p>So Nate's script was (among others) reducing the refresh interval. Let me add some other findings as well:</p><p>The refresh rate was stressing the cluster however I continued searching and found more ""errors"". One gotcha was that I have a deprecated <a href=""https://github.com/elasticsearch/elasticsearch-cloud-aws#s3-gateway"" rel=""nofollow noreferrer"">S3.Gateway</a>. S3 is persistent but slower than the EC2 volume.</p><p>Not only did I have S3 as data storage but on a different region (ec2 virginia -> s3 oregon). So sending documents over the network. I got down to that because some old tutorials have S3 as cloud data storage option.</p><p>After solving that, the ""Documents deleted"" below was better. When I was using S3 it was like 30%. This is from Elasticsearch HQ plugin.</p><p><img src=""https://i.stack.imgur.com/5RaYx.png"" alt=""FS Ops""></p><p>Since now we have optimized I/O. Let's see what else we can do.</p><p>I found out that CPU is an issue. Although big desk says that the workload was minimal, <code>t1.micros</code> are <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts_micro_instances.html"" rel=""nofollow noreferrer"">not to used for persistent CPU usage</a>. That means that although on the charts CPU it is not fully used that's because Amazon throttles it in intervals and in reality they are fully used.</p><p>If you put a big more complex documents it will stress the server.</p><p>Happy dev-oping.</p>",1447885,1218285,2014-04-11T13:55:17.043,2014-04-11T13:55:17.043,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23012207
132,23017359,2,23016500,2014-04-11T15:59:26.683,0,"<p>I was able to solve the issue by opening up the postgresql adapter for Activerecord and commenting out all of the SET commands.</p><p>However, this is not a best practice at all. At the end of editing the postgresql adapter I was faced with errors due to an outdated postgres version (80002).</p><p>The right answer is just updating postgres, which unfortunately isn't possible amongst my shared Redshift DB that lives at Amazon.</p>",2665588,,,2014-04-11T15:59:26.683,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23017359
133,23052163,2,23022442,2014-04-14T04:49:46.400,1,"<p>It looks like it could be a couple of things.  Maybe something you installed with the epel repo requires a specific version of glibc and so it is causing you to not be able to install the latest version.  Or maybe you have the repo metadata out of date.  A couple of things to try.  </p><p>You can clear the repo metadata with:</p><pre><code>sudo yum clean all </code></pre><p>and then</p><pre><code>sudo yum update openssl</code></pre><p>You can also try to disable the epel repo temporarily with </p><pre><code>sudo yum --disablerepo epel update openssl</code></pre><p>If you don't want to update glibc as part of the update you can also just upgrade openssl to the version that is compiled against glibc 2.12 with the following command:</p><pre><code>sudo yum --releasever=2013.09 update openssl</code></pre>",3530566,3530566,2014-04-14T21:21:50.140,2014-04-14T21:21:50.140,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23052163
134,23056579,2,23054668,2014-04-14T09:30:45.377,0,"<p>Strike that, reverse. Nobody should <em>ever</em> still be using the <code>__autoload()</code> function. This was deprecated many years ago in favor of the stackable <code>spl_autoload_register()</code> function.</p><p>From the code snippet you posted, FlourishLib is doing the <em>wrong</em> thing by having a function called <code>__autoload()</code> at all.</p>",228514,,,2014-04-14T09:30:45.377,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23056579
135,23141761,2,11250691,2014-04-17T19:25:20.033,9,"<p>You should have two processes, one that inserts messages into the queue and your worker threads. A typical worker thread will look something like this:</p><pre><code>while(true) {  $res = $client-&gt;receiveMessage(array(  'QueueUrl'  =&gt; $url,  'WaitTimeSeconds'   =&gt; 1  ));  if ($res-&gt;getPath('Messages')) {  foreach ($res-&gt;getPath('Messages') as $msg) {  echo ""Received Msg: "".$msg['Body'];  // Do something useful with $msg['Body'] here  $res = $client-&gt;deleteMessage(array(  'QueueUrl'  =&gt; $url,  'ReceiptHandle' =&gt; $msg['ReceiptHandle']  ));  }  }}</code></pre><p>The WaitTimeSeconds parameters means to do ""long polling"" and has various benefits (see <a href=""http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html"" rel=""noreferrer"">http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html</a>).</p><p>You should use supervise or monit to make sure your worker threads stay running.</p><p>UPDATE: for api v3 use get() instead of getPath() - getPath is now deprecated</p>",650664,1448874,2016-02-28T13:05:13.113,2016-02-28T13:05:13.113,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23141761
136,23289372,2,23282666,2014-04-25T09:36:00.450,1,"<p>There is some outdated blog posts so first time installs inevitably end up getting terminated.. As Jeff mentioned, shiny will be looking for apps in /srv. Try this: <code>sudo cp -R /usr/local/lib/R/site-library/shiny/examples/01_hello /srv/shiny-server/</code> Also, may be useful, for Ubuntu 12.04 did you sort out the sources.list file before installing R? Can copy paste the below in Terminal.</p><pre><code>######################################### UBUNTU CONFIG ######################################## # useful: http://withr.me/blog/2013/07/23/configure-shiny-server-under-ubuntu/## Unlike other Linux, need to follow the following steps to install R on Ubuntu:gpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9gpg -a --export E084DAB9 | sudo apt-key add -## Open sources.list, and add the repository deb http://&lt;myFAVcran&gt;/linux/ubuntu precise/ to its end, deb http://cran.csiro.au/bin/linux/ubuntu precise/## Where precise means the R version for Ubuntu 12.04. ## Note, if you use Š—…nanoŠ—È as the editor, press Ctrl+X to exit nano editor and press Y to save the change, then press Enter.sudo nano /etc/apt/sources.list## Update Ubuntu, and install R.sudo apt-get updatesudo apt-get install r-base r-base-dev</code></pre>",1897926,,,2014-04-25T09:36:00.450,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23289372
137,23349226,2,20664018,2014-04-28T19:04:58.567,32,"<p>I had a similar issue recently which turned out to be due to ssl_ciphers that I was using. </p><p>From <a href=""http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/RequestAndResponseBehaviorCustomOrigin.html"">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/RequestAndResponseBehaviorCustomOrigin.html</a>,  </p><p>""CloudFront forwards HTTPS requests to the origin server using the SSLv3 or TLSv1 protocols and the AES128-SHA1 or RC4-MD5 ciphers. If your origin server does not support either the AES128-SHA1 or RC4-MD5 ciphers, CloudFront cannot establish an SSL connection to your origin.""</p><p>I had to change my nginx confg to add AES128-SHA ( deprecated RC4:HIGH ) to ssl_ciphers to fix the 302 error. I hope this helps. I have pasted the line from my ssl.conf </p><pre><code>ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:RSA+3DES:AES128-SHA:!ADH:!AECDH:!MD5;</code></pre>",665478,665478,2015-05-29T03:46:18.437,2015-05-29T03:46:18.437,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23349226
138,23371813,2,20544518,2014-04-29T17:51:15.933,11,"<p>There is a lack of clear information in this area but we can make some pretty strong inferences.  Many people assume that DynamoDB implements all of the ideas from its predecessor ""Dynamo"", but that doesn't seem to be the case and it is important to keep the two separated in your mind.  The original Dynamo system was carefully described by Amazon in the <a href=""http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html"">Dynamo Paper</a>. In thinking about these, it is also helpful if you are familiar with the distributed databases based on the Dynamo ideas, like Riak and Cassandra.  In particular, <a href=""http://cassandra.apache.org/"">Apache Cassandra</a> which provides a full range of trade-offs with respect to CAP.</p><p>By comparing DynamoDB which is clearly distributed to the options available in Cassandra I think we can see where it is placed in the CAP space.  According to Amazon ""DynamoDB maintains multiple copies of each item to ensure durability. When you receive an 'operation successful' response to your write request, DynamoDB ensures that the write is durable on multiple servers. However, it takes time for the update to propagate to all copies."" (<a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/APISummary.html"">Data Read and Consistency Considerations</a>).  Also, DynamoDB does not require the application to do conflict resolution the way Dynamo does. Assuming they want to provide as much availability as possible, since they say they are writing to multiple servers, writes in DyanmoDB are equivalent to Cassandra <code>QUORUM</code> level.  Also, it would seem DynamoDB does not support <a href=""http://wiki.apache.org/cassandra/HintedHandoff"">hinted handoff</a>, because that can lead to situations requiring conflict resolution.  For maximum availability, an inconsistent read would only have to be at the equivalent of Cassandras's <code>ONE</code> level.  However, to get a consistent read given the quorum writes would require a <code>QUORUM</code> level read (following the R + W > N for consistency). For more information on levels in Cassandra see <a href=""http://www.datastax.com/docs/1.1/dml/data_consistency"">About Data Consistency in Cassandra</a>.</p><p>In summary, I conclude that:</p><ul><li>Writes are ""Quorum"", so a majority of the nodes the row is replicated to must be available for the write to succeed</li><li>Inconsistent Reads are ""One"", so only a single node with the row need be available, but the data returned may be out of date</li><li>Consistent Reads are ""Quorum"", so a majority of the nodes the row is replicated to must be available for the read to succeed</li></ul><p>So writes have the same availability as a consistent read.</p><p>To specifically address your question about two simultaneous conditional writes, one or both will fail depending on how many nodes are down.  However, there will never be an inconsistency.  The availability of the writes really has nothing to do with whether they are conditional or not I think.</p>",268898,,,2014-04-29T17:51:15.933,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23371813
139,23560825,2,20851872,2014-05-09T09:15:26.910,0,"<p>Try setting the content length field and locally declaring the S3 client:</p><pre><code>AmazonS3Client *s3 = [[AmazonS3Client alloc] initWithAccessKey:ACCESS_KEY_ID withSecretKey:SECRET_KEY]; //auto release deprecated, and not necessary with local declarations3.endpoint = [AmazonEndpoints s3Endpoint:US_WEST_2];//post full videopor = [[S3PutObjectRequest alloc] initWithKey:generatedString inBucket:@""dormpicbucket""];por.contentType = @""video/mp4"";por.data = capturedMovie;por.delegate = self;[por setDelegate:self];por.contentLength = [capturedMovie length];[s3 putObject:por];</code></pre><p>Also make sure to debug this: <code>[Constants pictureBucket]</code> because it looks like youre using the sample/tutorial code from aws docs almost explicitly lol.</p>",2584565,,,2014-05-09T09:15:26.910,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23560825
140,23637593,2,23619714,2014-05-13T17:10:39.017,0,"<p>You're running into two problems here: </p><ol><li>Due to <a href=""http://en.wikipedia.org/wiki/Same-origin_policy"" rel=""nofollow"">same origin policy</a>, the <code>&lt;iframe&gt;</code> in the resulting page is not going to display an arbitrary URL.</li><li>Even if it could display an arbitrary URL, that URL would have to be configured for SSL.</li></ol><p>So, you'll be able to load this (another amazon URL using SSL):</p><p><a href=""https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=https%3A%2F%2Fwww.mturk.com"" rel=""nofollow"">https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=https%3A%2F%2Fwww.mturk.com</a></p><p>But not this (amazon URL w/o SSL):</p><p><a href=""https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=http%3A%2F%2Fwww.mturk.com"" rel=""nofollow"">https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=http%3A%2F%2Fwww.mturk.com</a></p><p>And not an arbitrary URL (even with SSL):</p><p><a href=""https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=https%3A%2F%2Fwww.google.com"" rel=""nofollow"">https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=https%3A%2F%2Fwww.google.com</a></p><p>So, my guess is that this template is horribly outdated and used to work at some point in the past but is not compliant with modern web browser technology.</p><p>Best solution is just to provide a link for the worker to click to visit the URL.</p>",2338862,,,2014-05-13T17:10:39.017,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23637593
141,23766244,2,23748065,2014-05-20T17:22:31.933,3,<p>It's a long time since I configured a squid but as far as I remember the directive <code>http_access allow all</code> in line 4 of your configuration makes the 3 lines at the very bottom obsoleteŠ—_ You might try to remove that one.</p>,1741281,,,2014-05-20T17:22:31.933,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23766244
142,23822355,2,23822272,2014-05-23T06:26:24.650,0,"<p>just write the code <code>autorelease</code> has deprecated in <code>iOS</code> 6</p><pre><code>AmazonS3Client *s3 = [[AmazonS3Client alloc] initWithAccessKey:MY_ACCESS_KEY_ID withSecretKey:MY_SECRET_KEY]; </code></pre><p>you were imported   </p><pre><code>#import &lt;AWSS3/AWSS3.h&gt;#import &lt;AWSRuntime/AWSRuntime.h&gt;</code></pre><p>and call the .h file</p><pre><code>@interface ArtistProfileViewController : UIViewController&lt;AmazonServiceRequestDelegate&gt;</code></pre><p><img src=""https://i.stack.imgur.com/HHtDU.png"" alt=""enter image description here""></p>",2783370,2783370,2014-05-23T06:43:12.877,2014-05-23T06:43:12.877,7,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23822355
143,23985147,2,23830383,2014-06-01T22:26:13.303,2,"<p>I believe you need to use <code>verifyEmailIdentity</code> <em>not</em> <code>verifyEmailAddress</code>:</p><pre><code>$result = $sesClient-&gt;verifyEmailIdentity(array('EmailAddress'=&gt; $email));</code></pre><p>As stated in the AWS documentation:</p><blockquote>  <p>The VerifyEmailAddress action is deprecated as of the May  15, 2012 release of Domain Verification. The VerifyEmailIdentity  action is now preferred.</p></blockquote><p>Š—¢ <a href=""http://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.Ses.SesClient.html#_verifyEmailIdentity"" rel=""nofollow""><strong>Further Reading</strong></a></p>",499581,,,2014-06-01T22:26:13.303,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23985147
144,23993751,2,8722565,2014-06-02T11:45:04.380,16,"<p>EDIT: 4 years have past since I posted this answer, and it still seems valid. I hope someone from Amazon RDS documentation team would read it and update their documentation.</p><hr><p>I had a really hard time to figure such a simple thing out, because all online information in this regard seems outdated including one in Amazon Docs. Amazon has obviously changed how you do things since now the default parameters cannot be modified, and you need to create a custom set of parameters in order to modify them, including general_log. It is an obvious bug that you can still click the Edit button for default parameters, but when you try to save them, you get an error that default parameters can't be changed.</p><p><img src=""https://i.stack.imgur.com/4cowe.png"" alt=""enter image description here""></p><p>How you do it now, is that in the Parameters Groups, click on Create DB Parameter Group, and create a new group and select the same DB in 'DB Parameter Group Family' as in the default parameter group. See the attached screen shot. Once done, it'll create a copy identical to the default parameter group. Now edit the parameters, e.g. change general_log to '1'. According to the Docs is should be '0' by default but it is neither '0' nor '1' by default.</p><p>Now save it, go back to your instance, click on 'Instance Actions', select 'Modify' and in the setting which will appear, change 'Parameter Group' to your new custom parameter group. It'll take a few moments to apply it, after which you'll need to restart your DB instance.</p><p>This is how it is till June 2014. But there is no guarantee that it'll stay like this in future too, since in the technology industry things keep getting updated too fast (many times unnecessarily) but documents and tutorials don't get updated as fast.</p>",1198077,1198077,2018-04-14T03:22:10.307,2018-04-14T03:22:10.307,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/23993751
145,24012996,2,11228792,2014-06-03T10:37:32.367,19,"<p>The method <em>withPathStyleAccess</em> has been deprecated. Please use the following instead:</p><pre><code>AmazonS3 s3client = AmazonS3Client.builder()  .withCredentials((new AWSStaticCredentialsProvider(credentials)))  .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(""host"", ""region""))  .withPathStyleAccessEnabled(true)  .build();</code></pre><hr><p>Deprecated method:</p><p>This is now possible, I'm not sure when it was introduced, but it's available in at least the 1.7.8 version of the Java AWS SDK.</p><p>Just call <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3.html#setS3ClientOptions%28com.amazonaws.services.s3.S3ClientOptions%29"" rel=""noreferrer"">setClientOptions</a> on your AmazonS3 instance:</p><pre><code>AmazonS3 client = new AmazonS3Client(credentials);client.setS3ClientOptions(new S3ClientOptions().withPathStyleAccess(true));</code></pre>",590032,922313,2017-05-17T21:16:16.170,2017-05-17T21:16:16.170,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/24012996
146,24030938,2,24014493,2014-06-04T06:55:26.103,40,"<p>i found a way to do that, resize2fs not working in case not sure why but it says device or resource busy. i found a very good article on <a href=""https://forums.aws.amazon.com/thread.jspa?messageID=547507"">resizedisk</a> using fdisk we can increase block size by deleting and creating it and Make the partition bootable. all it requires is a reboot. it wont effect your data if you use same start cylinder. </p><pre><code># df -h  &lt;&lt;1&gt;&gt;Filesystem  Size  Used Avail Use% Mounted on/dev/xvda1  6.0G  2.0G  3.7G  35% / tmpfs  15G   0   15G   0% /dev/shm# fdisk -l  &lt;&lt;2&gt;&gt;Disk /dev/xvda: 21.5 GB, 21474836480 bytes97 heads, 17 sectors/track, 25435 cylindersUnits = cylinders of 1649 * 512 = 844288 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x0003b587  Device Boot  Start   End  Blocks   Id  System/dev/xvda1   *   2  7632   6291456   83  Linux# fdisk /dev/xvda  &lt;&lt;3&gt;&gt;WARNING: DOS-compatible mode is deprecated. It's strongly recommended to  switch off the mode (command 'c') and change display units to  sectors (command 'u').Command (m for help): u  &lt;&lt;4&gt;&gt;Changing display/entry units to sectorsCommand (m for help): p  &lt;&lt;5&gt;&gt;Disk /dev/xvda: 21.5 GB, 21474836480 bytes97 heads, 17 sectors/track, 25435 cylinders, total 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x0003b587  Device Boot  Start   End  Blocks   Id  System/dev/xvda1   *  2048  12584959   6291456   83  LinuxCommand (m for help): d  &lt;&lt;6&gt;&gt;Selected partition 1Command (m for help): n  &lt;&lt;7&gt;&gt;Command action  e   extended  p   primary partition (1-4)p  &lt;&lt;8&gt;&gt;Partition number (1-4): 1  &lt;&lt;9&gt;&gt;First sector (17-41943039, default 17): 2048  &lt;&lt;10&gt;&gt;Last sector, +sectors or +size{K,M,G} (2048-41943039, default 41943039): &lt;&lt;11&gt;&gt;Using default value 41943039Command (m for help): p &lt;&lt;12&gt;&gt;Disk /dev/xvda: 21.5 GB, 21474836480 bytes97 heads, 17 sectors/track, 25435 cylinders, total 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x0003b587  Device Boot  Start   End  Blocks   Id  System/dev/xvda1  2048  41943039  20970496   83  LinuxCommand (m for help): a  &lt;&lt;13&gt;&gt;Partition number (1-4): 1  &lt;&lt;14&gt;&gt;Command (m for help): w  &lt;&lt;15&gt;&gt;The partition table has been altered!Calling ioctl() to re-read partition table.WARNING: Re-reading the partition table failed with error 16: Device or resource busy.The kernel still uses the old table. The new table will be used atthe next reboot or after you run partprobe(8) or kpartx(8)Syncing disks.# reboot  &lt;&lt;16&gt;&gt;&lt;wait&gt;# df -h  &lt;&lt;17&gt;&gt;Filesystem  Size  Used Avail Use% Mounted on/dev/xvda1   20G  2.0G   17G  11% / tmpfs  15G   0   15G   0% /dev/shm# resize2fs /dev/xvda1  &lt;&lt;18&gt;&gt;resize2fs 1.41.12 (17-May-2010)The filesystem is already 5242624 blocks long.  Nothing to do!</code></pre>",2172800,,,2014-06-04T06:55:26.103,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/24030938
147,24148411,2,24146492,2014-06-10T18:31:54.817,0,"<p>I figured it out.. no thanks to amazon's own documentatio which is scattered and out of date..</p><p>you can set the permission to public read like so:</p><pre><code>PutObjectRequest por = new PutObjectRequest(  Constants.getPictureBucket(), Constants.PICTURE_NAME,  resolver.openInputStream(selectedImage),metadata);  por.setCannedAcl(CannedAccessControlList.PublicRead);</code></pre><p>then you end up getting a normal url: </p><blockquote>  <p><a href=""https://s3-us-west-2.amazonaws.com/%5bbucket%5d/%5bNameOfThePicture%5d"" rel=""nofollow"">https://s3-us-west-2.amazonaws.com/[bucket]/[NameOfThePicture]</a></p></blockquote>",1197638,,,2014-06-10T18:31:54.817,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/24148411
148,24208664,2,24208179,2014-06-13T15:12:46.647,0,"<p>Have you run <code>bundle update</code> and is <code>Gemfile.lock</code> in your <code>.gitignore</code> (it shouldn't be). When you set your gems the specific configuration you use gets added to your <code>Gemfile.lock</code> so that you have a consistent deployment and gem versions don't change. The error there is saying that your <code>Gemfile.lock</code> is outdated see <a href=""http://ryanbigg.com/2011/01/why-you-should-run-bundle-update/"" rel=""nofollow"">here for more details</a>.</p>",966097,,,2014-06-13T15:12:46.647,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/24208664
149,24642037,2,21933724,2014-07-08T21:41:27.033,1,"<p>I came across a similar issue while following a related tutorial<a href=""http://mobile.awsblog.com/post/Tx1U4RV2QI1MVWS/Amazon-DynamoDB-on-Mobile-Part-1-Loading-Data"" rel=""nofollow noreferrer"">http://mobile.awsblog.com/post/Tx1U4RV2QI1MVWS/Amazon-DynamoDB-on-Mobile-Part-1-Loading-Data</a><a href=""https://github.com/awslabs/aws-sdk-android-samples/tree/master/DynamoDBMapper_UserPreference"" rel=""nofollow noreferrer"">https://github.com/awslabs/aws-sdk-android-samples/tree/master/DynamoDBMapper_UserPreference</a></p><p>I'm using the latest version of Android Studio (0.8.1) and AWS SDK for Android (1.7.1.1). It looks like you're using the wrong package path.</p><p>For</p><pre><code>import com.amazonaws.services.dynamodbv2.model.DynamoDBAttribute;import com.amazonaws.services.dynamodbv2.model.DynamoDBHashKey;import com.amazonaws.services.dynamodbv2.model.DynamoDBMapper;import com.amazonaws.services.dynamodbv2.model.DynamoDBScanExpression;import com.amazonaws.services.dynamodbv2.model.DynamoDBTable;import com.amazonaws.services.dynamodbv2.model.PaginatedScanList;</code></pre><p>you're using <code>com.amazonaws.services.dynamodbv.model</code> when it should be <code>com.amazonaws.services.dynamodbv2.datamodeling</code>. For more proof/documentation about the packages you can look at dynmodbv2 <a href=""http://docs.aws.amazon.com/AWSAndroidSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/package-summary.html"" rel=""nofollow noreferrer"">here</a> to see that <strong>they are not deprecated</strong>. Change your package path to reflect this:</p><pre><code>import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBAttribute;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBHashKey;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBMapper;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBScanExpression;import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBTable;import com.amazonaws.services.dynamodbv2.datamodeling.PaginatedScanList;</code></pre><p>and you should hopefully only need to use one of the following:</p><ul><li>aws-android-sdk-VERSION.jar. Contains all necessary files (includingthird-party dependencies) for all supported AWS services. All classfiles were compiled without the debug option.</li><li>aws-android-sdk-VERSION-debug.jar. Contains all files (including third-party dependencies) necessary for debugging. The code was compiled with the debug option on with all symbols retained.</li></ul><p>For more on the jars in the AWS SDK for Android see the official documentation <a href=""http://docs.aws.amazon.com/mobile/sdkforandroid/gsg/Welcome.html"" rel=""nofollow noreferrer"">here</a></p><h2>Other Potential Issues</h2><p>(Should only be relevant for AWS SDK for 1.7.1.1 and below if the bug doesn't get fixed in the next rev)</p><p>My issue was different in that even though I had the correct package path as per documentation, Android Studio's Intellij couldn't find <code>dynamodbv2.datamodeling</code> that is, when typing <code>import com.amazonaws.services.dynamodbv2.</code> it didn't suggest datamodeling, only utility and model. Trying <code>import com.amazonaws.services.dynamodbv2.*</code> didn't fix the issue either. Because of this it seems that the datamodeling package isn't wrapped into the two above mentioned packages which in theory are supposed to be all encompassing. To get datamodeling and fix the issue I had in regards to the same objects not being found (DynamoDBAttribute, DynamoDBHashKey, etc.) I had to add as library/file dependencies <code>aws-android-sdk-1.7.1.1-ddb-mapper.jar</code> and <code>aws-android-sdk-1.7.1.1-ddb.jar</code> and cleaned my project at which point Intellij could see datamodeling when typing import and the sample project could compile.</p><p>I imagine you know how to import libraries in your platform as you mentioned importing, but make sure it's only the individual files you need and you don't include <code>aws-android-sdk-1.7.1.1-debug.jar</code> or <code>aws-android-sdk-1.7.1.1.jar</code> with them. So for example, my Android Studio module gradle build file dependencies looks like:</p><pre><code>dependencies {  compile files('libs/aws-android-sdk-1.7.1.1-ddb-mapper.jar')  compile files('libs/aws-android-sdk-1.7.1.1-ddb.jar')  compile files('libs/aws-android-sdk-1.7.1.1-autoscaling.jar')  compile files('libs/aws-android-sdk-1.7.1.1-cloudwatch.jar')  compile files('libs/aws-android-sdk-1.7.1.1-core.jar')  compile files('libs/aws-android-sdk-1.7.1.1-ec2.jar')  compile files('libs/aws-android-sdk-1.7.1.1-elb.jar')  compile files('libs/aws-android-sdk-1.7.1.1-s3.jar')  compile files('libs/aws-android-sdk-1.7.1.1-sdb.jar')  compile files('libs/aws-android-sdk-1.7.1.1-ses.jar')  compile files('libs/aws-android-sdk-1.7.1.1-sns.jar')  compile files('libs/aws-android-sdk-1.7.1.1-sqs.jar')  compile files('libs/aws-android-sdk-1.7.1.1-sts.jar')}</code></pre><p>The issue I had seems to be a bug where ddb and ddb-mapper weren't included in what should be the all encompassing <code>aws-android-sdk-1.7.1.1-debug.jar</code> and <code>aws-android-sdk-1.7.1.1.jar</code>. As such, I have reported on GitHub (<a href=""https://github.com/aws/aws-sdk-android/issues/18"" rel=""nofollow noreferrer"">https://github.com/aws/aws-sdk-android/issues/18</a>) and hopefully the next rev will fix this issue and it will no longer be relevant and you should only need <code>aws-android-sdk-1.7.1.1-debug.jar</code> or <code>aws-android-sdk-1.7.1.1.jar</code></p>",2288548,-1,2020-06-20T09:12:55.060,2014-07-09T16:46:15.847,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/24642037
150,24698876,2,24698664,2014-07-11T13:25:37.590,1,"<p>Whoops, turns out the <code>FileField.url</code> property doesn't use <code>MEDIA_URL</code>. According to <a href=""https://docs.djangoproject.com/en/dev/ref/models/fields/#django.db.models.fields.files.FieldFile.url"" rel=""nofollow noreferrer"">the Django docs</a>:</p><blockquote><p><strong>FieldFile.url</strong></p><p>A read-only property to access the fileŠ—Ès relative URL by calling the url() method of the underlying Storage class.</p></blockquote><p>In my case, the underlying storage class, was <code>S3BotoStorage</code> provided by <code>django-storages</code>. As noted by <a href=""https://github.com/incuna/django-storages-cloudfront"" rel=""nofollow noreferrer"">this now-deprecated fork</a>, this was previously not supported, but can now be done by adding the following to <code>settings.py</code>:</p><pre><code>AWS_S3_CUSTOM_DOMAIN = 'd2ynhpzeiwwiom.cloudfront.net'</code></pre><p>Now it works like a charm:</p><pre><code>&gt;&gt;&gt; design.image.urlu'https://d2ynhpzeiwwiom.cloudfront.net/media/images/designs/etc/etc'</code></pre>",2588818,-1,2020-06-20T09:12:55.060,2014-07-11T13:25:37.590,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/24698876
151,24741171,2,24728634,2014-07-14T16:22:56.950,14,"<p>In terms of load, they have the same goal, but they differ in other areas:</p><p>Up-to-dateness of data:</p><ul><li>A read replica will continuously sync from the master. So your results will probably lag 0 - 3s (depending on the load) behind the master.</li><li>A cache takes the query result at a specific point in time and stores it for a certain amount of time. The longer your queries are being cached, the more lag you'll have; but your master database will experience less load. It's a trade-off you'll need to choose wisely depending on your application.</li></ul><p>Performance / query features:</p><ul><li>A cache can only return results for queries it has already seen. So if you run the same queries over and over again, it's a good match. Note that queries must not contain changing parts like <code>NOW()</code>, but must be equal in terms of the actual data to be fetched.</li><li>If you have many different, frequently changing, or dynamic (<code>NOW()</code>,...) queries, a read replica will be a better match.</li><li>ElastiCache should be much faster, since it's returning values directly from RAM. However, this also limits the number of results you can store.</li></ul><p>So you'll first need to evaluate how outdated your data can be and how cacheable your queries are. If you're using ElastiCache, you might be able to cache more than queries Š—” like caching whole sections of a website instead of the underlying queries only, which should improve the overall load of your application.</p><p>PS: Have you tuned your indexes? If your main problems are writes that won't help. But if you are fighting reads, indexes are the #1 thing to check and they do make a huge difference.</p>",573153,,,2014-07-14T16:22:56.950,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/24741171
152,25098266,2,16912354,2014-08-02T18:41:33.267,0,"<p>I solved this by updating the fog gem by running:</p><pre><code>bundle update fog</code></pre><p>Aparently I was very outdated running fog 1.9.0. Now im running 1.23.0 and got to successfully deploy to EC2 by following Rubber's Railscasts (<a href=""http://railscasts.com/episodes/347-rubber-and-amazon-ec2"" rel=""nofollow"">http://railscasts.com/episodes/347-rubber-and-amazon-ec2</a>)</p><p>This also updated the following gems to these versions:</p><ul><li>fog-core 1.23.0 </li><li>net-scp 1.2.1 </li><li>fog-json 1.0.0</li><li>List item</li><li>inflecto 0.0.2</li><li>fog-brightbox 0.1.1</li><li>fog-softlayer 0.3.11</li><li>ipaddress 0.8.0</li><li>mini_portile 0.6.0</li></ul><p>Nokogiri depends on libxml2-2.9.0 library so be sure to have that installed</p><p>Hope it helps!!</p>",382279,,,2014-08-02T18:41:33.267,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/25098266
153,25186819,2,7599256,2014-08-07T15:47:56.187,1,"<p>A link to the answer is in the comments here, but it wouldn't hurt to have an actual answer with a summary.  There's details about simulating error conditions in the testing sandbox in the <a href=""https://amazonpayments.s3.amazonaws.com/FPS_ASP_Guides/FPS_Advanced_Quick_Start.pdf"" rel=""nofollow noreferrer"">advanced quick start guide</a>.</p><p>The following errors can be simulated by setting the value for <strong>SenderTokenId</strong>:</p><blockquote><p>Closed account:  <em>Z1LGRXR4HMDZBSFKXELA32KZASGWD8IHMHZCK4DETR784LDLD1GMFW4P3WT8VTGX</em></p><p>Email address not verified:  <em>E3FR7BARJV3PB631PMKV74PGKCJLBHI1Q1KMQN7BJ2JJICPDKN3N1CJIKFZ8D7NN</em></p><p>Suspended account:  <em>H216UECZ8ZM1G8G4QA3V7RKF8JDFZ9SI3SJAFSGUKBBNDHX1NVM8GUQRZNRNAHER</em></p></blockquote><p>The following errors can be simulated by setting the value for <strong>RecipientTokenId</strong>:</p><blockquote><p>Closed account:  <em>P1LL7A1LHK935DBGI5NAYCXOCLVEBHBNIU7PBXBAMRKKNLDEPI8M3MUSLZT2VANZ</em></p><p>Email address not verified:  <em>C4LGSEMXN11FTUXZ2X2C7QVFHN5DVBGQJNF17AIQXXXQSX4DRG4KJFCN2KRFUUZI</em></p><p>Suspended account:  <em>R3VK49XVGCAZTJSXKN7ZSBHPMFGKM5VEEQTXGMVE8CFUZ2G5RLLMAB4J6TQRL6BU</em></p></blockquote><p>And finally, setting the <strong>amount</strong> value such that it includes a decimal value between the following values will simulate the following errors:</p><blockquote><p>Temporary Decline:  <em>.60 to .69</em></p><p>Payment Error:  <em>.70 to .89</em></p></blockquote><p>Unfortunately, that last detail may be out of date.  <a href=""http://docs.aws.amazon.com/AmazonFPS/2008-09-17/FPSAdvancedGuide/index.html?CHAP_Sandbox.html"" rel=""nofollow noreferrer"">This</a> page seems to imply that having 7 or 8 after the decimal point will create the terribly generic &quot;Failure&quot;.  From testing, I can confirm that 0.6 does lead to a temporary failure (status is stuck in &quot;Pending&quot; for a while before eventually advancing to &quot;Success&quot;), and 0.7 and 0.8 result in the same &quot;TransactionDenied&quot; error.</p>",9330,-1,2020-06-20T09:12:55.060,2014-08-07T21:26:59.087,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/25186819
154,25303459,2,25284562,2014-08-14T08:28:26.547,1,"<p>Thanks for everyones help it was a combination of everyone</p><pre><code>-dontoptimize-dontusemixedcaseclassnames-dontskipnonpubliclibraryclasses-verbose-ignorewarnings-keepdirectories-dontnote-keepattributes Exceptions,InnerClasses,Signature,Deprecated,  SourceFile,LineNumberTable,*Annotation*,EnclosingMethod-optimizations !code/simplification/arithmetic,!field/*,!class/merging/*-libraryjars C:/Users/Hobbs/Closr/Closr/libs/aws-android-sdk-1.7.1.1.jar-keep public class com.google.vending.licensing.ILicensingService-keep public class com.android.vending.licensing.ILicensingService-keep public class org.apache.commons.** { *; }-keep public class com.nostra13.universalimageloader.** { *; }-keep public class uk.co.senab.actionbarpulltorefresh.** { *; }-keep public class * extends android.app.Application-keep public class * extends android.app.Service-keep public class * extends android.content.BroadcastReceiver-keep public class * extends android.content.ContentProvider-keep public class * extends android.content.Context-keep public class * extends android.app.Activity-keep public class * extends android.support.v4.app.Activity-keep public class * extends android.app.Fragment-keep public class * extends android.support.v4.app.Fragment-keep class com.google.inject.** { *; }-keep class com.facebook.** { *; }-keep class android.os.** { *; }-keep class com.google.android.gms.maps.** { *; }-keep class com.google.android.gms.auth.** { *; }-keep class com.google.android.gms.common.** { *; }-keep class com.google.android.gms.location.** { *; }-keep class com.google.android.gms.gcm.** { *; }-keep class com.google.android.gms.internal.** { *; }-keep class com.google.android.finsky.utils.** { *; }-keep class com.amazonaws.services.sqs.QueueUrlHandler  { *; }-keep class com.amazonaws.javax.xml.transform.sax.*   { public *; }-keep class com.amazonaws.javax.xml.stream.**   { *; }-keep class com.amazonaws.services.**.model.*Exception* { *; }-keep class org.codehaus.**   { *; }-keep class javax.inject.** { *; }-keep class javax.annotation.** { *; }-keep class com.amazonaws.** { *; }-keep class org.joda.convert.*  { *; }-keepnames class com.fasterxml.jackson.**   { *; }-keep class com.closr.closr.** { *; }-keep interface com.closr.closr.** { *; }-keepclassmembers class com.closr.closr.** {  public static &lt;fields&gt;;}-keepclasseswithmembernames class * {  native &lt;methods&gt;;}-keepclassmembers public class * extends android.view.View {  void set*(***);  *** get*();}-keepclassmembers class * extends android.app.Activity {  public void *(android.view.View);}-keepclassmembers enum * {  public static **[] values();  public static ** valueOf(java.lang.String);}-keep class * implements android.os.Parcelable {  public static final android.os.Parcelable$Creator *;}-keepclassmembers class **.R$* {  public static &lt;fields&gt;;}-dontwarn javax.xml.stream.events.**-dontwarn org.codehaus.jackson.**-dontwarn org.apache.commons.logging.impl.**-dontwarn org.apache.http.conn.scheme.**-dontwarn com.amazonaws.**</code></pre>",1408682,,,2014-08-14T08:28:26.547,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/25303459
155,25408240,2,25389624,2014-08-20T15:06:47.987,2,"<p>After reading documentation I managed to find, provided by AWS I found out the proper method to call on my AWS-S3 Object <code>AWS::S3::S3Object</code>. <a href=""http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/S3Object.html"" rel=""nofollow"">Here</a>, it state's that I must use the .write method on my <code>AWS::S3::S3Object</code>. </p><p>Following the documentation my problem was solved and I was not receiving the repeated <code>Digest::Digest is deprecated; use Digest</code> warning anymore. </p>",3040414,1816580,2015-03-17T14:34:55.987,2015-03-17T14:34:55.987,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/25408240
156,25546904,2,23090900,2014-08-28T10:45:54.440,3,"<p>Oh darn. I think I found the reason for this behavior. After facing this issue, I made sure that each token was only uploaded <em>once</em> to AWS SNS. When testing this, I realized that nevertheless I ended up with multiple endpoints with the <em>same</em> token - huh???It turned out that these duplicated tokens resulted from <em>outdated</em> tokens being uploaded to AWS SNS. After creating an endpoint using an outdated token, SNS would automagically <em>revive</em> the endpoint by updating it with the <em>current</em> device token (which afaik is delivered back from GCM as a canonical ID once you try to send push messages to outdated tokens).</p><p>So e.g. uploading these (made-up) tokens and custom data</p><pre><code>APA9...YFDw, {original_token: APA9...YFDw}APA9...XaSd, {original_token: APA9...XaSd} &lt;-- Assume this token is outdatedAPA9...sVQa, {original_token: APA9...sVQa}</code></pre><p>might result in something like this - i.e. different endpoints with identical tokens:</p><pre><code>APA9...YFDw, {original_token: APA9...YFDw}, arn:aws:sns:eu-west-1:4711:endpoint/GCM/myapp/daf64...5c204APA9...YFDw, {original_token: APA9...XaSd}, arn:aws:sns:eu-west-1:4711:endpoint/GCM/myapp/a980f...e3c82 &lt;-- Duplicate token!APA9...sVQa, {original_token: APA9...sVQa}, arn:aws:sns:eu-west-1:4711:endpoint/GCM/myapp/14777...7d9ff</code></pre><p>This scenario in turn seems to lead to above error on subsequent attempts to create endpoints using outdated tokens. On the hand, it seems correct that subsequent requests fail. On the other hand, intuitively I have the gut-feeling that the duplication of tokens that is taking place seems wrong, or at least difficult to handle. Maybe once SNS discovers that a token is outdated and needs to be changed, it could first check if there is already another endpoint existent with the same token...</p><p>I will research on this a bit more and see if I can find a way to handle this properly.</p><p>Cheers</p>",342875,,,2014-08-28T10:45:54.440,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/25546904
157,25836395,2,25687136,2014-09-14T18:07:54.880,1,"<p>One thing up front: You don't say which third party module you are using with PrestaShop, so we have to take wild guesses as to what that actually does and how to change its behavior.</p><p>You are sending one parent SKU (MSL110) and five children (MWS110-DGM-S, -M, -L, -XL and -XXL). MWS reports it has successfully processed one of those six. I assume it is the parent product, please check in Seller Central if that is the case.</p><p>Your feed doesn't validate with the XSDs I've got. Mine do not allow <code>CollectionName</code>, but may be it is outdated. The feed seems to have passed the validation stage of the MWS feed processing though, so I guess it is fine with Amazon anyways.</p><p>You are getting multiple errors:</p><ol><li><strong>Error 8008</strong> means Amazon couldn't establish the parent-child link. Since appearantly MWS rejected the five child products to begin with, you should focus on other errors first.</li><li><strong>Error 99001</strong> missing ""department_name"". This probably refers to the XML element <code>Department</code> in <code>ClassificationData</code>, which you <strong>did</strong> specify.</li><li><strong>Error 99001</strong> missing ""material_type"". This probably refers to the XML element <code>MaterialAndFabric</code> in <code>ClassificationData</code>, which you <strong>did not</strong> specify.</li><li><strong>Error 99042</strong> missing ""recommended_browse_nodes"". The XML field this refers to is <code>RecommendedBrowseNode</code> in <code>DescriptionData</code> which you <strong>did not</strong> specify</li></ol><p>Steps to solve these:</p><ol><li><p>The easiest one is the last error. Get the <a href=""http://g-ecx.images-amazon.com/images/G/01/rainier/help/btg/apparel_browse_tree_guide.xls"" rel=""nofollow"">Browse Tree Guide</a>, and pick the correct category from the list.In your case, the appropriate category seems to be ID 1045710, ""Clothing &amp; Accessories/Men/Underwear/Boxers"". Put right before your closing <code>DescriptionData</code> tag, like this: <code>...&lt;RecommendedBrowseNode&gt;1045710&lt;/RecommendedBrowseNode&gt;&lt;/DescriptionData&gt;</code>. This should get rid of error #4.</p></li><li><p>The same Browse Tree Guide shows two additional values in the next column: <code>department_name:mens AND item_type_keyword:boxer-shorts</code>. What that means is your ClassificationData should include those two values. Your Department already reads ""Mens"", but I'd change that to lower case to match the BTG, and see if error #2 goes away. The item type keyword is called <code>StyleKeywords</code> in XML and should contain ""boxer-shorts"".</p></li><li><p>While you're at it, you should also add <code>MaterialAndFabric</code>. <code>&lt;MaterialAndFabric&gt;cotton&lt;/MaterialAndFabric&gt;</code>. This should get rid of error #3.</p></li><li><p>Now you should see all six products in Seller Central, if you do, error #1 is likely to have vanished as well, as it now can create parent-child links since they actually are in the database.</p></li></ol>",2097290,,,2014-09-14T18:07:54.880,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/25836395
158,25877899,2,25810197,2014-09-16T20:30:20.953,1,"<p>Just to repeat what's in the comment above; i have noticed that most of the R-packages that connected to AWS service were out of date. So i have created a new package <a href=""http://github.com/lalas/awsConnect"" rel=""nofollow"">AWSConnect</a> that allows a user to do most basic operations with S3 and EC2. In that package, the function <code>s3.ls()</code> is designed to list the bucket on S3. </p><p>Please feel free to use it, and report any bugs/request/issues</p>",,,,2014-09-16T20:30:20.953,0,CC BY-SA 3.0,user2329215,,,https://stackoverflow.com/questions/25877899
159,26056837,2,23090900,2014-09-26T09:55:32.213,0,"<p>Had the same issue, with the device reporting one token (outdated according to GCM) and the SNS retrieving/storing another.</p><p>We solved it by clearing the app cache on the device and reopening the app (which in our case, re-registered the device on the gcm service), generating the same token (not outdated) that SNS was attempting to push to.</p>",1037989,243925,2014-09-26T13:38:12.830,2014-09-26T13:38:12.830,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/26056837
160,26089273,2,26084019,2014-09-28T20:23:43.510,1,"<p>Heroku syncs your dependencies.yml file everytime you push your app to them. One of my dependencies ended up being out of date, and heroku automatically grabbed a newer version of the file, which in turn broke the Amazon dependency. Updating my Amazon to the latest version ended up fixing the problem.</p><p>I hadn't touched my dependenies.yml file for weeks, and also hadn't run ""play deps --sync"" for weeks, so didn't think to look there on my local machine.</p>",1203703,,,2014-09-28T20:23:43.510,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/26089273
161,26161256,2,26158411,2014-10-02T12:53:03.883,41,"<h2>Update 4/15/2017: For EC2Launch and Windows Server 2016 AMIs</h2><p>Per AWS documentation for EC2Launch, Windows Server 2016 users can continue using the persist tags introduced in EC2Config 2.1.10:</p><blockquote>  <p>For EC2Config version 2.1.10 and later, or for EC2Launch, you can use  true in the user data to enable the plug-in after  user data execution. </p></blockquote><p><strong>User data example:</strong></p><pre><code>&lt;powershell&gt;  insert script here &lt;/powershell&gt; &lt;persist&gt;true&lt;/persist&gt;</code></pre><p><strong>For subsequent boots:</strong></p><p>Windows Server 2016 users must additionally enable <a href=""http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2launch.html#ec2launch-config"" rel=""noreferrer"">configure and enable EC2Launch</a> instead of EC2Config. EC2Config was deprecated on Windows Server 2016 AMIs in favor of EC2Launch. </p><p>Run the following powershell to schedule a Windows Task that will run the user data on next boot:</p><pre><code>C:\ProgramData\Amazon\EC2-Windows\Launch\Scripts\InitializeInstance.ps1 Š—–Schedule</code></pre><p>By design, this task is disabled after it is run for the first time. However, using the persist tag causes Invoke-UserData to schedule a separate task via Register-FunctionScheduler, to persist your user data on subsequent boots. You can see this for yourself at <code>C:\ProgramData\Amazon\EC2-Windows\Launch\Module\Scripts\Invoke-Userdata.ps1</code>.</p><p><strong>Further troubleshooting:</strong></p><p>If you're having additional issues with your user data scripts, you can find the user data execution logs at <code>C:\ProgramData\Amazon\EC2-Windows\Launch\Log\UserdataExecution.log</code> for instances sourced from the WS 2016 base AMI.</p><hr><h2>Original Answer: For EC2Config and older versions of Windows Server</h2><p>User data execution is automatically disabled after the initial boot. When you created your image, it is probable that execution had already been disabled.  This is configurable manually within <code>C:\Program Files\Amazon\Ec2ConfigService\Settings\Config.xml</code>.</p><p>The <a href=""http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/UsingConfig_WinAMI.html"" rel=""noreferrer"">documentation for ""Configuring a Windows Instance Using the EC2Config Service""</a> suggests several options:</p><ul><li><p>Programmatically create a scheduled task to run at system start using <code>schtasks.exe /Create</code>, and point the scheduled task to the user data script (or another script) at <code>C:\Program Files\Amazon\Ec2ConfigServer\Scripts\UserScript.ps1</code>.</p></li><li><p>Programmatically enable the user data plug-in in Config.xml.</p></li></ul><p>Example, from the documentation:</p><pre><code>&lt;powershell&gt;$EC2SettingsFile=""C:\Program Files\Amazon\Ec2ConfigService\Settings\Config.xml""$xml = [xml](get-content $EC2SettingsFile)$xmlElement = $xml.get_DocumentElement()$xmlElementToModify = $xmlElement.Pluginsforeach ($element in $xmlElementToModify.Plugin){  if ($element.name -eq ""Ec2SetPassword"")  {  $element.State=""Enabled""  }  elseif ($element.name -eq ""Ec2HandleUserData"")  {  $element.State=""Enabled""  }}$xml.Save($EC2SettingsFile)&lt;/powershell&gt;</code></pre><ul><li>Starting with EC2Config version 2.1.10, you can use <code>&lt;persist&gt;true&lt;/persist&gt;</code> to enable the plug-in after user data execution.</li></ul><p>Example, from the documentation:</p><pre><code>&lt;powershell&gt;  insert script here&lt;/powershell&gt;&lt;persist&gt;true&lt;/persist&gt;</code></pre>",775544,775544,2017-11-10T17:37:03.880,2017-11-10T17:37:03.880,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/26161256
162,26187284,2,26186062,2014-10-03T21:49:09.103,2,"<p>AmazonS3Resolver is deprecated, try to use AwsS3Resolver:</p><pre><code>#service.ymlawsS3:  class: Aws\S3\S3Client  factory_class: Aws\S3\S3Client  factory_method:  factory  arguments:  -  key:  %aws_key%  secret: %aws_secret%  region: %aws_region%liip_imagine.cache.resolver.amazon_s3:  class: Liip\ImagineBundle\Imagine\Cache\Resolver\AwsS3Resolver  arguments:  - ""@awsS3""  - %aws_bucket%  tags:  - {name: 'liip_imagine.cache.resolver', resolver: 'resolver_as3'}#config.ymlliip_imagine:  cache: resolver_as3  data_loader: import  filter_sets:  128_128_75_s3:  quality: 75  filters:  thumbnail: { size: [128, 128], mode: outbound }</code></pre>",3249470,,,2014-10-03T21:49:09.103,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/26187284
163,26201391,2,16897778,2014-10-05T09:33:54.103,2,"<p>I checked out your scenario. I think you can achieve that by using multiple load inpath statements for enabling multiple locations .Below are the steps I took for the test I ran.</p><pre><code>hive&gt; create external table xxx (uid int, name string, dept string) row format delimited fields terminated by '\t' stored as textfile;hive&gt; load data inpath '/input/tmp/user_bckt' into table xxx;hive&gt; load data inpath '/input/user_bckt' into table xxx;hive&gt; select count(*) from xxx;10hive&gt; select * from xxx;1   ankur   abinitio2   lokesh  cloud3   yadav   network4   sahu  td5   ankit   data1   ankur   abinitio2   lokesh  cloud3   yadav   network4   sahu  td5   ankit   data</code></pre><p>Let me know if this doesn't work for you</p><p><strong>EDIT:</strong> I just checked the data is getting moved in this case into the hive warehouse opposed to the concept of external table data being left at its original location which is demonstrated below:</p><pre><code>hduser@hadoopnn:~$ hls /input/tmpDEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.14/10/05 14:47:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 items-rw-r--r--   1 hduser hadoop   93 2014-10-04 18:54 /input/tmp/dept_bckt-rw-r--r--   1 hduser hadoop   71 2014-10-04 18:54 /input/tmp/user_bckthduser@hadoopnn:~$ hcp /input/tmp/user_bckt /input/user_bcktDEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.14/10/05 14:47:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicablehduser@hadoopnn:~$ logoutConnection to nn closed.hduser@hadoopdn2:~$ hls /input/tmp/DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.14/10/05 15:05:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 1 items-rw-r--r--   1 hduser hadoop   93 2014-10-04 18:54 /input/tmp/dept_bckthduser@hadoopdn2:~$ hls /hive/wh/xxxDEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.14/10/05 15:21:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 items-rw-r--r--   1 hduser hadoop   71 2014-10-04 18:54 /hive/wh/xxx/user_bckt-rw-r--r--   1 hduser hadoop   71 2014-10-05 14:47 /hive/wh/xxx/user_bckt_copy_1</code></pre><p>I am currently looking into the issue here and will get back once done.</p>",3032283,3032283,2014-10-05T09:54:07.697,2014-10-05T09:54:07.697,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/26201391
164,26443593,2,19478904,2014-10-18T19:19:04.023,5,"<p>The answers here are slightly outdated, spent a great deal of my day trying to get this work in Swift and the new AWS SDK. So here's how to do it in Swift by using the new <code>AWSS3PreSignedURLBuilder</code> (available in version 2.0.7+):</p><pre><code>class S3BackgroundUpload : NSObject {  // Swift doesn't support static properties yet, so have to use structs to achieve the same thing.  struct Static {  static var session : NSURLSession?  }  override init() {  super.init()  // Note: There are probably safer ways to store the AWS credentials.  let configPath = NSBundle.mainBundle().pathForResource(""appconfig"", ofType: ""plist"")  let config = NSDictionary(contentsOfFile: configPath!)  let accessKey = config.objectForKey(""awsAccessKeyId"") as String?  let secretKey = config.objectForKey(""awsSecretAccessKey"") as String?  let credentialsProvider = AWSStaticCredentialsProvider .credentialsWithAccessKey(accessKey!, secretKey: secretKey!)  // AWSRegionType.USEast1 is the default S3 endpoint (use it if you don't need specific endpoints such as s3-us-west-2.amazonaws.com)  let configuration = AWSServiceConfiguration(region: AWSRegionType.USEast1, credentialsProvider: credentialsProvider)  // This is setting the configuration for all AWS services, you can also pass in this configuration to the AWSS3PreSignedURLBuilder directly.  AWSServiceManager.defaultServiceManager().setDefaultServiceConfiguration(configuration)  if Static.session == nil {  let configIdentifier = ""com.example.s3-background-upload""  var config : NSURLSessionConfiguration  if NSURLSessionConfiguration.respondsToSelector(""backgroundSessionConfigurationWithIdentifier:"") {  // iOS8  config = NSURLSessionConfiguration.backgroundSessionConfigurationWithIdentifier(configIdentifier)  } else {  // iOS7  config = NSURLSessionConfiguration.backgroundSessionConfiguration(configIdentifier)  }  // NSURLSession background sessions *need* to have a delegate.  Static.session = NSURLSession(configuration: config, delegate: self, delegateQueue: nil)  }  }  func upload() {  let s3path = ""/some/path/some_file.jpg""  let filePath = ""/var/etc/etc/some_file.jpg""  // Check if the file actually exists to prevent weird uncaught obj-c exceptions.  if NSFileManager.defaultManager().fileExistsAtPath(filePath) == false {  NSLog(""file does not exist at %@"", filePath)  return  }  // NSURLSession needs the filepath in a ""file://"" NSURL format.  let fileUrl = NSURL(string: ""file://\(filePath)"")  let preSignedReq = AWSS3GetPreSignedURLRequest()  preSignedReq.bucket = ""bucket-name""  preSignedReq.key = s3path  preSignedReq.HTTPMethod = AWSHTTPMethod.PUT   // required  preSignedReq.contentType = ""image/jpeg""   // required  preSignedReq.expires = NSDate(timeIntervalSinceNow: 60*60)  // required  // The defaultS3PreSignedURLBuilder uses the global config, as specified in the init method.  let urlBuilder = AWSS3PreSignedURLBuilder.defaultS3PreSignedURLBuilder()  // The new AWS SDK uses BFTasks to chain requests together:  urlBuilder.getPreSignedURL(preSignedReq).continueWithBlock { (task) -&gt; AnyObject! in  if task.error != nil {  NSLog(""getPreSignedURL error: %@"", task.error)  return nil  }  var preSignedUrl = task.result as NSURL  NSLog(""preSignedUrl: %@"", preSignedUrl)  var request = NSMutableURLRequest(URL: preSignedUrl)  request.cachePolicy = NSURLRequestCachePolicy.ReloadIgnoringLocalCacheData  // Make sure the content-type and http method are the same as in preSignedReq  request.HTTPMethod = ""PUT""  request.setValue(preSignedReq.contentType, forHTTPHeaderField: ""Content-Type"")  // NSURLSession background session does *not* support completionHandler, so don't set it.  let uploadTask = Static.session?.uploadTaskWithRequest(request, fromFile: fileUrl)  // Start the upload task:  uploadTask?.resume()  return nil  }  }}extension S3BackgroundUpload : NSURLSessionDelegate {  func URLSession(session: NSURLSession, dataTask: NSURLSessionDataTask, didReceiveData data: NSData) {  NSLog(""did receive data: %@"", NSString(data: data, encoding: NSUTF8StringEncoding))  }  func URLSession(session: NSURLSession, task: NSURLSessionTask, didCompleteWithError error: NSError?) {  NSLog(""session did complete"")  if error != nil {  NSLog(""error: %@"", error!.localizedDescription)  }  // Finish up your post-upload tasks.  }}</code></pre>",1494796,,,2014-10-18T19:19:04.023,1,CC BY-SA 3.0,,2014-10-18T19:19:04.023,,https://stackoverflow.com/questions/26443593
165,26500697,2,4770635,2014-10-22T05:13:45.860,0,"<p>Using ntp may not work on all version of your Linux based server (e.g. an out of date Ubuntu server version that is no longer supported which will block you from downloading ntp if it is not already installed).  </p><p>If this is your situation, you can set independent time zones for your Linux VM:<a href=""https://community.rackspace.com/products/f/25/t/650"" rel=""nofollow"">https://community.rackspace.com/products/f/25/t/650</a> </p><p>After you do this you may need to reset the time/date.  Instructions for doing this are in this article:<a href=""http://codeghar.wordpress.com/2007/12/06/manage-time-in-ubuntu-through-command-line"" rel=""nofollow"">http://codeghar.wordpress.com/2007/12/06/manage-time-in-ubuntu-through-command-line</a></p>",211545,,,2014-10-22T05:13:45.860,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/26500697
166,26962712,2,26961425,2014-11-16T22:18:36.220,1,"<p>Yes, you can do this.  You want to use the <a href=""http://docs.aws.amazon.com/IAM/latest/UserGuide/AccessPolicyLanguage_ElementDescriptions.html#Principal"" rel=""nofollow"">Principal</a> element.</p><p>You can find examples <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html"" rel=""nofollow"">here.</a></p><p>(I know links are generally frowned upon, but AWS technologies change at such a rapid pace that actual examples may be obsolete within days or months)</p>",4782,,,2014-11-16T22:18:36.220,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/26962712
167,27069121,2,27068728,2014-11-21T19:45:29.567,2,"<p>The error: <code>WHERE</code> conditions are not executed in any given order.<br>Use a <code>CASE</code> statement to avoid the exception.</p><pre><code>SELECT exec_time FROM   myTableWHERE  <b>CASE WHEN exec_time NOT LIKE '%N%' THEN  split_part(exec_time,' ', 1)::int >= 60000  ELSE FALSE END</b>ORDER  BY length(exec_time) desc, exec_time descLIMIT  10;</code></pre><p>While being at it, if <code>'None Recorded'</code> is the only case to rule out, use a faster left-anchored check:</p><pre><code>exec_time NOT LIKE 'N%'</code></pre><p>If the above still errors out, check with this to find any offending rows you may have missed:</p><pre><code>SELECT DISTINCT exec_timeFROM   myTableWHERE  exec_time NOT LIKE '%N%'AND  exec_time !~ '^\\d+ '  -- not all digits before the first space</code></pre><p>In modern Postgres you only need a single backslash. <code>'^\d+ '</code>! Seems you have to double up on backslashes in Redshift, which seems to still use the outdated <a href=""https://stackoverflow.com/questions/12316953/insert-varchar-with-single-quotes-in-postgresql/12320729#12320729"">Posix escape syntax for strings</a> by default, and without explicit declaration (<code>E'^\\d+ '</code>)!</p><p>Generally, it's not a good idea to mix data this way. You should have an <code>integer</code> column to store execution time. Much cheaper, cleaner and faster.</p>",939860,-1,2017-05-23T12:34:15.037,2014-11-21T20:39:02.007,7,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/27069121
168,27136173,2,26797144,2014-11-25T20:34:52.077,1,"<p>You should switch to the new EB CLI 3.0.  Git aws.push will soon be deprecated. <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-getting-started.html"" rel=""nofollow"">http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-getting-started.html</a></p><p>Full reference is available at : <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb3-cmd-commands.html"" rel=""nofollow"">http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb3-cmd-commands.html</a></p>",663360,663360,2014-11-27T20:09:26.647,2014-11-27T20:09:26.647,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/27136173
169,27184715,2,27180320,2014-11-28T08:36:34.467,6,"<p>What you are doing is considered as the best practice : let the mobile devices upload directly and securely to S3.</p><p>Documentation :</p><ul><li><a href=""http://docs.aws.amazon.com/mobile/sdkforios/developerguide/s3transfermanager.html"" rel=""noreferrer"">http://docs.aws.amazon.com/mobile/sdkforios/developerguide/s3transfermanager.html</a></li><li><a href=""https://aws.amazon.com/articles/3002109349624271"" rel=""noreferrer"">https://aws.amazon.com/articles/3002109349624271</a> (a bit outdated)</li></ul><p>You must ensure only your users can upload objects to S3 by crafting a correct IAM policy.  Depending on how you authenticate your users, Cognito Identity might help to broker identity tokens received from third party providers (like Google, Facebook or Amazon) or your own (OpenID Connect Token) with AWS STS to receive a temporary Access Key and Secret Key.</p><p>Documentation :</p><ul><li><a href=""http://docs.aws.amazon.com/mobile/sdkforios/developerguide/cognito-auth.html"" rel=""noreferrer"">http://docs.aws.amazon.com/mobile/sdkforios/developerguide/cognito-auth.html</a></li></ul><p>Direct upload allows your application and your user base to scale without requiring additional compute power on the backend.  S3 is a massively parallel object storage, it will handle your mobile fleet traffic, offloading you from low level tasks such as monitoring, scaling, patching,... your backend.</p><p>Now that Lambda is available (in Preview), you can also consider to capture meta data about the S3 object in a Lambda function and upload meta-data to your backend store (DynamoDB or a relational database) directly from lambda.  Considering the generous free tier usage of Lambda, this solution would be much more cost effective than running your own backend.You are familiar with Node.JS, the framework used by Lambda, so their will be almost no learning curve for you.</p><p>Documentation:</p><ul><li><a href=""http://docs.aws.amazon.com/lambda/latest/dg/welcome.html"" rel=""noreferrer"">http://docs.aws.amazon.com/lambda/latest/dg/welcome.html</a></li><li><a href=""http://aws.amazon.com/lambda/pricing/"" rel=""noreferrer"">http://aws.amazon.com/lambda/pricing/</a></li></ul>",663360,663360,2014-11-28T17:32:55.787,2014-11-28T17:32:55.787,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/27184715
170,27294557,2,27292145,2014-12-04T12:51:04.687,37,"<p>By default, when you do a <code>get_bucket</code> call in boto it tries to validate that you actually have access to that bucket by performing a <code>HEAD</code> request on the bucket URL.  In this case, you don't want boto to do that since you don't have access to the bucket itself.  So, do this:</p><pre><code>bucket = conn.get_bucket('my-bucket-url', validate=False)</code></pre><p>and then you should be able to do something like this to list objects:</p><pre><code>for key in bucket.list(prefix='dir-in-bucket'):   &lt;do something&gt;</code></pre><p>If you still get a 403 Errror, try adding a slash at the end of the prefix.</p><pre><code>for key in bucket.list(prefix='dir-in-bucket/'):   &lt;do something&gt;</code></pre><p><strong>Note</strong>: this answer was written about the boto version 2 module, which is obsolete by now. At the moment (2020), boto3 is the standard module for working with AWS. See this question for more info: <a href=""https://stackoverflow.com/questions/32322503/what-is-the-difference-between-the-aws-boto-and-boto3"">What is the difference between the AWS boto and boto3</a></p>",653643,855475,2020-07-25T15:11:36.863,2020-07-25T15:11:36.863,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/27294557
171,27398208,2,27382398,2014-12-10T10:08:39.543,3,"<h3>Limited options in Redshift</h3><pre><code>regexp_replace(hour, '(^\\d{4}-\\d{2}-\\d{2})-(\\d{2}:\\d{2}:\\d{2}$)', '\\1 \\2') AS aregexp_replace(hour, '(^\\d{4}-\\d\\d-\\d\\d)-(\\d\\d:\\d\\d:\\d\\d)$', '\\1 \\2') AS bregexp_replace(hour, '(^[\\d-]{10})-([\\d:]+)$', '\\1 \\2')  AS cleft(hour,10) || ' ' ||  substring(hour FROM 12)   AS e</code></pre><h3>More options in modern Postgres (9.1+)</h3><pre><code>regexp_replace(hour, '(^\d{4}-\d{2}-\d{2})-(\d{2}:\d{2}:\d{2}$)', '\1 \2') AS aregexp_replace(hour, '(^\d{4}-\d\d-\d\d)-(\d\d:\d\d:\d\d)$', '\1 \2')  AS bregexp_replace(hour, '(^[\d-]{10})-([\d:]+)$', '\1 \2')  AS creverse(regexp_replace(reverse(hour), '-', ' '))   AS dleft(hour,10) || ' ' ||  right(hour, -11)  AS eoverlay(hour placing ' ' from 11)  AS fto_timestamp(hour, 'YYYY-MM-DD-HH24:MI:SS')  AS ts</code></pre><p><a href=""http://sqlfiddle.com/#!15/814f6/4"" rel=""nofollow noreferrer""><strong>SQL Fiddle.</strong></a></p><p>From ""restrictive"" to ""cheap"" in order of appearance. <code>ts</code> is special.</p><h3>a</h3><p>That's like the <a href=""https://stackoverflow.com/a/27382697/939860"">currently accepted answer by @Zeki</a>, completed with anchors at start and end with <code>^</code> and <code>$</code> to make it even less ambiguous and potentially faster.</p><p>You want the special meaning of <code>\d</code> as <a href=""http://www.postgresql.org/docs/current/interactive/functions-matching.html#POSIX-CLASS-SHORTHAND-ESCAPES-TABLE"" rel=""nofollow noreferrer"">class shorthand</a> for digits.<br>In Postgres, do not escape backslashes <code>\</code> with <code>\\</code>. That would be incorrect unless you are running with the long outdated, non-default setting <a href=""https://stackoverflow.com/questions/12316953/insert-varchar-with-single-quotes-in-postgresql/12320729#12320729""><code>standard_conforming_strings = off</code></a>.<br>Redshift is stuck at an old stage of development and does just that. Backslashes are interpreted unless escaped with another backslash.</p><h3>b</h3><p><code>\d\d</code> is shorter and cheaper than <code>\d{2}</code>.</p><h3>c</h3><p>Simplify with a character classes: digits + hyphen: <code>[\d-]</code> and digits + colon: <code>[\d:]</code>.</p><h3>d</h3><p>Since <code>regexp_replace()</code> without 4th parameter <code>'g'</code> only replaces the first match, you can <a href=""http://www.postgresql.org/docs/current/interactive/functions-string.html#FUNCTIONS-STRING-OTHER"" rel=""nofollow noreferrer""><code>reverse()</code></a> the string, replace the first hyphen and <code>reverse()</code> back.<br>Doesn't work in Redshift, since it uses a <a href=""http://docs.aws.amazon.com/redshift/latest/dg/REGEXP_REPLACE.html"" rel=""nofollow noreferrer"">simpler version of <code>regexp_replace()</code></a> that <em>always</em> replaces all occurrences.</p><h3>e</h3><p>If the format is fixed as shown, just take the first 10 character, a blank and the rest of the string.<br>Redshift uses <a href=""http://docs.aws.amazon.com/redshift/latest/dg/r_LEFT.html"" rel=""nofollow noreferrer"">simpler versions of <code>left()</code> and <code>right()</code></a> that don't accept negative parameters, so I substituted with <a href=""http://docs.aws.amazon.com/redshift/latest/dg/r_SUBSTRING.html"" rel=""nofollow noreferrer""><code>substring()</code></a>.</p><h3>f</h3><p>Or, simpler yet, just <a href=""http://www.postgresql.org/docs/current/interactive/functions-string.html#FUNCTIONS-STRING-SQL"" rel=""nofollow noreferrer"">overlay()</a> the 11th character with a blank.<br><a href=""http://docs.aws.amazon.com/redshift/latest/dg/c_unsupported-postgresql-functions.html"" rel=""nofollow noreferrer"">Not implemented in Redshift.</a></p><h3>ts</h3><p>Unlike the rest, <a href=""http://www.postgresql.org/docs/current/interactive/functions-formatting.html#FUNCTIONS-FORMATTING-TABLE"" rel=""nofollow noreferrer""><strong><code>to_timestamp()</code></strong></a> returns a proper <code>timestamp with time zone</code> type, not <code>text</code>. You can assign the result to <code>timestamp without time zone</code> just as well. <a href=""https://stackoverflow.com/questions/9571392/ignoring-timezones-altogether-in-rails-and-postgresql/9576170#9576170"">Details.</a>. By far the best option if you want to convert your string. <a href=""http://docs.aws.amazon.com/redshift/latest/dg/c_unsupported-postgresql-functions.html"" rel=""nofollow noreferrer"">Not implemented in Redshift.</a></p>",939860,-1,2017-05-23T12:07:50.200,2014-12-13T19:38:58.890,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/27398208
172,27433142,2,25943805,2014-12-11T21:55:25.167,1,"<p>CuddlyBuddly is out of date and queries for the entire list of files in the bucket every time it wants to post one, which eventually slows down and stops working entirely. The django-storages app using s3Boto is more up to date and works great.</p><p><a href=""https://django-storages.readthedocs.org/en/latest/backends/amazon-S3.html"" rel=""nofollow"">https://django-storages.readthedocs.org/en/latest/backends/amazon-S3.html</a></p>",1021343,,,2014-12-11T21:55:25.167,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/27433142
173,27494547,2,27477730,2014-12-15T22:46:27.280,25,"<p>The underlying Hadoop API that Spark uses to access S3 allows you specify input files using a <a href=""http://en.wikipedia.org/wiki/Glob_%28programming%29"" rel=""noreferrer"">glob expression</a>.</p><p>From <a href=""http://spark.apache.org/docs/1.1.1/programming-guide.html#external-datasets"" rel=""noreferrer"">the Spark docs</a>:</p><blockquote>  <p>All of SparkŠ—Ès file-based input methods, including textFile, support running on directories, compressed files, and wildcards as well. For example, you can use <code>textFile(""/my/directory"")</code>, <code>textFile(""/my/directory/*.txt"")</code>, and <code>textFile(""/my/directory/*.gz"")</code>.</p></blockquote><p>So in your case you should be able to open all those files as a single RDD using something like this:</p><pre><code>rdd = sc.textFile(""s3://bucket/project1/20141201/logtype1/logtype1.*.gz"")</code></pre><p>Just for the record, you can also specify files using a comma-delimited list, and you can even mix that with the <code>*</code> and <code>?</code> wildcards.</p><p>For example:</p><pre><code>rdd = sc.textFile(""s3://bucket/201412??/*/*.gz,s3://bucket/random-file.txt"")</code></pre><p>Briefly, what this does is:</p><ul><li>The <code>*</code> matches all strings, so in this case all <code>gz</code> files in all folders under <code>201412??</code> will be loaded.</li><li>The <code>?</code> matches a single character, so <code>201412??</code> will cover all days in December 2014 like <code>20141201</code>, <code>20141202</code>, and so forth.</li><li>The <code>,</code> lets you just load separate files at once into the same RDD, like the <code>random-file.txt</code> in this case.</li></ul><p>Some notes about the appropriate URL scheme for S3 paths:</p><ul><li>If you're running Spark on EMR, <a href=""https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html"" rel=""noreferrer"">the correct URL scheme is <code>s3://</code></a>.</li><li>If you're running open-source Spark (i.e. no proprietary Amazon libraries) built on Hadoop 2.7 or newer, <code>s3a://</code> is the way to go.</li><li><a href=""https://wiki.apache.org/hadoop/AmazonS3"" rel=""noreferrer""><code>s3n://</code> has been deprecated</a> on the open source side in favor of <code>s3a://</code>. You should only use <code>s3n://</code> if you're running Spark on Hadoop 2.6 or older.</li></ul>",877069,877069,2018-06-12T21:40:21.757,2018-06-12T21:40:21.757,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/27494547
174,27586721,2,27584823,2014-12-21T04:27:20.047,3,"<blockquote>  <p>unable to load Private Key  6312:error:0906D06C:PEM routines:PEM_read_bio:no start line:pem_lib.c:647:Expecting: ANY PRIVATE KEY</p></blockquote><p>I ran your commands on OS X, and I could not reproduce the results.</p><p>I <em>did</em> use the <code>-config</code> option because I have an ""OpenSSL server config template"" that makes it easy to generate CSRs and self signed certificates:</p><pre><code>$ mkdir test$ cd test$ openssl req -new -key privatekey.pem -out csr.pem -config example-com.conf</code></pre><p>The configuration file is named <code>example-com.conf</code>, and you can find it at <a href=""https://stackoverflow.com/questions/26019957/how-do-i-edit-a-self-signed-certificate-created-using-openssl-xampp/26029695#26029695"">How do I edit a self signed certificate created using openssl xampp?</a>. Edit it to suit your taste (in particular, the DNS names).</p><p>If interested, here's the OpenSSL man pages on the <a href=""https://www.openssl.org/docs/apps/req.html"" rel=""nofollow noreferrer""><code>req</code> sub-command</a>.</p><hr><blockquote>  <p>I checked the generated key and it looks like<br></p>   <p>-----BEGIN RSA PRIVATE KEY-----<br>  {lots of characters}<br>  -----END RSA PRIVATE KEY-----<br></p></blockquote><p>You can validate the key you just created with:</p><pre><code>$ openssl rsa -in privatekey.pem -inform PEM -text -nooutPrivate-Key: (2048 bit)modulus:  00:b0:91:ce:57:28:0f:5c:3a:c3:29:d7:23:6a:71:  ca:64:49:fc:24:ea:69:a3:09:d6:49:94:17:b9:09:  65:fa:5a:10:47:a4:9b:b8:cd:6d:32:74:19:8d:5c:  79:92:f0:a6:43:9c:75:a3:7b:ef:c4:c3:d9:c2:db:  b9:bd:ec:14:a8:b1:52:73:8f:56:c8:5c:16:08:56:  ff:c2:2b:35:3c:0a:0f:34:d0:91:c1:54:7e:72:e8:  97:bf:ea:46:69:5f:e4:21:8d:7a:f5:a5:6b:6a:e8:  00:56:bc:02:f6:b4:ae:6e:89:a6:50:aa:5b:2f:d8:  7d:99:04:61:51:76:b3:5e:9e:30:52:99:54:26:e2:  3a:54:ec:78:34:e6:9a:b7:c2:58:5c:51:3d:39:52:  d4:6e:0c:6e:a1:a0:a5:f1:4d:5a:f5:0b:1a:6e:dc:  f3:bb:0d:d0:53:51:b0:1a:04:ee:86:35:d5:f3:8b:  0d:bc:19:61:6c:0c:b2:7b:a9:7c:47:97:01:bb:a2:  6a:74:d9:19:e9:df:60:07:d4:95:4c:83:f8:3b:84:  c2:b8:3d:b9:a7:34:0a:9b:a3:c6:70:cc:ef:de:f4:  64:88:f1:56:d3:2a:fd:5a:82:88:96:66:93:6c:a0:  b8:ec:e4:4c:e8:76:5c:9c:fc:c4:60:72:b6:9a:3f:  98:a3publicExponent: 65537 (0x10001)privateExponent:  00:87:ab:f1:65:ac:e5:68:93:ca:64:3a:e7:fe:a1:  62:c7:7e:c5:dc:c3:b5:d9:cd:f4:36:e3:30:fb:40:  0a:78:bc:7d:67:df:46:bc:50:34:88:a1:07:05:44:  ba:31:ba:f1:b6:5f:e1:50:76:29:bd:02:54:2f:d2:  cf:bc:ec:4a:cf:78:39:07:8c:6b:3d:56:ec:a3:09:  de:49:9f:13:af:87:77:39:b8:cd:56:45:0b:48:56:  0a:4c:2f:c2:5c:b3:8e:c2:6d:48:be:b9:95:79:36:  bd:13:e8:31:4a:c9:78:82:7d:08:2b:51:4a:f1:cf:  a2:6a:52:20:49:0d:31:34:10:88:02:d7:a7:07:70:  32:b5:f5:8c:cc:d4:b2:8d:b9:aa:bb:33:82:1a:74:  bd:4d:4f:e9:e0:cc:f2:27:fb:98:34:2c:77:56:6f:  88:3a:66:32:5d:7d:57:c6:5b:63:39:fa:32:04:9d:  e3:cc:a5:b6:44:91:fd:7d:d1:b6:2d:16:47:59:81:  3d:cf:d9:a7:58:2a:d6:61:5d:c6:69:3b:7a:70:50:  4f:80:f4:d9:fb:c8:7d:5e:44:9e:ac:c8:e6:aa:49:  c3:d6:df:6b:03:68:25:a3:2b:89:8f:9a:35:3a:58:  7d:71:b4:08:d9:04:7b:b9:96:17:f3:a5:19:c5:07:  4e:c1prime1:  00:d7:d0:d8:8c:b5:86:ed:0e:06:70:c9:54:00:25:  d7:8c:e4:65:51:1b:c5:ba:33:c2:02:1a:dc:80:a6:  ae:8e:1e:e8:c0:b7:04:11:5a:e3:98:52:8f:4a:7a:  43:b8:e8:1b:c8:d6:d3:b2:dc:70:59:a5:ca:83:bb:  35:f1:6c:f5:cb:d0:f4:04:5e:aa:7c:d0:ec:d7:4a:  d5:1c:7c:e2:67:e4:e8:17:95:9b:4e:2b:a0:26:74:  61:d0:a0:15:27:18:e5:84:b5:54:ef:be:82:35:7e:  78:e0:49:6b:4e:ae:93:53:a0:81:a3:8e:de:d3:e5:  dc:c5:ba:03:36:14:47:97:03prime2:  00:d1:72:3b:f5:34:b1:11:78:b2:79:f4:3e:d7:be:  bf:cc:b3:09:ea:24:a4:cc:7f:64:73:96:d2:48:9e:  55:bc:79:23:c2:d9:80:81:7d:a4:a5:4b:43:33:8e:  62:04:ec:8d:22:d7:43:5e:41:b6:4d:e9:b0:cc:70:  63:17:70:93:88:81:f5:84:a6:3f:2b:98:33:a3:69:  53:11:c7:95:8c:30:ea:e8:58:c7:77:10:b4:a8:f5:  bf:5e:cf:e1:99:bb:b3:4e:57:d2:4c:f7:73:de:8a:  98:8e:7c:26:37:6c:e4:77:c6:d2:ed:5d:53:a7:15:  c3:9c:67:61:d3:24:9a:f5:e1exponent1:  00:83:34:59:e2:b9:9d:8c:d2:e1:01:82:b4:89:de:  77:bc:15:42:af:5b:c6:0a:dc:da:8e:f3:0b:a9:3f:  2c:92:04:a2:96:3e:ed:bf:2b:55:80:ce:78:84:db:  ed:fe:25:46:77:04:7b:f1:9a:68:c7:67:ae:c6:05:  73:d7:11:da:21:0e:28:bb:db:5d:a4:c2:53:aa:d3:  b8:da:37:e6:61:29:5e:1c:b0:7c:99:ba:96:03:aa:  ef:a8:a9:1a:13:09:e4:c7:98:82:49:ba:b5:68:96:  3a:20:89:22:2e:d4:9d:86:d2:e6:dd:ab:c7:36:65:  e1:a1:67:e3:f9:e5:bc:5c:47exponent2:  00:81:6d:b9:55:8f:09:39:05:c0:2d:12:dd:5e:cf:  56:91:35:b6:93:c5:af:3d:5c:20:04:3a:18:9a:9d:  95:d7:d1:78:62:e9:ab:ba:d9:9c:cc:34:95:43:9f:  e2:3c:ae:bd:8c:e1:3f:95:58:c0:42:a7:7e:04:e8:  12:a4:22:82:59:22:0e:49:b9:be:61:bf:3d:71:e7:  1d:59:68:5f:a6:f1:77:c8:bb:4c:0f:ec:f7:e7:4d:  6d:c4:36:6c:70:67:08:a8:0a:27:40:3e:ce:90:a0:  4f:24:05:de:4b:f3:f3:bf:7c:d3:4d:b1:95:87:34:  30:dc:4f:1a:a9:b2:fe:3b:a1coefficient:  6d:51:b3:6e:87:8d:aa:f0:55:c4:22:21:62:a9:ea:  24:b3:b7:91:40:f5:78:5d:f1:40:45:7e:0d:a2:a3:  54:46:ba:42:33:b6:cd:57:a1:85:bc:3d:ba:1c:eb:  87:33:a9:e9:63:1e:7c:2c:89:98:b9:0f:4b:e8:c4:  79:bd:00:6a:f5:3e:ea:63:f1:9e:aa:47:35:5a:22:  fc:4e:e3:61:7e:eb:dc:a6:c0:2c:d5:fd:22:9f:01:  59:32:15:db:41:99:b7:a8:c1:eb:1e:42:c7:1b:c7:  c8:56:86:a8:34:fe:1c:48:b6:6e:f1:c1:5c:96:bf:  9d:fa:e5:4c:d0:2a:d9:09</code></pre><hr><blockquote>  <p>unable to write 'random state'</p></blockquote><p>This is a well known problem. OpenSSL uses a default configuration file. You can locate the configuration file with <a href=""https://stackoverflow.com/q/21477210/608639"">correct location of openssl.cnf file</a>.</p><p>The default configuration file includes these lines:</p><pre><code>$ cat /usr/local/ssl/macosx-x64/openssl.cnf ...HOME  = .RANDFILE  = $ENV::HOME/.rnd...</code></pre><p>To save the random file, you should point <code>HOME</code> and <code>RANDFILE</code> to a valid location. On Windows, you type <code>set HOME=...</code> and <code>set RANDFILE=...</code> in the command prompt. Or better, change it in the OpenSSL configuration file you use.</p><p>Also see <a href=""https://stackoverflow.com/q/12507277/608639"">How to fix Š—“unable to write 'random state' Š— in openssl</a> and <a href=""https://stackoverflow.com/q/2229723/608639"">How do I make OpenSSL write the RANDFILE on Windows Vista?</a>.</p><hr><blockquote>  <p>I'm trying to configure HTTPS for my ElasticBeanstalk environment following these instructions.</p></blockquote><p>The instructions are wrong in the image below. <strong><em>Do not place a DNS name in the Common Name (CN)</em></strong>.</p><p><img src=""https://i.stack.imgur.com/3PYVL.png"" alt=""enter image description here""></p><p>Placing a DNS name in the Common Name is deprecated by both the IETF (the folks who publish RFCs) and the CA/B Forums (the cartel where browsers and CAs collude). You should pay articular attention to what the CA/B recommends because Browsers and CAs come up with those rules, and the browsers follow them (and they don't follow the RFCs). For reference, see <a href=""http://www.ietf.org/rfc/rfc5280.txt"" rel=""nofollow noreferrer"">RFC 5280</a>, <a href=""http://www.ietf.org/rfc/rfc6125.txt"" rel=""nofollow noreferrer"">RFC 6125</a> and the <a href=""https://cabforum.org/baseline-requirements-documents/"" rel=""nofollow noreferrer"">CA/B Baseline Requirements</a>.</p><p>Instead, <strong><em>place DNS names in the Subject Alternate Name (SAN)</em></strong>. Both the IETF and CA/B specifies it.</p><p>The custom OpenSSL configuration file handles this for you. You just have to change the DNS names listed under the section <code>[ alternate_names ]</code>. For example, here's a set of names set up for the domain <code>example.com</code>. Notice there is no DNS name in the CN:</p><pre><code>[ subject ]...commonName  = Common Name (e.g. server FQDN or YOUR name)commonName_default  = Example Company[ alternate_names ]DNS.1   = example.comDNS.2   = www.example.comDNS.3   = mail.example.comDNS.4   = ftp.example.com</code></pre>",608639,-1,2017-05-23T12:16:18.020,2014-12-21T20:22:25.653,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/27586721
175,27658878,2,15778047,2014-12-26T15:24:57.340,2,"<p>Good answer - The blog post is out of date now as their naming has changed, but it's probably still based on MD5.</p>",2692198,,,2014-12-26T15:24:57.340,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/27658878
176,27768332,2,26314316,2015-01-04T17:34:31.273,30,"<p>Boto and the underlying EMR API is currently mixing the terms <em>cluster</em> and <em>job flow</em>, and job flow is being <a href=""http://docs.aws.amazon.com/ElasticMapReduce/latest/API/API_DescribeJobFlows.html"">deprecated</a>. I consider them synonyms.</p><p>You create a new cluster by calling the <code>boto.emr.connection.run_jobflow()</code> function. It will return the cluster ID which EMR generates for you.</p><p>First all the mandatory things:</p><pre><code>#!/usr/bin/env pythonimport botoimport boto.emrfrom boto.emr.instance_group import InstanceGroupconn = boto.emr.connect_to_region('us-east-1')</code></pre><p>Then we specify instance groups, including the spot price we want to pay for the TASK nodes:</p><pre><code>instance_groups = []instance_groups.append(InstanceGroup(  num_instances=1,  role=""MASTER"",  type=""m1.small"",  market=""ON_DEMAND"",  name=""Main node""))instance_groups.append(InstanceGroup(  num_instances=2,  role=""CORE"",  type=""m1.small"",  market=""ON_DEMAND"",  name=""Worker nodes""))instance_groups.append(InstanceGroup(  num_instances=2,  role=""TASK"",  type=""m1.small"",  market=""SPOT"",  name=""My cheap spot nodes"",  bidprice=""0.002""))</code></pre><p>Finally we start a new cluster:</p><pre><code>cluster_id = conn.run_jobflow(  ""Name for my cluster"",  instance_groups=instance_groups,  action_on_failure='TERMINATE_JOB_FLOW',  keep_alive=True,  enable_debugging=True,  log_uri=""s3://mybucket/logs/"",  hadoop_version=None,  ami_version=""2.4.9"",  steps=[],  bootstrap_actions=[],  ec2_keyname=""my-ec2-key"",  visible_to_all_users=True,  job_flow_role=""EMR_EC2_DefaultRole"",  service_role=""EMR_DefaultRole"")</code></pre><p>We can also print the cluster ID if we care about that:</p><pre><code>print ""Starting cluster"", cluster_id</code></pre>",1571302,,,2015-01-04T17:34:31.273,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/27768332
177,27772632,2,27764533,2015-01-05T02:22:51.193,0,"<p><code>@filename</code> has been deprecated in PHP >= 5.5.0 as stated <a href=""http://www.php.net/manual/en/function.curl-setopt.php"" rel=""nofollow noreferrer"">here</a> under the CURLOPT_POSTFIELDS description , So thats the reason why you got the error .</p><p>you have your answer here at this <a href=""https://stackoverflow.com/questions/17032990/can-anyone-give-me-an-example-for-phps-curlfile-class"">stack overflow thread</a>, where different solutions are discussed . Also here is a snippet from <a href=""https://wiki.php.net/rfc/curl-file-upload"" rel=""nofollow noreferrer"">RFC</a> for the code. </p><blockquote>  <p><strong>Currently, cURL file uploading is done as:</strong></p><pre><code> curl_setopt($curl_handle, CURLOPT_POST, 1);  $args['file'] = '@/path/to/file'; curl_setopt($curl_handle, CURLOPT_POSTFIELDS, $args);</code></pre>   <p>This API is both invonvenient and insecure, it is impossible to send  data starting with '@' to the POST, and any user data that is being  re-sent via cURL need to be sanitized so that the data value does not  start with @. In general, in-bound signalling usually vulnerable to  all sorts of injections and better not done in this way.</p>   <p>Instead of using the above method, <strong>the following should be used to  upload files with CURLOPT_POSTFIELDS</strong>:</p><pre><code> curl_setopt($curl_handle, CURLOPT_POST, 1);  $args['file'] = new CurlFile('filename.png', 'image/png'); curl_setopt($curl_handle, CURLOPT_POSTFIELDS, $args);</code></pre></blockquote>",939299,-1,2017-05-23T12:28:27.807,2015-01-05T02:22:51.193,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/27772632
178,27893162,2,27747455,2015-01-11T23:07:43.600,5,"<p>Homebrew relies on volunteers to keep the formulas updated.  If you notice an outdated formula, please submit a bug or pull request.</p>",98530,,,2015-01-11T23:07:43.600,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/27893162
179,28089460,2,10940271,2015-01-22T13:01:45.220,3,"<p>Instead of using CommonCrypto, which is deprecated in modern OS X, you can also use SecTransforms:</p><pre><code>CFErrorRef error = NULL;SecTransformRef digestRef = SecDigestTransformCreate(kSecDigestHMACSHA2, 256, &amp;error);SecTransformSetAttribute(digestRef, kSecTransformInputAttributeName, (__bridge CFDataRef)self, &amp;error);SecTransformSetAttribute(digestRef, kSecDigestHMACKeyAttribute, (__bridge CFDataRef)key, &amp;error);CFDataRef resultData = SecTransformExecute(digestRef, &amp;error);NSData* hashData = (__bridge NSData*)resultData;CFRelease(digestRef);</code></pre>",573035,,,2015-01-22T13:01:45.220,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28089460
180,28098998,2,28093087,2015-01-22T21:26:46.120,1,"<p>For future reference, the problem was caused by using an obsolete 2.x version of the aws-eb-cli tools package. Upgrading it to 3.x made the error obvious - building the docker image has failed on AWS. </p><p>What I was looking for was running an existing docker image, I found instruction for this scenario at <a href=""https://aws.amazon.com/blogs/aws/aws-elastic-beanstalk-for-docker/"" rel=""nofollow"">https://aws.amazon.com/blogs/aws/aws-elastic-beanstalk-for-docker/</a>.</p><p>Thanks a lot for Nick for asking the right questions which made me realize the obsolete tools package!</p>",548915,,,2015-01-22T21:26:46.120,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28098998
181,28159780,2,21223048,2015-01-26T22:19:09.497,0,"<p>The vagrant-rsync is deprecated as of Vagrant 1.5. One solution out there is <a href=""https://github.com/dmatora/vagrant-unison"" rel=""nofollow"">vagrant-unison</a>. You may also check out <a href=""https://github.com/mitchellh/vagrant-aws/issues/340"" rel=""nofollow"">this discussion</a>. What should also work is a <code>vagrant reload</code>.</p>",1200707,,,2015-01-26T22:19:09.497,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28159780
182,28162117,2,28060017,2015-01-27T02:12:20.220,0,"<p>The cluster feature was deprecated in VSE6.7 which is why you receive the error.  </p><p>If you want the cluster feature you will have to use the older core release.</p><p>If you are interested in other virtualized functions:</p><p>The latest incarnation (commercial) from Brocade is the 5600 ( <a href=""http://www.brocade.com/products/all/network-functions-virtualization/index.page"" rel=""nofollow"">http://www.brocade.com/products/all/network-functions-virtualization/index.page</a> ) and they recently announced a controller, the Brocade Vyatta Controller  ( <a href=""https://github.com/BRCDcomm/BVC/wiki"" rel=""nofollow"">https://github.com/BRCDcomm/BVC/wiki</a> ).  It is based on the open source OpenDaylight (ODL) controller ( <a href=""https://wiki.opendaylight.org/view/Main_Page"" rel=""nofollow"">https://wiki.opendaylight.org/view/Main_Page</a> ).  </p><p>The OpenDaylight controller allows you to write programs and/or scripts to monitor, configure and control a heterogenous network of virtual (and physical) devices.</p><p>SDxCentral maintains a list of virtual switching and routing components:<a href=""https://www.sdxcentral.com/comprehensive-list-virtual-switching-routing/"" rel=""nofollow"">https://www.sdxcentral.com/comprehensive-list-virtual-switching-routing/</a></p>",2430212,,,2015-01-27T02:12:20.220,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28162117
183,28197504,2,28176056,2015-01-28T16:34:33.580,0,"<p>I found the issue. I did a huge mistake by importing: </p><pre><code>com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBRangeKey</code></pre><p>instead of:</p><pre><code>com.amazonaws.mobileconnectors.dynamodbv2.dynamodbmapper.DynamoDBRangeKey</code></pre><p>for @DynamoDBRangeKey annotation since I'm using Android mobile SDK. I found the issue by downloading and adding the DynamoDBMapper.jar for 2.1.8, and this version shows the </p><pre><code>com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBRangeKey</code></pre><p>as deprecated.   </p>",2292615,,,2015-01-28T16:34:33.580,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28197504
184,28350203,2,28308210,2015-02-05T17:22:08.440,1,"<p>From your description it looks like you may be mixing ups steps of root and non-root installation of DB2. </p><p>In a non-root installation all DB2 binaries and other files are installed in the home directory of the user performing the install. Certain tasks still require root privileges, this is where you run <code>db2rfe</code>. Non-root installations have many limitations though, so you should only choose it if you have no other alternative.</p><p>A root installation copies binaries under <code>/opt</code> and creates a DB2 instance in the home directory of the instance owner user that you specify during installation. From there symlinks are created to the actual binaries. </p><p>Root installation is not less secure than non-root, may be it's even more secure as the binaries are owned by root.</p><p>Console vs. GUI installation has no relation to root vs. non-root. In the past <code>db2setup</code> was used for GUI-based installations while <code>db2_install</code> for console installations. However, <code>db2_install</code> is now deprecated; to perform a console installation use a response file option (<code>db2setup -r &lt;response file name&gt;</code>) as explained in <a href=""http://www-01.ibm.com/support/knowledgecenter/SSEPGG_10.5.0/com.ibm.db2.luw.qb.server.doc/doc/t0007298.html?cp=SSEPGG_10.5.0%2F2-0-5-4&amp;lang=en"" rel=""nofollow"" title=""the manual"">the manual</a>.</p>",1227152,,,2015-02-05T17:22:08.440,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28350203
185,28432988,2,28387499,2015-02-10T13:32:44.943,3,"<p>In the end, I figures it out (and it was, of course, obvious):</p><p>Here's how I have done it:</p><ol><li><p>add a bootstrap action that downloads the JARs on every node, for example you can upload the JARs in your bucket, make them public and then do:</p><pre><code>wget https://yourbucket/path/somejar.jar -O $HOME/somejar.jarwget https://yourbucket/path/avro-1.7.7.jar -O $HOME/avro-1.7.7.jarwget https://yourbucket/path/avro-mapred-1.7.7.jar -O $HOME/avro-mapred-1.7.7.jar</code></pre></li><li><p>when you specify <code>-libjars</code> in the optional arguments <em>use the <strong>abosolute</strong> path</em>, so:</p><pre><code>-libjars /home/hadoop/somejar.jar,$HOME/avro-1.7.7.jar,/home/hadoop/avro-mapred-1.7.7.jar</code></pre></li></ol><p>I have lost a number of hours that I am ashamed to say, hope this helps somebody else.</p><p><strong>Edit (Feb 10th, 2015)</strong></p><p>I have double checked, and I want to point out that it seems that environment variable are not expanded when passed to the optional arguments field. So, use the explicit $HOME path (i.e. <code>/home/hadoop</code>)</p><p><strong>Edit (Feb 11th, 2015)</strong></p><p>If you want to launch the a streaming job on Amazon EMR using the AWS cli you can use the following command.</p><pre><code>aws emr create-cluster  --ami-version '3.3.2' \  --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType='m1.medium' InstanceGroupType=CORE,InstanceCount=2,InstanceType='m3.xlarge' \  --name 'TestStreamingJob' \  --no-auto-terminate \  --log-uri 's3://path/to/your/bucket/logs/' \  --no-termination-protected \  --enable-debugging \  --bootstrap-actions Path='s3://path/to/your/bucket/script.sh',Name='ExampleBootstrapScript' Path='s3://path/to/your/bucket/another_script.sh',Name='AnotherExample' \  --steps file://./steps_test.json</code></pre><p>and you can specify the steps in a JSON file:</p><pre><code>[ {  ""Name"": ""Avro"",  ""Args"": [""-files"",""s3://path/to/your/mapper.py,s3://path/to/your/reducer.py"",""-libjars"",""/home/hadoop/avro-1.7.7.jar,/home/hadoop/avro-mapred-1.7.7.jar"",""-inputformat"",""org.apache.avro.mapred.AvroAsTextInputFormat"",""-mapper"",""mapper.py"",""-reducer"",""reducer.py"",""-input"",""s3://path/to/your/input_directory/"",""-output"",""s3://path/to/your/output_directory/""],  ""ActionOnFailure"": ""CONTINUE"",  ""Type"": ""STREAMING"" }]</code></pre><p>(please note that the <a href=""http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/CLI_CreateStreaming.html"" rel=""nofollow"">official Amazon documentation</a> is somewhat outdated, in fact it uses the old Amazon EMR CLI tool which is <a href=""http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-cli-install.html"" rel=""nofollow"">deprecated</a> in favor of the more recente <a href=""https://aws.amazon.com/cli/"" rel=""nofollow"">AWS CLI</a>)</p>",2377454,2377454,2015-02-11T17:59:39.780,2015-02-11T17:59:39.780,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28432988
186,28447858,2,28364723,2015-02-11T06:26:48.950,4,"<p>Since <code>DescribeJobFlows</code> is deprecated, monitor cluster status is an alternate way to monitor job run progress.</p><pre><code>  RunJobFlowResult runJobResult = emr.runJobFlow(runJobFlowRequest);  System.out.printf(""Run JobFlowId is: %s\n"", runJobResult.getJobFlowId());  while(true) {  DescribeClusterRequest desc = new DescribeClusterRequest()  .withClusterId(runJobResult.getJobFlowId());  DescribeClusterResult clusterResult = emr.describeCluster(desc);  Cluster cluster = clusterResult.getCluster();  String status = cluster.getStatus().getState();  System.out.printf(""Status: %s\n"", status);  if(status.equals(ClusterState.TERMINATED.toString()) || status.equals(ClusterState.TERMINATED_WITH_ERRORS.toString())) {  break;  }  try {  TimeUnit.SECONDS.sleep(30);  } catch (InterruptedException e) {  e.printStackTrace();  }  // maybe other handle  }</code></pre>",3275167,,,2015-02-11T06:26:48.950,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28447858
187,28455874,2,28449363,2015-02-11T13:42:44.297,79,"<p>Of course, I was misunderstanding the problem. <a href=""https://aws.amazon.com/blogs/compute/container-reuse-in-lambda/"" rel=""noreferrer"">As AWS themselves put it</a>:</p><blockquote>  <p>For those encountering nodejs for the first time in Lambda, a common   error is forgetting that callbacks execute asynchronously and calling   <code>context.done()</code> in the original handler when you really meant to wait   for another callback (such as an S3.PUT operation) to complete, forcing  the function to terminate with its work incomplete.</p></blockquote><p>I was calling <code>context.done</code> way before any callbacks for the request fired, causing the termination of my function ahead of time.</p><p>The working code is this:</p><pre><code>var http = require('http');exports.handler = function(event, context) {  console.log('start request to ' + event.url)  http.get(event.url, function(res) {  console.log(""Got response: "" + res.statusCode);  context.succeed();  }).on('error', function(e) {  console.log(""Got error: "" + e.message);  context.done(null, 'FAILURE');  });  console.log('end request to ' + event.url);}</code></pre><p><strong>Update:</strong> starting 2017 AWS has deprecated the old Nodejs 0.10 and only the newer 4.3 run-time is now available (old functions should be updated). This runtime introduced some changes to the handler function. The new handler has now 3 parameters.</p><pre><code>function(event, context, callback)</code></pre><p>Although you will still find the <code>succeed</code>, <code>done</code> and <code>fail</code> on the context parameter, AWS suggest to use the <code>callback</code> function instead or <code>null</code> is returned by default.</p><pre><code>callback(new Error('failure')) // to return errorcallback(null, 'success msg') // to return ok</code></pre><p>Complete documentation can be found at <a href=""http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-handler.html"" rel=""noreferrer"">http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-handler.html</a></p>",473467,345014,2017-01-31T20:30:08.497,2017-01-31T20:30:08.497,7,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28455874
188,28485802,2,28480220,2015-02-12T19:09:58.857,1,"<p><code>AmazonS3Client</code> is from the version 1 of the AWS Mobile SDK for iOS, which has been deprecated. You should use <code>AWSS3TrasnferManager</code> or <code>AWSS3</code> in the version 2 of the SDK instead. You can take a look at <a href=""http://docs.aws.amazon.com/mobile/sdkforios/developerguide/"" rel=""nofollow"">AWS Mobile SDK Guide</a> and the S3TransferManager Sample at our <a href=""https://github.com/awslabs/aws-sdk-ios-samples"" rel=""nofollow"">GitHub repo</a> for further details.</p>",1721492,,,2015-02-12T19:09:58.857,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28485802
189,28579201,2,28578079,2015-02-18T08:38:57.887,1,"<p>AWS SDK for iOS is depreciated now; so I believe the documentation link also must have been taken out.</p><blockquote>  <p>Version 1 of the AWS Mobile SDK is deprecated as of September 29, 2014  and will continue to be available until December 31, 2014. If you are  building new apps, we recommend you use Version 2. If you are working  on existing apps that use Version 1 (1.7.x or lower) of the AWS Mobile  SDK, you can download v1 for Android here and iOS here. The API  reference guides are included in the respective downloads. Apps built  using Version 1 will continue to function after December 31, 2014.  However, we highly recommend that you update your apps to the latest  version so you can take advantage of the latest features and bug  fixes.  <strong>Source</strong> : <a href=""http://aws.amazon.com/mobile/sdk/"" rel=""nofollow"">http://aws.amazon.com/mobile/sdk/</a></p></blockquote><p>I managed to find a sample code from AWS Mobile Blog [<a href=""http://mobile.awsblog.com/post/Tx15F6J3B8B4YKK/Creating-Mobile-Apps-with-Dynamic-Content-Stored-in-Amazon-S3]"" rel=""nofollow"">http://mobile.awsblog.com/post/Tx15F6J3B8B4YKK/Creating-Mobile-Apps-with-Dynamic-Content-Stored-in-Amazon-S3]</a> to get the S3 object, you can extrapolate from there.</p><pre><code>-(void)getRemoteImage:(AmazonS3Client*)s3  withName:(NSString*)imageName  fromBucket:(NSString*)bucketName{  S3GetObjectRequest *request =   [[S3GetObjectRequest alloc] initWithKey:imageName withBucket:bucketName];  S3GetObjectResponse *response = [s3 getObject:request];  [self storeImageLocally:response.body withName:imageName];}</code></pre><p>Download Link for v1 iOS SDK : <a href=""http://sdk-for-ios.amazonwebservices.com/aws-ios-sdk-1.7.1.zip"" rel=""nofollow"">http://sdk-for-ios.amazonwebservices.com/aws-ios-sdk-1.7.1.zip</a></p>",649408,,,2015-02-18T08:38:57.887,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28579201
190,28706771,2,28684565,2015-02-24T21:34:13.427,5,"<p><a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/model/QueryRequest.html"" rel=""nofollow""><code>com.amazonaws.services.dynamodbv2.model.QueryRequest</code></a> is not deprecated, and it doesn't look like it is targeted to be. The <code>com.amazonaws.services.dynamodb.model.QueryRequest</code> (notice no v2) was deprecated and, I believe, removed in the 1.9 AWS Java SDK release.</p><p>DynamoDB released another library building on top of the low-level Java client called the <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/document/DynamoDB.html"" rel=""nofollow"">Document API</a>. AWS Developer Blog published a blog explaining its usage (<a href=""http://java.awsblog.com/post/Tx1DDAWQGXWITSG/Introducing-DynamoDB-Document-API-Part-1"" rel=""nofollow"">Part 1</a> and <a href=""http://java.awsblog.com/post/Tx3C0ANZBMQY1RL/Introducing-DynamoDB-Document-API-Part-2"" rel=""nofollow"">Part 2</a>) and also mentions the reasons behind it, but it essentially makes code both easier to write and easier to read. Quoting from <a href=""http://java.awsblog.com/post/Tx1DDAWQGXWITSG/Introducing-DynamoDB-Document-API-Part-1"" rel=""nofollow"">Part 1</a>:</p><blockquote>  <p>As you see, the new Document API allows the direct use of plain old  Java data types and has less boilerplate.  In fact, the Dynamo  Document API can be used to entirely subsume what you can do with the  low level client (i.e. AmazonDynamoDBClient) but with a much cleaner  programming model and less code.</p>   <p>As you can see, saving JSON as a structured document in <a href=""https://aws.amazon.com/dynamodb/"" rel=""nofollow"">Amazon  DynamoDB</a>, or updating, retrieving and converting the document back  into JSON is as easy as 1-2-3. :)  You can find more examples in the  <a href=""https://github.com/aws/aws-sdk-java/tree/master/src/samples/AmazonDynamoDBDocumentAPI/quick-start/com/amazonaws/services/dynamodbv2/document/quickstart"" rel=""nofollow"">A-Z Document API quick-start folder</a> at GitHub. Happy coding until  next time!</p></blockquote><p>It is mostly up to how you want to write your code, but I would say that using the Document API is much easier to read and write than using the low-level Java client.</p>",627727,627727,2015-02-25T02:12:43.400,2015-02-25T02:12:43.400,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28706771
191,28781862,2,28736850,2015-02-28T12:46:21.990,1,"<p>Finally i was able to set the bootstrap option:</p><pre><code>Set Replication Factor  s3://elasticmapreduce/bootstrap-actions/configure-hadoop  -h, dfs.replication=2Set Block Size  s3://elasticmapreduce/bootstrap-actions/configure-hadoop  -h, dfs.block.size=67108864</code></pre><p>-s option is deprecated. This URL tell about the option to use while creating bootstrap option:<a href=""http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-bootstrap.html#PredefinedbootstrapActions_ConfigureHadoop"" rel=""nofollow"">http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-bootstrap.html#PredefinedbootstrapActions_ConfigureHadoop</a></p>",2163943,,,2015-02-28T12:46:21.990,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28781862
192,28841565,2,28151518,2015-03-03T20:46:57.033,1,"<p>I recommend two things.</p><p>First, use instance fields and methods instead of static fields and methods. You might have a race condition because the responses are not coming in as you expect, and you're closing streams in one invocation of <code>registerInvoice</code> and then trying to read from them in another invocation. So, your class would look like</p><pre><code>public class BPXProxy {  private HttpClient client;  public void init(){  client = new HttpClient();  }  public InvoiceCreationInfo registerInvoice(InvoiceData invoice) throws SQLException, HttpException, IOException {  JSONObject invoiceJSON = new JSONObject();  invoiceJSON.put(""invoicer"", invoice.getInvoicerIdentification().getDocument());  invoiceJSON.put(""buyer"", invoice.getBuyerIdentification().getDocument());  String serviceURL = ConfigurationManager.getBPXServicesPath() + ""Invoice?operation=registerExternalInvoice&amp;branchCode=""+ branchCode;  PostMethod updatePage = new PostMethod(serviceURL);  updatePage.addParameter(""invoice"", invoiceJSON.toJSONString());  updatePage.addParameter(""invoiceSubject"", invoice.getInvoiceSubject());  client.executeMethod(updatePage); // java.io.IOException: Stream closed, java.net.SocketException: Socket closed  String response = updatePage.getResponseBodyAsString();  // java.io.IOException: chunked stream ended unexpectedly, java.io.IOException: CRLF expected at end of chunk: -1/-1  updatePage.releaseConnection();  JSONObject jsonResponse = JSONSimpleHelper.parseJSON(response);  return jsonResponse;  }}</code></pre><p>and you would invoke it like</p><pre><code>BPXProxy instance = new BPXProxy();instance.init();InvoiceCreationInfo info = instance.registerInvoice(invoice);</code></pre><p>Second, it looks like you're using Apache HttpClient 3.x, which is severely outdated. Upgrade to version 4.3 (or later if it exists). You'll have to change around your code, but based on the <a href=""https://hc.apache.org/httpcomponents-client-ga/quickstart.html"" rel=""nofollow"">quick start</a>, it shouldn't be too drastic:</p><pre><code>public class BPXProxy {  private CloseableHttpClient client = HttpClients.createDefault();  public InvoiceCreationInfo registerInvoice(InvoiceData invoice) throws SQLException, HttpException, IOException {  JSONObject invoiceJSON = new JSONObject();  invoiceJSON.put(""invoicer"", invoice.getInvoicerIdentification().getDocument());  invoiceJSON.put(""buyer"", invoice.getBuyerIdentification().getDocument());  String serviceURL = ConfigurationManager.getBPXServicesPath() + ""Invoice?operation=registerExternalInvoice&amp;branchCode=""+ branchCode;  HttpPost httpPost = new HttpPost(serviceURL);  List &lt;NameValuePair&gt; nvps = new ArrayList &lt;NameValuePair&gt;();  nvps.add(""invoice"", invoiceJSON.toJSONString());  nvps.add(""invoiceSubject"", invoice.getInvoiceSubject());  client.setEntity(new UrlEncodedFormEntity(nvps));  CloseableHttpResponse rsp = client.execute(httpPost);  try {  HttpEntity entity = rsp.getEntity();  String rspJson = EntityUtils.toString(entity);  EntityUtils.consume(entity);  return JSONSimpleHelper.parseJSON(rspJson);  } finally {  rsp.close();  }  }}</code></pre><p>(Warning: I have not tried to compile or test this code.)</p>",2657036,,,2015-03-03T20:46:57.033,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/28841565
193,29171876,2,14693852,2015-03-20T16:54:24.030,16,"<blockquote>  <p><strong>EDIT: While I love this answer, it is now <em>very old</em>.</strong> AWS has come up with new services (like <a href=""https://aws.amazon.com/certificate-manager"" rel=""nofollow noreferrer"">Certificate Manager</a>) that make part of this answer obsolete. Additionally, using the <code>.ebextensions</code> folder with Apache is a cleaner way to handle this redirect as explained above.</p></blockquote><p>If you are hosting your website on S3, parts of this answer may still be useful to you.</p><hr><p>This worked for me:</p><ol><li><p>Upload the certificate to AWS using the <a href=""http://aws.amazon.com/cli"" rel=""nofollow noreferrer""><code>aws</code></a> console command. The command structure is:</p><pre><code>aws iam upload-server-certificate --server-certificate-name CERTIFICATE_NAME --certificate-body ""file://PATH_TO_CERTIFICATE.crt"" --private-key ""file://YOUR_PRIVATE_KEY.pem"" --certificate-chain ""file://YOUR_CERTIFICATE_CHAIN.ca-bundle"" --path /cloudfront/</code></pre></li><li><p>In your Elastic Beanstalk application, go to <strong>Configuration</strong> -> <strong>Network Tier</strong> -> <strong>Load Balancing</strong> and click the <strong>gear icon</strong>.</p></li><li><p>Select <strong>Secure listener port</strong> as <strong>443</strong>. Select <strong>Protocol</strong> as <strong>HTTPS</strong>. Select the <code>CERTIFICATE_NAME</code> from <strong>step 2</strong> for <strong>SSL certificate ID</strong>. Save the configuration.</p></li><li><p>Go to your <strong>Console</strong>. Click <strong>EC2 Instances</strong>. Click <strong>Load Balancers</strong>. Click through the load balancers. Click <strong>Instances</strong> and scroll down to see the EC2 instances assigned to that load balancer. If the EC2 instance has the same name as your Application URL (or something close), take note of the <strong>DNS Name</strong> for the load balancer. It should be in the format <code>awseb-e-...</code></p></li><li><p>Go back to your <strong>Console</strong>. Click <strong>CloudFront</strong>. Click <strong>Create Distribution</strong>. Select a <strong>Web</strong> distribution.</p></li><li><p>Set up the distribution. Set your <strong>Origin Domain Name</strong> to the load balancer DNS name you found in <strong>step 5</strong>. Set the <strong>Viewer Protocol Policy</strong> to <strong>Redirect HTTP to HTTPS</strong>. Set <strong>Forward Query Strings</strong> to <strong>Yes</strong>.   Set <strong>Alternate Domain Names (CNAMEs)</strong> to the URL(s) you want to use for your application. Set <strong>SSL Certificate</strong> to the <code>CERTIFICATE_NAME</code> you uploaded in <strong>step 2</strong>. Create your distribution.</p></li><li><p>Click on your distribution name in CloudFront. Click <strong>Origins</strong>, select your origin, and click <strong>Edit</strong>. Ensure your <strong>Origin Protocol Policy</strong> is <strong>Match Viewer</strong>. Go back. Click <strong>Behaviors</strong>, select your origin, and click <strong>Edit</strong>. Change <strong>Forward Headers</strong> to <strong>Whitelist</strong> and add <strong>Host</strong>. Save.</p></li></ol><p><strong>Note:</strong> <a href=""https://stackoverflow.com/questions/19967788/laravel-redirect-all-requests-to-https/29171680#29171680"">I wrote a longer guide as well</a>.</p>",4060460,-1,2017-05-23T12:26:43.657,2016-11-11T23:48:31.970,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29171876
194,29185978,2,28784528,2015-03-21T17:49:31.403,2,"<p>At the bottom of <a href=""http://arxiv.org/help/bulk_data_s3"" rel=""nofollow"">this page</a> arXiv explains that s3cmd gets denied because it does not support access to requester pays bucket as a non-owner and you have to apply a patch to the source code of s3cmd.  However, the version of s3cmd they used is outdated and the patch does not apply to the latest version of s3cmd.</p><p>Basically you need to allow s3cmd to add ""x-amz-request-payer"" header to its HTTP request to buckets.  Here is how to fix it:</p><ol><li>Download the source code of s3cmd.</li><li>Open S3/S3.py with a text editor.</li><li><p>Add this two lines of code at the bottom of <code>__init__</code> function:</p><pre><code>if self.s3.config.extra_headers:  self.headers.update(self.s3.config.extra_headers)</code></pre></li><li>Install s3cmd as instructed.</li></ol>",1441311,,,2015-03-21T17:49:31.403,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29185978
195,29280730,2,10435334,2015-03-26T14:13:52.110,211,"<p>There are now 3 ways to get this done: <strong>via the AWS Console</strong>, <strong>via the command line</strong>, or <strong>via the s3cmd command line tool</strong>.</p><hr /><h2>AWS Console Instructions</h2><p>This is now the recommended solution.  It is straight forward, but it can take some time.</p><ul><li>Log in to AWS Management Console</li><li>Go into S3 bucket</li><li>Select all files by route</li><li>Choose &quot;More&quot; from the menu</li><li>Select &quot;Change metadata&quot;</li><li>In the &quot;Key&quot; field, select &quot;Cache-Control&quot; from the drop down menumax-age=604800Enter (7 days) for Value</li><li>Press &quot;Save&quot; button</li></ul><p>(<a href=""https://stackoverflow.com/questions/10435334/set-cache-control-for-entire-s3-bucket-automatically-using-bucket-policies/47072736#47072736"">thanks to @biplob - please give him some love below</a>)</p><hr /><h2>AWS Command Line Solution</h2><p>Originally, when I created this bucket policies were a no go, so I figured how to do it using aws-cli, and it is pretty slick.  When researching I couldn't find any examples in the wild, so I thought I would post some of my solutions to help those in need.</p><p><strong>NOTE: By default, aws-cli only copies a file's current metadata, EVEN IF YOU SPECIFY NEW METADATA.</strong></p><p>To use the metadata that is specified on the command line, you need to add the '--metadata-directive REPLACE' flag.  Here are a some examples.</p><p><em>For a single file</em></p><pre><code>aws s3 cp s3://mybucket/file.txt s3://mybucket/file.txt --metadata-directive REPLACE \--expires 2034-01-01T00:00:00Z --acl public-read --cache-control max-age=2592000,public</code></pre><p><em>For an entire bucket (note --recursive flag):</em></p><pre><code>aws s3 cp s3://mybucket/ s3://mybucket/ --recursive --metadata-directive REPLACE \--expires 2034-01-01T00:00:00Z --acl public-read --cache-control max-age=2592000,public</code></pre><p>A little gotcha I found, if you only want to apply it to a specific file type, you need to exclude all the files, then include the ones you want.</p><p><em>Only jpgs and pngs:</em></p><pre><code>aws s3 cp s3://mybucket/ s3://mybucket/ --exclude &quot;*&quot; --include &quot;*.jpg&quot; --include &quot;*.png&quot; \--recursive --metadata-directive REPLACE --expires 2034-01-01T00:00:00Z --acl public-read \--cache-control max-age=2592000,public</code></pre><p>Here are some links to the manual if you need more info:</p><ul><li><a href=""http://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html"" rel=""noreferrer"">http://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html</a></li><li><a href=""http://docs.aws.amazon.com/cli/latest/reference/s3/cp.html#options"" rel=""noreferrer"">http://docs.aws.amazon.com/cli/latest/reference/s3/cp.html#options</a></li></ul><p><strong>Known Issues:</strong></p><pre><code>&quot;Unknown options: --metadata-directive, REPLACE&quot;</code></pre><p>this can be caused by an out of date awscli - see <a href=""https://stackoverflow.com/a/40075537/1991663"">@eliotRosewater's answer below</a></p><hr /><h2>S3cmd tool</h2><p>S3cmd is a &quot;Command line tool for managing Amazon S3 and CloudFront services&quot;.  While this solution requires a git pull it might be a simpler and more comprehensive solution.</p><p>For full instructions, <a href=""https://stackoverflow.com/a/35202296/1991663"">see @ashishyadaveee11's post below</a></p><hr /><p>Hope it helps!</p>",1991663,-1,2020-06-20T09:12:55.060,2018-12-18T12:47:35.237,10,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/29280730
196,29303152,2,28861620,2015-03-27T14:31:16.853,5,"<p>In the code, ""Total time spent by all maps in occupied slots (ms)"" is represented by the enum SLOTS_MILLIS_MAPS (or SLOTS_MILLIS_REDUCES) in JobCounter.java.  Those constants are deprecated.  They get their numbers by multiplying the task duration by the ratio of the MB used for the map task vs the minimum MB needed for one yarn slot.</p><p>So, if your map task used 4 MB and the minimum slot size is 1 MB, then your task took 4*duration of time that could have been used for other tasks.  That would explain why you see different ratios for different setups.  I don't find that metric to be particularly useful (especially since it isn't clear what it even means without diving into the code).</p>",971723,,,2015-03-27T14:31:16.853,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29303152
197,29305249,2,28029134,2015-03-27T16:12:00.300,3,"<p>@Ashrith's answer worked for me with one modification: I had to use <code>$HADOOP_PREFIX</code> rather than <code>$HADOOP_HOME</code> when running v2.6 on Ubuntu. Perhaps this is because it sounds like <code>$HADOOP_HOME</code> is being <a href=""https://stackoverflow.com/questions/9286983/hadoop-home-is-deprecated"">deprecated</a>?</p><p><code>export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:${HADOOP_PREFIX}/share/hadoop/tools/lib/*</code></p><p>Having said that, neither worked for me on my Mac with v2.6 installed via Homebrew. In that case, I'm using this extremely cludgy export:</p><p><code>export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$(brew --prefix hadoop)/libexec/share/hadoop/tools/lib/*</code></p>",612166,-1,2017-05-23T12:34:44.037,2016-03-11T01:05:42.923,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29305249
198,29355052,2,29354208,2015-03-30T19:53:38.800,0,"<p>Seems to be that S3 is deprecated in favor of s3api, this worked</p><pre><code>aws s3api get-object --bucket bucketname-vagrant --key or_vagrant.sql.tar.gz or_vagrant.sql.tar.gz</code></pre>",411141,,,2015-03-30T19:53:38.800,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29355052
199,29358202,2,29354208,2015-03-30T23:39:22.587,11,"<p><code>s3</code> is not deprecated. <a href=""http://docs.aws.amazon.com/cli/latest/userguide/cli-s3.html"" rel=""noreferrer""><code>s3</code> and <code>s3api</code> are on different tiers</a>. <code>s3api</code> is the API-level, while <code>s3</code> has the high-level commands.</p><h1>ls</h1><p>The problem is that you have a typo in <code>s3://</code> in your first command.</p><pre><code>$ aws s3 ls s://bucketname-vagrantA client error (NoSuchBucket) occurred when calling the ListObjects operation: The specified bucket does not exist</code></pre><p>I can replicate that error with my own bucket. This works:</p><pre><code>$ aws s3 ls s://bucketname-vagrant</code></pre><h1>cp</h1><pre><code>$ aws s3 cp bucketname-vagrant/or_vagrant.sql.xz /tmp/</code></pre><p>The problem here is that aws-cli doesn't know if you have a local directory named <code>bucketname-vagrant</code> or not. You can fix that by using the <code>s3://</code> syntax:</p><pre><code>$ aws s3 cp s3://bucketname-vagrant/or_vagrant.sql.xz /tmp/</code></pre><p>Again, I replicated that locally.</p><pre><code>$ aws s3 cp bucket/test.txt /tmp/usage: aws s3 cp &lt;LocalPath&gt; &lt;S3Path&gt; or &lt;S3Path&gt; &lt;LocalPath&gt; or &lt;S3Path&gt; &lt;S3Path&gt;Error: Invalid argument type$ aws s3 cp s3://bucket/test.txt /tmp/download: s3://bucket/test.txt to /tmp/keybase.txt</code></pre>",659298,,,2015-03-30T23:39:22.587,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29358202
200,29391974,2,17931073,2015-04-01T13:21:37.007,7,"<p>Bear in mind, I've only experimented with MongoDB...</p><p>From what I've read, DynamoDB has come a long way in terms of features. It used to be a super-basic key-value store with extremely limited storage and querying capabilities. It has since grown, now supporting <a href=""https://gigaom.com/2014/10/08/amazon-web-services-adds-json-support-to-dynamodb/"">bigger document sizes + JSON support</a> and <a href=""http://www.infoq.com/news/2015/02/dynamodb-online-indexing"">global secondary indices</a>. The gap between what DynamoDB and MongoDB offers in terms of features grows smaller with every month. The new features of DynamoDB are expanded on <a href=""http://www.allthingsdistributed.com/2014/10/document-model-dynamodb.html"">here</a>.</p><p>Much of the MongoDB vs. DynamoDB comparisons are out of date due to the recent addition of DynamoDB features. However, <a href=""https://www.linkedin.com/pulse/20140612181108-4788696-5-reasons-why-dynamodb-is-better-than-mongodb"">this post</a> offers some other convincing points to choose DynamoDB, namely that it's simple, low maintenance, and often low cost. <a href=""http://news.dice.com/2013/02/21/why-my-team-went-with-dynamodb-over-mongodb/"">Another discussion here</a> of database choices was interesting to read, though slightly old. </p><p>My takeaway: if you're doing serious database queries or working in languages not supported by DynamoDB, use MongoDB. Otherwise, stick with DynamoDB.</p>",2672869,2672869,2015-04-01T13:30:51.860,2015-04-01T13:30:51.860,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29391974
201,29416167,2,29352879,2015-04-02T15:00:26.997,0,"<p>For everyone that have any problems setup the river-plugin for elastic search, I've contacted the guy who maintain the plugin and he've written a step-by-step setup tutorial on linux <a href=""https://github.com/jprante/elasticsearch-river-jdbc/wiki/JDBC-plugin-feeder-mode-as-an-alternative-to-the-deprecated-Elasticsearch-River-API"" rel=""nofollow"">here</a></p><p>I hope that is help anyone </p>",525287,,,2015-04-02T15:00:26.997,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29416167
202,29436189,2,5187496,2015-04-03T16:39:59.150,1,"<p>Tools recommend by other answers are out of date.</p><p>This one is up to date: <a href=""https://github.com/schickling/git-s3"" rel=""nofollow"">https://github.com/schickling/git-s3</a></p>",938380,,,2015-04-03T16:39:59.150,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29436189
203,29517794,2,29502907,2015-04-08T14:39:00.303,0,"<p>It is possibly because WebFaction as far as I know still uses an out of date mod_wsgi and possibly even Apache 2.2. There is an issue with large file uploads if they are trickled in very slowly in small chunks. Use the latest mod_wsgi version and preferably Apache 2.4, which has other fixes in it related to memory usage and you may see better results.</p>",128141,,,2015-04-08T14:39:00.303,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29517794
204,29553199,2,18968387,2015-04-10T04:06:47.670,0,"<p>The accepted answer no longer works, its outdated. This worked for me:</p><pre><code># Install Oracle JDKrpm --erase --nodeps java-1.6.0-openjdk java-1.6.0-openjdk-develrpm -Uvh .ebextensions/jdk-6u45-linux-amd64.rpm/usr/sbin/alternatives --install /usr/bin/java java /usr/java/default/bin/java 3/usr/sbin/alternatives --set java /usr/java/default/bin/java/usr/sbin/alternatives --install /usr/bin/java_sdk java_sdk /usr/java/default/bin/java 3/usr/sbin/alternatives --set java_sdk /usr/java/default/bin/java</code></pre><p>This is for java 6, since I needed it to be. Also, the jdk downloaded from oracle is actually a bin file now (oracle's custom sh script extractor), so what I did is I downloaded the bin file from oracle, extracted it to get the RPM, and then included the RPM inside the ebextensions.</p><p>Just include that sh script to run in an ebextensions config file (google ebextenions config if you're unsure).</p><p>Hope this helps somebody.</p>",219843,,,2015-04-10T04:06:47.670,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29553199
205,29685453,2,12096241,2015-04-16T20:35:19.193,3,"<pre><code>instances = get_only_instances(instance_ids=['i-12345678'])</code></pre><p>Regarding the above answer using </p><pre><code>get_all_instances()</code></pre><p>, from the <a href=""http://boto.readthedocs.org/en/latest/ref/ec2.html#boto.ec2.connection.EC2Connection.get_all_instances"" rel=""nofollow"">BOTO API </a>--</p><pre><code>get_all_instances() is deprecated in favor of get_all_reservations(). A future major release will change get_all_instances() to return a list of boto.ec2.instance.Instance objects as its name suggests. To obtain that behavior today, use get_only_instances().</code></pre>",1236537,,,2015-04-16T20:35:19.193,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29685453
206,29774677,2,29752988,2015-04-21T14:10:55.390,2,"<p>sorry to hear you are having this issue. We added a check in excon for openssl version because openssl had a release with a bug in it (so we set it up to give a better warning/instruction when you hit that bug to update). We didn't realize that this constant might not be defined in some cases. I've updated the code to skip the check when the constant is undefined and released excon 0.45.3, if you update to that it should avoid this issue.</p><p>All that said, I think it is also worth noting that the openssl version you have there is quite outdated (though it maybe is not obviously so, due to backported security fixes). It would probably be a good idea to bump up to something in the 1.0.1 or even 1.0.2 series (probably using a package manager, I use homebrew for this). This might also fix your issue, but regardless is probably a good idea.</p><p>Hope that helps!</p>",967276,,,2015-04-21T14:10:55.390,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29774677
207,29787610,2,29785925,2015-04-22T04:11:11.093,5,"<p>Your case is a typical Time Series Data scenario where your records become obsolete as the time goes by. There are two main factors you need to be careful about:</p><ul><li>Make sure your tables have even access patterns</li></ul><p>If you put all your posts in a single table and the most recent ones are accessed more frequently, your provisioned throughput will not be used efficiently. You should group the most accessed items in a single table so the provisioned throughput can be properly adjusted for the required access. Additionally, make sure you properly define a <a href=""https://stackoverflow.com/questions/29501670/dynamodb-when-to-use-what-pk-type"">Hash Key that will allow even distribution of your data across multiple partitions</a>. </p><ul><li>The obsolete data is deleted with the most efficient way (effort, performance and cost wise)</li></ul><p>The documentation suggests segmenting the data in different tables so you can delete or backup the entire table once the records become obsolete (see more details below).</p><p>For example, You could have your tables segmented by month:</p><pre><code>Posts_April, Posts_May, etc</code></pre><p>Or by Count, each table containing a max number of records:</p><pre><code>Posts_1, Posts_2, Posts_3, etc</code></pre><p>On this case you create a new table once the current one has reached the max number of records, and delete/backup the oldest one when you need to do the cleanup.</p><p>I might need some additional information about your use cases to give you better examples on how you can take advantage of this approach.</p><p>Find below some references to the operations that you will need to programmatically create and delete tables:</p><p><strong>Create Table</strong><a href=""http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateTable.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateTable.html</a></p><p><strong>Delete Table</strong><a href=""http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DeleteTable.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DeleteTable.html</a></p><p>Here is the section from the documentation that explains best practices related to Time Series Data: </p><blockquote>  <p><strong>Understand Access Patterns for Time Series Data</strong></p>   <p>For each table that you create, you specify the throughput  requirements. DynamoDB allocates and reserves resources to handle your  throughput requirements with sustained low latency. When you design  your application and tables, you should consider your application's  access pattern to make the most efficient use of your table's  resources.</p>   <p>Suppose you design a table to track customer behavior on your site,  such as URLs that they click. You might design the table with hash and  range type primary key with Customer ID as the hash attribute and  date/time as the range attribute. In this application, customer data  grows indefinitely over time; however, the applications might show  uneven access pattern across all the items in the table where the  latest customer data is more relevant and your application might  access the latest items more frequently and as time passes these items  are less accessed, eventually the older items are rarely accessed. If  this is a known access pattern, you could take it into consideration  when designing your table schema. Instead of storing all items in a  single table, you could use multiple tables to store these items. For  example, you could create tables to store monthly or weekly data. For  the table storing data from the latest month or week, where data  access rate is high, request higher throughput and for tables storing  older data, you could dial down the throughput and save on resources.</p>   <p>You can save on resources by storing ""hot"" items in one table with  higher throughput settings, and ""cold"" items in another table with  lower throughput settings. You can remove old items by simply deleting  the tables. You can optionally backup these tables to other storage  options such as Amazon Simple Storage Service (Amazon S3). <strong>Deleting an  entire table is significantly more efficient than removing items  one-by-one, which essentially doubles the write throughput as you do  as many delete operations as put operations.</strong></p></blockquote><p>Source:<a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html#GuidelinesForTables.TimeSeriesDataAccessPatterns"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html#GuidelinesForTables.TimeSeriesDataAccessPatterns</a></p><p>UPDATED ANSWER BASED ON ADDITIONAL COMMENTS:</p><p><em>""So user ID will be my hash key. What I need is clean up procedure... So obviously based on date separate table approach will not gonna work as data is not filtered by timeframe but by count. In other works I need to have x amount of recent records per each user. And to keep it growing beyond that x amount I need to have clean up process.""</em></p><p>On this case you can pretty much define the <code>Hash Key</code> as the <code>UserId</code>  and the <code>PostId</code> as the <code>Range Key</code>. </p><p>If each User can have a maximum of 10 Posts then the <code>Range Key</code> maximum value would be 10. As you reach the maximum number and the User adds a new post, you start over from 1 automatically replacing the oldest Post from that User (see DynamoDB <code>PutItem</code> operation for more details). In the end you are just creating a circular post list per user.</p><p>By doing that you are essentially adding the new post AND instantly executing the clean up process with a single write operation.</p><p>You might need to create a support table containing the last <code>PostId</code> published by each <code>User</code>. If you choose to have only a Hash Key defined as the <code>UserId</code>, you will be able to lookup the last PostId for a specific user using the <code>GetItem</code> operation (which is pretty cheap and fast). The schema for this table could be something as simple as:</p><p><code>UserId</code> (<code>Hash Key</code>)</p><p><code>LastPostId</code> (Number Attribute) - NOT A RANGE KEY</p><p>As an example, say that you need to get the last three most recent posts from <code>UserId</code> <code>= ABC</code>:</p><p><strong>Step1.</strong> Use <code>GetItem</code> on <code>LastPostIds_Table</code> providing UserId <code>(Hash Key) = ""ABC""</code></p><p>if <code>LastPostId = 4</code> then</p><p><strong>Step2.</strong> Use <code>BatchGetItem</code> on <code>Posts_Table</code> to get the records with <code>UserId (Hash Key) = ""ABC""</code> and <code>PostId (Range Key) = 4, 3 and 2</code>.</p><p>From the returned <code>PostId</code>s you will know that 4 is the most recent one and 2 the oldest. </p><p><strong>WARNING</strong>: The use of <code>BatchGetItem</code> to return many records <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/QueryAndScanGuidelines.html#QueryAndScanGuidelines.BurstsOfActivity"" rel=""nofollow noreferrer"">may cause sudden bursts of reading activity</a>. This issue is easily resolved by simply breaking the read operation into several smaller batches.</p><p>The <code>PutItem</code> can be helpful to implement the Post persistence logic:</p><blockquote>  <p><strong>PutItem</strong>  Creates a new item, or replaces an old item with a new item. If an  item that has the same primary key as the new item already exists in  the specified table, the new item completely replaces the existing  item. You can perform a conditional put operation (add a new item if  one with the specified primary key doesn't exist), or replace an  existing item if it has certain attribute values.</p></blockquote><p>Source: <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html</a></p>",3183795,-1,2017-05-23T12:14:24.863,2015-04-23T05:06:13.550,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29787610
208,29836295,2,29772285,2015-04-23T23:44:01.203,0,"<p>It seems that AMI deployment of MongoDB has been deprecated and replaced by <a href=""https://docs.mms.mongodb.com/tutorial/nav/automation/"" rel=""nofollow"">MMS Automation</a>; however it is stated that </p><blockquote>  <p>MMS Automation is currently in Beta with a limited initial user group</p></blockquote><p>(so folks at MongoDB, before deprecating one method please try to make sure that replacement is actually available).</p><p>That said; it seems that MMS provides quite a few nice features:interface for provisioning machines, configuring MongoDB nodes and clusters, and upgrading your MongoDB deployment.</p><p>However it follows a freemium model (8 free servers; 1GB free per replica set; see <a href=""https://mms.mongodb.com/#pricing"" rel=""nofollow"">details</a>).  Providing only 1GB free for back up (on your own ec2/aws hardware) doesn't seem quite right ... </p><p>More details are available at <a href=""https://mms.mongodb.com/"" rel=""nofollow"">MMS site</a>.</p>",2705777,,,2015-04-23T23:44:01.203,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29836295
209,29898270,2,29871162,2015-04-27T14:17:02.093,0,"<p>The problem turned out to be a database access issue. Specifically, the version of PHP on the new server turned out to not support deprecated calls to php-mysql APIs (e.g. @mysql_pconnect or @mysql_connect)</p><p>The solution was to</p><p>1) Download the mysql library which supports the legacy calls. In the case of AmazonLinux, this is accomplished as follows:</p><pre><code>sudo yum install php-mysql</code></pre><p>2) Find where the library was downloaded e.g.</p><pre><code>find / -name mysql.so</code></pre><p>This may have been shown in the install as well, but that is a way to find it anytime. YMMV, but in my case, the library name was:</p><pre><code>/usr/lib64/php-zts/modules/mysql.so</code></pre><p>3) Change php.ini (as shown in phpinfo when called from a php page on the httpd/apached server) as follows:</p><p>a) change ""extension_dir"" to show where the mysql.so is installed:</p><pre><code>extension_dir = ""/usr/lib64/php-zts/modules"" </code></pre><p>b) Then add the following lines:</p><pre><code>extension=mysqli.soextension=mysql.soextension=pdo_mysql.so</code></pre><p>Reboot apache and you should be good to go.</p><p>Note: the specific legacy call not supported, which is part of of the version of CodeIgnitor I'm working with, was this:</p><pre><code>   return @mysql_pconnect($this-&gt;hostname, $this-&gt;username, $this-// etc. </code></pre><p>The php error log showed this error:</p><pre><code> PHP Fatal error:  Call to undefined function mysql_connect() in xxx.php on line xx</code></pre>",64895,,,2015-04-27T14:17:02.093,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29898270
210,29987284,2,29951270,2015-05-01T12:59:04.847,7,"<blockquote>  <p><strong>Motivation Note:</strong> When using a Cloud Storage like DynamoDB we have to be aware of the Storage Model because that will directly impact  your performance, scalability, and financial costs. It is different  than working with a local database because you pay not only for the  data that you store but also the operations that you perform against  the data. Deleting a record is a WRITE operation for example, so if  you don't have an efficient plan for clean up (and your case being  Time Series Data specially needs one), you will pay the price. Your  Data Model will not show problems when dealing with small data volume  but can definitely ruin your plans when you need to scale. That being  said, decisions like creating (or not) an index, defining proper  attributes for your keys, creating table segmentation, and etc will  make the entire difference down the road. Choosing DynamoDB (or more  generically speaking, a Key-Value store) as any other architectural  decision comes with a trade-off, you need to clearly understand  certain concepts about the Storage Model to be able to use the tool  efficiently, choosing the right keys is indeed important but only the  tip of the iceberg. For example, if you overlook the fact that you are  dealing with Time Series Data, no matter what primary keys or index  you define, your provisioned throughput will not be optimized because  it is spread throughout your entire table (and its partitions) and NOT  ONLY THE DATA THAT IS FREQUENTLY ACCESSED, meaning that unused data is  directly impacting your throughput just because it is part of the same  table. This leads to cases where the  <code>ProvisionedThroughputExceededException</code> is thrown ""unexpectedly"" when  you know for sure that your provisioned throughput should be enough for your  demand, however, the TABLE PARTITION that is being unevenly accessed   has reached its limits (more details <a href=""https://aws.amazon.com/blogs/aws/optimizing-provisioned-throughput-in-amazon-dynamodb/"" rel=""nofollow noreferrer"">here</a>).</p></blockquote><p>The post below has more details, but I wanted to give you some motivation to read through it and understand that although you can certainly find an easier solution for now, it might mean starting from the scratch in the near future when you hit a wall (the ""wall"" might come as high financial costs, limitations on performance and scalability, or a combination of all).</p><p><strong>Q: Could I have a unique notification ID as the hash key and the user ID as the range key? Would that allow me to do lookups only by the range key, i.e. without providing the hash key?</strong></p><p><strong>A:</strong> DynamoDB is a Key-Value storage meaning that the most efficient queries use the entire Key (Hash or Hash-Range). Using the <code>Scan</code> operation to actually perform a query just because you don't have your Key is definitely a sign of deficiency in your Data Model in regards to your requirements. There are a few things to consider and many options to avoid this problem (more details below).</p><p>Now before moving on, I would suggest you reading this quick post to clearly understand the difference between Hash Key and Hash+Range Key:</p><p><a href=""https://stackoverflow.com/questions/29501670/dynamodb-when-to-use-what-pk-type"">DynamoDB: When to use what PK type?</a></p><p>Your case is a typical Time Series Data scenario where your records become obsolete as the time goes by. There are two main factors you need to be careful about:</p><ul><li>Make sure your tables have even access patterns</li></ul><p>If you put all your notifications in a single table and the most recent ones are accessed more frequently, your provisioned throughput will not be used efficiently. You should group the most accessed items in a single table so the provisioned throughput can be properly adjusted for the required access. Additionally, make sure you properly define a <a href=""https://stackoverflow.com/questions/29501670/dynamodb-when-to-use-what-pk-type"">Hash Key that will allow even distribution of your data across multiple partitions</a>. </p><ul><li>The obsolete data is deleted with the most efficient way (effort, performance and cost wise)</li></ul><p>The documentation suggests segmenting the data in different tables so you can delete or backup the entire table once the records become obsolete (see more details below).</p><p>Here is the section from the documentation that explains best practices related to Time Series Data: </p><blockquote>  <p><strong>Understand Access Patterns for Time Series Data</strong></p>   <p>For each table that you create, you specify the throughput  requirements. DynamoDB allocates and reserves resources to handle your  throughput requirements with sustained low latency. When you design  your application and tables, you should consider your application's  access pattern to make the most efficient use of your table's  resources.</p>   <p>Suppose you design a table to track customer behavior on your site,  such as URLs that they click. You might design the table with hash and  range type primary key with Customer ID as the hash attribute and  date/time as the range attribute. In this application, customer data  grows indefinitely over time; however, the applications might show  uneven access pattern across all the items in the table where the  latest customer data is more relevant and your application might  access the latest items more frequently and as time passes these items  are less accessed, eventually the older items are rarely accessed. If  this is a known access pattern, you could take it into consideration  when designing your table schema. Instead of storing all items in a  single table, you could use multiple tables to store these items. For  example, you could create tables to store monthly or weekly data. For  the table storing data from the latest month or week, where data  access rate is high, request higher throughput and for tables storing  older data, you could dial down the throughput and save on resources.</p>   <p>You can save on resources by storing ""hot"" items in one table with  higher throughput settings, and ""cold"" items in another table with  lower throughput settings. You can remove old items by simply deleting  the tables. You can optionally backup these tables to other storage  options such as Amazon Simple Storage Service (Amazon S3). <strong>Deleting an  entire table is significantly more efficient than removing items  one-by-one, which essentially doubles the write throughput as you do  as many delete operations as put operations.</strong></p>   <p>Source:</p>   <p><a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html#GuidelinesForTables.TimeSeriesDataAccessPatterns"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html#GuidelinesForTables.TimeSeriesDataAccessPatterns</a></p></blockquote><p>For example, You could have your tables segmented by month:</p><pre><code>Notifications_April, Notifications_May, etc</code></pre><p><strong>Q: I would like to be able to query for the most recent X notifications for a given user.</strong></p><p><strong>A:</strong> I would suggest using the <code>Query</code> operation and querying using only the <code>Hash Key</code> (<code>UserId</code>) having the <code>Range Key</code> to sort the notifications by the <code>Timestamp</code> (Date and Time).</p><pre><code>Hash Key: UserIdRange Key: Timestamp</code></pre><p><strong>Note:</strong> A better solution would be the <code>Hash Key</code> to not only have the <code>UserId</code> but also another concatenated information that you could calculate before querying to make sure your <code>Hash Key</code> grants you even access patterns to your data. For example, you can start to have <em>hot partitions</em> if notifications from specific users are more accessed than others... having an additional information in the <code>Hash Key</code> would mitigate this risk.</p><p><strong>Q: I'd like to fetch the number of unread notifications for a particular user.</strong></p><p><strong>A:</strong> Create a <code>Global Secondary Index</code> as a <em>Sparse Index</em> having the <code>UserId</code> as the <code>Hash Key</code> and <code>Unread</code> as the <code>Range Key</code>.</p><p>Example:</p><pre><code>Index Name: Notifications_April_UnreadHash Key: UserIdRange Key : Unuread</code></pre><p>When you query this index by Hash Key (UserId) you would automatically have all unread notifications with no unnecessary scans through notifications which are not relevant to this case. Keep in mind that the original Primary Key from the table is automatically projected into the index, so in case you need to get more information about the notification you can always resort to those attributes to perform a <code>GetItem</code> or <code>BatchGetItem</code> on the original table.</p><p><strong>Note:</strong> You can explore the idea of using different attributes other than the 'Unread' flag, the important thing is to keep in mind that a Sparse Index can help you on this Use Case (more details below).</p><p>Detailed Explanation:</p><p>I would have a sparse index to make sure that you can query a reduced dataset to do the count. In your case you can have an attribute ""unread"" to flag if the notification was read or not, and use that attribute to create the Sparse Index. When the user reads the notification you simply remove that attribute from the notification so it doesn't show up in the index anymore. Here are some guidelines from the documentation that clearly apply to your scenario:</p><blockquote>  <p><strong>Take Advantage of Sparse Indexes</strong> </p>   <p>For any item in a table, DynamoDB will only write a corresponding  index entry if the index range key  attribute value is present in the item. If the range key attribute  does not appear in every table item, the index is said to be sparse.  [...]</p>   <p>To track open orders, you can create an index on CustomerId (hash) and  IsOpen (range). Only those orders in the table with IsOpen defined  will appear in the index. Your application can then quickly and  efficiently find the orders that are still open by querying the index.  If you had thousands of orders, for example, but only a small number  that are open, the application can query the index and return the  OrderId of each open order. Your application will perform  significantly fewer reads than it would take to scan the entire  CustomerOrders table. [...]</p>   <p>Instead of writing an arbitrary value into the IsOpen attribute, you  can use a different attribute that will result in a useful sort order  in the index. To do this, you can create an OrderOpenDate attribute  and set it to the date on which the order was placed (and still delete  the attribute once the order is fulfilled), and create the OpenOrders  index with the schema CustomerId (hash) and OrderOpenDate (range).  This way when you query your index, the items will be returned in a  more useful sort order.[...]</p>   <p>Such a query can be very efficient, because the number of items in the  index will be significantly fewer than the number of items in the  table. In addition, the fewer table attributes you project into the  index, the fewer read capacity units you will consume from the index.</p>   <p>Source:  <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForGSI.html#GuidelinesForGSI.SparseIndexes"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForGSI.html#GuidelinesForGSI.SparseIndexes</a></p></blockquote><p>Find below some references to the operations that you will need to programmatically create and delete tables:</p><p><strong>Create Table</strong><a href=""http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateTable.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateTable.html</a></p><p><strong>Delete Table</strong><a href=""http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DeleteTable.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DeleteTable.html</a></p>",3183795,-1,2017-05-23T12:24:41.687,2015-05-01T22:00:02.383,6,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/29987284
211,30250483,2,30138094,2015-05-15T02:17:52.663,5,"<p>I'm answering my own question.  This is what I posted on AWS support forum as well:</p><p>You can't do this with the high level API -- AWSDynamoDBObjectMapper. When using AWSDynamoDBObjectMapper, you need to provide an AWSDynamoDBQueryExpression object to the query method to specify the query conditions. AWSDynamoDBQueryExpression doesn't give you the option to set filters(conditions) on non-key attributes. I wonder why this isn't supported! However, AWSDynamoDBScanExpression lets you specify conditions on non-key attributes when you use the scan method. But you don't want to scan when you actually mean a query.</p><p>Fortunately, you can do this using the low level API by directly calling query on AWSDynamoDB providing an AWSDynamoDBQueryInput which lets you specify a lot of low level parameters. AWSDynamoDBQueryInput lets you specify the filter conditions on non-key attributes using either queryFilter or filterExpression. queryFilter is deprecated, it's recommended to use filterExpression. Here are the two documents that helped me to figure this out:</p><p><a href=""http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html"" rel=""nofollow"">http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html</a><a href=""http://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSDynamoDBQueryInput.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSDynamoDBQueryInput.html</a></p><p>Here's a code example in swift.  In this code I'm filtering based on ""approved"" field that is a non-key attribute. recId is the primary key:</p><pre><code>  func getApprovedRecords(recId: Int) {  let dynamoDB = AWSDynamoDB.defaultDynamoDB()  var startKey = nil  var queryInput = AWSDynamoDBQueryInput()  queryInput.tableName = TABLE_NAME  queryInput.limit = QUERY_SIZE  queryInput.exclusiveStartKey = startKey  var recIdValue = AWSDynamoDBAttributeValue()  recIdValue.N = String(recId)  var recIdCondition = AWSDynamoDBCondition()  recIdCondition.comparisonOperator = AWSDynamoDBComparisonOperator.EQ  recIdCondition.attributeValueList = [recIdValue]  queryInput.keyConditions = [ ""recId""\"" : recIdCondition]  var oneValue = AWSDynamoDBAttributeValue()  oneValue.N = ""1""  queryInput.expressionAttributeValues = [ "":one"" : oneValue ]    queryInput.filterExpression = ""approved = :one""  dynamoDB.query(queryInput).continueWithBlock { (task: BFTask!) -&gt; AnyObject! in  if ((task.error) != nil) {  NSLog(""The request failed. Error: \(task.error)"")  }  if ((task.exception) != nil) {  NSLog(""The request failed. Exception: \(task.exception)"")  }  if ((task.result) != nil) {  NSLog(""The request  succeeded."")  let results = task.result as! AWSDynamoDBQueryOutput  for r in results.items {  // do whatever with the result  }  }  return nil  }  }</code></pre>",2732722,2732722,2015-06-12T20:26:58.193,2015-06-12T20:26:58.193,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30250483
212,30282155,2,30270319,2015-05-17T00:37:08.027,4,"<blockquote>  <p>Can I use HTTP Authorization method to achieve the same thing?</p></blockquote><p>Sometimes.  The key difference is that, as a developer, you don't always have enough control over the user agent to inject a header.  The most obvious example of this is a simple <code>GET</code> request launched by a web browser in response to the user clicking a link.  In that situation, you don't have the a ability to inject an <code>Authorization:</code> header for the browser to send ... so pre-signing the URL is all you can do.</p><p>Importantly, there's no information in a signed URL that is considered sensitive, so there's no particularly strong motivation to use the header instead of a signed URL.  Your AWS Access Key ID is not secret, and your AWS Secret can't be derived from the other elements and the signature in a computationally-feasible time frame, particularly if you use Signature Version 4, which you should.  Signature Version 2 is not officially deprecated in older regions, but newer S3 never supported it and likely never will.</p><p>When you do control the user agent, such as in back-end server code, adding the header may be preferable, because you don't need to do any manipulation of the URL string you already have in-hand.</p>",1695906,,,2015-05-17T00:37:08.027,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30282155
213,30416677,2,30399068,2015-05-23T19:21:24.243,0,"<p>A little more complicated, and assuming that you have just a few easily-identified chunks of ""too-big"" directory trees (such as <code>/opt</code> and <code>/var/log</code>) would be to create filesystem(s) on the unallocated 28GB, use rsync to copy your working files to the new filesystems, and wipe the obsolete files (to free up diskspace), mounting the new filesystems in their place (by editing <code>/etc/fstab</code>, of course).</p><p>If you do this for <code>/var/log</code>, you have to do a reboot immediately after migrating the files, since the system still references the original files.  Of course, you might want to practice the procedure on a test-machine.</p><p>It would be a lot simpler if Amazon's root volumes used LVM.</p>",4518274,,,2015-05-23T19:21:24.243,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30416677
214,30437647,2,30425447,2015-05-25T11:47:07.927,1,"<p>I think you're mixing up the Cloud Provisioner and the AWS module. </p><p>The <code>node_aws</code> command is from the <a href=""https://docs.puppetlabs.com/pe/latest/cloudprovisioner_aws.html"" rel=""nofollow"">Cloud Provisioner</a> (which is now deprecated).</p>",2417812,,,2015-05-25T11:47:07.927,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30437647
215,30489054,2,26814829,2015-05-27T17:12:48.177,1,"<p>Just a heads up for those that stumble upon this post. <code>AWSDynamoDBModeling</code> protocol has been changed in the latest SDK (v2.1.1). Required functions: <code>dynamoDBTableName</code> and <code>hashKeyAttribute</code> must be static. The documentation as of today (5/27/2015) appears to be out of date.</p><p>Example:</p><pre><code>class Dingle:AWSDynamoDBObjectModel, AWSDynamoDBModeling {  static func dynamoDBTableName() -&gt; String! {  return ""dev_coupons""  }  static func hashKeyAttribute() -&gt; String! {  return ""status ""  }  func rangeKeyAttribute() -&gt; String! {  return ""post_date""  }  override func isEqual(object: AnyObject?) -&gt; Bool {  return super.isEqual(object)  }}</code></pre>",56256,1402846,2015-12-28T09:32:54.830,2015-12-28T09:32:54.830,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30489054
216,30607159,2,30606751,2015-06-02T21:45:02.897,4,"<p>You create AMIs directly from EC2 instances, not from snapshots. Snapshots are for EBS volumes. Check that you created your AMI correctly from a running EC2 instance on which you have Apache/Tomcat installed and running (and configured to autostart on reboot).</p><p>No, you do not have to use Puppet/Chef or any other CM tool. You can do what you want in a couple of ways:</p><ol><li>The simplest way is to create an AMI from your runningEC2 instance and then configure your Auto Scaling Group to launchnew instances from that AMI based on some metric.</li><li>Use a base AMI without Apache/Tomcat or your software and then bootstrap new instances at launch time to download and configure everything needed.</li></ol><p>The disadvantage of #1 is that your AMIs will get out of date quickly. The disadvantage of #2 is that your instances will taken longer to come into service. I would recommend a combination of #1 and #2, specifically that you capture a new AMI every few months and that becomes your base AMI for launching and you update the instance at launch time via userdata init script.</p>",271415,,,2015-06-02T21:45:02.897,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30607159
217,30672766,2,29093644,2015-06-05T17:46:51.487,2,"<p>The basic advice would be not to resize images on-the-fly as this may take a while and your users may experience a huge response times during this operation. In case you have some predefined set of styles it would be wise to generate them in advance and just return back when required.</p><p>Well, here is what you could do if there is no other option.</p><pre><code>def download_from_s3 url_to_s3, filename  uri = URI(url_to_s3)  response = Net::HTTP.get_response(uri)  File.open(filename, 'wb'){|f| f.write(response.body)}end</code></pre><p>Here we basically downloaded an image located at a given URL and saved it as a file locally. Resizing may be done in a couple of different ways (it depends on whether you want to serve the downloaded file as a <code>Paperclip</code> attachment).The most common approach here would be to use <code>image-magick</code> and its <code>convert</code> command-line script. Here is an example of resizing an image to width of <code>30</code>:</p><pre><code>convert  -strip -geometry 30 -quality 100 -sharpen 1 '/photos/aws_images/000/000/015/original/index.jpg' '/photos/aws_images/000/000/015/original/S_30_WIDTH__q_100__index.jpg' 2&gt;&amp;1 &gt; /dev/null</code></pre><p>You can find documentation for <code>convert</code> <a href=""http://www.imagemagick.org/script/convert.php"" rel=""nofollow"">here</a>, it's suitable not only for image resizing, but also for converting between image formats, bluring, cropping and much more! Also you could be intrested in <a href=""https://github.com/drpentode/Attachment-on-the-Fly"" rel=""nofollow"">Attachment-on-the-Fly gem</a>, which seems a little bit outdated, but has some insights of how to resize images using <code>convert</code>.</p><p>The last step is to upload resized image to some <code>S3 bucket</code>. I assume that you've already got <code>aws-sdk</code> gem and <code>AWS::S3</code> instance (the docs can be found <a href=""http://www.rubydoc.info/gems/aws-sdk-v1/1.64.0/AWS/S3"" rel=""nofollow"">here</a>).</p><pre><code>def upload_to_s3 bucket_name, key, file  s3 = AWS::S3.new(:access_key_id =&gt; 'YOUR_ACCESS_KEY_ID', :secret_access_key =&gt; 'YOUR_SECRET_ACCESS_KEY')  bucket = s3.buckets[bucket_name]  obj = bucket.objects[key]  obj.write(File.open(file, 'rb'), :acl =&gt; :public_read)end</code></pre><p>So, here you obtain an <code>AWS::S3</code> object to communicate with <code>S3</code> server, provide your bucket name and desired key, and basically upload an image with an option to make it visible to everybody on the web. Note that there are lots of additional upload options (including file encryption, access permissions, metadata and much more).</p>",1276552,,,2015-06-05T17:46:51.487,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30672766
218,30809937,2,30802773,2015-06-12T18:29:57.290,2,"<p>You are not retaining a strong reference to the <code>transferManager</code> object in <code>- getImageData:</code>. Please remember that <code>- getObject:</code> is an asynchronous method, and it returns immediately. You need to retain a strong reference to the service client until the request finishes processing.</p><p>If you use the AWS Mobile SDK for iOS 2.1.2, Xcode should give you a compiler warning for the use of <code>- initWithConfiguration:</code>. The method was deprecated to mitigate the misuse of the API such as this case. Please use <code>+ defaultS3TransferManager</code> or <code>+ S3TransferManagerForKey:</code> to retrieve the <code>AWSS3TransferManager</code> object.</p><p>(Also, the log indicates you are using 2.0.17 instead of 2.1.2.)</p>",1721492,,,2015-06-12T18:29:57.290,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30809937
219,30866481,2,18941673,2015-06-16T11:38:31.367,16,"<ol><li><p>Publish a TXT record:</p><pre><code>""v=spf1 include:_spf.google.com include:amazonses.com ~all""</code></pre><p>Amazon SES <a href=""http://docs.aws.amazon.com/ses/latest/DeveloperGuide/spf.html"" rel=""noreferrer"">documentation</a> says that no additional SPF configuration is required for a domain, but it turns out that adding <code>include:amazonses.com</code> to the record makes <a href=""https://en.wikipedia.org/wiki/Sender_ID"" rel=""noreferrer"">Sender ID</a> pass as well. Even though Sender ID is <a href=""http://www.openspf.org/SPF_vs_Sender_ID"" rel=""noreferrer"">considered</a> <a href=""https://wordtothewise.com/2012/12/hotmail-moves-to-spf-authentication/"" rel=""noreferrer"">obsolete</a>, some receivers could implement it.</p><p>If Amazon SES is <a href=""http://docs.aws.amazon.com/ses/latest/DeveloperGuide/mail-from.html"" rel=""noreferrer"">configured</a> to use a custom MAIL-FROM subdomain, publish another TXT record for the subdomain:</p><pre><code>""v=spf1 include:amazonses.com ~all""</code></pre><p>It's good to have a custom subdomain set up for better deliverability and customer experience. For example, the domain will be displayed in the <code>mailed-by</code> field in Gmail.</p><p>You can use <strong>-all</strong> instead of <strong>~all</strong>. In this case, emails sent from sources not covered in SPF record may be rejected by recipients.</p></li><li><p>According to Section 3.1 of RFC 7208:</p><blockquote>  <p>SPF records MUST be published as a DNS TXT (type 16) Resource Record (RR) [RFC1035] only.</p></blockquote><p>Thus, SPF record type is now obsolete.</p></li><li><p>Regarding your comment, here is one simple way to test whether SPF works:</p><ul><li>Send emails to <code>check-auth@verifier.port25.com</code> from both Gmail and Amazon SES Test Email form.</li><li>Afterwards, search the automated reply for <code>SPF check: pass</code>.</li></ul></li></ol>",2542208,2542208,2017-07-17T13:19:30.020,2017-07-17T13:19:30.020,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30866481
220,30923227,2,30909554,2015-06-18T18:40:03.547,1,"<p><code>- download:</code> is an asynchronous method and returns immediately. Since you are not retaining a strong reference to an instance of <code>S3TransferManager</code>, it can be released before the download completes. You need to keep a strong reference to <code>transferManager</code>.</p><p>Please note that you are using a deprecated version of the AWS SDK. It may be worthwhile to migrate to the version 2 of the AWS Mobile SDK for iOS. With the latest version of the SDK, you do not need to worry about the memory retention issue you are encountering.</p>",1721492,,,2015-06-18T18:40:03.547,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30923227
221,30961451,2,30889902,2015-06-21T05:04:53.877,1,"<p>It looks like you are running a sample from Github <a href=""https://github.com/jgilfelt/android-simpl3r"" rel=""nofollow"">https://github.com/jgilfelt/android-simpl3r</a>. The SDK version 1.4.3 is released 3 years ago. It uses Android's built-in Apache HttpClient which is very buggy and has been deprecated by Android. The 'Content has been consumed' issue can occur sometimes.</p><p>Please update the SDK to the latest version v2.2.2. It should be backward compatible. Check it out at <a href=""http://aws.amazon.com/mobile/sdk/"" rel=""nofollow"">http://aws.amazon.com/mobile/sdk/</a>.</p>",2026747,,,2015-06-21T05:04:53.877,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30961451
222,30995231,2,30994401,2015-06-23T06:19:55.850,6,"<p>In order to stream an S3 bucket. you need to provide the path to S3 bucket. And it will stream all data from all the files in this bucket. Then whenever w new file is created in this bucket, it will be streamed. If you are appending data to existing file which are read before, these new updates will not be read.</p><p>here is small piece of code that works</p><pre><code>import org.apache.spark.streaming._val conf = new SparkConf().setAppName(""Simple Application"").setMaster(""local[*]"")  val sc = new SparkContext(conf)val hadoopConf=sc.hadoopConfiguration;hadoopConf.set(""fs.s3.impl"", ""org.apache.hadoop.fs.s3native.NativeS3FileSystem"")hadoopConf.set(""fs.s3.awsAccessKeyId"",myAccessKey)hadoopConf.set(""fs.s3.awsSecretAccessKey"",mySecretKey)//ones above this may be deprecated?hadoopConf.set(""fs.s3n.awsAccessKeyId"",myAccessKey)hadoopConf.set(""fs.s3n.awsSecretAccessKey"",mySecretKey)val ssc = new org.apache.spark.streaming.StreamingContext(  sc,Seconds(60))val lines = ssc.textFileStream(""s3n://path to bucket"")lines.print()ssc.start()   // Start the computationssc.awaitTermination()  // Wait for the computation to terminate</code></pre><p>hope it will help.</p>",3987417,27657,2016-06-08T00:33:55.890,2016-06-08T00:33:55.890,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/30995231
223,31038374,2,28706078,2015-06-24T22:41:37.210,3,"<p>I had the same issue. I was using aws.push to update my application. Then I moved to a new computer and I had to set everything up again.</p><p>You can use</p><pre><code>eb deploy</code></pre><p>However, depending upon how you have your project setup, you might need to map your deployment to a branch. Use:</p><pre><code>eb branch</code></pre><p>I was in a bind and wanted to make sure that I did not screw up a deployment by introducing any new issues into the production environment and I wanted to use:</p><pre><code>git aws.push</code></pre><p>This can still be done.</p><p>Download the deprecated version of the AWS Elastic Beanstalk Command Line Tool <a href=""http://aws.amazon.com/code/6752709412171743"" rel=""nofollow"">here</a> </p><p>Then from within your repo run <strong>AWSDevTools-RepositorySetup.sh</strong>. You can find this file in the zip file you just downloaded, AWS-ElasticBeanstalk-CLI-2.6.4 / AWSDevTools / Linux</p><p>Now run</p><pre><code>git aws.config</code></pre><p>Once configured you should be able to run git aws.push without any problems. </p><p><strong>I am now using eb deploy, but I was in a bind and had never used it and did not have time to test it. So this worked for me.</strong></p>",967195,,,2015-06-24T22:41:37.210,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31038374
224,31160416,2,31131817,2015-07-01T11:47:35.757,0,"<p>Was able to solve the problem. Just removed and re-added the file and file-transfer plugins.</p><p><code>cordova plugin rm org.apache.cordova.file</code> and <code>cordova plugin rm org.apache.cordova.file-transfer</code> to remove.</p><p><code>cordova plugin add org.apache.cordova.file</code> and <code>cordova plugin add org.apache.cordova.file-transfer</code> to add.</p><p>It may tell you that these plugins are soon to be deprecated or something like that. You also have the option of using the later versions. To add you can use </p><p><code>cordova plugin add cordova-plugin-file</code> and <code>cordova plugin add cordova-plugin-file-transfer</code>. Just check if they are available under the respective builds in the platforms folder.</p>",4874431,,,2015-07-01T11:47:35.757,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31160416
225,31348128,2,31347430,2015-07-10T18:45:31.850,1,"<p>This repository <code>dockerfile/nodejs</code> has been deprecated.</p><p>You can instead use <code>FROM node:latest</code> instead of <code>FROM dockerfile/nodejs</code></p><p><code>node</code> is the official repository for nodejs.<br></p><p>Here is the link to the official repo page for node:<a href=""https://registry.hub.docker.com/_/node/"" rel=""nofollow"">https://registry.hub.docker.com/_/node/</a></p><p>And here is the open github issue for dockerfile/nodejs not found:<a href=""https://github.com/dockerfile/nodejs/issues/11"" rel=""nofollow"">https://github.com/dockerfile/nodejs/issues/11</a></p>",2167517,,,2015-07-10T18:45:31.850,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31348128
226,31357213,2,31354601,2015-07-11T12:31:28.870,0,"<p>What you probably want is to keep cache of the generated urls in say a HashMap, and if there is no value for a certain key, then generate the url with <code>getAmazonTempUrl()</code> and store it in the cache.</p><p>If you set the URL expiry time to an hour then you can record the activity load time into a variable, and check if that has been longer than an hour. If so, clear the cache.</p><p>What you can also do is do a callback with Picasso in case the image fails and recreate the url:</p><pre><code>Picasso.with(this).load(imageUrl).into(imgaeView, new Callback() {  @Override  public void onSuccess() {  }  @Override  public void onError() {  //recreate the url here and store in cache  }  }  );</code></pre><p><strong>Edit:</strong></p><p>So in your case you could persist the above cache and load it next time the app is started. To deal with image changing under the same url you can try do it the dirty way or the clean way.</p><p>The clean way is to change the url for the image if you can. If that's not an option - you can perhaps check the image's last modified date and append that as a timestamp on the url. e.g <a href=""http://www.sample.com/image.jpg?14869246263"" rel=""nofollow"">http://www.sample.com/image.jpg?14869246263</a> - this works fine with Picasso's cache as long as the the url is exactly the same.</p><p>The dirty way is to load the cached version and then immediately after force-reload the same image around the cache (used to be done with skipCache() I believe, but was deprecated in favour of somethingCachePolicy).</p>",3046504,3046504,2015-07-11T14:24:02.890,2015-07-11T14:24:02.890,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31357213
227,31379500,2,3142388,2015-07-13T09:16:03.320,26,"<p>I had the same problem, solution by @DanielVonFange is outdated, as new version of SDK is out.</p><p>Adding code snippet that works for me right now with AWS Ruby SDK:</p><pre><code>require 'aws-sdk'Aws.config.update({  region: 'REGION_CODE_HERE',  credentials: Aws::Credentials.new(  'ACCESS_KEY_ID_HERE',  'SECRET_ACCESS_KEY_HERE'  )})bucket_name = 'BUCKET_NAME_HERE's3 = Aws::S3::Resource.news3.bucket(bucket_name).objects.each do |object|  puts object.key  object.acl.put({ acl: 'public-read' })end</code></pre>",2336662,,,2015-07-13T09:16:03.320,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31379500
228,31381151,2,16307992,2015-07-13T10:38:57.803,10,"<blockquote>  <p>Amazon deprecated the S3 <strong>Transfer Manager</strong> and migrated to the new  <strong>Transfer Utility</strong>.The Transfer Utility is a simple interface for handling the most common uses of S3.It has a single constructor, which  requires an instance of <strong>AmazonS3Client</strong>. Working with it is so easy  and let the develpers perform all operations with less code.</p></blockquote><p><strong>Following are key features of using Transfer Utility over Transfer Manager</strong></p><ul><li>When uploading large files, TransferUtility uses <strong>multiple threads</strong> toupload multiple parts of a single upload at once.When dealing withlarge content sizes and <strong>high bandwidth</strong>, this can <strong>increase throughput</strong>significantly.TransferUtility detects if a file is large and switches intomultipart upload mode. The <strong>multipart upload</strong> gives the benefit ofbetter performance as the parts can be uploaded simultaneously aswell, and if there is an error, only the individual part has to beretried.</li><li>Mostly we people try to upload large files on S3 that take too muchtime to upload,at those situations we required progress informationsuch as the total number of <strong>bytes transferred</strong> and <strong>remaining amount ofdata</strong> to transfer.To track current progress of transfer with theTransfer Manager, developers pass an <strong>S3ProgressListener</strong> callback toupload or download, which periodically fires the method below.</li><li>Pausing transfers using the Transfer Manager is not possible withstream based uploads or downloads.But Transfer Utility provide us<strong>pause and resume</strong> option, it also has one single-file-based method foruploads, and downloads.</li></ul><blockquote>  <p>transferUtility.upload(MY_BUCKET,OBJECT_KEY,FILE_TO_UPLOAD)  transferUtility.download(MY_BUCKET,OBJECT_KEY,FILE_TO_UPLOAD)</p></blockquote><ul><li>The Transfer Manager only requires the INTERNET permission. However,since the Transfer Utility <strong>automatically detects network state</strong> and<strong>pauses/resumes transfers</strong> based on the network statepause functionality to the Transfer Utility is easy, since all transfers can be paused and resumed.A transfer is paused because of a loss of network connectivity, it will automatically be resumed and there is no action you need to take.Transfers that are automatically paused and waiting for network connectivity will have the state.Additionally, the Transfer Utility stores all of the metadata about transfers to the local SQLite database, so developers do not need to persist anything.</li></ul><p><strong>Note :</strong>Every thing else is good.But Transfer Utility does not support a copy() API.To accomplish it use <strong>AmazonS3Client</strong> class <strong>copyObject()</strong> method.</p>",4859919,4859919,2016-04-26T11:53:02.827,2016-04-26T11:53:02.827,7,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31381151
229,31387625,2,31345871,2015-07-13T15:37:56.710,0,"<p>The issue was using DynamoDB version 1 (i.e. the deprecated one). Once I switched to connecting with the new version of the table and performed the scan with the appropriate method, it worked. </p><p>Hope this helps other tormented souls who get confused by cross-version mismatches. </p>",4490681,,,2015-07-13T15:37:56.710,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31387625
230,31416369,2,31413718,2015-07-14T20:12:15.340,2,"<p>Finally resolve the problem.</p><p>On generating ssl.pem, do not follow completely the guide mentioned in mup doc. That guide is outdated.</p><p>The whole process is follows:</p><ol><li><p>create Aws ubuntu  ec2. Note the user is ubuntu, not ec2-user</p></li><li><p>Š—¢ First you need to generate a CSR file and the private keyAWSuse openssl, so follow: <a href=""https://support.comodo.com/index.php?/Default/Knowledgebase/Article/View/1/19/csr-generation-using-openssl-apache-wmod_ssl-nginx-os-x"" rel=""nofollow"">https://support.comodo.com/index.php?/Default/Knowledgebase/Article/View/1/19/csr-generation-using-openssl-apache-wmod_ssl-nginx-os-x</a>Š—¢ Then purchase a SSL certificate.</p><p>Š—¢ Then generate a SSL certificate from your SSL providers UI.</p><p>Š—¢ Then that'll ask to provide the CSR file. Upload the CSR file we've generated. only csr, no private key.</p><p>Š—¢ When asked to select your SSL server type, select it as nginx.</p><p>Š—¢ Then you'll get a set of files (your domain certificate and CA files).</p></li></ol><p>I bought ssl certificate from namecheap, and got four files in one .zip:  Š—¢ Root CA Certificate - AddTrustExternalCARoot.crt  Š—¢ Intermediate CA Certificate - COMODORSAAddTrustCA.crt  Š—¢ Intermediate CA Certificate - COMODORSADomainValidationSecureServerCA.crt  Š—¢ Your PositiveSSL Certificate - www_carllo_us.crt</p><ol start=""3""><li><p>Get ssl.pem:cat www_domain_com.crt ComodoRSADomainValidationSecureServerCA.crt COMODORSAAddTrustCA.crt AddTrustExternalCARoot.crt private.key >> ssl.pem</p></li><li><p>Set mup.json:</p><pre><code>""env"": {""ROOT_URL"": ""https://domain.com""},""ssl"": {""pem"": ""carllous-bundle.pem""},</code></pre></li><li><p>Mup setup</p></li></ol><p>Done</p>",5107277,,,2015-07-14T20:12:15.340,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31416369
231,31426645,2,31313955,2015-07-15T09:32:47.027,1,"<p>I suggest you to read this <a href=""https://medium.com/aws-activate-startup-blog/a-guide-to-amazon-simple-notification-service-mobile-push-self-registration-for-ios-a2502e8d5fbd"" rel=""nofollow"">article</a> to know the whole workflow. Though it's written for iOS development and somewhat outdated, the concepts are helpful.</p><p>AWS Cognito helps you get credentials for connecting your application users with AWS SNS Service. It's important to first set up Cognito to authorize any further action.</p><p>For the auto registration part,you'll need to make a request to AWS console and create an <code>endpoint</code> (related to a specific device installed your application) there, thus you can push notifications to those registered endpoints later.</p><p>check the <strong>createPlatformEndpoint</strong> (keyword for more searching) section at this <a href=""http://docs.aws.amazon.com/AWSAndroidSDK/latest/javadoc/com/amazonaws/services/sns/AmazonSNSClient.html"" rel=""nofollow"">AWS doc</a>.</p><p>Notice that two request parameters <code>PlatformApplicationArn</code> and <code>Token</code> are required, see the <strong>Request Parameters</strong> section <a href=""http://docs.aws.amazon.com/sns/latest/APIReference/API_CreatePlatformEndpoint.html"" rel=""nofollow"">here</a>.</p>",4062660,4062660,2015-07-15T09:55:50.950,2015-07-15T09:55:50.950,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31426645
232,31452261,2,31426011,2015-07-16T10:57:41.680,-1,"<p>You have to many application versions. You can manually delete application version using the AWS Console:</p><p>AWS Console / Elastic Beanstalk / <em>Your App</em> / Application Versions</p><ul><li>Select some obsolete versions and delete them.</li></ul>",479023,,,2015-07-16T10:57:41.680,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31452261
233,31500170,2,26177059,2015-07-19T10:21:37.523,3,"<p><strong>Update: This answer is obsolete as Docker now supports the <code>docker run --sysctl</code> option!</strong></p><p>The solution I use for my OpenVPN container is to enter the container namespace with full capabilities using <code>nsenter</code>, remounting <code>/proc/sys</code> read-write temporarily, setting stuff up and remounting it read-only again.</p><p>Here an example, enabling IPv6 forwarding in the container:</p><pre><code>CONTAINER_NAME=openvpn# enable ipv6 forwarding via nsentercontainer_pid=`docker inspect -f '{{.State.Pid}}' $CONTAINER_NAME`nsenter --target $container_pid --mount --uts --ipc --net --pid \  /bin/sh -c '/usr/bin/mount /proc/sys -o remount,rw;  /usr/sbin/sysctl -q net.ipv6.conf.all.forwarding=1;  /usr/bin/mount /proc/sys -o remount,ro;  /usr/bin/mount /proc -o remount,rw # restore rw on /proc'</code></pre><p>This way the container does not need to run privileged.</p>",2177070,2177070,2017-08-01T13:08:56.567,2017-08-01T13:08:56.567,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31500170
234,31564979,2,31086368,2015-07-22T13:45:06.030,24,"<p>I was facing the same issue.</p><p>The problem for me:</p><p>My local mongo shell was v2.6.10. It uses an authentication method called MONGODB-CR <a href=""http://docs.mongodb.org/manual/core/authentication/"" rel=""noreferrer"">that has been deprecated</a>.</p><p>My server version is v3.0.4. It uses an authentication method called SCRAM-SHA-1.</p><p>Try to check your local shell and remote server versions with:</p><pre><code>mongo --versionmongod --version</code></pre><p>If they are different, upgrade your local shell to v3. (I had to uninstall and install it again.)</p>",5115166,,,2015-07-22T13:45:06.030,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31564979
235,31581489,2,31581388,2015-07-23T08:03:39.670,3,"<p>This can happen when some api calls get outdated. I would recommend using the <a href=""https://github.com/aws/aws-sdk-php/"" rel=""nofollow"">official php libraries</a>. They work almost the same way this library works, but they are being kept up-to-date by Amazon programmers.</p>",743016,,,2015-07-23T08:03:39.670,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31581489
236,31781192,2,31781102,2015-08-03T07:00:26.913,1,"<p>From <a href=""https://stackoverflow.com/questions/31781102/how-to-get-array-of-key-from-amazon-s3-object/31781192?noredirect=1#comment51563041_31781192"">comment</a>: <code>Please note that v1 of the SDK is deprecated</code>. If you're still using it you can use the following snippet:</p><pre><code>$ObjectsListResponse = s3-&gt;list_objects([ 'Bucket' =&gt; $somebucketName]);$Objects = $ObjectsListResponse-&gt;body-&gt;Contents;foreach ($Objects as $Object) {  $keyArray[] = $Object-&gt;Key;}</code></pre><p><a href=""http://ceph.com/docs/master/radosgw/s3/php/"" rel=""nofollow noreferrer"">PHP S3 Examples</a></p>",1747433,-1,2017-05-23T12:02:16.157,2015-08-04T20:59:44.457,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31781192
237,31886425,2,31885443,2015-08-07T20:50:30.570,0,"<p>A helpful voice in the #elasticsearch IRC channel asked me if I had the same version running. Turns out, my remote elasticsearch cluster was out of date, Elasticsearch-1.4.1 (likely because I installed it using a script copied from a tutorial). I <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-upgrade.html"" rel=""nofollow"">upgraded Elasticsearch</a> to version 1.7.1 and now my Haystack queries are working.</p>",1359529,,,2015-08-07T20:50:30.570,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31886425
238,31969535,2,31363144,2015-08-12T15:27:49.827,5,"<p>I ended up with this function in Cordova:</p><pre><code>$scope.uploadPhoto = function () {  $scope.getSignedRequest(function (signedRequest) {  if (!signedRequest)  return;  var options = new FileUploadOptions();  options.chunkedMode = false;  options.httpMethod = 'PUT';  options.headers = {  'Content-Type': 'image/jpeg',  'X-Amz-Acl': 'public-read'  };  var ft = new FileTransfer();  ft.upload($scope.photoURI, signedRequest.signedUrl, function () {  $scope.$apply(function () {  // success  });  }, function () {  $scope.$apply(function () {  // failure  });  }, options);  });};</code></pre><p>The important bits are setting the <code>Content-Type</code> header, so that <code>multipart/form-data</code> won't be used, and <code>chunkedMode = false</code> to send the file with a single request.</p><p>EDIT: Removed changes to the plugin code which were, in hindsight, useless (outdated plugin).</p>",2811496,2811496,2015-08-12T16:11:59.387,2015-08-12T16:11:59.387,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/31969535
239,32027260,2,32020472,2015-08-15T17:20:13.910,3,"<p>In practice, neither of these records are necessary.  Here's why:</p><ul><li>The two records you've listed are SPF (v=spf1) and Sender-ID (spf2.0/pra).  The latter protocol, Sender-ID, is now obsolete and this record is not required.</li><li>SPF works off the 'mfrom' address - that's the Return Path address.  Amazon SES uses amazonses.com in the Return Path, meaning that receivers won't even check the SPF record you're creating.  So it is not necessary to add it to the SPF record for your domain.</li></ul><p>What you need to do is set up DKIM.  Authenticating email from Amazon SES requires the use of Amazon's Easy DKIM system (<a href=""http://docs.aws.amazon.com/ses/latest/DeveloperGuide/easy-dkim.html"" rel=""nofollow"">http://docs.aws.amazon.com/ses/latest/DeveloperGuide/easy-dkim.html</a>) .</p>",1599793,,,2015-08-15T17:20:13.910,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32027260
240,32040035,2,31999668,2015-08-16T21:16:04.760,7,"<ol><li><p>At present the AWS Aurora documentation is linking to an out of date SSL certificate to use, hence the problem. This has been confirmed by the AWS support staff. Use this instead: <a href=""https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem"" rel=""noreferrer"">https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem</a></p></li><li><p>Even when using that certificate, connecting to the cluster end-point over SSL still doesn't work for the command line using mysql -h connection. If I change from the cluster end-point to the instance end-point strangely it works.</p></li><li><p>Bizarrely, mysql workbench does connect over ssl, both to the instance end-point AND the cluster-end point.</p></li></ol>",964634,,,2015-08-16T21:16:04.760,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32040035
241,32084940,2,32084637,2015-08-19T01:07:38.837,2,"<p>Yup. /utc/now is is off by one hour. At the moment, it is returning </p><pre><code>2015-08-19T01:58:07+01:00</code></pre><p>(notice the +01:00)</p><p><a href=""http://chronic.herokuapp.com"" rel=""nofollow"">http://chronic.herokuapp.com</a> returns </p><pre><code>2015-08-19 00:57:27 +00:00</code></pre><p>timeapi.org is based on an out of date fork of <a href=""https://github.com/zh/timeapi"" rel=""nofollow"">https://github.com/zh/timeapi</a>, which is the basis for the heroku app linked above.</p>",5241372,,,2015-08-19T01:07:38.837,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32084940
242,32162415,2,32157262,2015-08-23T01:45:04.320,0,"<p>There is no Amazon EC2 API call to see ""inside"" the Amazon EC2 instance. However, you can <strong>obtain information about the AMI used to launch the instance</strong>.</p><p>Here's an example, using the <a href=""http://aws.amazon.com/cli/"" rel=""nofollow"">AWS Command-Line Interface (CLI)</a>, which makes similar API calls to Java.</p><pre><code>$ aws ec2 describe-instances --query 'Reservations[*].Instances[*].ImageId' --filter Name=instance-id,Values=i-xxxxxxxx --output textami-d9fe9be3$ aws ec2 describe-images --image-ids ami-d9fe9be3{  ""Images"": [  {  ""VirtualizationType"": ""hvm"",   ""Name"": ""amzn-ami-hvm-2014.03.2.x86_64-ebs"",   ""Hypervisor"": ""xen"",   ""ImageOwnerAlias"": ""amazon"",   ""SriovNetSupport"": ""simple"",   ""ImageId"": ""ami-d9fe9be3"",   ""State"": ""available"",   ""BlockDeviceMappings"": [  {  ""DeviceName"": ""/dev/xvda"",   ""Ebs"": {  ""DeleteOnTermination"": true,   ""SnapshotId"": ""snap-c90a03fd"",   ""VolumeSize"": 8,   ""VolumeType"": ""standard"",   ""Encrypted"": false  }  }  ],   ""Architecture"": ""x86_64"",   ""ImageLocation"": ""amazon/amzn-ami-hvm-2014.03.2.x86_64-ebs"",   ""RootDeviceType"": ""ebs"",   ""OwnerId"": ""137112412989"",   ""RootDeviceName"": ""/dev/xvda"",   ""CreationDate"": ""2014-06-11T19:46:45.000Z"",   ""Public"": true,   ""ImageType"": ""machine"",   ""Description"": ""Amazon Linux AMI x86_64 HVM EBS""  }  ]}</code></pre><p>Information can be extracted from the <code>Description</code> field. However, please note that <strong>Windows AMIs are deprecated each month</strong> as updates are made available by Microsoft. This means that old Windows instances might not have information accessible about their AMIs.</p>",174777,,,2015-08-23T01:45:04.320,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32162415
243,32215884,2,31709257,2015-08-25T23:48:07.003,1,<p>It sounds like your AWS SDK associated with the document client may be out of date and does not support the new KeyConditionExpression feature. Could you please try re-installing your AWS SDK and the document SDK? Please also attach the versions you are installing if you continue to have issues after re-installing.</p>,4131111,,,2015-08-25T23:48:07.003,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32215884
244,32335110,2,32227782,2015-09-01T15:09:35.657,0,"<p>Ok, the issue has been fixed. A guy from PG community helped me:</p><p><a href=""http://community.phonegap.com/nitobi/topics/phonegap-file-transfer-how-to-prevent-multipart-data?utm_source=notification&amp;utm_medium=email&amp;utm_campaign=new_comment&amp;utm_content=reply_button&amp;reply[id]=16076747#reply_16076747"" rel=""nofollow"">http://community.phonegap.com/nitobi/topics/phonegap-file-transfer-how-to-prevent-multipart-data?utm_source=notification&amp;utm_medium=email&amp;utm_campaign=new_comment&amp;utm_content=reply_button&amp;reply[id]=16076747#reply_16076747</a></p><p>I used PG build and the plugin there was outdated. Once I installed the last version of cordova-plugin-media-transfer from npm and built locally the app started to upload to Amazon with no issues and no multipart data were used.</p>",3595434,,,2015-09-01T15:09:35.657,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32335110
245,32517878,2,26475486,2015-09-11T07:26:27.097,15,"<p>Way overdue, but there is a <a href=""https://www.npmjs.com/package/aws-sdk-promise"" rel=""noreferrer"">aws-sdk-promise</a> npm module that simplifies this.</p><p>This just adds a promise() function which can be used like this:</p><pre><code>ddb.getItem(params).promise().then(function(req) {  var x = req.data.Item.someField;});</code></pre><p><strong>EDIT</strong>: It's been a few years since I wrote this answer, but since it seems to be getting up-votes lately, I thought I'd update it: <code>aws-sdk-promise</code> is deprecated, and newer (as in, the last couple of <em>years</em>) versions of aws-sdk includes built-in promise support. The promise implementation to use can be configured through <code>config.setPromisesDependency()</code>. </p><p>For example, to have <code>aws-sdk</code> return <code>Q</code> promises, the following configuration can be used:</p><pre><code>const AWS = require('aws-sdk')const Q = require('q')AWS.config.setPromisesDependency(Q.Promise)</code></pre><p>The <code>promise()</code> function will then return <code>Q</code> promises directly (when using <code>aws-sdk-promise</code>, you had to wrap each returned promise manually, e.g. with <code>Q(...)</code> to get <code>Q</code> promises).</p>",1226020,1226020,2018-04-19T07:22:50.073,2018-04-19T07:22:50.073,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32517878
246,32664598,2,27350634,2015-09-19T05:25:57.067,64,"<p>The accepted answer gave me node 0.10.36 and npm 1.3.6 which are very out of date. I grabbed the latest linux-x64 tarball from the nodejs downloads page and it wasn't too difficult to install: <a href=""https://nodejs.org/dist/latest/"" rel=""noreferrer"">https://nodejs.org/dist/latest/</a>.</p><pre><code># start in a directory where you like to install things for the current user(For noobs : it downloads node package as node.tgz file in your directlry)curl (paste the link to the one you want from the downloads page) &gt;node.tgz</code></pre><p>Now upzip the tar you just downloaded -  </p><pre><code>tar xzf node.tgz</code></pre><p>Run this command and then also add it to your <code>.bashrc</code>:</p><pre><code>export PATH=""$PATH:(your install dir)/(node dir)/bin""</code></pre><p>(example : export PATH =""$PATH:/home/ec2-user/mydirectory/node/node4.5.0-linux-x64/bin"")</p><p>And update <code>npm</code> (only once, don't add to <code>.bashrc</code>):</p><pre><code>npm install -g npm</code></pre><p>Note that the <code>-g</code> there which means global, really means global <em>to that npm instance</em> which is the instance we just installed and is limited to the current user. This will apply to all packages that npm installs 'globally'.</p>",1048668,609782,2016-09-01T13:40:23.590,2016-09-01T13:40:23.590,6,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32664598
247,32725928,2,32703553,2015-09-22T20:08:41.657,2,"<p>Before version 5.6, MySQL makes an assumption when you declare a timestamp column... specifically, it assumes the <em>first</em> timestamp column on a table will be the one that has the automatic update attributes.</p><p>If you don't explicitly disable the behavior on the first timestamp, it's implicitly enabled, which causes the explicit automatic value on a later timestamp to be rejected.  It isn't enough not to ask.</p><blockquote><p>It need not be the first <code>TIMESTAMP</code> column in a table that is automatically initialized or updated to the current timestamp. However, to specify automatic initialization or updating for a different <code>TIMESTAMP</code> column, you must suppress the automatic properties for the first one. Then, for the other <code>TIMESTAMP</code> column, the rules for the <code>DEFAULT</code> and <code>ON UPDATE</code> clauses are the same as for the first <code>TIMESTAMP</code> column, except that if you omit both clauses, no automatic initialization or updating occurs.</p><p>To suppress automatic properties for the first <code>TIMESTAMP</code> column, do either of the following:</p><p>Define the column with a <code>DEFAULT</code> clause that specifies a constant default value.</p><p>Specify the <code>NULL</code> attribute. This also causes the column to permit <code>NULL</code> values, which means that you cannot assign the current timestamp by setting the column to <code>NULL</code>. Assigning <code>NULL</code> sets the column to <code>NULL</code>.</p><p>Š—” <a href=""https://dev.mysql.com/doc/refman/5.5/en/timestamp-initialization.html"" rel=""nofollow noreferrer"">https://dev.mysql.com/doc/refman/5.5/en/timestamp-initialization.html</a></p></blockquote><p>So, for your first timestamp -- if it's not the one you want to be the automatic timestamp -- use either one of these column type declarations (they're identical):</p><pre><code>TIMESTAMP NOT NULL DEFAULT '0000-00-00 00:00:00'TIMESTAMP NOT NULL DEFAULT 0 -- automatically expanded to '0000-00-00 00:00:00'</code></pre><p>This should allow you to copy this table definition between systems without issue.</p><hr /><p>This silliness was fixed in MySQL Server 5.6, where the system variable <a href=""https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_explicit_defaults_for_timestamp"" rel=""nofollow noreferrer""><code>explicit_defaults_for_timestamp</code></a> disables the implicit automatic behavior for the first timestamp in a table.</p><p>If you start a server running 5.6 without setting this option, a warning is written to the error log.</p><pre><code>[Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).</code></pre><p>The warning is reminding you that you still have the legacy behavior, which is deprecated in 5.6.</p>",1695906,-1,2020-06-20T09:12:55.060,2015-09-22T20:08:41.657,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32725928
248,32881772,2,32881396,2015-10-01T07:27:45.043,0,"<p>Seems aws cli doesn't work well with python 3.</p><blockquote><p>inspect.getargspec(func)</p><p>Get the names and default values of a Python functionŠ—Ès arguments. A named tuple ArgSpec(args, varargs, keywords, defaults) is returned. args is a list of the argument names. varargs and keywords are the names of the * and ** arguments or None. defaults is a tuple of default argument values or None if there are no default arguments; if this tuple has n elements, they correspond to the last n elements listed in args.</p><p>Deprecated since version 3.0: Use signature() and Signature Object, which provide a better introspecting API for callables. This function will be removed in Python 3.6.</p></blockquote><p>Can you try with python 2.6 or 2.7?</p><pre><code>Requires Python 2.6.5 or higher.</code></pre><p>Refer:</p><p><a href=""https://docs.python.org/3/library/inspect.html#inspect.getargspec"" rel=""nofollow noreferrer"">https://docs.python.org/3/library/inspect.html#inspect.getargspec</a></p><p><a href=""https://aws.amazon.com/cli/"" rel=""nofollow noreferrer"">https://aws.amazon.com/cli/</a></p>",498256,-1,2020-06-20T09:12:55.060,2015-10-01T07:33:56.330,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32881772
249,32881955,2,32881396,2015-10-01T07:37:59.290,4,"<p>You probably have just upgraded python to 3.4.1.</p><p>The version of the AWS CLI and botocore hosted for Ubuntu 14.04 is incompatible with python 3.4.1+. (<a href=""https://bugs.launchpad.net/ubuntu/+source/awscli/+bug/1501042"" rel=""nofollow"">source</a>)</p><p>The version of the CLI installed via apt-get is out of date. Python 3.4.1 introduced a breaking change with the getargspec() function, which we use in botocore. (<a href=""https://github.com/aws/aws-cli/issues/800"" rel=""nofollow"">source</a>)</p><p>You can try: (I don't have testbox with me to verify)</p><pre><code>sudo apt-get remove awsclisudo apt-get install python-pipsudo pip install awsclisudo pip install upgrade botocore</code></pre>",1145196,,,2015-10-01T07:37:59.290,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32881955
250,32898784,2,32898692,2015-10-02T00:25:55.927,0,"<p>It runs great in my system, I just cloned the project:</p><pre><code>shiva@ubuntu:~/projects/amazon-ecs-sample (master)$ rails sDigest::Digest is deprecated; use Digest=&gt; Booting WEBrick=&gt; Rails 3.2.5 application starting in development on http://0.0.0.0:3000=&gt; Call with -d to detach=&gt; Ctrl-C to shutdown server[2015-10-02 06:05:38] INFO  WEBrick 1.3.1[2015-10-02 06:05:38] INFO  ruby 2.1.1 (2014-02-24) [x86_64-linux][2015-10-02 06:05:38] INFO  WEBrick::HTTPServer#start: pid=4008 port=3000</code></pre><p>May be, you need to verify changes you made in few of the latest commits. or make sure you did <code>bundle install</code></p><pre><code>shiva@ubuntu:~/projects/amazon-ecs-sample (master)$ bundle installFetching gem metadata from https://rubygems.org/...........Fetching version metadata from https://rubygems.org/...Fetching dependency metadata from https://rubygems.org/..Installing rake 0.9.2.2Installing i18n 0.6.0Installing multi_json 1.3.6Installing activesupport 3.2.5Installing builder 3.0.0Installing activemodel 3.2.5Using erubis 2.7.0Installing journey 1.0.3Installing rack 1.4.1Installing rack-cache 1.2Installing rack-test 0.6.1Installing hike 1.2.1Installing tilt 1.3.3Installing sprockets 2.1.3Installing actionpack 3.2.5Installing mime-types 1.18Installing polyglot 0.3.3Installing treetop 1.4.10Installing mail 2.4.4Installing actionmailer 3.2.5Installing arel 3.0.2Installing tzinfo 0.3.33Installing activerecord 3.2.5Installing activeresource 3.2.5Installing nokogiri 1.5.4Using ruby-hmac 0.4.0Installing amazon-ecs 2.2.4Installing coffee-script-source 1.3.3Installing execjs 1.4.0Using coffee-script 2.2.0Installing rack-ssl 1.3.2Installing json 1.7.3Installing rdoc 3.12Installing thor 0.15.2Installing railties 3.2.5Installing coffee-rails 3.2.2Installing jquery-rails 2.0.2Using bundler 1.9.2Installing rails 3.2.5Installing sass 3.1.19Installing sass-rails 3.2.5Installing sqlite3 1.3.6Installing uglifier 1.2.4Bundle complete! 7 Gemfile dependencies, 43 gems now installed.Use `bundle show [gemname]` to see where a bundled gem is installed.Post-install message from rdoc:Depending on your version of ruby, you may need to install ruby rdoc/ri data:&lt;= 1.8.6 : unsupported = 1.8.7 : gem install rdoc-data; rdoc-data --install = 1.9.1 : gem install rdoc-data; rdoc-data --install&gt;= 1.9.2 : nothing to do! Yay!</code></pre>",3437900,,,2015-10-02T00:25:55.927,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32898784
251,32901014,2,32900928,2015-10-02T05:24:18.653,2,"<p>As your link suggests this option was available till Rails 2.3.8.</p><p>It was deprecated from Rails 3.</p><p>Here's an old day discussion showing problems with this feature: <a href=""https://groups.google.com/forum/#!topic/rubyonrails-core/BnVHfD-yO_I"" rel=""nofollow"">https://groups.google.com/forum/#!topic/rubyonrails-core/BnVHfD-yO_I</a>.</p>",1426097,,,2015-10-02T05:24:18.653,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/32901014
252,33181296,2,31709257,2015-10-17T00:12:30.980,0,"<p>Previous <a href=""https://github.com/awslabs/dynamodb-document-js-sdk"" rel=""nofollow"">DynamoDB Document SDK</a> was deprecated, new client from standard Javascript SDK should be used from now on:</p><p><a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html</a></p>",5455783,,,2015-10-17T00:12:30.980,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33181296
253,33287596,2,15867047,2015-10-22T18:00:18.097,2,"<p>This question was filed a few years ago, and the current answer I believe is out of date.  EC2 now runs the above script with a successful response without the need for a proxy.  I came across this question while investigating my own similar issue with Google App Engine.</p>",3037408,,,2015-10-22T18:00:18.097,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33287596
254,33306070,2,33298186,2015-10-23T15:24:54.277,1,"<p><strong>Update</strong></p><p>At some point after this answer was written, most of the information about Signature Version 3 has been removed from the AWS docs.  The documentation at the link mentioned below is still present, but the page no longer describes how to actually sign requests using Signature Version 3.  It does still mention that SES supports both Signature V3 and V4, but it also states that Signature V4 is recommended.</p><p>For the historically curious, the instructions for signing SES requests using Signature Version 3 can still be found at an <a href=""https://web.archive.org/web/20150905231547/https://docs.aws.amazon.com/ses/latest/DeveloperGuide/query-interface-authentication.html"" rel=""nofollow noreferrer"">archived version of the page</a> but using (or switching to) Signature Version 4 would be the future-proof course of action, as it is highly unlikely for Signature V3 to be supported if SES launches any new regions in the future (Signature V2 has not been supported in any region launched since 2014, but SES does not have endpoints in any of those regions).</p><p>Signature V3 had significant limitations compared to Signature V4, so it may at some point (or may already) be deprecated and may eventually be discontinued. </p><p>tl;dr: Always use Signature Version 4.</p><p><strike><a href=""https://docs.aws.amazon.com/ses/latest/DeveloperGuide/query-interface-authentication.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/ses/latest/DeveloperGuide/query-interface-authentication.html</a> explains how to use Signature Version 3 with SES to sign requests you will submit over HTTPS.</strike></p><p><strike>This is an algorithm that is different from most other AWS services, but is very simple to implement.  The standard Signature V4 is also supported by SES.</strike></p>",1695906,1695906,2019-01-26T21:59:38.773,2019-01-26T21:59:38.773,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/33306070
255,33369633,2,25701632,2015-10-27T13:41:50.497,0,"<p>Use <code>RVM</code> to Fix <code>SSL</code> Certificates</p><p>Recent versions of <code>RVM</code>, the <strong>Ruby Version Manager</strong>, include a utility to diagnose and resolve errors caused by outdated certificate files. See the article Installing Rails for instructions and advice. The <code>RVM</code> website explains how to install RVM.</p><p>If you have installed <code>3RVM</code>, try this:</p><pre><code>$ rvm -v # rvm 1.19.1 (stable)$ rvm osx-ssl-certs status all # Certificates for$ rvm osx-ssl-certs update all # Updating certificates</code></pre><p>For more on the issue, see a discussion at <a href=""https://github.com/rvm/rvm/pull/1764"" rel=""nofollow"">https://github.com/rvm/rvm/pull/1764</a></p>",5409748,6056677,2016-08-11T10:35:59.127,2016-08-11T10:35:59.127,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33369633
256,33482970,2,33479851,2015-11-02T17:01:52.427,1,"<p>It looks like <code>invokeAsync</code> is deprecated, so I used <code>invoke</code> for this example, but it is fairly similar.  </p><p>The <code>Payload</code> argument in the <code>invoke</code> params becomes the <code>event</code> parameter in ChainFunc2.</p><p><strong>ChainFunc1</strong></p><pre><code>var AWS = require(""aws-sdk"");exports.handler = function(event, context) {  console.log('Received event:', JSON.stringify(event, null, 2));  var params = {  FunctionName: ""ChainFunc2"",  InvocationType: ""RequestResponse"",  Payload: JSON.stringify({""greeting"": ""Hello, Lambda""})  };  var lambdaClient = new AWS.Lambda();  lambdaClient.invoke(params, function(err, data) {  if (err) {  console.log(""invoke failed:"" + err, err.stack);  context.fail(err);  } else {  console.log(""invoke succeeded"", data);  context.succeed(data);  }  });};</code></pre><p><strong>ChainFunc2</strong></p><pre><code>exports.handler = function(event, context) {  console.log(""Received event:"", JSON.stringify(event, null, 2));  console.log(""Greeting:"", event.greeting);  context.succeed({""message"": ""ChainFunc2 processed this"", ""payload"": event});};</code></pre>",3587167,,,2015-11-02T17:01:52.427,8,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33482970
257,33504677,2,33500359,2015-11-03T16:47:27.920,0,"<p>You have only created a single <code>AttributeDefinition</code>. Your method chaining only produces a single <code>AttributeDefinition</code>, rather than a list of them. You also only need to create an <code>AttributeDefinition</code> for attributes that are for your <code>KeySchema</code> of your table or your GSIs.</p><pre><code>attributeDefinitions.add(new AttributeDefinition()  .withAttributeName(""ServiceID"")  .withAttributeType(ScalarAttributeType.N));</code></pre><p>For your deprecation question, look at the <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/util/Tables.html"" rel=""nofollow"">Javadoc</a> of <code>Tables</code>:</p><blockquote>  <p><strong>Deprecated.</strong> </p>   <p>Use <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/util/TableUtils.html"" rel=""nofollow""><code>TableUtils</code></a>.</p></blockquote><pre><code>TablesUtils.waitUntilActive(dynamoDBClient, serviceRef);</code></pre>",627727,,,2015-11-03T16:47:27.920,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33504677
258,33506948,2,33506726,2015-11-03T18:56:23.327,7,"<p>(Note: This answer is now somewhat obsolete, since Dynamo has added a TTL setting on their tables.  Please see link in @Sindhu's answer for more information.  Leaving the answer because chronologically splitting Dynamo tables may still be useful.)</p><p>There's no way to do this automatically with a setting.  You can create a job that periodically scans your table and deletes old entries-- you have to be careful when doing this, however, since such a job could easily consume all your configured read and write capacity-- you may have to add delays so this job does not bottleneck the rest of your processing.</p><p>Another way to approach the same problem is to break up your table into chunks by week or month-- so you might have a separate table for each month.  Then when the data is old enough to be discarded, you just drop the entire table.  This may or may not work for your data access patterns-- but another advantage is you can configure a lot of capacity for your recent data which you are more likely to access, and less capacity on the older tables you won't need as often.</p>",183203,183203,2018-04-06T15:50:15.570,2018-04-06T15:50:15.570,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33506948
259,33522444,2,32977688,2015-11-04T12:50:50.520,9,"<p><strong>Update</strong>Since time of writing, lambda has updated the invocation signature and now passes <code>event, context, callback</code>.</p><p>Instead of calling <code>context.done(err, res)</code> you should use <code>callback(err, res)</code>. Note that what was true for context.done still applies to the callback pattern.</p><p>Should also add that with API Gateways proxy and integration implementation this entire thread is pretty much obsolete.I recommend reading this article if you are integrating API Gateway with Lambda: <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html</a></p>Original response below<p>First things first, let's clear a few things up.</p><h2>context.done() vs. context.fail()/context.success</h2><p><code>context.done(error, result);</code> is nothing but a wrapper around <code>context.fail(error);</code> and <code>context.success(response);</code>The Lambda documentation clearly states that result is ignored if error is non null:</p><blockquote>  <p>If the Lambda function was invoked using the RequestResponse (synchronous) invocation type, the method returns response body as follows:  If the error is null, set the response body to the string representation of result. This is similar to the context.succeed().  <strong>If the error is not null, set the response body to error.</strong>  If the function is called with a single argument of type error, the error value will be populated in the response body.  <a href=""http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-context.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-context.html</a></p></blockquote><p>What this means is that it won't matter whether you use a combination of fail/success or done, the behaviour is exactly the same.</p><h2>API Gateway and Response Code Mapping</h2><p>I have tested every thinkable combination of response handling from Lambda in combination with Response code mapping in API Gateway.</p><p>The conclusion of these tests are that the <strong>""Lambda Error RegExp"" is only executed against a Lambda error</strong>, i.e: you have to call <code>context.done(error);</code>or <code>context.fail(error);</code> for the RegExp to actually trigger.</p><p>Now, this presents a problem as, has already been noted, Lambda takes your error and sticks it in an object and calls <code>toString()</code> on whatever you supplied:  </p><pre><code>{ errorMessage: yourError.toString() }</code></pre><p>If you supplied an error object you'll get this:</p><pre><code>{ errorMessage: ""[object Object]"" }</code></pre><p>Not very helpful at all.</p><p>The only workaround I have found thus far is to call </p><pre><code>context.fail(JSON.stringify(error));</code></pre><p>and then in my client do:</p><pre><code>var errorObject = JSON.parse(error.errorMessage);</code></pre><p>It's not very elegant but it works.As part of my error I have a property called ""code"". It could look something like this:</p><pre><code>{   code: ""BadRequest"",   message: ""Invalid argument: parameter name"" }</code></pre><p>When I stringify this object I get:</p><pre><code>""{\""code\"":\""BadRequest\"",\""message\"":\""Invalid argument: parameter name\""}""</code></pre><p>Lambda will stick this string in the errorMessage property of the response and I can now safely grep for <code>.*""BadRequest"".*</code> in the API Gateway response mapping.</p><p>It's very much a hack that works around two somewhat strange quirks of Lambda and API Gateway:</p><ol><li>Why does Lambda insist on wrapping the error instead of just givingit back as is? </li><li>Why doesn't API Gateway allow us to grep in theLambda result, only the error?</li></ol><p>I am on my way to open a support case with Amazon regarding these two rather odd behaviours.</p>",832287,832287,2017-06-26T08:25:30.597,2017-06-26T08:25:30.597,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33522444
260,33641020,2,12908599,2015-11-10T22:39:59.500,1,"<p>The answer by Godsaur is essentially correct. However, it appears to be outdated, perhaps for SDK version 1? </p><p>This worked for me for version 2:</p><pre><code>s3 = Aws::S3::Client.new(endpoint:'https://s3-ap-southeast-1.amazonaws.com')</code></pre><p>See <a href=""http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Client.html#initialize-instance_method."" rel=""nofollow"">docs</a>.</p>",336920,,,2015-11-10T22:39:59.500,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33641020
261,33685242,2,33684609,2015-11-13T03:01:54.463,2,"<p>I would remove the <code>provider</code> key. The <code>carrierwave-aws</code> gem <a href=""https://github.com/sorentwo/carrierwave-aws#usage"" rel=""nofollow"">readme</a> (I'm guessing you are using that or something similar) does not even mention the <code>provider</code> key. That might have been an old requirement that has been deprecated. </p>",733721,,,2015-11-13T03:01:54.463,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33685242
262,33757615,2,31978347,2015-11-17T13:00:30.470,4,"<p><strong>Update</strong> <em>Sept 2017</em>: <code>fs-promise</code> has been deprecated in favour of <a href=""https://www.npmjs.com/package/fs-extra"" rel=""nofollow noreferrer""><code>fs-extra</code></a>.</p><hr><p>I haven't used it, but you could look into <a href=""https://www.npmjs.com/package/fs-promise"" rel=""nofollow noreferrer"">fs-promise</a>. It's a node module that:</p><blockquote>  <p>Proxies all async fs methods exposing them as Promises/A+ compatible  promises (when, Q, etc). Passes all sync methods through as values.</p></blockquote>",1688034,1688034,2017-09-26T13:33:16.223,2017-09-26T13:33:16.223,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33757615
263,33934630,2,28676307,2015-11-26T09:18:18.130,0,"<p>@filename has been deprecated in PHP >= 5.5.0: </p><p>Use <strong><em>new CurlFile</em></strong> instead, will not require any changes on the receiving end.</p>",3387127,,,2015-11-26T09:18:18.130,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33934630
264,33998898,2,33995463,2015-11-30T12:39:24.750,0,"<p>The version you are trying to use is deprecated and IAM roles are required. Follow the example as given in the documentation <a href=""http://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/calling-emr-with-java-sdk.html"" rel=""nofollow"">http://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/calling-emr-with-java-sdk.html</a>. </p>",4394783,,,2015-11-30T12:39:24.750,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/33998898
265,34177254,2,34168923,2015-12-09T11:06:59.843,31,"<p>There's a problem with the premise of your question.  The fact that CloudFront <em>may</em> cache your objects for some period of time actually has little relevance when selecting an S3 storage class.</p><p><code>REDUCED_REDUNDANCY</code> is sometimes less expensive&sup1; <em>because S3 stores your data on fewer physical devices</em>, reducing the reliability somewhat in exchange for lower pricing... and in the event of failures, the object is statistically more likely to be lost by S3.  If S3 loses the object because of the reduced redundancy, CloudFront will at some point begin returning errors.  </p><p>The deciding factor in choosing this storage class is whether the object is easily replaced.</p><blockquote>  <p>Reduced Redundancy Storage (RRS) is an Amazon S3 storage option that enables customers to reduce their costs by storing noncritical, reproducible data at lower levels of redundancy than Amazon S3Š—Ès standard storage. It provides a cost-effective, highly available solution for distributing or sharing content that is durably stored elsewhere, or for storing thumbnails, transcoded media, or other processed data that can be easily reproduced. </p>   <p><a href=""https://aws.amazon.com/s3/reduced-redundancy/"" rel=""noreferrer"">https://aws.amazon.com/s3/reduced-redundancy/</a></p></blockquote><p><code>STANDARD_IA</code> (infrequent access) is less expensive for a different reason: the storage savings are offset by retrieval charges.  If an object is downloaded <em>more than once per month</em>, the combined charge will exceed the cost of <code>STANDARD</code>.  It is intended for objects that will genuinely be accessed infrequently.  Since CloudFront has multiple edge locations, each with its own independent cache,&sup2; whether an object is ""currently stored in"" CloudFront is not a question with a simple yes/no answer.  It is also not possible to ""game the system"" by specifying large <code>Cache-Control: max-age</code> values.  CloudFront has no charge for its cache storage, so it's only sensible that an object can be purged from the cache before the expiration time you specify.  Indeed, anecdotal observations confirm what the docs indicate, that objects are sometimes purged from CloudFront due to a relative lack of ""popularity.""</p><p>The deciding factor in choosing this storage class is whether the increased data transfer (retrieval) charges will be low enough to justify the storage charge savings that they offset.  Unless the object is expected to be downloaded less than once or twice a month, this storage class does not represent a cost savings.</p><p>Standard/Infrequent Access should be reserved for things you really don't expect to be needed often, like tarballs and database dumps and images unlikely to be reviewed after they are first accessed, such as (borrowing an example from my world) a proof-of-purchase/receipt scanned and submitted by a customer for a rebate claim.  Once the rebate has been approved, it's very unlikely we'll need to look at that receipt again, but we do need to keep it on file.  Hello, Standard_IA.  (Note that S3 does this automatically for me, after the file has been stored for 30 days, using a lifecycle policy on the bucket).</p><blockquote>  <p>Standard - IA is ideally suited for long-term file storage, older data from sync and share, backup data, and disaster recovery files.</p>   <p><a href=""https://aws.amazon.com/s3/faqs/#sia"" rel=""noreferrer"">https://aws.amazon.com/s3/faqs/#sia</a></p></blockquote><p>Side note: one alternative mechanism for saving some storage cost is to <code>gzip -9</code> the content before storing, and set <code>Content-Encoding: gzip</code>.  I have been doing this for years with S3 and am still waiting for my first support ticket to come in reporting a browser that can't handle it.  Even content that is allegedly already compressed -- such as <code>.xlsx</code> spreadsheets -- will often shrink a little bit, and every byte you squeeze out means slightly lower storage <em>and download bandwidth</em> charges. </p><p>Fundamentally, if your content is easily replaceable, such as resized images where you still have the original... or reports that can easily be rerun from source data... or content backed up elsewhere (AWS is essentially always my first choice for cloud services, but I do have backups of my S3 assets stored in another cloud provider's storage service, for example)... then reduced redundancy is a good option.  </p><hr><p>&sup1; <em><code>REDUCED_REDUNDANCY</code> is sometimes less expensive</em> <strong>only in some regions</strong> as of late 2016.  Prior to that, it was priced lower than <code>STANDARD</code>, but in an odd quirk of the strange world of competitive pricing, as a result of <a href=""https://aws.amazon.com/blogs/aws/aws-storage-update-s3-glacier-price-reductions/"" rel=""noreferrer"">S3 price reductions announced in November, 2016</a>, in some AWS regions, the <code>STANDARD</code> storage class is now slightly <em>less</em> expensive than <code>REDUCED_REDUNDANCY</code> (""RRS"").  For example, in us-east-1, Standard was reduced from $0.03/GB to $0.023/GB, but RRS remained at $0.024/GB... leaving <em>no</em> obvious reason to ever use RRS in that region.  The structure of the pricing pages leaves the impression that RRS may no longer be considered a current-generation offering by AWS.  Indeed, it's an older offering than both <code>STANDARD_IA</code> and <code>GLACIER</code>.  It is unlikely to ever be fully deprecated or eliminated, but they may not be inclined to reduce its costs to a point that lines up with the other storage classes if it's no longer among their primary offerings.</p><p>&sup2; <em>""CloudFront has multiple edge locations, each with its own independent cache""</em> is still a technically true statement, but CloudFront quietly began to roll out and then announced some significant architectural changes in late 2016, with the introduction of the <a href=""https://aws.amazon.com/about-aws/whats-new/2016/11/announcing-regional-edge-caches-for-amazon-cloudfront/"" rel=""noreferrer"">regional edge caches</a>.  It is now, in a sense, ""less true"" that the global edge caches are indepenent.  They still are, but it makes less of a difference, since CloudFront is now a two-tier network, with the global (outer tier) edge nodes <em>sometimes</em> fetching content from the regional (inner tier) edge nodes, instead of directly from the origin server.  This should have the impact of increasing the likelihood of an object being considered to be ""in"" the cache, since a cache miss in the outer tier might be transformed into a hit by the inner tier, which is also reported to have more available cache storage space than some or all of the outer tier. It is not yet clear from external observations how much of an impact this has on hit rates on S3 origins, as the documentation indicates the regional edges are not used for S3 (only custom origins) but it seems less than clear that this universally holds true, particularly with the introduction of <a href=""http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html"" rel=""noreferrer"">Lambda@Edge</a>. It might be significant, but as of this writing, I do not believe it to have any material impact on my answer to the question presented here.</p>",1695906,1695906,2017-07-21T20:25:36.820,2017-07-21T20:25:36.820,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34177254
266,34280684,2,31745831,2015-12-15T03:33:53.523,11,"<p>Don't know if you still need it but here you go: </p><pre><code>let credentialsProvider = AWSStaticCredentialsProvider(accessKey: ""ACCESS KEY"", secretKey: ""SECRET KEY"")  let configuration = AWSServiceConfiguration(region: .USWest2, credentialsProvider: credentialsProvider)  AWSS3.registerS3WithConfiguration(configuration, forKey: ""defaultKey"")  let s3 = AWSS3.S3ForKey(""defaultKey"")  let listRequest: AWSS3ListObjectsRequest = AWSS3ListObjectsRequest()  listRequest.bucket = ""BUCKET""  s3.listObjects(listRequest).continueWithBlock { (task) -&gt; AnyObject? in  print(""call returned"")  let listObjectsOutput = task.result;  for object in (listObjectsOutput?.contents)! {  print(object.key)  }  return nil  }</code></pre><p>(Thanks to Daniel for reminding me not to use deprecated code) ;)</p>",4024732,4024732,2017-02-16T02:34:56.277,2017-02-16T02:34:56.277,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34280684
267,34330629,2,34294720,2015-12-17T09:10:09.200,0,"<p>I trashed the original instance and created a new instance and this time I've installed node.js using the command that is on their website:</p><pre><code>sudo yum install nodejs npm --enablerepo=epel</code></pre><p>Instead of installing it from source as I did the first time. By executing the command above I ensured that I installed the latest version instead of an outdated version which I believe caused my problems with CERT_UNTRUSTED.</p>",4423934,,,2015-12-17T09:10:09.200,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34330629
268,34379703,2,33675453,2015-12-20T09:44:52.290,2,"<p>Elasticsearch binds to localhost by default now (2.x).  Use these settings in your elasticsearch.yml to bind to a physical interface.  Keep in mind that making your Elasticsearch server publicly accessible to the internet is insecure, bordering on suicidal, so be sure you have a good firewall and are only listening on a LAN interface.</p><pre><code>network.bind_host: 0.0.0.0network.publish_host: _non_loopback_</code></pre><p>If you're using the <code>cloud-aws</code> plugin for automatic discovery on AWS/EC2, you may want to automatically choose your VPC interface's IP address:</p><pre><code>network.bind_host: 0.0.0.0network.publish_host: _ec2_</code></pre><p>It's possible that <code>_ec2_</code> may be deprecated at some point and replaced with <code>_ec2:privateIpv4_</code>.</p><p>Sources:</p><ul><li><a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html"" rel=""nofollow"">Network Settings</a></li><li><a href=""https://www.elastic.co/guide/en/elasticsearch/plugins/2.1/cloud-aws-discovery.html#cloud-aws-discovery-network-host"" rel=""nofollow"">EC2 Discovery</a></li></ul>",1188377,1188377,2015-12-20T09:50:25.363,2015-12-20T09:50:25.363,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34379703
269,34479582,2,34479284,2015-12-27T10:34:07.993,2,"<p>The I2 family <a href=""https://aws.amazon.com/ec2/instance-types/"" rel=""nofollow"">are indeed</a> the I/O optimised successors to the H1 family. AWS themselves recommend them for NoSQL database operations.</p><p>Although a little out of date now <a href=""http://www.datastax.com/dev/blog/ec2-series-doc"" rel=""nofollow"">Datastax themselves recommend</a> as a progression in price and performance: M series, C series, and finally the I series.</p>",3846501,,,2015-12-27T10:34:07.993,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34479582
270,34502774,2,23687081,2015-12-29T00:28:14.737,0,"<p>NOTE: EC2 specific answer, not a general Spark answer. Just trying to round out an answer to a question asked a year ago, one that has the same symptom but often different causes and trips up a lot of people.</p><p>If I am understanding the question correctly, you are asking, ""Do I need to manually copy the jar file? If so, to which dir?"" You say, ""and set SparkConf to distribute the classes"" but you are not clear if this is done via spark-env.sh or spark-defaults.conf? So making some assumptions, the main one being your are running in cluster mode, meaning your driver runs on one of the workers and you don't know which one in advance... then...</p><p>The answer is yes, to the dir named in the classpath. In EC2 the only persistent data storage is /root/persistent-hdfs, but I don't know if that's a good idea.</p><p><a href=""http://spark.apache.org/docs/latest/ec2-scripts.html"" rel=""nofollow"">In the Spark docs on EC2 I see this line</a>: </p><pre><code>To deploy code or data within your cluster, you can log in and usethe provided script ~/spark-ec2/copy-dir, which, given a directory path, RSYNCs it to the same location on all the slaves.</code></pre><p><code>SPARK_CLASSPATH</code> </p><p>I wouldn't use SPARK_CLASSPATH because it's deprecated as of Spark 1.0 so a good idea is to use its replacement in $SPARK_HOME/conf/spark-defaults.conf: </p><pre><code>spark.executor.extraClassPath /path/to/jar/on/worker</code></pre><p>This should be the option that works. If you need to do this on the fly, not in a conf file, the recommendation is ""./spark-submit with --driver-class-path to augment the driver classpath"" (<a href=""http://spark.apache.org/docs/latest/configuration.html"" rel=""nofollow"">from Spark docs about spark.executor.extraClassPath</a> and see end of answer for another source on that). </p><p>BUT ... you are not using spark-submit ... I don't know how that works in EC2, looking at the script I didn't figure out where EC2 let's you supply these parameters on a command line. You mention you already do this in setting up your SparkConf object so stick with that if that works for you.</p><p>I see in Spark-years this is a very old question so I wonder how you resolved it? I hope this helps someone, I learned a lot researching the specifics of EC2.</p><hr><p>I must admit, as a limitation on this, it confuses me in the <a href=""http://spark.apache.org/docs/latest/configuration.html#spark.executor.extraClassPath"" rel=""nofollow"">Spark docs that for spark.executor.extraClassPath</a> it says:</p><blockquote>  <p>Users typically should not need to set this option </p></blockquote><p>I assume they mean most people will get the classpath out through a driver config option. I know most of the docs for spark-submit make it should like the script handles moving your code around the cluster but I think that's only in ""standalone client mode"" which I assume you are not using, I assume EC2 must be in ""standalone cluster mode.""</p><hr><p>MORE / BACKGROUND ON SPARK_CLASSPATH deprecation:</p><p>More background that leads me to think SPARK_CLASSPATH <a href=""https://mail-archives.apache.org/mod_mbox/spark-user/201503.mbox/%3C01a901d0547c$a23ba480$e6b2ed80$@innowireless.com%3E"" rel=""nofollow"">is deprecated is this archived thread.</a> and <a href=""https://mail-archives.apache.org/mod_mbox/spark-user/201502.mbox/%3CCAH2_pyJv78gRJuEP18YKn++5ZOJBtzP5mdqOSM6a1BaZWR-W8g@mail.gmail.com%3E"" rel=""nofollow"">this one, crossing the other thread</a> and <a href=""http://apache-spark-user-list.1001560.n3.nabble.com/SPARK-CLASSPATH-Warning-td9233.html"" rel=""nofollow"">this one about a WARN message when using SPARK_CLASSPATH</a>:</p><pre><code>14/07/09 13:37:36 WARN spark.SparkConf:SPARK_CLASSPATH was detected (set to 'path-to-proprietary-hadoop-lib/*:/path-to-proprietary-hadoop-lib/lib/*').This is deprecated in Spark 1.0+.Please instead use: - ./spark-submit with --driver-class-path to augment the driver classpath - spark.executor.extraClassPath to augment the executor classpath</code></pre>",3255525,3255525,2015-12-31T05:09:57.383,2015-12-31T05:09:57.383,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34502774
271,34553346,2,30385981,2016-01-01T03:37:11.340,53,"<p>Having experienced first hand the difference between s3a and s3n - 7.9GB of data transferred on s3a was around ~7 minutes while 7.9GB of data on s3n took 73 minutes [us-east-1 to us-west-1 unfortunately in both cases; Redshift and Lambda being us-east-1 at this time] this is a very important piece of the stack to get correct and it's worth the frustration.</p><p>Here are the key parts, as of December 2015:</p><ol><li><p>Your Spark cluster will need a Hadoop version 2.x or greater. If you use the Spark EC2 setup scripts and maybe missed it, the switch for using something other than 1.0 is to specify <code>--hadoop-major-version 2</code> (which uses CDH 4.2 as of this writing).</p></li><li><p>You'll need to include what may at first seem to be an out of date AWS SDK library (built in 2014 as version 1.7.4) for versions of Hadoop as late as 2.7.1 (stable): aws-java-sdk 1.7.4. As far as I can tell using this along with the specific AWS SDK JARs for 1.10.8 hasn't broken anything.</p></li><li><p>You'll also need the hadoop-aws 2.7.1 JAR on the classpath. This JAR contains the class <code>org.apache.hadoop.fs.s3a.S3AFileSystem</code>.</p></li><li><p>In <code>spark.properties</code> you probably want some settings that look like this:</p><p>spark.hadoop.fs.s3a.access.key=ACCESSKEY<br>  spark.hadoop.fs.s3a.secret.key=SECRETKEY</p></li></ol><p>I've detailed this list in more detail on a <a href=""http://deploymentzone.com/2015/12/20/s3a-on-spark-on-aws-ec2/"" rel=""nofollow noreferrer"">post I wrote</a> as I worked my way through this process. In addition I've covered all the exception cases I hit along the way and what I believe to be the cause of each and how to fix them.</p>",5645,2261274,2020-03-06T11:06:13.917,2020-03-06T11:06:13.917,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/34553346
272,34570519,2,34570396,2016-01-02T20:40:56.017,6,"<p>You have a very old 1.2.9 version. According to the <a href=""https://github.com/aws/aws-cli/blob/develop/CHANGELOG.rst"">changelog</a> glacier support was added in 1.7.40, and current is 1.9.15.</p><p>Perhaps it is better to install it via pip, your ubuntu package repo seems outdated.</p>",13189,,,2016-01-02T20:40:56.017,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34570519
273,34643666,2,34615932,2016-01-06T22:02:33.487,4,"<p>If you want to stick to Sidekiq as your worker backend, then you don't need to launch an  extra Elastic Beanstalk worker environment. You could start the Sidekiq process within your web environment. Please, have a look at this <a href=""https://stackoverflow.com/questions/14538940/how-can-i-run-rails-background-jobs-with-resque-on-aws-elastic-beanstalk/34575986#34575986"">question and answers from a  similar situation</a>.</p><p>Concerning the actual idea and architecture behind Elastic Beanstalk's web and worker environment, I'll try to summarize Amazon's <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html"" rel=""nofollow noreferrer"">official documentation</a>:</p><p>A <strong>Web environment</strong> hosts the HTTP server and web application that responds to the HTTP requests from your users.</p><p>A <strong>Worker environment</strong> hosts an application (can be the same as the web application, but does not have to) which executes background jobs, long running tasks, etc. </p><p>The Web and the Worker environment should be connected via an <strong><a href=""https://aws.amazon.com/de/sqs/"" rel=""nofollow noreferrer"">Amazon SQS message queue</a></strong> (an extra AWS feature/service).Your web applications should send messages to this queue, and the worker environment will consume the messages from this queue. When you launch a worker environment, you can choose which SQS queue it should be connected to. Elastic Beanstalk will automatically install and start a daemon which consumes messages from this queue and transform them into HTTP POST requests. These requests are sent to the localhost and a path that you can also define. The request's body will contain the content of the message from the queue. Your application should parse the content and trigger the background job accordingly.</p><p>You see, this would make Sidekiq actually obsolete, since the SQS queue will handle the queuing logic.</p><p>If you are not constrained to use Sidekiq and your application is written with Rails >= 4.2 then you can use the <a href=""https://github.com/tawan/active-elastic-job"" rel=""nofollow noreferrer"">Active Elastic Job</a> gem for your background tasks. It takes care of all the message sending and parsing, and allows you to keep your deployment setup simple. No need to write and maintain ebextension scripts.</p>",2375107,-1,2017-05-23T12:16:40.297,2016-01-06T22:02:33.487,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34643666
274,34665095,2,19662715,2016-01-07T21:02:05.760,10,"<p>I think it is suboptimal to run queues, like Resque, inside Elastic Beanstalk web environments. A web environment is intended to host web applications and spawn new instances when traffic and load increases. However, it would not make sense to have multiple Resque queues, each running on one of the instances.</p><p>Elastic Beanstalk offers <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html"" rel=""noreferrer"">worker environments</a> which are intended to host code that executes background tasks. These worker environments pull jobs from an <a href=""https://aws.amazon.com/de/sqs/"" rel=""noreferrer"">Amazon SQS queue</a> (which makes an additional queue solution, like Resque, obsolete). An Amazon SQS queue scales easily and is easier to maintain (AWS just does it for you).</p><p>To use worker environments, which come with Amazon SQS queues, makes more sense, as it is out of the box supported and fits nicely into the Elastic Beanstalk landscape. There also exists a gem, <a href=""https://github.com/tawan/active-elastic-job"" rel=""noreferrer"">Active Elastic Job</a>, which makes it simple for Rails &gt;= 4.2 applications to run background tasks in worker environments.</p><p>Disclaimer: I'm the author of <a href=""https://github.com/tawan/active-elastic-job"" rel=""noreferrer"">Active Elastic Job</a>.</p>",2375107,-1,2020-06-20T09:12:55.060,2016-01-13T15:52:17.580,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34665095
275,34739115,2,34736988,2016-01-12T08:51:35.193,0,"<p>You can use <code>org.apache.http.conn.ssl.SSLConnectionSocketFactory</code> instead of <code>org.apache.http.conn.ssl.SSLSocketFactory</code>.</p><p>From the JavaDoc of SSLSocketFactory (<a href=""https://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/org/apache/http/conn/ssl/SSLSocketFactory.html"" rel=""nofollow"">https://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/org/apache/http/conn/ssl/SSLSocketFactory.html</a>) - </p><blockquote>  <p>Deprecated.   (4.3) use SSLConnectionSocketFactory</p></blockquote>",3872500,,,2016-01-12T08:51:35.193,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34739115
276,34775544,2,34736988,2016-01-13T19:37:40.397,0,"<p>Some services like Cognito support TLSv1.0+, while some say DynamoDB supports only TLSv1.0 (not above). If your device supports TLSv1.0, it should work then. Since you said the code works on emulator but not on the tablet, it's hard to say where the problem is. I suggest you try these:</p><ul><li>Visit <a href=""https://dynamodb.us-west-2.amazonaws.com"" rel=""nofollow"">https://dynamodb.us-west-2.amazonaws.com</a> in your browser. If your device can handle TLSv1.0, you should see <code>healthy: dynamodb.us-west-2.amazonaws.com</code>.</li><li>Run the same code on a different device.</li></ul><p>Some notes from comments:</p><ul><li>SSLv3 has been deprecated on all AWS services.</li><li>Remove aws-java-sdk and use aws-android-sdk. The latter has lots of optimizations for Android.</li><li>aws-android-sdk uses HttpURLConnection as the default HTTP library.</li></ul><p>To test what protocols a service supports, use this command:</p><pre><code>openssl s_client -connect dynamodb.us-west-2.amazonaws.com:443</code></pre>",2026747,,,2016-01-13T19:37:40.397,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34775544
277,34864764,2,34854438,2016-01-18T21:55:13.170,0,"<p>It seems you are trying to use <code>boto</code>, which is slowly becoming obsolete.</p><p>In long term, changing to <code>boto3</code> is inevitable as older <code>boto</code> is not really maintained. See <a href=""https://github.com/boto/boto/issues/3306"" rel=""nofollow noreferrer""><code>boto3</code> is not a replacement of <code>boto</code> (yet?)</a></p><p>You may find example of uploading files here: <a href=""https://stackoverflow.com/a/29636604/346478"">https://stackoverflow.com/a/29636604/346478</a></p>",346478,-1,2017-05-23T11:44:52.873,2016-01-18T21:55:13.170,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34864764
278,34880340,2,33613068,2016-01-19T15:13:12.903,0,"<p>I had a similar problem where the culprit was an outdated system clock. EC2 instances can sometimes drift and IAM API is very sensitive to it. Relevant information can be found here: <a href=""https://github.com/boto/boto/issues/2885"" rel=""nofollow"">https://github.com/boto/boto/issues/2885</a>.</p>",5811137,,,2016-01-19T15:13:12.903,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/34880340
279,35068909,2,35067388,2016-01-28T18:11:31.233,7,"<p>Solved!</p><p>It appears that the tutorial is outdated.</p><p>I needed to update wercker.yml to work with Wercker v2. </p><p>To do this, I changed:<code>box: wercker/ruby</code>to<code>box: ruby</code>.</p>",3574603,,,2016-01-28T18:11:31.233,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/35068909
280,35100734,2,35100394,2016-01-30T10:37:40.780,1,"<p>That is because of space in your <code>:rvm_ruby_version</code> causing deprecated-like rvm commands to be generated, try replacing to:</p><pre><code>set :rvm_ruby_version, 'ruby-2.2.1p85'</code></pre>",177053,,,2016-01-30T10:37:40.780,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/35100734
281,35101653,2,35101317,2016-01-30T12:14:28.150,9,"<p>Add a valid version to your $params array, e.g:</p><pre><code>""ResponseGroup"" =&gt; ""Images,ItemAttributes,Offers"",""Version"" =&gt; ""2015-10-01""</code></pre><p>I tested your script and found it works with the above.</p><p>The cause of the issue seems to be that the API defaults to a deprecated value if you omit the version parameter.</p>",4341900,4341900,2016-02-23T11:28:00.163,2016-02-23T11:28:00.163,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/35101653
282,35165401,2,27350634,2016-02-02T22:57:34.640,226,"<p>Like others, the accepted answer also gave me an outdated version.</p><p>Here is another way to do it that works very well:</p><pre class=""lang-sh prettyprint-override""><code>$ curl --silent --location https://rpm.nodesource.com/setup_12.x | bash -$ yum -y install nodejs</code></pre><p>You can also replace the 12.x with another version, such as 10.x, 8.x, etc.</p><p>You can see all available versions on the <a href=""https://github.com/nodesource/distributions/tree/master/rpm"" rel=""noreferrer"">NodeSource Github page</a>, and pull from there as well if desired.</p><p>Note: you may need to run using <code>sudo</code> depending on your environment.</p>",2518231,2518231,2019-09-17T01:46:27.070,2019-09-17T01:46:27.070,7,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/35165401
283,35174546,2,35174076,2016-02-03T10:32:45.423,4,"<p>Don't think of CloudFront as primarily <em>""hosting""</em> your site. CloudFront <em>caches</em> your site to a) reduce traffic to the origin web server (not a big concern when hosting on S3 to begin with) and b) to speed up delivery to clients since CloudFront caches are geographically distributed throughout the globe, instead of just having one centralised server.</p><p>Yes, you can host a website this way. Put the actual ""master copy"" on S3, configure CloudFront as a front-end cache to it. The master copy needs to stay where it is; in case you were thinking of removing it from S3 once it's cached in CloudFront, that would be a bad idea. CloudFront makes no guarantees about keeping your data available; caches may expire at any time, entire CloudFront nodes may be taken offline, replaced, or added at any time, and in all those cases the CloudFront node needs an origin server to get its copy from.</p><p>The only problem left then is <em>cache expiry</em>. As always, there are two approaches:</p><ol><li><p>Configure a sensible cache timeout for your content. If you set your content to expire after, say, 1 hour, then any client may see data which is outdated by up to an hour after you update your content. This may or may not be a problem, you decide. You can and should configure different expiry times for different kinds of content; images and such can probably be cached indefinitely, while the HTML of an often-updated front page should probably have much shorter life times.</p></li><li><p>Explicitly flush the cache with an invalidation request after you update your content. The problem with this is that you only have a limited number of free invalidation requests on CloudFront. It is not something AWS likes you to do and is mostly reserved as a tool for emergencies when incorrect data got pushed out. AWS prefers you to let content expire naturally, since this puts the least stress on the network.</p></li></ol><p>""Pre-warming"" is not typically necessary and difficult to do anyway, since you cannot <em>push</em> content to CloudFront; CloudFront <em>pulls</em> content from the origin as needed. I'm not sure what ""armfront"" you're referring to exactly or what it supposedly does. Without pre-warming, the very first request for a particular page in a particular geographic region will be ever so slightly slower, that's all.</p>",476,,,2016-02-03T10:32:45.423,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/35174546
284,35181781,2,34834290,2016-02-03T16:00:04.407,1,"<p>Docker Hub has deprecated pulls from Docker clients on 1.5 and earlier. Make sure that your docker client version is at least above 1.5. See <a href=""https://blog.docker.com/2015/10/docker-hub-deprecation-1-5/"" rel=""nofollow"">https://blog.docker.com/2015/10/docker-hub-deprecation-1-5/</a> for more information.</p>",2780476,,,2016-02-03T16:00:04.407,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/35181781
285,35186240,2,25578219,2016-02-03T19:44:39.027,3,"<p>The accepted answer is out of date. This is possible using the ""Origin Path"" setting in AWS which will rewrite the request to a sub-folder on the origin:</p><p><a href=""http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesOriginPath"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesOriginPath</a></p>",1528315,,,2016-02-03T19:44:39.027,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/35186240
286,35639822,2,35453029,2016-02-25T22:37:53.640,0,"<p>It does not seem that registerCognitoWithConfiguration() is deprecated. </p><p><a href=""https://github.com/aws/amazon-cognito-ios/blob/master/Cognito/AWSCognitoService.m#L118"" rel=""nofollow"">https://github.com/aws/amazon-cognito-ios/blob/master/Cognito/AWSCognitoService.m#L118</a></p><p><a href=""https://github.com/aws/amazon-cognito-ios/blob/master/Cognito/AWSCognitoService.h#L232"" rel=""nofollow"">https://github.com/aws/amazon-cognito-ios/blob/master/Cognito/AWSCognitoService.h#L232</a></p>",4785056,,,2016-02-25T22:37:53.640,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/35639822
287,35644902,2,35644640,2016-02-26T06:21:57.947,1,"<p>This books seems to be obsolete, it's better to use the doc on the <a href=""https://www.npmjs.com/package/multer"" rel=""nofollow"">npmjs page</a>. For example:</p><pre><code>var express = require('express');var app = express();var router = express.Router();var multer = require('multer');var upload = multer({ dest: __dirname + '/uploads' });// single file uploadrouter.post('/upload', upload.single('test'), function (req, res, next) {  var file = req.file;  console.log(file);});// multiple filesrouter.post('/photos/upload', upload.array('photos', 12), function (req, res, next) {  // req.files is array of `photos` files   // req.body will contain the text fields, if there were any })</code></pre>",5388620,,,2016-02-26T06:21:57.947,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/35644902
288,35745599,2,27421972,2016-03-02T11:13:54.630,0,"<p>With the aws-sdk v2: I had to do this: (see <a href=""http://docs.aws.amazon.com/sdkforruby/api/Aws/Resources/Batch.html"" rel=""nofollow"">Doc</a>)</p><pre><code>$s3.bucket(""my-bucket"").objects(prefix: 'my_folder/').batch_delete!</code></pre><p>(<code>delete</code> is deprecated in favor of <code>batch_delete</code>)</p><p>Useful post: <a href=""https://ruby.awsblog.com/post/Tx1H87IVGVUMIB5/Using-Resources"" rel=""nofollow"">https://ruby.awsblog.com/post/Tx1H87IVGVUMIB5/Using-Resources</a></p>",311744,311744,2016-03-03T16:27:13.003,2016-03-03T16:27:13.003,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/35745599
289,36154877,2,36118668,2016-03-22T12:50:42.170,1,"<p>The <code>BroadcastReceiver</code> has been deprecated and AWS have not updated their docs yet. But you can simply create an <code>Intent</code> from the supplied data <code>Bundle</code> in the <code>GcmListenerService</code>:</p><pre><code>@Overridepublic void onMessageReceived(String from, Bundle data) {  CognitoSyncManager manager;  try {  manager = CognitoSyncClientManager.getInstance();  } catch (IllegalStateException ise) {  Log.w(TAG, ""sync manager is not initialized"");  return;  }  Intent intent = new Intent();  intent.putExtras(data);  PushSyncUpdate update = manager.getPushSyncUpdate(intent);  //...</code></pre>",6098584,,,2016-03-22T12:50:42.170,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36154877
290,36212531,2,36198573,2016-03-25T01:03:16.720,0,"<p>Unless you can do all activities in shell or CLI, it is not possible to do everything in the same instance.</p><p>One suggestion I can give is to move on to new technologies. AWS Data Pipeline is outdated (4 years old). You should use AWS Lambda which will cost you a fraction of what you are paying and you can load the files into Redshift as soon as the files are uploaded to S3. Clean up is automatic and Lambda is much more powerful than AWS Data Pipeline. The tutorial <a href=""https://blogs.aws.amazon.com/bigdata/post/Tx24VJ6XF1JVJAA/A-Zero-Administration-Amazon-Redshift-Database-Loader"" rel=""nofollow"">A Zero-Administration Amazon Redshift Database Loader</a> is the one you want. Yes, there is some learning curve, but as the title suggest it is a zero administration load.</p>",4237701,,,2016-03-25T01:03:16.720,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36212531
291,36339430,2,36239224,2016-03-31T16:59:51.683,2,"<p><strong>Update: <a href=""https://developer.amazon.com/docs/custom-skills/literal-slot-type-reference.html"" rel=""nofollow noreferrer"">This is no longer possible</a> as of October 2018.</strong></p><blockquote>  <p>AMAZON.LITERAL is deprecated as of October 22, 2018. Older skills  built with AMAZON.LITERAL do continue to work, but you must migrate  away from AMAZON.LITERAL when you update those older skills, and for  all new skills.</p></blockquote><p><s>You can use the <a href=""https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-interaction-model-reference#LITERAL%20Slot%20Type%20Reference"" rel=""nofollow noreferrer""><code>AMAZON.LITERAL</code></a> slot type to capture freeform text. Amazon recommends providing sample phrases, <a href=""https://forums.developer.amazon.com/forums/message.jspa?messageID=17841"" rel=""nofollow noreferrer"">but according to this thread</a>, you may be able to get away with not providing them.</s></p>",662761,662761,2019-01-03T15:32:57.243,2019-01-03T15:32:57.243,7,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/36339430
292,36343469,2,36342920,2016-03-31T20:47:41.677,7,"<p>You are trying to use <code>boto</code> library, which is rather obsolete and not maintained. The number ofissues with this library is growing.</p><p>Better use currently developed <code>boto3</code>.</p><p>First, let us define parameters of our search:</p><pre><code>&gt;&gt;&gt; bucket_name = ""bucket_of_m""&gt;&gt;&gt; prefix = ""region/cz/""</code></pre><p>Do import <code>boto3</code> and create s3 representing S3 resource:</p><pre><code>&gt;&gt;&gt; import boto3&gt;&gt;&gt; s3 = boto3.resource(""s3"")</code></pre><p>Get the bucket:</p><pre><code>&gt;&gt;&gt; bucket = s3.Bucket(name=bucket_name)&gt;&gt;&gt; buckets3.Bucket(name='bucket_of_m')</code></pre><p>Define filter for objects with given prefix:</p><pre><code>&gt;&gt;&gt; res = bucket.objects.filter(Prefix=prefix)&gt;&gt;&gt; ress3.Bucket.objectsCollection(s3.Bucket(name='bucket_of_m'), s3.ObjectSummary)</code></pre><p>and iterate over it:</p><pre><code>&gt;&gt;&gt; for obj in res:...   print obj.key...   print obj.size...   print obj.last_modified...</code></pre><p>Each <code>obj</code> is ObjectSummary (not Object itself), but it holds enought to learn something about it</p><pre><code>&gt;&gt;&gt; objs3.ObjectSummary(bucket_name='bucket_of_m', key=u'region/cz/Ostrava/Nadrazni.txt')&gt;&gt;&gt; type(obj)boto3.resources.factory.s3.ObjectSummary</code></pre><p>You can get Object from it and use it as you need:</p><pre><code>&gt;&gt;&gt; o = obj.Object()&gt;&gt;&gt; os3.Object(bucket_name='bucket_of_m', key=u'region/cz/rodos/fusion/AdvancedDataFusion.xml')</code></pre><p>There are not so many options for filtering, but prefix is available.</p>",346478,,,2016-03-31T20:47:41.677,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36343469
293,36368873,2,31799589,2016-04-02T03:35:38.763,0,"<p>I have faced this very issue and tried the current accepted answer but it looks like Kubernetes is changing quite fast what may make this answer also outdated soon.</p><p>To this date, I've tested the solution below that might become or not a definitive solution in the future:</p><p>There is <a href=""https://github.com/kubernetes/kubernetes/pull/23776/files"" rel=""nofollow"">this PR on Kubernetes' github project</a> that implements an easy way to ignore the SSD storage by setting <code>KUBE_AWS_STORAGE=ebs</code> before running <code>kubernetes/cluster/kube-up.sh</code>.</p><p>Hope it is helpful!</p>",1463744,,,2016-04-02T03:35:38.763,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36368873
294,36585871,2,32592571,2016-04-12T23:27:47.317,70,"<p>This was an incredibly difficult issue to deal with, for two reasons:</p><ol><li><p>The fact that CloudFront is <strong>mirroring</strong> our Rails appŠ—Ès response headers requires you to twist your mind around.  The CORS protocol is hard enough to understand as it is, but now you have to follow it at two levels: between the browser and CloudFront (when our Rails app uses it as a CDN), and between the browser and our Rails app (when some malicious site wants to abuse us).</p><p>CORS is really about a dialog between the browser and the 3rd-party resources a web page wants to access.   (In our use-case, thatŠ—Ès the CloudFront CDN, serving assets for our app.)   But since CloudFront gets its Access-Control response headers <em>from</em> our app, our app needs to serve those headers <em>as if</em> it is CloudFront talking, and <em>simultaneously</em> not grant permissions that would expose itself to the type of abuse that led to the Same-Origin Policy / CORS being developed in the first place.  In particular, we should not grant <code>*</code> access to <code>*</code> resources on our site.</p></li><li><p>I found <strong>so much</strong> outdated information out there -- an endless line of blog posts and SO threads.   CloudFront has improved its CORS support significantly since many of those posts, although it is still not perfect.   (CORS should really be handled out-of-the-box.)  And the gems themselves have evolved.</p></li></ol><p>My setup: Rails 4.1.15 running on Heroku, with assets served from CloudFront.  My app responds to both http and https, on both ""www."" and the zone apex, without doing any redirection.</p><p>I looked briefly at the font_assets gem mentioned in the question, but quickly dropped it in favor of rack-cors, which seemed more on point.  I did not want to simply open up all origins and all paths, as that would defeat the point of CORS and the security of the Same-Origin Policy, so I needed to be able to specify the few origins I would allow.  Finally, I personally favor configuring Rails via individual <code>config/initializers/*.rb</code> files rather than editing the standard config files (like <code>config.ru</code> or <code>config/application.rb</code>)  Putting all that together, here is my solution, which I believe is the best available, as of 2016-04-16:</p><ol><li><p><strong>Gemfile</strong></p><pre><code>gem ""rack-cors""</code></pre><p>The rack-cors gem implements the CORS protocol in a Rack middleware.In addition to setting Access-Control-Allow-Origin and related headers on approved origins, it adds a <code>Vary: Origin</code> response header, directing CloudFront to cache the responses (including the response headers) for each origin separately.  This is crucial when our site is accessible via multiple origins (e.g. via both http and https, and via both ""www."" and the bare domain)</p></li><li><p><strong>config/initializers/rack-cors.rb</strong></p><pre><code>## Configure Rack CORS Middleware, so that CloudFront can serve our assets.## See https://github.com/cyu/rack-corsif defined? Rack::Cors  Rails.configuration.middleware.insert_before 0, Rack::Cors do  allow do  origins %w[  https://example.com  http://example.com  https://www.example.com  http://www.example.com  https://example-staging.herokuapp.com  http://example-staging.herokuapp.com  ]  resource '/assets/*'  end  endend</code></pre><p>This tells the browser that it may access resources on our Rails app (and by extension, on CloudFront, since it is mirroring us) only on behalf of our Rails app (and <em>not</em> on behalf of malicious-site.com) and only for <code>/assets/</code> urls (and <em>not</em> for our controllers).  In other words, allow CloudFront to serve assets but don't open the door any more than we have to.</p><p>Notes:</p><ul><li>I tried inserting this <em>after</em> rack-timeout instead of at the head of the middleware chain.It worked on dev but was not kicking in on Heroku, despitehaving the same middleware (other than Honeybadger).</li><li><p>The origins list could also be done as Regexps.Be careful to anchor patterns at the end-of-string.</p><pre><code>origins [  /\Ahttps?:\/\/(www\.)?example\.com\z/,  /\Ahttps?:\/\/example-staging\.herokuapp\.com\z/]</code></pre><p>but I think itŠ—Ès easier just to read literal strings.</p></li></ul></li><li><p><strong>Configure CloudFront to pass the browser's Origin request header on to our Rails app.</strong></p><p>Strangely, it appears that CloudFront forwards the Origin header from the browser to our Rails app <em>regardless</em> whether we add it here, but that CloudFront honors our appŠ—Ès <code>Vary: Origin</code> caching directive only if Origin is explicitly added to the headers whitelist (as of April 2016).</p><p>The request header whitelist is kind of buried.</p><p>If the distribution already exists, you can find it at:</p><ul><li><a href=""https://console.aws.amazon.com/cloudfront/home#distributions"">https://console.aws.amazon.com/cloudfront/home#distributions</a></li><li>select the distribution</li><li>click Distribution Settings</li><li>go to the Behaviors tab</li><li>select the behavior (there will probably be only one)</li><li>Click Edit</li><li><strong>Forward Headers: Whitelist</strong></li><li><strong>Whitelist Headers:</strong> Select <strong>Origin</strong> and click <strong>Add >></strong></li></ul><p><br>If you have not created the distribution yet, create it at:</p><ul><li><a href=""https://console.aws.amazon.com/cloudfront/home#distributions"">https://console.aws.amazon.com/cloudfront/home#distributions</a></li><li><p>Click Create Distribution</p><p>(For the sake of completeness and reproducibility, I'm listing all the settings I changed from the defaults, however the Whitelist settings are the only ones that are relevant to this discussion)</p></li><li><p>Delivery Method: Web (not RTMP)</p></li><li><p>Origin Settings</p><ul><li>Origin Domain Name: example.com</li><li>Origin SSL Protocols: TLSv1.2 ONLY</li><li>Origin Protocol Policy: HTTPS only</li></ul></li><li><p>Default Cache Behavior Settings</p><ul><li>Viewer Protocol Policy: Redirect HTTP to HTTPS</li><li><strong>Forward Headers: Whitelist</strong></li><li><strong>Whitelist Headers:</strong> Select <strong>Origin</strong> and click <strong>Add >></strong></li><li>Compress Objects Automatically: Yes</li></ul></li></ul></li></ol><p>After changing all these things, remember that it can take some time for any old, cached values to expire from CloudFront.  You can explicitly invalidate cached assets by going to the CloudFront distribution's Invalidations tab and creating an invalidation for <code>*</code>.</p>",577438,,,2016-04-12T23:27:47.317,12,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36585871
295,36655183,2,35601198,2016-04-15T19:14:08.667,0,"<p>I've had this problem before and it turned out the EC2 I was on had an old version of docker. There was no error, it just died trying to transfer stuff. Eventually I found an error buried in some system log that mentioned that my docker version was deprecated so I updated Docker and everything magically worked.</p><p>In a nutshell: try using the latest ECS optimized image for your region from <a href=""http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html</a> and see if that fixes the problem.</p>",5748922,,,2016-04-15T19:14:08.667,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36655183
296,36676157,2,36637964,2016-04-17T11:56:20.850,1,"<p>I would debug this by placing a <code>set -x</code> at the top of the file so that you can see all the command output.</p><p>I've suffered a lot lately from using quotation marks too much or too little in my bash scripts lately. I'm not an expert, but it seems to me you might be double wrapping some variables in quotations. That could create an incorrect digest.</p><p>Also, the way you're creating the digest seems a bit error prone since you're doing some modifications on the openssl output. There must be a way to avoid that our get openssl to create the digest as Amazon specifies. Going to peek at that.</p><p><strong>Also</strong>, in the <a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html"" rel=""nofollow"">documentation </a> they provide a way for you to test that you are creating the signature correctly. I would try out your signature logic on that and that would narrow it down to being a problem with the way you're doing the SHA256.</p><p>And here's a <a href=""https://gist.github.com/chrismdp/6c6b6c825b07f680e710"" rel=""nofollow"">gist</a> with some like minded individuals. Saw some blog posts that were outdated, and this gist seems to diverge from the documentation, but I appreciated their simple take on SHAing. Perhaps you should share your question there and ask them to update the gist.</p><p>I'm sorry this isn't more of an answer, but was too long for a comment. I hope it does get answered better! Always interested in learning more about openssl.</p>",456809,456809,2016-04-17T12:34:47.957,2016-04-17T12:34:47.957,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36676157
297,36701425,2,36701317,2016-04-18T18:15:40.890,2,"<p>There is a class with the same name in another package which is not <code>@Deprecated</code>:</p><p><a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/cloudfront/AmazonCloudFrontClient.html"" rel=""nofollow"">Docs for <code>com.amazonaws.services.cloudfront.AmazonCloudFrontClient</code></a></p><p>The deprecated class is in the namespace <code>com.amazonaws.services.cloudfront_2012_03_15.AmazonCloudFrontClient</code>.</p>",4464702,,,2016-04-18T18:15:40.890,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36701425
298,36708152,2,36706182,2016-04-19T03:42:17.277,1,"<p>It's likely that your kubectl client is out of date, because your command line works for me:</p><pre><code>$ kubectl versionClient Version: version.Info{Major:""1"", Minor:""2"", GitVersion:""v1.2.0"", GitCommit:""5cb86ee022267586db386f62781338b0483733b3"", GitTreeState:""clean""}Server Version: version.Info{Major:""1"", Minor:""2"", GitVersion:""v1.2.2"", GitCommit:""528f879e7d3790ea4287687ef0ab3f2a01cc2718"", GitTreeState:""clean""}$ kubectl run -i --tty busybox --image=busybox --restart=NeverWaiting for pod default/busybox-dikev to be running, status is Pending, pod ready: falseHit enter for command prompt/ #</code></pre>",4215791,,,2016-04-19T03:42:17.277,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36708152
299,36755011,2,29633635,2016-04-20T20:55:15.630,5,"<p>You can't change username. You can check the following links that describe how to change master password and if Amazon adds the ability to change username you will find there:</p><p>Try to find at <a href=""http://docs.aws.amazon.com/cli/latest/reference/rds/modify-db-instance.html"" rel=""noreferrer"">AWS CLI for RDS</a>:</p><pre><code> modify-db-instance --db-instance-identifier &lt;value&gt; --master-user-password (string)</code></pre><p>--master-user-password (string)</p><blockquote>  <p>The new password for the DB instance master user. Can be any printable  ASCII character except ""/"", """""", or ""@"".</p>   <p>Changing this parameter does not result in an outage and the change is  asynchronously applied as soon as possible. Between the time of the  request and the completion of the request, the MasterUserPassword  element exists in the PendingModifiedValues element of the operation  response. Default: Uses existing setting</p>   <p>Constraints: Must be 8 to 41 alphanumeric characters (MySQL, MariaDB,  and Amazon Aurora), 8 to 30 alphanumeric characters (Oracle), or 8 to  128 alphanumeric characters (SQL Server).</p></blockquote><p><em>The Amazon RDS Command Line Interface (CLI) has been deprecated. Instead, use the AWS CLI for RDS.</em></p><p>Via the <a href=""http://docs.aws.amazon.com/AmazonRDS/latest/CommandLineReference/CLIReference-cmd-ModifyDBInstance.html"" rel=""noreferrer"">AWS Management Console</a>, choose the instance you need to reset the password for, click Š—…ModifyŠ—È then choose a new master password.</p><blockquote>  <p>If  you donŠ—Èt want to use the AWS Console, you can use the  rds-modify-db-instance command (as per AmazonŠ—Ès documentation for RDS)  to reset it directly, given the AWS command line tools:   rds-modify-db-instance instance-name --master-user-password  examplepassword</p></blockquote>",3479860,3479860,2016-05-19T04:25:18.980,2016-05-19T04:25:18.980,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36755011
300,36969249,2,36969184,2016-05-01T16:51:44.537,6,"<p>'aws s3 ls --recursive' was added in version <a href=""https://github.com/aws/aws-cli/blob/develop/CHANGELOG.rst#1211"" rel=""nofollow"">1.2.11</a> - you are using version 1.2.9 - an outdated version. Please upgrade to the latest version.</p><pre><code>pip install -U awscli</code></pre>",1779712,1779712,2016-05-03T21:17:35.093,2016-05-03T21:17:35.093,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36969249
301,36988951,2,36964256,2016-05-02T18:15:19.697,0,"<p>This line has a bug:</p><pre><code>AWSSNS *snsManager = [[AWSSNS new] initWithConfiguration:configuration];</code></pre><p><code>- new</code> is an equivalent of <code>- alloc</code> plus <code>- init</code>. You are calling two <code>init</code> methods. It should be changed to:</p><pre><code>AWSSNS *snsManager = [[AWSSNS alloc] initWithConfiguration:configuration];</code></pre><p>Also, <code>- initWithConfiguration:</code> had been deprecated for a while, and now it's removed from the SDK. You need to use <a href=""http://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSSNS.html#//api/name/defaultSNS"" rel=""nofollow""><code>+ defaultSNS</code></a> or <a href=""http://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSSNS.html#//api/name/SNSForKey:"" rel=""nofollow""><code>+ SNSForKey:</code></a> instead.</p>",1721492,,,2016-05-02T18:15:19.697,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/36988951
302,37164062,2,37137269,2016-05-11T13:23:53.303,0,"<p>This is typical cargo-cult coding that you don't understand what are you copying , especially the argparse part, the <strong>name</strong> part, even the for next part. Also boto is deprecated! boto is deprecated! boto is deprecated! Use boto3 .</p><pre><code>import argparseimport boto3def get_ec2_instances(Env,Role):  ec2 = boto3.client(""ec2"")  reservations = ec2.describe_instances(Filters=  [{  ""Name"": ""tag:environment"",  ""Values"" : [Env],  },{  ""Name"": ""tag:role"",  ""Values"" : [Role]  }])  # following for diagnostic purposes  # reservations = ec2.describe_instances()  for reservation in reservations[""Reservations""] :  for instance in reservation[""Instances""]:  print ""instance {} : Name:{} "".format(instance[""InstanceId""], instance[""Tags""][0]['Value'])if  __name__ == '__main__':  parser = argparse.ArgumentParser()  parser.add_argument('env', default=""environment"", help='value for tag:environment');  parser.add_argument('role', default=""role"", help='value for tag:role');  args = parser.parse_args()  get_ec2_isntance(args.env, args.role )</code></pre><p><a href=""http://sysadminandnetworking.blogspot.de/2013/12/aws-ec2-cli-filter-by-tag-name-and-value.html"" rel=""nofollow"">filters is one of the poorly documented boto3 ""magic"" </a></p>",6017840,6017840,2016-05-30T09:08:59.677,2016-05-30T09:08:59.677,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37164062
303,37341589,2,37247975,2016-05-20T08:22:37.410,1,"<p>Short answer: the wrapper you are using doesn't support custom encryption key, due to obsoleted boto2 implementation.</p><p>Long answer : Here is the source of <a href=""https://www.google.com/?gws_rd=ssl#q=S3BotoStorage"" rel=""nofollow"">S3BotoStorage</a>. Now here come the puzzle of <a href=""http://boto.cloudhackers.com/en/latest/ref/s3.html"" rel=""nofollow"">how boto2 saving file</a>. Where did you find the header? So I see they suggest something like this </p><pre><code>AWS_HEADERS = {  'Expires': 'Thu, 15 Apr 2010 20:00:00 GMT',  'Cache-Control': 'max-age=86400',}</code></pre><p>There is no example for encryption headers passing to boto. The header you given, is for REST API, not the S3BotoStorage wrapper.  So you may only able to use this and forget about setting the AWS_HEADERS for the encryption algorithm, give custom encryption key. </p><p>And the confusing part is , in boto2, S3 object name are call key (in boto3, they improve it and call it key_name explicitly) . This has nothing to do with encryption key. </p>",6017840,6017840,2016-05-20T09:34:46.563,2016-05-20T09:34:46.563,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37341589
304,37353748,2,37257296,2016-05-20T18:41:45.660,6,"<p>With the investigative help of @RodrigoM and your question update, it all started to make sense. There are actually two distinct problems that contribute to the error you observe:</p><ul><li>Your openssl installation does not have the certificate chain needed to verify the Amazon server in its trusted certs store...</li><li>...which is the exact situation that should be solved by adding <code>Aws.use_bundled_cert!</code> to an initializer, <a href=""https://github.com/aws/aws-sdk-ruby/blob/aeee0c99f0466e0435755bf81a80cbb2e4d1b569/aws-sdk-core/lib/aws-sdk-core.rb#L339"" rel=""noreferrer"">according to the docs</a>. But in this case it does not work because even though this command instructs the ruby openssl library to add various CA certs to the trusted store from the <code>aws-sdk-core</code> gem's <a href=""https://github.com/aws/aws-sdk-ruby/blob/ef9f2d3655c4bacb3adff580a6b295df510f78fe/aws-sdk-core/ca-bundle.crt"" rel=""noreferrer"">CA bundle file</a>, <strong>the file also does not contain the proper CA certificate as it is itself almost 2 years old and outdated</strong>. The intermediate CA cert <a href=""https://www.digicert.com/CACerts/DigiCertBaltimoreCA-2G2.crt"" rel=""noreferrer""><code>CN=DigiCert Baltimore CA-2 G2</code></a> has been published Dec 8, 2015, so no wonder that the CA bundle file does not contain it.</li></ul><p>Now, you have two options:</p><ul><li><p>You can try to install this intermediate CA certificate, probably including the root CA cert (<a href=""https://www.digicert.com/CACerts/BaltimoreCyberTrustRoot.crt"" rel=""noreferrer""><code>CN=Baltimore CyberTrust Root</code></a>), to your openssl trusted certs store. This should make the <code>s_client</code> command work. But you might still run into issues using these trusted certs from ruby code. For concrete steps for making it work under ruby on OSX, refer to the <strong><em>Solution</em></strong> section of <a href=""https://stackoverflow.com/questions/36966650/ruby-nethttp-responds-with-opensslsslsslerror-certificate-verify-failed"">this SO question</a>.</p></li><li><p>Also, since you are using a forked <code>aws-sdk-ruby</code> gem repository anyway, you may as well <strong>update the <a href=""https://github.com/StevenHarlow/aws-sdk-ruby/blob/master/aws-sdk-core/ca-bundle.crt"" rel=""noreferrer""><code>ca-bundle.crt</code> file in your repo</a></strong> by adding the intermediate CA cert yourself (the root CA cert seems to be already present in the bundle). For this you need to do the following:</p><ul><li>download the intermediate CA cert from the official page of the <a href=""https://www.digicert.com/digicert-root-certificates.htm"" rel=""noreferrer"">DigicertCA certificates</a> (you can as well use the direct link above, but to obey security rules precisely you should also check the fingerprints)</li><li><p>convert it to the PEM format (it gets downloaded in DER format) and add it to the cert bundle using the following openssl command:</p><pre><code>openssl x509 -in DigiCertBaltimoreCA-2G2.crt -inform DER &gt;&gt; ca-bundle.crt</code></pre><p>after running this command, your <code>ca-bundle.crt</code> should contain the intermediate CA certificate at the end of the file.</p></li><li><p>Now simply push this updated bundle file to your repo and the <strong><code>Aws.use_bundled_cert!</code> should start working</strong>!</p></li><li>If you care, perhaps the best would be also to start a github issue at the <code>aws-sdk-ruby</code> gem so that they update the cert bundle in their repo too...</li></ul></li></ul>",1544012,-1,2017-05-23T11:59:52.367,2016-05-20T18:41:45.660,7,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37353748
305,37357678,2,37348878,2016-05-21T00:29:15.900,0,"<p>The short answer is NO. You have to buy a domain name.</p><p>You can read the gritty details <a href=""https://cabforum.org/wp-content/uploads/Guidance-Deprecated-Internal-Names.pdf"" rel=""nofollow"">here</a></p><p>You can read the friendly GoDaddy version <a href=""https://www.godaddy.com/help/can-i-request-a-certificate-for-an-intranet-name-or-ip-address-6935"" rel=""nofollow"">here</a></p><p>But the sum of it is:</p><blockquote>  <p>Effective October 1, 2016, Certification Authorities (CAs) must revoke SSL certificates that use intranet names or IP addresses.</p></blockquote><p>So, just buy a domain name. They are cheap. You can even buy one directly from <a href=""https://console.aws.amazon.com/route53/home"" rel=""nofollow"">AWS Route 53</a></p>",4336500,,,2016-05-21T00:29:15.900,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37357678
306,37360108,2,37335496,2016-05-21T07:23:05.403,1,"<p>The <a href=""https://www.rabbitmq.com/management.html"" rel=""nofollow"">RabbitMQ management client</a> plug-in has an api to get a list of all available queues: <code>/api/queues</code></p><p>Follow the link to the API on that page for more detail (haven't placed link in answer because link will fast go out of date with new API releases).</p>",4151019,,,2016-05-21T07:23:05.403,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37360108
307,37363867,2,37347003,2016-05-21T13:59:28.177,2,"<p>Your demo repo does not appear to be including the AWS SDK &amp; setting the region as noted in the <a href=""https://github.com/awslabs/dynamodb-document-js-sdk"" rel=""nofollow"">Getting Started guide</a>.  I.e.:</p><pre><code>var AWS = require(""aws-sdk"");var DOC = require(""dynamodb-doc"");AWS.config.update({region: ""us-west-1""});var docClient = new DOC.DynamoDB();... </code></pre><p>Note that <code>dynamo-doc</code> was deprecated almost a year ago.  You may want to try the <a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html"" rel=""nofollow"">DynamoDB DocumentClient</a> instead.  This updated API has much more clear error-handling semantics that will probably help point out where the problem is.</p>",467512,,,2016-05-21T13:59:28.177,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37363867
308,37365182,2,37365009,2016-05-21T16:04:00.787,6,"<p>I'm looking at the latest API docs <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html"" rel=""noreferrer"">here</a>, and it looks like only <code>AWSLambdaAsyncClient.invokeAsyncAsync()</code> is deprecated. The <code>AWSLambdaAsyncClient.invokeAsync()</code> method is not marked as deprecated. It looks like they are just doing some code cleanup by removing the need for the <code>InvokeAsyncRequest</code> and <code>InvokeAsyncResult</code> classes and the extra <code>invokeAsyncAsync()</code> methods.</p><p>You should be able to use the <code>AWSLambdaAsyncClient.invokeAsync()</code> method which uses <code>InvokeRequest</code> and returns <code>InvokeResult</code>. You might have to set the <code>InvocationType</code> on the <code>InvokeRequest</code> to <code>InvocationType.Event</code>. It's not clear if that's needed if you are using the Async client.</p><p>Regarding your second question about calling Lambda functions asynchronously without using the SDK, I would look into using <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-lambda.html"" rel=""noreferrer"">API Gateway as a service proxy</a>. This is the recommended way to expose Lambda functions for asynchronous calls.</p>",13070,,,2016-05-21T16:04:00.787,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37365182
309,37427657,2,37426707,2016-05-25T04:06:56.533,1,"<p>You haven't specified which MongoDB version you're using, but since you're showing YAML configuration, I'll assume it's 2.6 or later.</p><p>You can enable the REST interface via mongodb.conf with the following (per <a href=""https://docs.mongodb.com/v3.0/reference/configuration-options/"" rel=""nofollow"">https://docs.mongodb.com/v3.0/reference/configuration-options/</a>):</p><pre><code>net:  http:  enabled: true  RESTInterfaceEnabled: true</code></pre><p>According to the <a href=""https://docs.mongodb.com/ecosystem/tools/http-interfaces/#"" rel=""nofollow"">MongoDB docs</a>, the REST interface (which is deprecated in 3.2) listens on port 28017 (1000 + the mongod port), so you will have to open the firewall for that port.</p><p>Also, I strongly recommend NOT opening up any DB ports to the world (0.0.0.0).  Find your laptop's IP (or probably your router's IP as assigned by your ISP) and add that instead.</p><p>Your browser likely cannot connect (you didn't specify the exact error) because mongodb doesn't use HTTP or any other browser protocol and your browser doesn't know how to to talk to it.  You won't be able to do much with your browser even with the REST interface enabled anyway.  Try getting the mongo shell (make sure you get the same version as mongodb on your server) on your laptop and seeing if you can connect to port 27017 with that.</p>",6047960,6047960,2016-05-25T04:18:48.647,2016-05-25T04:18:48.647,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37427657
310,37494803,2,37494755,2016-05-28T03:10:24.670,0,"<p>Well... after posting this I just went back and ran</p><pre><code>$ mupx deploy</code></pre><p>again...  And it just worked fine - deployed with no problems.</p><p>Another thing I found to avoid this in the future is to install NodeJS on my AWS instance.  Following <a href=""https://nodejs.org/en/download/package-manager/#debian-and-ubuntu-based-linux-distributions"" rel=""nofollow"">this link</a> will do the trick.  I'm not sure why they don't just add this to the mupx setup...</p><p>I also found some documentation which was helpful (and my next step to turn to if things were not working)</p><p><a href=""https://www.npmjs.com/package/mupx"" rel=""nofollow"">https://www.npmjs.com/package/mupx</a> has some great documentation and there they say</p><p>""One of the most common problems is your Node version getting out of date. In that case, see <a href=""https://www.npmjs.com/package/mupx#updating-mup"" rel=""nofollow"">Š—“UpdatingŠ—</a> section above.""</p><p>So, if you run into this problem - maybe the best thing to do is run</p><pre><code>$ npm update mupx -g</code></pre><p>as per: <a href=""https://www.npmjs.com/package/mupx#updating-mup"" rel=""nofollow"">https://www.npmjs.com/package/mupx#updating-mup</a></p><p>However, for me it was as simple as running <code>$ mupx deploy</code> one more time after waiting a few minutes (kind of disturbing actually)</p>",484732,484732,2016-08-23T23:01:07.440,2016-08-23T23:01:07.440,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37494803
311,37556167,2,36917382,2016-05-31T21:48:00.117,2,"<p>I'm not sure if the MongoDB documentation is outdated, or just has a typo, but I think the correct file/directory is <code>/etc/yum.repos.d/mongodb.repo</code>.</p><p>So the full command would be:</p><pre><code>echo ""[MongoDB]name=MongoDB Repositorybaseurl=http://downloads-distro.mongodb.org/repo/redhat/os/x86_64gpgcheck=0enabled=1"" | sudo tee -a /etc/yum.repos.d/mongodb.repo</code></pre>",1310642,,,2016-05-31T21:48:00.117,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37556167
312,37735884,2,37731116,2016-06-09T20:48:37.763,0,"<p>You are trying to use </p><pre><code>let qe = AWSDynamoDBQueryExpression()qe.hashKeyAttribute = ""id""qe.hashKeyValues = id</code></pre><p>which is deprecated. I would suggest you use 'keyConditionExpression' and 'expressionAttributeValues' instead.</p><p>Thanks,Rohan</p>",5045692,,,2016-06-09T20:48:37.763,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37735884
313,37757550,2,37753691,2016-06-10T21:42:57.407,1,"<p>Apparently we have in out .NET 4.0 project the following references to </p><ol><li>Elasticsearch.Net - official low level elasticsearch clientpackages\Elasticsearch.Net.2.3.2\lib\net45\Elasticsearch.Net.dll</li><li>NEST - official high level elasticsearch clientpackages\NEST.2.3.2\lib\net45\Nest.dll</li></ol><p>interesting fact they still use deprecated queries. as mentioned by Frederick here</p><blockquote>  <p>the client you're using but the server is 1.5.2 (since that is the  only version provided by the aws elasticsearch service, which you  appear to be using)</p></blockquote><p>see below - it works:</p><pre><code>  var query = new QueryContainer[8];//Size is based on hte number of search parameters  var descriptor = new QueryContainerDescriptor&lt;EsNoteModel&gt;();/*........... skipping here some code for other parameters ..........*/ if (hasFollowUpDate.HasValue)  {  if ((bool) hasFollowUpDate)  {  //If true shows only with a non-blank follow-up date  query[7] = descriptor.Filtered(p =&gt; p.Filter(f =&gt; f.Exists(r =&gt; r.Field(u =&gt; u.FollowUpDateTime))));  }  else  {  ////If false shows only notes with a blank follow-up date  query[7] = descriptor.Filtered(p =&gt; p.Filter(f =&gt; f.Missing(r =&gt; r.Field(u =&gt; u.FollowUpDateTime))));  }  }  var result = ElasticSearchClient.Search&lt;EsNoteModel&gt;(body =&gt; body  .From(offset - 1)  .Size(rows)  .Query(q =&gt; q  .Bool(b =&gt; b  .Must(query)  )  )  .Sort(s =&gt; s  .Field(f =&gt; f  .Field(p =&gt; p.NoteDate)  .Order(SortOrder.Descending)))  );</code></pre><p>So simply replacing word BOOL to Filtered - did the trick</p>",2262047,,,2016-06-10T21:42:57.407,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37757550
314,37928102,2,37624794,2016-06-20T17:06:04.283,0,"<p>Edit: Answer was wrong. Only useful information was this</p><p>This context.fail syntax is actually deprecated. Look up the Lambda context object properties, it should look more like ""callback(null, resultObj)"" now.</p>",560050,560050,2016-07-20T02:03:46.513,2016-07-20T02:03:46.513,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37928102
315,37985081,2,37898686,2016-06-23T07:37:45.293,1,"<p>You're ember.js configuration file defines <code>rootURL</code> to be <code>""https://cdn.lorem.io""</code> which is wrong. When accessing lorem.io your <code>rootURL</code> is supposed to be <code>""https://www.lorem.io""</code>.</p><p>Perhaps you meant <code>baseURL</code>. See <a href=""https://til.hashrocket.com/posts/d1aff13969-the-difference-between-rooturl-and-baseurl"" rel=""nofollow"">this explanation</a> for the difference between <code>rootURL</code> and <code>baseURL</code>.</p><p>Warning: Keep in mind, that <code>baseURL</code> <a href=""http://emberjs.com/blog/2016/04/28/baseURL.html"" rel=""nofollow"">gets deprecated</a> in ember-cli 2.7.</p>",2471991,,,2016-06-23T07:37:45.293,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/37985081
316,38003665,2,37914184,2016-06-23T23:53:19.307,1,"<p>The key that you set the ID token against in logins should be of the format <code>cognito-idp.&lt;region&gt;.amazonaws.com/&lt;YOUR_USER_POOL_ID&gt;</code> not your USER_POOL_NAME. <a href=""https://mobile.awsblog.com/post/TxGNH1AUKDRZDH/Announcing-Your-User-Pools-in-Amazon-Cognito"" rel=""nofollow noreferrer"">This blog</a> along with the link in your post for our dev guide should explain the steps and code you need.</p><p>As for the solution to deprecated logins dictionary, you need to use <a href=""http://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSCognitoCredentialsProvider.html#//api/name/initWithRegionType:identityPoolId:identityProviderManager:"" rel=""nofollow noreferrer"">this constructor</a> to create the credentials provider. The <em>identityProviderManager</em> here should be an implementation of <a href=""http://docs.aws.amazon.com/AWSiOSSDK/latest/Protocols/AWSIdentityProviderManager.html#//api/name/logins"" rel=""nofollow noreferrer"">AWSIdentityProviderManager Protocol</a> and the <em>logins</em> method should return the dictionary mapping for your provider name to the token. The credentials provider will call this method every time it needs the identity provider token. Check <a href=""https://stackoverflow.com/questions/37127831/aws-expiredtokenexception-after-app-relaunch/37146730#37146730"">this answer</a> for more details.</p>",3204480,-1,2017-05-23T12:10:12.657,2016-06-23T23:53:19.307,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38003665
317,38136519,2,35977787,2016-07-01T03:20:25.763,0,"<p>According to user MichaelB@AWS (Amazon staff account):</p><blockquote>  <p>For the issues with ORDS, you should skip (by entering ""2"") steps  involving the ORDS schema and PL/SQL gateway. The ORDS schema is not  required to support the APEX RESTful listener, and is not currently  supported on RDS Oracle. The specific options may vary between  versions of ORDS.  <a href=""https://forums.aws.amazon.com/thread.jspa?messageID=711534"" rel=""nofollow"">https://forums.aws.amazon.com/thread.jspa?messageID=711534</a></p></blockquote><p>The APEX+APEX_DEV options do not create the correct ORDS users, in particular there's no ORDS_METADATA created which is where most of the important ORDS packages and data are stored.  Without SYSDBA (not available on RDS) you won't be able to install ORDS either.</p><p>From the documentation:<a href=""http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.Options.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.Options.html</a>the minimal ORDS.war configuration and associated schemas needed to get APEX working is available on Amazon's RDS as a replacement for the deprecated APEX Express Listener.  It isn't a complete install of ORDS for RESTful services.</p><p>The solutions are:</p><ol><li>Wait for Amazon to support ORDS on RDS</li><li>Install Oracle yourself on an EC2 instance and forget RDS</li></ol>",148346,,,2016-07-01T03:20:25.763,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38136519
318,38293614,2,38293292,2016-07-10T15:43:08.150,6,"<p>In my experience, as soon as you ask the question ""how can I scale up performance?"" you know you have outgrown RDS (edit: I admit my experience that leads me to this opinion may be outdated).</p><p>It sounds like your query load is pretty write-heavy. Lots of inserts and updates. You should increase the innodb_log_file_size if you can on your version of RDS. Otherwise you may have to abandon RDS and move to an EC2 instance where you can tune MySQL more easily.</p><p>I would also disable the MySQL query cache. On every insert/update, MySQL has to scan the query cache to see if there any results cached that need to be purged. This is a waste of time if you have a write-heavy workload. Increasing your query cache to 2.56GB makes it even worse! Set the cache size to 0 and the cache type to 0.</p><p>I have no idea what queries you run, or how well you have optimized them. MySQL's optimizer is limited, so it's frequently the case that you can get huge benefits from redesigning SQL queries. That is, changing the query syntax, as well as adding the right indexes.</p><p>You should do a query audit to find out which queries are accounting for your high load. A great free tool to do this is <a href=""https://www.percona.com/doc/percona-toolkit/2.2/pt-query-digest.html"" rel=""nofollow"">https://www.percona.com/doc/percona-toolkit/2.2/pt-query-digest.html</a>, which can give you a report based on your slow query log. Download the RDS slow query log with the <a href=""http://docs.aws.amazon.com/cli/latest/reference/rds/download-db-log-file-portion.html"" rel=""nofollow"">http://docs.aws.amazon.com/cli/latest/reference/rds/download-db-log-file-portion.html</a> CLI command.</p><p>Set your long_query_time=0, let it run for a while to collect information, then change long_query_time back to the value you normally use. It's important to collect all queries in this log, because you might find that 75% of your load is from queries under 2 seconds, but they are run so frequently that it's a burden on the server.</p><p>After you know which queries are accounting for the load, you can make some informed strategy about how to address them:</p><ul><li>Query optimization or redesign</li><li>More caching in the application</li><li>Scale out to more instances</li></ul>",20860,20860,2016-07-10T16:22:02.197,2016-07-10T16:22:02.197,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38293614
319,38299104,2,38142093,2016-07-11T04:19:47.647,0,"<p>At this point in time, the question really becomes somewhat subjective, at least with Google -- it really depends on your specific site, like how quickly your pages render, how much content renders after the DOM loads, etc. Certainly (as @birju-shaw mentions) if Google can't read your page at all, you know you need to do something else.</p><p>Google has officially <a href=""https://webmasters.googleblog.com/2015/10/deprecating-our-ajax-crawling-scheme.html"" rel=""nofollow"">deprecated the _escaped_fragment_</a> approach as of October 14, 2015, but that doesn't mean you might not want to still pre-render.</p><p>YMMV on trusting Google (and other crawlers) for reasons stated here, so the only definitive way to find out which is best in your scenario would be to test it out. There could be other reasons you may want to pre-render, but since you mentioned SEO specifically, I'll leave it at that.</p>",786662,786662,2016-07-11T04:25:41.503,2016-07-11T04:25:41.503,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38299104
320,38313021,2,38307640,2016-07-11T17:41:18.890,-1,"<p>I think the <code>s3n://</code> URL style has been deprecated and/or removed. </p><p>Try defining your keys as <code>""fs.s3.awsAccessKeyId""</code>.</p>",139490,,,2016-07-11T17:41:18.890,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38313021
321,38337569,2,38336752,2016-07-12T19:39:20.463,0,"<p>Try this and let me know if it works, I know from experience that DynamoDB is very painful to filter. </p><pre><code>result = dynamodb.scan(  table_name: 'my_table',  expression_attribute_values: {   ':one' =&gt; 1,  ':two' =&gt; 2,  ':three' =&gt; 3,  ':four' =&gt; 4  },  filter_expression: 'contains(numbers, :one) OR contains(numbers, :two) OR contains(numbers, :three) OR contains(numbers, :four)')</code></pre><p>I can't think of anything simpler currently, the method you linked is marked as deprecated, instead you should use <code>expression_attribute_values</code> and <code>filter_expression</code>.</p>",4412925,,,2016-07-12T19:39:20.463,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38337569
322,38461850,2,38460457,2016-07-19T14:48:24.247,7,"<p>So it turns out the issue was that my security keys were ""out of date"". This is due to my IAM permissions being changed between the generation of the first set of keys and the attempt to deploy my application.</p><p>Generating a new set of keys solved the problem.</p>",2485799,,,2016-07-19T14:48:24.247,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38461850
323,38467582,2,34294693,2016-07-19T20:07:51.770,13,"<p>Create two Lambdas and in the first use <a href=""http://boto3.readthedocs.io/en/latest/reference/services/lambda.html#Lambda.Client.invoke"" rel=""nofollow noreferrer"">Lambda.Client.invoke</a> with InvocationType=Event in an dedicated ApiGateway request handling Lambda.  The second executes the logic you want the ApiGateway request to asynchronously invoke.</p><p>Example dedicated ApiGateway Lambda handler:</p><pre><code>from __future__ import print_functionimport boto3import jsondef lambda_handler(event, context):  response = client.invoke(  FunctionName='&lt;your_long_task_running_function_executer&gt;',  InvocationType='Event',  Payload=json.dumps(event)  )  return { ""result"": ""OK"" }</code></pre><p>You would likely want to detect a failure to send the request and other conditions of that sort but as I don't primarily use python, I'll leave that logic up to you.</p><p>p.s. note that <a href=""http://boto3.readthedocs.io/en/latest/reference/services/lambda.html#Lambda.Client.invoke_async"" rel=""nofollow noreferrer"">invoke_async</a> is deprecated<br>p.p.s. sorry, my account is new and I don't have the rep to add these as a comment: 0. I borrowed from what you answered; 1. you are using a deprecated api; and 2. you ought (clearly it's been fine) to add <code>InvocationType = 'Event'</code> into your call.</p>",4825613,4023420,2019-03-25T20:18:03.437,2019-03-25T20:18:03.437,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/38467582
324,38677287,2,38665929,2016-07-30T18:35:24.320,0,"<p>AWS constantly updates its Elastic Beanstalk solution stacks and makes its old stacks obsolete, so you will need to change your solution stack string to the currently-supported version. See the <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html#concepts.platforms.nodejs"" rel=""nofollow"">Supported Platforms (Node.js)</a> page for the latest supported solution stack name. This is currently (as of 7/30/2016) <code>64bit Amazon Linux 2016.03 v2.1.3 running Node.js</code>, and will likely change again in the future.</p>",2518355,,,2016-07-30T18:35:24.320,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38677287
325,38680284,2,26817395,2016-07-31T02:39:57.313,1,"<p>CollectionFS is now deprecated, but there are other options:</p><ul><li>Only upload, without S3 integration*: <a href=""https://github.com/tomitrescak/meteor-uploads"" rel=""nofollow"">https://github.com/tomitrescak/meteor-uploads</a></li></ul><p>Use the <a href=""https://blueimp.github.io/jQuery-File-Upload/"" rel=""nofollow"">jQuery-File-Upload</a> (which is great), it generates thumbs, has size and format validation, etc. Using basically these two packages together:</p><p><a href=""https://atmospherejs.com/tomi/upload-jquery"" rel=""nofollow"">https://atmospherejs.com/tomi/upload-jquery</a></p><p><a href=""https://atmospherejs.com/tomi/upload-server"" rel=""nofollow"">https://atmospherejs.com/tomi/upload-server</a></p><p>You can use other package for S3 integration.</p><p>Like: <a href=""https://github.com/peerlibrary/meteor-aws-sdk/"" rel=""nofollow"">https://github.com/peerlibrary/meteor-aws-sdk/</a></p><hr><ul><li>Upload + Integration with S3: <a href=""https://github.com/Lepozepo/S3"" rel=""nofollow"">https://github.com/Lepozepo/S3</a></li></ul><p>Good, but if you need to generate thumbs for example you will need to integrate with other package or do it yourself. I not tested, but I got this suggestion: <a href=""https://github.com/jamgold/cropuploader"" rel=""nofollow"">https://github.com/jamgold/cropuploader</a></p><hr><ul><li>Upload only, but with examples of how to generate thumbs or integrate with S3 / DropBox / GridFS /: <a href=""https://github.com/VeliovGroup/Meteor-Files/"" rel=""nofollow"">https://github.com/VeliovGroup/Meteor-Files/</a></li></ul><p>Rich documentation and does well which proposes: Upload images.</p><hr><p>Use that adapt best to your needs.</p>",1424473,1424473,2016-08-17T14:22:07.747,2016-08-17T14:22:07.747,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38680284
326,38887301,2,38883025,2016-08-11T04:30:34.940,8,"<p>The recommended way is using Zeppelin's <a href=""https://zeppelin.apache.org/docs/0.6.0/manual/dependencymanagement.html"" rel=""noreferrer"">Dependency Management</a></p><p>It can add jar file or maven artifact.</p><p>the dependency jar will be downloaded to local-repo. </p><blockquote>  <p>NOTE: If the jar file is compiled from source, when you compile again, it will NOT be synchronised automatically(download again). You need go to <code>interpreter</code> setting, click edit and OK will trigger another download to local-repo.</p></blockquote><hr><blockquote>  <p>NOTE: If you use one scala version first, and compiled again with another version. It will report <code>Exception in thread Š—“mainŠ— java.lang.NoSuchMethodError: scala.reflect.api.JavaUniverse.runtimeMirror</code>. remove already downloaded jar with <code>rm -rf local-repo/*</code></p></blockquote><hr><blockquote>  <p>NOTE: z.dep is deprecated. </p></blockquote>",2015518,2015518,2016-08-12T16:01:41.743,2016-08-12T16:01:41.743,6,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38887301
327,38895935,2,38887061,2016-08-11T12:06:33.217,0,"<p>My guess would be that you missed a step on setup. There's one where you have to set the ""event source"". IF you don't do that, I think you get that message.</p><p>But the debug options are limited. I wrote EchoSim (the original one on GitHub) before the service simulator was written and, although it is a bit out of date, it does a better job of giving diagnostics.</p><p>Lacking debug options, the best is to do what you've done. Partition and re-test. Do static replies until you can work out where the problem is.</p>",553903,,,2016-08-11T12:06:33.217,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38895935
328,38919361,2,38487965,2016-08-12T13:37:14.397,5,"<p>This answer is now out of date, please see the other answer about using CloudFormation to codify CloudWatch Dashboards.  For historical purposes here is my original reply:  The accepted answer is effectively ""no"".  However it may be worth noting that if you were to create a graph through the web interface and inspect the URL that is generated for you through the ""Copy URL"" button, you can see the patterns it uses and you could come up with your own URL.  Note that when you click the ""Copy URL"" button in the CloudWatch web interface, it doesn't do anything special on its backend other than programmatically generate a URL for you based on your currently selected options.  Therefore in a way this offers you a way to generate your own graphs programmatically, albeit in a rather unintuitive and messy way since as far as I can tell there isn't a published specification for how those URLs are assembled and you must figure it out on your own through trial and error.</p>",2099868,2099868,2018-09-27T13:05:03.223,2018-09-27T13:05:03.223,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/38919361
329,38986651,2,38897266,2016-08-17T01:04:32.693,1,"<p>API gateway now has native integration with 'Cognito Your User Pool', so you can pass the identity token directly - <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html"" rel=""nofollow"">api gateway docs</a>. The post you have linked is outdated</p>",6031617,,,2016-08-17T01:04:32.693,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38986651
330,38995722,2,38993458,2016-08-17T11:44:59.553,2,"<p>There is no any single CLI command that you will run and get the monthly pricing. you will have to integrate the AWS Price List API to get the pricing.</p><p><a href=""https://aws.amazon.com/blogs/aws/new-aws-price-list-api/"" rel=""nofollow"">https://aws.amazon.com/blogs/aws/new-aws-price-list-api/</a></p><p>This is also a good tool available to get the costs for the EC2 instances.</p><p><a href=""https://github.com/colinbjohnson/aws-missing-tools/blob/master/_deprecated/ec2-cost-calculate/ec2-cost-calculate.sh"" rel=""nofollow"">https://github.com/colinbjohnson/aws-missing-tools/blob/master/_deprecated/ec2-cost-calculate/ec2-cost-calculate.sh</a></p>",5017586,,,2016-08-17T11:44:59.553,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/38995722
331,39014840,2,39014641,2016-08-18T09:40:21.743,0,"<p><code>gcm.register(""project number"");</code> to create a new token</p><p><code>gcn.unregister();</code> to unregister the previous token;</p><p>above code is deprecated so use</p><pre><code>InstanceID.deleteToken() or InstanceID.deleteInstanceID().</code></pre>",4268500,4268500,2016-08-18T09:52:46.773,2016-08-18T09:52:46.773,7,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39014840
332,39088822,2,38214633,2016-08-22T21:13:59.777,7,"<p>If your JSON is uniformly structured I would advise you to give Spark the schema for your JSON files and this should speed up processing tremendously.</p><p>When you don't supply a schema Spark will read all of the lines in the file first to infer the schema which, as you have observed, can take a while.</p><p>See this documentation for how to create a schema: <a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema"" rel=""noreferrer"">http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema</a></p><p>Then you'd just have to add the schema you created to the jsonFile call:</p><pre><code>val df = sqlContext.jsonFile(""s3://testData/*/*/*"", mySchema)</code></pre><p>At this time (I'm using Spark 1.6.2) it seems as if <code>jsonFile</code> has been deprecated, so switching to <code>sqlContext.read.schema(mySchema).json(myJsonRDD)</code> (where <code>myJsonRDD</code> is of type <code>RDD[String]</code>) might be preferable.</p>",308013,,,2016-08-22T21:13:59.777,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39088822
333,39239314,2,39239035,2016-08-31T01:16:13.923,1,"<p>When you say the call succeeds with no error i am assuming you mean the call to the lambda </p><p>If you are using node version 4.3 the way you are returning from the function is deprecated as seen from this extract from the aws lambda documentation</p><blockquote>  <p>The Node.js runtime v4.3 supports the optional callback parameter. You >can use it to explicitly return information back to the caller. The general syntax is:</p>   <blockquote>  <p>callback(Error error, Object result);</p>  </blockquote>   <p>Using the callback parameter is optional. If you don't use the optional callback parameter, the behavior is same as if you called the callback() without any parameters. You can specify the callback in your code to return information to the caller.  If you don't use callback in your code, AWS Lambda will call it implicitly and the return value is null.</p>   <blockquote>  <p>This is the correct way to return from an aws lambda function if using node version 4.3</p>  </blockquote></blockquote><p>first add a 3rd parameter to the handler function like so</p><pre><code>  exports.handler = function(event,context,callback)</code></pre><p>then when returning from the function follow this form</p><pre><code>  function(err,data){  if(err){  callback(err);  } else {  callback(null,data);  }  }</code></pre>",6017886,,,2016-08-31T01:16:13.923,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39239314
334,39272105,2,39260811,2016-09-01T12:53:37.187,0,"<p>When I ran the code you provided, I got the following returned:</p><pre><code>$ aws --versionaws-cli/1.10.60 Python/2.7.11 Darwin/15.6.0 botocore/1.4.50 $ aws deploy create-deployment --application-name ...snipAn error occurred (ApplicationDoesNotExistException) when calling the CreateDeployment operation: No application found for name: MyApp</code></pre><p>And this(AWS server side error) is the expected behaviour(I guess).</p><p>One possibility that your CLI raised the validation error is that your AWS CLI is out of date.</p><p>Which version is your AWS CLI?</p>",2239016,,,2016-09-01T12:53:37.187,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39272105
335,39292876,2,10528540,2016-09-02T12:50:26.450,2,"<p>Updating this for Apache Hadoop 2.7+, and ignoring Amazon EMR as they've changed things there.</p><ol><li>If you are using Hadoop 2.7 or later, use s3a over s3n. This also applies to recent versions of HDP and, AFAIK, CDH.</li><li>This supports 5+GB files, has other nice features, etc. It is tangibly better when reading files Š—”and will only get better over time.</li><li>Apache s3:// should be considered deprecated -you don't need it any more, and shouldn't be using it.</li><li>Amazon EMR use ""s3://"" to refer to their own, custom, binding to S3. That's what you should be using if you are running on EMR.</li></ol><p>Improving distcp reliability and performance working with object stores is still and ongoing piece of work...contributions are, as always, welcome.</p>",2261274,,,2016-09-02T12:50:26.450,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39292876
336,39359930,2,39358139,2016-09-07T01:18:33.033,2,"<p>try changing it to this:<code>  public void progressChanged(ProgressEvent progressEvent) {  LOG.info(myUpload.getProgress().getPercentTransferred() + ""%"");  LOG.info(""progressEvent.getEventCode():"" + progressEvent.getEventType());  if (progressEvent.getEventType() == ProgressEventType.TRANSFER_COMPLETED_EVENT) {  LOG.info(""Upload complete!!!"");  }  }</code></p><p>It looks like you are running some deprecated code.</p><p>In <code>com.amazonaws.event.ProgressEventType</code>, value 8 refers to <code>HTTP_REQUEST_COMPLETED_EVENT</code></p><ul><li><code>COMPLETED_EVENT_CODE</code> is deprecated</li><li><code>getEventCode</code> is deprecated</li></ul><p>refer to this -> <a href=""https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/java/com/amazonaws/event/ProgressEvent.java"" rel=""nofollow"">https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/java/com/amazonaws/event/ProgressEvent.java</a></p>",96332,,,2016-09-07T01:18:33.033,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39359930
337,39376228,2,39362847,2016-09-07T17:42:36.347,1,"<p>Unfortunately, CodeDeploy doesn't have a good/elegant way to handle those obsolete revisions at the moment. It'd be great if there is an overwrite option when bitbucket pushes to S3.</p>",2288838,,,2016-09-07T17:42:36.347,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39376228
338,39459919,2,39451096,2016-09-12T22:56:24.520,1,<p>I had the exact same problem.  Turned out that my dependency was out of date: jackson-databind.  Update to 2.6.6 or later.</p>,4040254,,,2016-09-12T22:56:24.520,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39459919
339,39499329,2,39362847,2016-09-14T20:41:12.363,1,"<p>CodeDeploy is purely a deployment tool, it cannot handle the revisions in S3 bucket.</p><p>I would recommend you look into the ""lifecycle management"" for S3. Since you are using version controlled bucket (I assume), there is always one latest version and 0 to many obsolete version. You can set a lifecycle configuration of type ""NoncurrentVersionExpiration"" so that the obsolete version will be deleted after some days.</p><p>This method is still not possible to maintain a fixed number of deployments as AWS only allows specifying lifecycle in number of days. But it's probably the best alternative to your use-case. </p><p>[1] <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-set-lifecycle-configuration-intro.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-set-lifecycle-configuration-intro.html</a>[2] <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/intro-lifecycle-rules.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonS3/latest/dev/intro-lifecycle-rules.html</a></p>",2939614,,,2016-09-14T20:41:12.363,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39499329
340,39543998,2,39543404,2016-09-17T07:17:32.023,0,"<p>One thing - a StringSet is a value type for a field of a DynamoDB item, and not something you can use to query for multiple things at once.</p><p>I looked around a bit and found some methods that would work elegantly with your design, but they're deprecated. What I would recommend is building up the query string yourself. For example...</p><pre><code>String[] countries = {""Japan"", ""Vietnam"", ""Thailand""};List&lt;String&gt; queries = new ArrayList&lt;&gt;();Map&lt;String, AttributeValue&gt; eav = new HashMap&lt;&gt;();for (Integer i = 0; i &lt; countries.length; i++) {  String placeholder = "":val"" + i;  eav.put(placeholder, new AttributeValue().withS(countries[i]));  queries.add(""Place = "" + placeholder);}String query = String.join("" or "", queries);DynamoDBQueryExpression queryExpression = new DynamoDBQueryExpression()  .withFilterExpression(query)  .withExpressionAttributeValues(eav)  .withHashKeyValues(someHashValue);</code></pre><p>I haven't tested it, but this is the sort of thing you'd be looking at doing (unless there's a better way that I still haven't seen).</p>",433380,,,2016-09-17T07:17:32.023,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39543998
341,39702583,2,39696264,2016-09-26T12:17:10.847,0,"<p>Maybe you should paste some code here. Also, there are two versions of AWS SDK, are you using the latest version, or the deprecated version?</p>",2537914,,,2016-09-26T12:17:10.847,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39702583
342,39779480,2,39737414,2016-09-29T20:32:17.197,2,"<p>The accepted solution works, but the method <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3.html#changeObjectStorageClass-java.lang.String-java.lang.String-com.amazonaws.services.s3.model.StorageClass-"" rel=""nofollow"">changeObjectStorageClass</a> is actually deprecated.The deprecation note suggests to use the method <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3.html#copyObject-com.amazonaws.services.s3.model.CopyObjectRequest-"" rel=""nofollow"">copyObject</a>, as in the following snippet:</p><pre><code>  CopyObjectRequest copyObjectRequest=   new CopyObjectRequest(bucket, key, bucket, key)  .withStorageClass(StorageClass.StandardInfrequentAccess);  s3.copyObject(copyObjectRequest);</code></pre><p>I could not notice any performance difference between the two approaches.</p>",391346,,,2016-09-29T20:32:17.197,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39779480
343,39832482,2,39832289,2016-10-03T13:09:51.427,9,"<p>If you want the browser to treat the file as pdf you should let him know that his file is a pdf. The way to do so is to send the relevant header:</p><pre><code>Content-type: application/pdf</code></pre><p>The current header that is sent by <code>s3</code> is:</p><pre><code>Content-type: application/octet-stream</code></pre><p>Since you upload the files to s3 - you can either set the correct mime-type for the file during uploading or afterward, using some of the s3 tools (for example s3cmd for windows).</p><p>According to the <a href=""http://undesigned.org.za/2007/10/22/amazon-s3-php-class/documentation"" rel=""nofollow noreferrer"">link to the documentation</a> you provided, the <code>putObjectFile</code> function is deprecated, but this is the definition:</p><pre><code>putObjectFile (string $file, string $bucket, string $uri, [constant $acl = S3::ACL_PRIVATE], [array $metaHeaders = array()], [string $contentType = null])</code></pre><p>As you can see - the last parameter is <code>contentType</code> - so you can set it to <code>application/pdf</code> when uploading the file.<br>This will set the correct header when downloading.</p>",5037551,10977559,2019-11-21T15:21:21.627,2019-11-21T15:21:21.627,7,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/39832482
344,39834740,2,39813634,2016-10-03T15:06:13.050,0,"<p>First thing, <code>s3n://</code> is now deprecated, start using <code>s3://</code> for S3 paths.</p><p>Secondly, if you're merely copying a file into S3 from a local file on your cluster, you can use <code>aws s3 cp</code>:</p><pre><code>aws s3 cp /user/hive/warehouse/abc.text s3://bucket/abc.text</code></pre>",680578,,,2016-10-03T15:06:13.050,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39834740
345,39844490,2,39844092,2016-10-04T05:03:08.483,0,"<p>The most obvious issue in your code is that HTTPS is served from port 443.  Besides that looks like outdated code.  Why not use a recent version of Express?  If you look at the example for HTTPS they give here its pretty different from what you wrote: <a href=""http://expressjs.com/en/api.html"" rel=""nofollow"">http://expressjs.com/en/api.html</a> search for 'HTTPS'</p><pre><code>var express = require('express');var https = require('https');var http = require('http');var app = express();http.createServer(app).listen(80);https.createServer(options, app).listen(443);</code></pre>",281920,,,2016-10-04T05:03:08.483,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39844490
346,39966132,2,39778587,2016-10-10T20:20:51.140,10,"<p>The solution I found for this problem is to update Hadoop to 2.7 and set </p><pre><code>spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2</code></pre><p>In Spark 1.6 there was an alternative version of the fileoutputcommiter that wrote directly to s3, but it got deprecated in spark 2.0.0: <a href=""https://issues.apache.org/jira/browse/SPARK-10063"" rel=""noreferrer"">https://issues.apache.org/jira/browse/SPARK-10063</a></p>",1560163,,,2016-10-10T20:20:51.140,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39966132
347,39989331,2,27677113,2016-10-12T01:53:31.893,1,"<p>I believe the GUI only shows parameters that can be changed, so no, you cannot change that on RDS.</p><p>Also, it seems this parameter is deprecated for current versions of MySQL. According to the official documentation on MySQL site:</p><blockquote>  <p><a href=""http://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_locks_unsafe_for_binlog"" rel=""nofollow"">http://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_locks_unsafe_for_binlog</a></p>   <p>As of MySQL 5.6.3, innodb_locks_unsafe_for_binlog is deprecated and  will be removed in a future MySQL release.</p></blockquote><p>So that would be a likely reason why this parameter cannot be changed.</p><p>Cheers,</p>",6088494,,,2016-10-12T01:53:31.893,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/39989331
348,40008653,2,40000735,2016-10-12T21:27:46.557,2,"<p>Federated Identities is not about to be deprecated. We will be updating the docs.  In the meantime, I can provide some stopgap instructions.  In 2.4, the logins dictionary switched to a pull model.  The SDK will ask you for an updated logins dictionary whenever the AWS credentials need to be refreshed.  To use it, provide an implementation of <a href=""http://docs.aws.amazon.com/AWSiOSSDK/latest/Protocols/AWSIdentityProviderManager.html"" rel=""nofollow"">AWSIdentityProviderManager</a> to your credentials provider.  Below is some partial code which shows you how to implement the logins method.  It shows how to do it both synchronously if you have a current token and asynchronously if you have to call a service to get one.</p><p>Synchronously get the token</p><pre><code>- (AWSTask&lt;NSDictionary&lt;NSString *, NSString *&gt; *&gt; *)logins {  return [AWSTask taskWithResult: @{ @""login.provider.com"" : token}];}</code></pre><p>Asynchronously get the token</p><pre><code>- (AWSTask&lt;NSDictionary&lt;NSString *, NSString *&gt; *&gt; *)logins {  AWSTaskCompletionSource&lt;NSString*&gt; *token = [AWSTaskCompletionSource new];  [self getOpenIdToken:token];  return [token.task continueWithSuccessBlock:^id _Nullable(AWSTask&lt;NSString *&gt; * _Nonnull task) {  return [AWSTask taskWithResult: @{ @""login.provider.com"" : task.result }];  }];}- (void) getOpenIdToken: (AWSTaskCompletionSource &lt;NSString*&gt;*) token {  //call your server to get the token  //...  if(success){  token.result = oidcTokenFromServer;  }else {  [token setError:[NSError errorWithDomain:@""oidc""  code:-1  userInfo:@{@""error"":@""Unable to get open id connect token""}]];  }}</code></pre>",3006039,3006039,2016-10-13T01:08:44.487,2016-10-13T01:08:44.487,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40008653
349,40098463,2,40083329,2016-10-18T02:14:26.410,2,"<p>What type of EC2 instance are you using?  The error is pretty descriptive and indicates that CM is unable to access memory. Maybe you are using an instance type with too little RAM. </p><p>Also - the docs you are referencing are out of date. The latest docs on deploying CDH5 in the cloud can be found here: <a href=""https://www.cloudera.com/documentation/director/latest/topics/director_get_started_aws.html"" rel=""nofollow"">https://www.cloudera.com/documentation/director/latest/topics/director_get_started_aws.html</a></p><p>These docs also recommend using Cloudera Director which will simplify much of the deployment and configuration of your cluster. </p>",7034040,,,2016-10-18T02:14:26.410,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40098463
350,40153954,2,40152379,2016-10-20T12:05:51.523,28,"<p>The <strong><code>save()</code></strong> method will perform the <code>putItem</code> or <code>updateItem</code> based on the value set in <strong>SaveBehavior</strong>. Please refer the below description. There is no update method in DynamoDBMapper class because of this reason. However, there is a separate delete method available.</p><blockquote>  <p>Saves an item in DynamoDB. The service method used is determined by  the DynamoDBMapperConfig.getSaveBehavior() value, to use either  AmazonDynamoDB.putItem(PutItemRequest) or  AmazonDynamoDB.updateItem(UpdateItemRequest): </p>   <p><strong>UPDATE (default) :</strong>  UPDATE will not affect unmodeled attributes on a save operation and a  null value for the modeled attribute will remove it from that item in  DynamoDB. Because of the limitation of updateItem request, the  implementation of UPDATE will send a putItem request when a key-only  object is being saved, and it will send another updateItem request if  the given key(s) already exists in the table.</p>   <p><strong>UPDATE_SKIP_NULL_ATTRIBUTES :</strong> Similar to UPDATE except that it ignores  any null value attribute(s) and will NOT remove them from that item in  DynamoDB. It also guarantees to send only one single updateItem  request, no matter the object is key-only or not. </p>   <p><strong>CLOBBER :</strong> CLOBBER  will clear and replace all attributes, included unmodeled ones,  (delete and recreate) on save. Versioned field constraints will also  be disregarded. Any options specified in the saveExpression parameter  will be overlaid on any constraints due to versioned attributes.</p></blockquote><p><strong>Example usage:-</strong></p><pre><code>DynamoDBMapperConfig dynamoDBMapperConfig = new DynamoDBMapperConfig(SaveBehavior.UPDATE);</code></pre><p><strong>UPDATE</strong> DynamoDBMapperConfig (aws sdk 1.11.473) constructor seems to be deprecated and the builder should be used instead:</p><pre><code>DynamoDBMapperConfig dynamoDBMapperConfig = new DynamoDBMapperConfig.Builder()  .withConsistentReads(DynamoDBMapperConfig.ConsistentReads.CONSISTENT)  .withSaveBehavior(DynamoDBMapperConfig.SaveBehavior.UPDATE)  .build();dynamoDBMapper.save(yourObject, dynamoDBMapperConfig);</code></pre>",6337748,5793987,2019-01-24T08:02:31.527,2019-01-24T08:02:31.527,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/40153954
351,40160012,2,28077435,2016-10-20T16:41:27.543,23,"<p>No AWS DynamoDB Java SDK can't map java.time.LocalDateTime natively without using any annotation.</p><p>To do this mapping, you have to use <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBTypeConverted.html"" rel=""noreferrer""><code>DynamoDBTypeConverted</code></a> annotation introduced in the version 1.11.20 of the AWS Java SDK. Since this version, the annotation <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBMarshalling.html"" rel=""noreferrer""><code>DynamoDBMarshalling</code></a> is deprecated.</p><p>You can do that like this:</p><pre><code>class MyClass {  ...  @DynamoDBTypeConverted( converter = LocalDateTimeConverter.class )  public LocalDateTime getStartTime() {  return startTime;  }  ...  static public class LocalDateTimeConverter implements DynamoDBTypeConverter&lt;String, LocalDateTime&gt; {  @Override  public String convert( final LocalDateTime time ) {  return time.toString();  }  @Override  public LocalDateTime unconvert( final String stringValue ) {  return LocalDateTime.parse(stringValue);  }  }}</code></pre><p>With this code, the stored dates are saved as string in the <a href=""https://en.wikipedia.org/wiki/ISO_8601"" rel=""noreferrer"">ISO-8601</a> format like that: <code>2016-10-20T16:26:47.299</code>.</p>",4698204,4698204,2016-11-06T07:18:39.680,2016-11-06T07:18:39.680,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40160012
352,40268100,2,37260766,2016-10-26T16:57:58.040,27,"<p>Your question exposes certain terminology problems:</p><ul><li>When you say <strong><em>developer</em></strong> I am not sure you mean the same thing that AWSmeans.<ul><li>AWS use the term ""developer"" (as in ""developer identities"") to mean<br>externally maintained pools as opposed to privately maintained.  AWShas  it's own product (Cognito User Pools) which you can privatelymaintain as an AWS service. AWS does not consider user pools to be adeveloper identity provider (but in practice, it is simply a fully implement renamed version of a developer identity provider).</li></ul></li><li>Regarding maintaining tokens<ul><li>The iOS SDK maintains the tokens you need for access to AWSServices.You should use the Mobile Hub, (it has a much nicer object andinterface design than the SDK).  But regardless of whether you usemobile-hub-helper or the SDK directly,  you never have to manage atoken (the SDK does it for you). The documentation (almost cruelly) obscures this fact (and yes, it is lagging behind the SDK). </li></ul></li></ul><p><strong>3 ways to use Cognito</strong></p><p>You must understand that there are 3 different interface apis.</p><ol><li>COGNITO API and <strong>up to date</strong> API documentation (the RESTFUL interaction)</li><li>iOS SDK and <strong>out of date</strong> SDK documentation (the SDK is not RESTFUL, it has ton's of state).</li><li>Mobile Hub Helper (MHH) SDK - MHH is documented (a little) by the hub, and pretty well in the .h files used to produce appledoc documentation.</li></ol><p>With respect to Identity and SignIn/Authentication (the topics of this question) aws-mobile-hub-helper (hereafter MHH) has an elegant design and works well. I would recommend anyone using Cognito start with the Mobile Hub site (or at least with aws-mobile-hub-helper). The MHH is basically a wrapper to the SDK and helps clarify and separate the issues of <strong><em>persistent federated identity and credentials/authorization for AWS services</em></strong> from issues of <strong><em>Identity, authentication and attributes/claims for that Identity</em></strong>.</p><ul><li>Identity in MHH has only 1 class <strong>AWSIdentityManager</strong>.</li><li>SignIn in MHH has one protocol, <strong>AWSSignInProvider</strong>, and two implementations of that protocol (plus one I made):<ol><li><strong>AWSGoogleSignInProvider</strong>: OpenID-Connect/OAuth implementation ofAWSSignInProvider for Google+</li><li><strong>AWSFacebookSignInProvider</strong>: An OAuth/Proprietary implementation of AWSSignInProvider for Facebook</li><li><strong>AWSCUPIdPSignInProvider</strong>: OpenID-Connect/OAuth implementation ofAWSSignInProvider for Amazon AWS Cognito Your User Pools service (this is available on a forked repository)</li></ol></li></ul><p>The mobile-hub-helper is documented only in the .h files.  These can be processed into documentation by appledocs, and the comments there are pretty good if you had an overview of the class structure (which does not exist but I will attempt to provide).</p><p><strong>SDK Authentication Flow</strong></p><p>The authentication flow documented by AWS, is an oversimplification and does not aid in understanding how the authentication is accomplished using the SDK and Mobile Hub Helper.  The following diagrams attempt to convey how identity authentication(login) and authorization(credentials) to use AWS Services (like S3, and DynamoDB) works.</p><p><a href=""https://i.stack.imgur.com/CYnj9.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/CYnj9.jpg"" alt=""Cognito SDK Authentication Flow (Single Identity Provider)""></a><a href=""https://i.stack.imgur.com/h6fBN.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/h6fBN.jpg"" alt=""Cognito SDK Authentication Flow (Multiple Identity Provider)""></a></p><p><strong>Understanding Cognito</strong></p><ul><li>Understanding Cognito is initially confusing for a variety of<br>reasons: Authentication, Authorization and Identity Management in a<br>distributed system is complex. There are many parties with different roles and elaborate prescribed interactions with keys, tokens and<br>signatures.  The end user, the relying party (RP) the Identity<br>Provider (IdP) the resource being used (RS) and the resource owner<br>(RO). This terminology is used by the OpenId Connect and OAuth2.0<br>standards documents.  For reasons that will become clear, this<br>terminology is not consistently used by AWS.  But they concepts and<br>the entities are all there when using Cognito.<ul><li>There are some good  OpenID connect overviews online (ex:   <a href=""http://nordicapis.com/api-security-oauth-openid-connect-depth/"" rel=""noreferrer"">http://nordicapis.com/api-security-oauth-openid-connect-depth/</a> ) and a review of those will help.</li></ul></li><li>Cognito allows <strong>non-OpenID Connect</strong>  Identity Providers, this isan  advantage (ex: allowing the OAuth/Proprietary Facebook identityAPI) but it also means that Cognito is playing a Š—“federatingŠ—<br>role.  This role is outside the scope of OpenID Connect standards<br>documents (Update 1: Lately I have begun to wonder whether theCognito Credentials Provider is really the RP (Relying Party) whichthen issues credentials for AWS Services.  But the main point here isthat OpenId Connect does not prescribe the way that Identities fromdifferent IdP's could be combined.), Amazon are essentially inventingthe role, and in  doing so have had some naming challenges.</li></ul><p><strong>Cognito Naming</strong></p><ul><li><p>Cognito is a single name created by AWS to cover many functionalitiesand roles.</p><ol><li>There is the RESTFUL web API to Cognito, but there is also the<br>Cognito SDK. The SDK calls and the API messages are not named the<br>same, and SDK calls make multiple and conditional API calls.</li><li>Cognito  can federate identity providers. It can persist and<br>association  betwene authenticated users from different identity<br>providers (So it  can remember your google+ and your facebook<br>identities and associated  them with a single Cognito identityId.)</li><li>Cognito can provide  persistent identityId (which, if anonymous,follow an iOS device  using keychain data) for users as well asauthenticated users. These  are stored in what is called anIdentity Pool (NOT to be confused  with a User Pool). Your appreceives the same identityId for a user  on different devices forauthenticated users. Unauthenticated (Guest)  identityIdŠ—Ès follow asingle device.</li><li>Cognito can store (known as  Š—“SyncŠ—) state data IdentityIdŠ—Ès (onthe AWS server), which works for  authenticated and unauthenticatedusers.</li><li>Cognito has a  AWSCredentialsProvider (a source for AWS Credentialsfor using AWS  Services (Cognito but also S3, DynamoDB, etc)</li><li>Cognito can create an  OpenID Connect server called a User Pool,which can be used by  Cognito Identity to Authenticate users.</li><li>Cognito is new, but AWS Federated Identities and AWS IdentityManagement and AWS Credentials are not, so there are lots of classeswith overlapping responsibility.  And the naming conventions areconfusing (consider the nameAWSCognitoIdentityCognitoIdentityProvider! ). The use of the Š—“cognitoŠ—brand name for userpools, really is a nightmare.  AnAWSCognitoIdentity thing is Cognito Federated Identity CFI but anAWSCognitoIdentityProvider thing is a thing like userpools anauthentication provider also called an identity provider.</li></ol></li><li><p>SDK class names are confusing. But with few exceptions, classes starting with <strong>AWSCognitoIdentity</strong> (but NOT AWSCognitoIdentityProvider) are about the credentialsProvider/IdentityProvider, classes starting with <strong>AWSCognitoIdentityProvider</strong> relate to Oauth/Open Id Connect providers and other distributed identity providers (facebook).</p></li></ul><p><strong>Glossary/Synonyms</strong></p><p>These terms are used loosely throughout the AWS documentation and marketing material.  This is an attempt to sort out the terminology by grouping terms that are used interchangeably by AWS.</p><ul><li>Identity provider, authentication provider, Login provider, federatedidentity provider(s)</li><li>Amazon Cognito, Cognito credentials provider, cognito identity (allseem to refer to the same class/process)</li><li>Cognito user pool, Cognito Your User Pools, user pool. CUP <strong>is</strong> an identity provider aka authentication provider</li><li>Cognito identity pool, pool, cognito pool, identity pool. Occasionally called an identity provider (which seems incorrect) but it is never called an authentication provider</li><li>Developer identity, developer authenticated identities, developerprovider, developer identity provider, all used to refer to private external Identity Providers.</li><li>Identity is a term often misused in Cognito documentation.  It is important to understand there are two different kinds of identity that Cognito manages. <strong>identityId</strong> (which should be in lower case) is the persistent unique name that Cognito associates with credentials and uses to federate different <strong>Identity</strong> providers, and Identity (upper case) which is an authenticated identifier from an Identity Provider.</li><li><strong>identityId</strong> Identity ID, id (as in get-id), identity, identityId</li><li><strong>Identity</strong>   </li><li>Federation means multiple things.  </li><li>Web identity federation - an earlier way of federating identity at AWS</li><li>Cognito federated identities</li><li>BYOI (bring your own idenity) where a user may use google, facebook or another identity provider (perhaps a developer provided identity) usually through OpenId-Connect.  </li></ul><p><strong>IdentityId Behaviors</strong></p><ul><li>An identity id looks something like this:us-east-1:982396fs-841e-3cdd-9r43-e7ac41bhbcb28</li><li>The identityId is maintained on an iOS device in a keychain entry.For an unauthenticated IdentityId it remains the same until you clearthe keychain (This can be done in simulator by Simulator ->  ResetContent and SettingsŠ—_).  At that point that IdentityId is abandoned.It is not disabled, it is just never used again.</li><li><p>When the user authenticates, authenticating disables the unauthenticated identityId (theidentityId will be permanently marked with DISABLED in the Logins array in theidentityPool entry. You can see this in the Cognito console.) that iscurrently on the device.  There is one exception: If this is the first time theauthentication takes place for this Identity then the unauthenticated identityId is not abandoned butis associated with the Identity and used as the authenticatedidentityID going forward.</p></li><li><p>Merging multiple Identities (meaning usernames not IdentityIdŠ—Ès) fromdifferent Identity providers abandons (disables) one of the identityId's, andassociates both Identities with the other identityId. DisabledIdŠ—Ès get created whenever this happens. These abandoned identityId's are marked with DISABLED in the Logins array inthe cognito identityPool.</p></li><li><p>In practice this process creates a reasonable use of unique identityIds withdisabled ones only getting created when a user authenticates on a newdevice (It can be bothersome in testing as it creates a barrage ofdisabled and unused identityIdŠ—Ès as the tester logs out and inmultiple times with multiple idŠ—Ès). But in practice the common use case would not create this barrage of disabled identityIds. A user would: </p></li><li><p>Connect Š—– get an unauthenticated id - authenticate Š—– and use the sameID. No abandoned id is created.</p></li><li>Connect on another device Š—– here he/she would momentarily get a newunauthenticated id Š—– and when he/she authenticated and got theidentityId for his/her identity, that unauthenticated id would bedisabled and abandoned.</li><li>Each merging of identities from two identity providers would alsocreate a disabled and abandoned identityId.</li></ul><p><strong>AWSIdentityProviderManager</strong></p><ul><li><p>AWSIdentityProviderManager is the protocol that manages federated AWSIdentityProviders</p></li><li><p>In mobile-hub-helper AWSIdentityManager is the AWSIdentityProviderManager</p><ul><li><p>All it needs to do is return to credentials provider  a loginsdictionary, with providers name and ID Token.  AWSIdentityManageronly returns the providername and  token for a single identityprovider. It simply gets the name and token from theAWSSignInProvider and returns.  (There is a <a href=""https://github.com/BruceBuckland/aws-mobilehub-helper-ios"" rel=""noreferrer"">fork</a> with a modificationthat adds the ability to return all of the current logged inproviders in the logins dictionary.)</p></li><li><p>As modified AWSIdentityManager maintains an NSDictionarycalled cachedLogins. Each new login adds an login (an identityprovider name and id token) to the cache. Then logins always returnsthe whole loginCache.  This is what supports identity merging.</p></li></ul></li><li><p>When the credentials provider calls itŠ—Ès associatedAWSIdentityProviderManager logins method, and finds a list oflogins instead of just one it will merge the identityId's for those loginsin itŠ—Ès database and disable the identityId of one of them.  How doesit know which ID goes with which login?  The ID Token contains anencoded decryptable (paste the token into <a href=""https://jwt.io"" rel=""noreferrer"">https://jwt.io</a> to see foryourself) set of claims, one of which is the identity (ex: username)</p></li><li><p>Note: Even though you have an identityId that has multiple related logins, in Mobile Hub Helper you are only ever authenticated by one AWSSignInProvider.  Credentials get associated with the merged identityId, but in mobile-hub-helper access to that identityId is always via the active AWSSignInProvider (authentication provider) even if you are logged with multiple identity providers.  Your app can keep track of all of the AWSSignInProviders and access them independently of AWSIdentityManager, but from AWSIdentityManagers point of view you are logged in with one of them. In practice this has little impact (until you try to get ""claims"" like imageURL from different providers for instance).</p></li></ul><p><strong>About Merging Identities</strong></p><ul><li><p>Currently the AWSIdentityManager does not support identity merging. I have a <a href=""https://github.com/BruceBuckland/aws-mobilehub-helper-ios"" rel=""noreferrer"">forked repository https://github.com/BruceBuckland/aws-mobilehub-helper-ios</a> from the github repository that adds that capability, and adds a Cognito User Pools Identity Provider AWSSignInProvider (AWSCUPIdPSignInProvider.swift). </p></li><li><p>You can probably think of all sorts of gotchaŠ—Ès when mergingidentities.</p></li><li><p>What if I try to merge two identities from the same provider(wouldnŠ—Èt the dictionary keys be the same?)</p></li><li><p>What if I try to merge two identities, each of which has a differentidentity from the same provider associated with it (and again theywould create two entities with the same keys).</p></li><li><p>Cognito manages this beautifully and rejects attempts to merge<br>identites that cannot be merged.  The rejection happens at login time(when you would try get credentials, the credentials provider willreject the logins dictionary that contains an un-mergeableidentityId)</p></li></ul><p><strong>Where Cognito buries its data</strong></p><ul><li><p>Cognito stores a keychain on the device that contains the last identityId that was used. This is used by the credentialsProvider/identityProvider object upon a call to credentialsProvider.credentials (iOS SDK name) to re-use an existing identity (for example unauthenticated) and avoid creating unused identities unless the user truly is not going to log in or resume.</p></li><li><p>Mobile-Hub-HelperŠ—Ès AWSSignInProviderŠ—Ès and AWSIdentityManager store an indication of an open session state in NSUserDefaults.  These are used to re-start the session if the app is terminated and restarted.</p></li><li><p>AWSSignInProviderŠ—Ès store NSUserDefaults too, and sometimes in the iOS Keychain, for their own internal purposes (like retaining easy persistent access to a username or imageURL or token)</p></li></ul>",6491337,4907037,2018-10-25T18:07:27.110,2018-10-25T18:07:27.110,7,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/40268100
353,40275134,2,30793481,2016-10-27T02:26:54.810,0,"<p><code>Interface DynamoDBMarshaller&lt;T extends Object&gt;</code> is deprecated already, the replacement is <code>Interface DynamoDBTypeConverter&lt;S,T&gt;</code>.</p><p>Inside your model class, add the annotation to your list of objects.</p><pre><code>@DynamoDBTypeConverted(converter = PhoneNumberConverter.class)  public List&lt;MyObject&gt; getObjects() { return this.objects; }</code></pre><p>public void setObjects(List objects) { this.objects = objects; }</p><p>And this is the implementation of <code>DynamoDBTypeConverter</code>.</p><pre><code>public class PhoneNumberConverterimplements DynamoDBTypeConverter&lt;String, PhoneNumber&gt;{  private static final ObjectMapper mapper = new ObjectMapper();  private static final ObjectWriter writer = mapper.writerWithType(new TypeReference&lt;List&lt;MyObject&gt;&gt;(){});  @Override  public String convert(List&lt;MyObject&gt; obj) {  try {  return writer.writeValueAsString(obj);  } catch (JsonProcessingException e) {  System.out.println(  ""Unable to marshall the instance of "" + obj.getClass()  + ""into a string"");  return null;  }  }  @Override  public List&lt;MyObject&gt; unconvert(String s) {  TypeReference&lt;List&lt;MyObject&gt;&gt; type = new TypeReference&lt;List&lt;MyObject&gt;&gt;() {};  try {  List&lt;MyObject&gt; list = mapper.readValue(s, type);  return list;  } catch (Exception e) {  System.out.println(""Unable to unmarshall the string "" + s  + ""into "" + s);  return null;  }  }  }</code></pre>",5353589,,,2016-10-27T02:26:54.810,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40275134
354,40468034,2,40467083,2016-11-07T14:50:19.733,-1,"<p>Use boto3, it is easier. boto2 is not supported by AWS and boto already deprecated and there is no intention for features </p><p>The error pop up because it can't find the region. Boto/boto3 API  will check the region name inside the boto/boto3 service initialization stage. If you didn't specify it, it will look for default region name define inside credential file or environment variable (e.g. ~/.aws/config). </p><p>This is true even you explicitly specify S3 endpoint URL. If you don't want to hard code the credential, region name, then you must setup AWS credential as specified here: <a href=""http://boto3.readthedocs.io/en/latest/guide/configuration.html"" rel=""nofollow noreferrer"">credential configuration</a>. </p><p>Making boto/boto3 using credential file/environment variable will make your code cleaner and more flexible, e.g. you can even using STS without changing the code. e.g.: </p><pre><code>import boto3# You can choose between service resource or service client.s3 = boto3.client(""s3"")response = s3.list_objects_v2(  Bucket=""jd-eu01-isg-analytics-data-from-us01"",   Prefix=""EU_Scripts_For_Frankfurt"")for content in response[""Contents""]:  print content[""Key""]</code></pre><p>Nevertheless, you can still hardcode access key id, secre_key, region name, etc by passing the parameter when you initialize boto/boto3 resources. (boto API is similar) </p><pre><code>import boto3s3 = boto3.client(  ""s3"",  region_name = ""eu-central-1"",  aws_access_key_id = 'xxxxxxxx"",  aws_secret_access_key= = ""yyyyyyy"")</code></pre>",6017840,6017840,2017-06-22T10:43:23.637,2017-06-22T10:43:23.637,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40468034
355,40480087,2,40479737,2016-11-08T06:08:00.430,4,"<p>You CLI version is outdated by 3 years and it doesn't know the new regions. Can you upgrade the CLI to <strong>1.10.x</strong> and try?</p><pre><code>$ aws --versionaws-cli/1.10.66 Python/2.7.12 Linux/3.14.35-28.38.amzn1.x86_64 botocore/1.4.56$ aws ec2 describe-regions{  ""Regions"": [  {  ""Endpoint"": ""ec2.us-east-2.amazonaws.com"",  ""RegionName"": ""us-east-2""  },</code></pre>",4237701,,,2016-11-08T06:08:00.430,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40480087
356,40485886,2,6271222,2016-11-08T11:35:04.423,1,"<p><a href=""https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/core-default.xml"" rel=""nofollow noreferrer"">https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/core-default.xml</a></p><p>fs.default.name is deprecated, and maybe fs.defaultFS is better.</p>",6037289,,,2016-11-08T11:35:04.423,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40485886
357,40508134,2,40504704,2016-11-09T13:26:35.277,0,"<p>Yes in latest  SDK <strong>logins</strong> property get deprecated, so we need to assign <strong>IdentityProvider</strong> to <strong>logins</strong> by using <strong>AWSIdentityProviderManager</strong> delegate. So do as follows,</p><ol><li><p>Create one custom class which adopt AWSIdentityProviderManager delegate.</p><pre><code>import UIKitimport AWSCognitoIdentityProviderimport AWSCoreimport Foundationclass DVCustomIdentityProvider: NSObject, AWSIdentityProviderManager { var tokens: NSDictionary = [String : String]() as NSDictionary init(tokens: [String : String]) {  self.tokens = tokens as NSDictionary } func logins() -&gt; AWSTask&lt;NSDictionary&gt; { // AWSIdentityProviderManager delegate method  return AWSTask(result: tokens) }}</code></pre></li><li><p>Add following code in view controller you want.</p><pre><code>@IBAction func loginButtonPressed(_ sender: UIButton) {if (phoneNumberTextField.text != nil) &amp;&amp; (passwordTextField.text != nil) {  // Change userName.getSession.... with your Facebook method to get authenticate from Facebook, in success block add what I added in my success block.   userName.getSession(phoneNumberTextField.text!, password: passwordTextField.text!, validationData: nil).continue(with: AWSExecutor.mainThread(), withSuccessBlock: { (task: AWSTask&lt;AWSCognitoIdentityUserSession&gt;) -&gt; Any? in // Your Facebook call will go here   if task.error != nil {  // Error  } else {   // SUCCESS BLOCK  self.updateCredentials()  }  return nil  })} else {  // Credential empty}}  func updateCredentials() {  let customcedentialProvider = DVCustomIdentityProvider(tokens: [""graph.facebook.com"" : token]))  let credentialsProvider = AWSCognitoCredentialsProvider(regionType: ""Your region"", identityPoolId: ""Your pool id"", unauthRoleArn: ""Your unearth role name"", authRoleArn: ""Your auth role name"", identityProviderManager: customcedentialProvider)  let configuration = AWSServiceConfiguration(region: ""Your region"", credentialsProvider:credentialsProvider)  AWSServiceManager.default().defaultServiceConfiguration = configuration  credentialsProvider.getIdentityId().continue(with: AWSExecutor.mainThread(), withSuccessBlock: { (taskTask: AWSTask&lt;NSString&gt;) -&gt; Any? in  if taskTask.error == nil &amp;&amp; taskTask.exception == nil {  kUserIdentityID = taskTask.result as String? // Im saving user identity id in constant variable called ""kUserIdentityID""  } else {  // Do Nothing  }  return nil  })  }</code></pre></li></ol><p>import following in your view controller </p><pre><code>import AWSCognitoIdentityProviderimport AWSCoreimport AWSCognito</code></pre><p>Note: This code is written in <strong>swift 3</strong></p>",6355922,,,2016-11-09T13:26:35.277,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40508134
358,40537717,2,40535314,2016-11-10T21:57:50.967,2,"<p>You have few issues here:</p><ol><li>Your user agent is outdated, so you get a totally different response than you expect. Use a newer one.</li><li><code>document.select</code> returns an <code>Element</code>, not <code>String</code>.</li><li><p>Your seletor is not right.Use the following code:  </p><pre><code>String url = ""http://www.amazon.de/DJI-CP-PT-000498-Mavic-Drohne-grau/dp/B01M0AVO1P/"";Document document = Jsoup.connect(url).userAgent(""Mozilla/49.0"").get();Elements question = document.select(""#priceblock_ourprice"");System.out.println(""Price is "" + question.html());</code></pre></li></ol>",3426328,,,2016-11-10T21:57:50.967,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40537717
359,40602349,2,40601941,2016-11-15T04:58:44.550,5,"<p>You cannot use bare variables in 2.2. The syntax has been deprecated and users were warned since version 2.0.</p><p>You should read the error message you have pasted, and although it suggests a different reason, you should follow the example given:</p><blockquote><pre><code>Should be written as:  with_items:  - ""{{ foo }}""</code></pre></blockquote><p>In your case it's enough to replace all <code>with_items: ec2.instances</code> with:</p><pre><code>with_items: ""{{ ec2.instances }}""</code></pre>",2947502,2947502,2016-11-15T05:04:02.060,2016-11-15T05:04:02.060,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40602349
360,40674961,2,40649605,2016-11-18T10:38:17.823,39,"<p>we are doing the same.We started with Cognito but moved to Firebase because we were not satisfied with the way AWS Android SDK implements the authentication flow with Google and Facebook: the code is quite old, it makes use of deprecated methods and generally requires rewriting. On the other hand, Firebase authentication is obviously working seamlessly.When you don't use Cognito, you need to implement your custom authenticator in AWS API Gateway which is quite easy and is described in <a href=""https://aws.amazon.com/blogs/mobile/integrating-amazon-cognito-user-pools-with-api-gateway/"" rel=""noreferrer"">https://aws.amazon.com/blogs/mobile/integrating-amazon-cognito-user-pools-with-api-gateway/</a>. Firebase instructions for token validation are in <a href=""https://firebase.google.com/docs/auth/admin/verify-id-tokens"" rel=""noreferrer"">https://firebase.google.com/docs/auth/admin/verify-id-tokens</a></p><p>The following is an excerpt of my authenticator's code:</p><pre><code>'use strict';// Firebase initialization// console.log('Loading function');const admin = require(""firebase-admin"");admin.initializeApp({  credential: admin.credential.cert(""xxx.json""),  databaseURL: ""https://xxx.firebaseio.com""});// Standard AWS AuthPolicy - don't touch !!...// END Standard AWS AuthPolicy - don't touch !!exports.handler = (event, context, callback) =&gt; {  // console.log('Client token:', event.authorizationToken);  // console.log('Method ARN:', event.methodArn);  // validate the incoming token  // and produce the principal user identifier associated with the token  // this is accomplished by Firebase Admin  admin.auth().verifyIdToken(event.authorizationToken)  .then(function(decodedToken) {  let principalId = decodedToken.uid;  // console.log(JSON.stringify(decodedToken));  // if the token is valid, a policy must be generated which will allow or deny access to the client  // if access is denied, the client will recieve a 403 Access Denied response  // if access is allowed, API Gateway will proceed with the backend integration configured on the method that was called  // build apiOptions for the AuthPolicy  const apiOptions = {};  const tmp = event.methodArn.split(':');  const apiGatewayArnTmp = tmp[5].split('/');  const awsAccountId = tmp[4];  apiOptions.region = tmp[3];  apiOptions.restApiId = apiGatewayArnTmp[0];  apiOptions.stage = apiGatewayArnTmp[1];  const method = apiGatewayArnTmp[2];  let resource = '/'; // root resource  if (apiGatewayArnTmp[3]) {  resource += apiGatewayArnTmp[3];  }  // this function must generate a policy that is associated with the recognized principal user identifier.  // depending on your use case, you might store policies in a DB, or generate them on the fly  // keep in mind, the policy is cached for 5 minutes by default (TTL is configurable in the authorizer)  // and will apply to subsequent calls to any method/resource in the RestApi  // made with the same token  // the policy below grants access to all resources in the RestApi  const policy = new AuthPolicy(principalId, awsAccountId, apiOptions);  policy.allowAllMethods();  // policy.denyAllMethods();  // policy.allowMethod(AuthPolicy.HttpVerb.GET, ""/users/username"");  // finally, build the policy and exit the function  callback(null, policy.build());  })  .catch(function(error) {  // Firebase throws an error when the token is not valid  // you can send a 401 Unauthorized response to the client by failing like so:  console.error(error);  callback(""Unauthorized"");  });};</code></pre><p>We are not in production, yet, but tests on the authenticator show that it behaves correctly with Google, Facebook and password authentication and it is also very quick (60 - 200 ms).The only drawback I can see is that you will be charged for the authenticator lambda function, while the Cognito integrated authenticator is free.</p>",4620414,,,2016-11-18T10:38:17.823,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40674961
361,40690288,2,40690224,2016-11-19T07:21:59.653,6,"<p>The trope about MyISAM being faster than InnoDB is a holdover from code that was current in the mid-2000's.</p><p>MyISAM is <em>not</em> faster than InnoDB anymore, for most types of queries. Look at the benchmarks in this blog from 2007: <a href=""https://www.percona.com/blog/2007/01/08/innodb-vs-myisam-vs-falcon-benchmarks-part-1/"" rel=""nofollow noreferrer"">https://www.percona.com/blog/2007/01/08/innodb-vs-myisam-vs-falcon-benchmarks-part-1/</a></p><p>InnoDB has just gotten better, faster, and more reliable since then. MyISAM is not being developed.</p><p><em>Update:</em> In MySQL 8.0, even the system tables have been converted to InnoDB. There is clearly an intention to phase out MyISAM. I expect that it will be deprecated and then removed in future versions of MySQL (but I can't say how many years from now that will be).</p><p>There were a couple of edge cases where MyISAM might be faster, like table-scans. But you really shouldn't be optimizing your database for table-scans. You should be creating the right indexes to <em>avoid</em> table-scans.</p><blockquote>  <p><em>Update Feb 2018:</em> MyISAM just suffered an additional 40% performance hit from the recent fix for the Meltdown CPU bug, and this affects table-scans. Assuming you are responsible and patch your systems to fix the Meltdown vulnerability, MyISAM is now a performance liability. See current tests of MyISAM performance with the patch: <a href=""https://mariadb.org/myisam-table-scan-performance-kpti/"" rel=""nofollow noreferrer"">https://mariadb.org/myisam-table-scan-performance-kpti/</a></p></blockquote><p>But what trumps that is the fact that InnoDB supports ACID behavior, and MyISAM doesn't support <em>any</em> of the four qualities of ACID. See my answer to <a href=""https://stackoverflow.com/questions/20148/myisam-versus-innodb/17706717#17706717"">MyISAM versus InnoDB</a></p><p>Failing to support ACID isn't just an academic point. It translates into things like table-locks during updates, and global locks during backups.</p>",20860,20860,2018-02-20T17:10:58.547,2018-02-20T17:10:58.547,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40690288
362,40766457,2,38755214,2016-11-23T13:58:39.703,8,"<p>Yes so the correct answer is that an outdated sdk version was used. To fix it set aws-sdk to * in your package.json file and run</p><p>npm install aws-sdk</p><p>With the latest version this code will run fine!</p>",1052141,1052141,2018-12-07T20:46:20.620,2018-12-07T20:46:20.620,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/40766457
363,40861054,2,40749213,2016-11-29T08:30:36.910,2,"<p>AWS does not offer a library for geospatial indexing for node.js. They only have the java version that you referred to. You could use the java library with a lambda function (I tried that), but the library is outdated and not maintained (last commit was 3 years ago)My <a href=""https://github.com/awslabs/dynamodb-geo/pull/15"" rel=""nofollow noreferrer"">pull request</a> with a fix was ignored. Even after fixing things myself I ran into issues like <a href=""https://stackoverflow.com/questions/36153366/update-user-location-with-aws-dynamodb-geo-and-lambda"">this one</a>.</p><p>To answer your questions 1 and 2: AWS wrote <a href=""https://aws.amazon.com/blogs/mobile/geo-library-for-amazon-dynamodb-part-1-table-structure/"" rel=""nofollow noreferrer"">a document series</a> about the way their library calculates geohashes and how it executes queries on DynamoDB. It's too much to summarize here.</p><p>The lack of maintenance and support by AWS and the issues I encountered made me conclude that AWS gave up on the idea of geo querying DynamoDB. Besides, I could not find any other developers that have applied this library successfully.</p><p>My current solution is based on CloudSearch. I write the items (with their location) into a DynamoDB table. I use a DynamoDB stream that uploads all changes in the table to CloudSearch. CloudSearch has geoquery features out-of-the-box.</p>",3568263,-1,2017-05-23T11:46:33.490,2016-11-29T08:30:36.910,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40861054
364,40866205,2,27658147,2016-11-29T12:41:22.683,3,"<p>The TS asked for a working SHA-1 version of the script. However, SHA-1 is outdated and Amazon has datacenters that only accept SHA-256 encryption, hereby the download script that can be used for all S3 datacenters:It also follows HTTP 307 redirects.</p><pre><code>#!/bin/sh#USAGE:# download-aws.sh &lt;bucket&gt; &lt;region&gt; &lt;source-file&gt; &lt;dest-file&gt;set -es3Key=xxxxxxxxxxxxxxxxxxxxs3Secret=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxfile=$3bucket=$1host=""${bucket}.s3.amazonaws.com""resource=""/${file}""contentType=""text/plain""dateValue=""`date +'%Y%m%d'`""X_amz_date=""`date +'%Y%m%dT%H%M%SZ'`""X_amz_algorithm=""AWS4-HMAC-SHA256""awsRegion=$2awsService=""s3""X_amz_credential=""$s3Key%2F$dateValue%2F$awsRegion%2F$awsService%2Faws4_request""X_amz_credential_auth=""$s3Key/$dateValue/$awsRegion/$awsService/aws4_request""signedHeaders=""host;x-amz-algorithm;x-amz-content-sha256;x-amz-credential;x-amz-date""contentHash=""e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855""HMAC_SHA256_asckey () {  var=`/bin/echo -en $2 | openssl sha256 -hmac $1 -binary | xxd -p -c256`  echo $var}HMAC_SHA256 () {  var=`/bin/echo -en $2 | openssl dgst -sha256 -mac HMAC -macopt hexkey:$1 -binary | xxd -p -c256`  echo $var}REQUEST () {  canonicalRequest=""GET\n$resource\n\n""\""host:$1\n""\""x-amz-algorithm:$X_amz_algorithm""""\n""\""x-amz-content-sha256:$contentHash""""\n""\""x-amz-credential:$X_amz_credential""""\n""\""x-amz-date:$X_amz_date""""\n\n""\""$signedHeaders\n""\""$contentHash""  #echo $canonicalRequest  canonicalHash=`/bin/echo -en ""$canonicalRequest"" | openssl sha256 -binary | xxd -p -c256`  stringToSign=""$X_amz_algorithm\n$X_amz_date\n$dateValue/$awsRegion/s3/aws4_request\n$canonicalHash""  #echo $stringToSign  s1=`HMAC_SHA256_asckey ""AWS4""""$s3Secret"" $dateValue`  s2=`HMAC_SHA256 ""$s1"" ""$awsRegion""`  s3=`HMAC_SHA256 ""$s2"" ""$awsService""`  signingKey=`HMAC_SHA256 ""$s3"" ""aws4_request""`  signature=`/bin/echo -en $stringToSign | openssl dgst -sha256 -mac HMAC -macopt hexkey:$signingKey -binary | xxd -p -c256`  #echo signature  authorization=""$X_amz_algorithm Credential=$X_amz_credential_auth,SignedHeaders=$signedHeaders,Signature=$signature""  result=$(curl --silent -H ""Host: $1"" -H ""X-Amz-Algorithm: $X_amz_algorithm"" -H ""X-Amz-Content-Sha256: $contentHash"" -H ""X-Amz-Credential: $X_amz_credential"" -H ""X-Amz-Date: $X_amz_date"" -H ""Authorization: $authorization"" https://${1}/${file} -o ""$2"" --write-out ""%{http_code}"")  if [ $result -eq 307 ]; then  redirecthost=`cat $2 | sed -n 's:.*&lt;Endpoint&gt;\(.*\)&lt;/Endpoint&gt;.*:\1:p'`  REQUEST ""$redirecthost"" ""$2""  fi}REQUEST ""$host"" ""$4""</code></pre><p>Tested on Ubuntu</p><p>If someone knows a solution to remove the HMAC-ASCII step, you're welcome to reply. I got this only working in this way.</p>",2906692,,,2016-11-29T12:41:22.683,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40866205
365,40872184,2,33356041,2016-11-29T17:34:52.387,57,"<p>in Apache Hadoop, ""s3://"" refers to the original S3 client, which used a non-standard structure for scalability. That library is deprecated and soon to be deleted,</p><p>s3n is its successor, which used direct path names to objects, so you can read and write data with other applications. Like s3://, it uses jets3t.jar to talk to S3.</p><p>On Amazon's EMR service, s3:// refers to Amazon's own S3 client, which is different. A path in s3:// on EMR refers directly to an object in the object store.</p><p>In Apache Hadoop, S3N and S3A are both connectors to S3, with S3A the successor built using Amazon's own AWS SDK.Why the new name? so we could ship it side-by-side with the one which was stable. S3A is where all ongoing work on scalability, performance, security, etc, goes. S3N is left alone so we don't break it. S3A shipped in Hadoop 2.6, but was still stabilising until 2.7, primarily with some minor scale problems surfacing.</p><p>If you are using Hadoop 2.7 or later, use s3a. If you are using Hadoop 2.5 or earlier. s3n, If you are using Hadoop 2.6, it's a tougher choice. -I'd try s3a and switch back to s3n if there were problems-</p><p>For more of the history, see <a href=""http://hortonworks.com/blog/history-apache-hadoops-support-amazon-s3/"" rel=""noreferrer"">http://hortonworks.com/blog/history-apache-hadoops-support-amazon-s3/</a> </p><p><strong>2017-03-14 Update</strong> actually, partitioning is broken on S3a in Hadoop 2.6, as the block size returned in a <code>listFiles()</code> call is 0: things like Spark &amp; pig partition the work into one task/byte. You cannot use S3a for analytics work in Hadoop 2.6, even if core filesystem operations &amp; data generation is happy. Hadoop 2.7 fixes that.</p><p><strong>2018-01-10 Update</strong> Hadoop 3.0 has cut its s3: and s3n implementations: s3a is all you get. It is now significantly better than its predecessor and performs as least as good as the Amazon implementation. Amazon's ""s3:"" is still offered by EMR, which is their closed source client. Consult the <a href=""https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html"" rel=""noreferrer"">EMR docs</a> for more info.</p>",2261274,542442,2018-01-11T03:26:30.387,2018-01-11T03:26:30.387,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40872184
366,40905002,2,40868530,2016-12-01T07:37:04.687,0,"<p>It sounds like maybe the issue might be an outdated certificate on CloudFront. </p><p>If so, you can either upload your RapidSSL certificate using aws iam upload-server-certificate, or (I'd prefer) request a new certificate that you'll only use with CloudFront from AWS Certificate Manager. The latter is free, and AWS will autoupdate the certificate before it expires. </p><p>You can start this simple and quick process by going to your CloudFront distribution in AWS Console, and clicking <em>Edit</em> > <em>Request or Import a Certificate with ACM</em></p><p>Once that is done, you will be able to choose the certificate from the Custom Certificate dropdown at the same location <a href=""https://i.stack.imgur.com/aRuss.png"" rel=""nofollow noreferrer"">(screenshot)</a>.</p><p><strong>Note: If you upload your RapidSSL certificate, it must be uploaded to eu-east-1 (N. Virginia) in order to be used with CloudFront. Requesting ACM Certificates must also be done in this region</strong></p><hr><p>If you provide an URL to your website, it's easier to confirm this issue.</p><p>I scribbled some notes on this <a href=""http://notes.webutvikling.org/howto-aws-cloudfront-with-ssl-and-your-domain/"" rel=""nofollow noreferrer"">here</a> with more details on uploading your own/RapidSSL certificate.</p>",2403169,,,2016-12-01T07:37:04.687,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/40905002
367,41001940,2,41000657,2016-12-06T18:03:18.240,3,"<p>Ansible &lt; 2.2 (deprecated since 2.1) Š—– bare variables:</p><pre><code>with_items: ec2.instances</code></pre><p>Ansible >= 2.2 Š—– templated:</p><pre><code>with_items: '{{ ec2.instances }}'</code></pre><p>From <a href=""https://github.com/ansible/ansible/blob/stable-2.2/CHANGELOG.md#removed-deprecated"" rel=""nofollow noreferrer"">release notes</a>:</p><blockquote>  <p><strong>Removed Deprecated:</strong>  </p>   <ul>  <li>with_ 'bare variable' handling, now loop items must always be templated {{ }} or they will be considered as plain strings.</li>  </ul></blockquote><p>Single or double quotes doesn't matter in this case.<br>You can read about differences in quotes from other SO <a href=""https://stackoverflow.com/questions/19109912/do-i-need-quotes-for-strings-in-yaml"">questions</a>.</p>",2795592,-1,2017-05-23T10:30:13.120,2016-12-06T18:03:18.240,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41001940
368,41064542,2,41061857,2016-12-09T16:13:22.587,4,"<p>Short answer: don't.</p><p>Longer answer: A master URL like ""spark://..."" is for Spark Standalone, but EMR uses Spark on YARN, so the master URL should be just ""yarn"". This is already configured for you in spark-defaults.conf, so when you run spark-submit, you don't even have to include ""--master ..."".</p><p>However, since you are asking about cluster execution mode (actually, it's called ""deploy mode""), you may specify either ""--master yarn-cluster"" (deprecated) or ""--deploy-mode cluster"" (preferred). This will make the Spark driver run on a random cluster mode rather than on the EMR master.</p>",2205987,,,2016-12-09T16:13:22.587,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41064542
369,41079360,2,41078151,2016-12-10T19:23:08.403,1,"<p>Buddy, you are using the deprecated version of AWS SDK. The new SDK is integrated directly in Unity without jars and java files. It's a .NET library. Check it out on github:</p><p><a href=""https://github.com/aws/aws-sdk-net"" rel=""nofollow noreferrer"">https://github.com/aws/aws-sdk-net</a></p><p>and here you can see how you can build it for Unity:</p><p><a href=""https://github.com/aws/aws-sdk-net/blob/master/Unity.README.md"" rel=""nofollow noreferrer"">https://github.com/aws/aws-sdk-net/blob/master/Unity.README.md</a></p>",3183423,,,2016-12-10T19:23:08.403,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41079360
370,41083194,2,18839216,2016-12-11T05:07:46.730,0,"<p>The answer is somewhat outdated with the new SDK. The following works with v3 SDK.</p><pre class=""lang-php prettyprint-override""><code>$client-&gt;registerStreamWrapper();$result = $client-&gt;headObject([  'Bucket' =&gt; $bucket,  'Key'  =&gt; $key]);$headers = $result-&gt;toArray();header('Content-Type: ' . $headers['ContentType']);header('Content-Disposition: attachment');// Stop output bufferingif (ob_get_level()) {  ob_end_flush();}flush();// stream the outputreadfile(""s3://{$bucket}/{$key}"");</code></pre>",1234452,,,2016-12-11T05:07:46.730,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41083194
371,41190825,2,31654994,2016-12-16T19:00:10.967,0,"<p>Looks like you are using AWS SDK V2 code with AWS SDK V3.<code>Aws\Common\Enum\Region</code> is obsolete in V3.</p>",3562288,,,2016-12-16T19:00:10.967,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41190825
372,41191255,2,41190793,2016-12-16T19:29:47.533,0,"<p>Here is a complete sample using the <a href=""http://docs.aws.amazon.com/cognito/latest/developerguide/tutorial-integrating-user-pools-android.html#tutorial-integrating-user-pools-sign-up-users-android"" rel=""nofollow noreferrer"">link</a> sugested by Jeff: </p><pre><code>  CognitoUserAttributes attributes = new CognitoUserAttributes();  attributes.addAttribute(""phone_number"",""+15555555555"");  attributes.addAttribute(""email"",""email@mydomain.com"");  cognitoUserPool.signUp('username', 'password', attributes, null, new SignUpHandler() {  @Override  public void onSuccess(CognitoUser cognitoUser, boolean b, CognitoUserCodeDeliveryDetails cognitoUserCodeDeliveryDetails) {  // If the sign up was successful, ""user"" is a CognitoUser object of the user who was signed up.  // ""codeDeliveryDetails"" will contain details about where the confirmation codes will be delivered.  }  @Override  public void onFailure(Exception e) {  // Sign up failed, code check the exception for cause and perform remedial actions.  }  });</code></pre><p>The sections <em>Examples of Using User Pools with the Mobile SDK for Android</em> seems to be outdated.</p><p>Hope to help somebody else ;)</p>",4848308,,,2016-12-16T19:29:47.533,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41191255
373,41271068,2,41201750,2016-12-21T20:06:54.137,2,"<p>I think the solution here is likely very easy.</p><p>Instead of <code>s3://</code> use <code>s3a://</code> as a scheme for your job accessing the bucket.See <a href=""https://wiki.apache.org/hadoop/AmazonS3"" rel=""nofollow noreferrer"">here</a>, the <code>s3://</code> scheme is deprecated and requires the bucket in question to be exclusive to your Hadoop data. Quote from the above doc link:</p><blockquote>  <p>This filesystem requires you to dedicate a bucket for the filesystem -  you should not use an existing bucket containing files, or write other  files to the same bucket. The files stored by this filesystem can be  larger than 5GB, but they are not interoperable with other S3 tools.</p></blockquote>",5509152,,,2016-12-21T20:06:54.137,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41271068
374,41304804,2,41302414,2016-12-23T16:27:13.753,1,"<p>If you would like to keep the <code>channel</code> attribute as <code>List&lt;Map&lt;String, Object&gt;&gt;</code>, the simple answer is that you can't save the values as List of Map objects. The only solution available is to convert the data as String and store in DynamoDB.</p><p><strong>Couple of options to convert to String or JSON:-</strong></p><p>1) <code>@DynamoDBTypeConvertedJson</code> - This annotation can be used to convert the data into JSON and store it as String attribute in DynamoDB</p><p><code>@DynamoDBMarshalling</code> - is deprecated</p><p>2) Write a custom converter. Again, this will save the data as String in DynamoDB.</p><p><a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBTypeConverted.html"" rel=""nofollow noreferrer"">DynamoDBTypeConverted</a></p><p><strong>Note:-</strong></p><p>Though DynamoDB supports Document data types such as List and Map, it will be impossible to query the data if you have Map inside List. The DynamoDB API doesn't support filtering the data for this data model.</p><p>If you need to support any use case querying/updating the <code>channels</code> attribute, I would strongly recommend to rethink about the data model to flatten the data structure.</p><p>Another <strong>drawback</strong> is that index can't be created on Document data types (List and Map).</p>",6337748,,,2016-12-23T16:27:13.753,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41304804
375,41348697,2,20412599,2016-12-27T16:28:08.980,2,"<p>For remote editing, there are <em>lots</em> of options here: This answer, like any other, is sure to become outdated as more options enter the field.</p><ul><li>For <code>vim</code>, the <a href=""http://www.vim.org/scripts/script.php?script_id=1075"" rel=""nofollow noreferrer""><code>netrw</code> module</a> meets this need, and is shipped with the editor by default.</li><li>For <code>emacs</code>, this is available with <a href=""https://www.gnu.org/software/emacs/manual/html_node/emacs/Remote-Files.html"" rel=""nofollow noreferrer"">TRAMP</a>.</li><li>For the ATOM editor, see <a href=""https://atom.io/packages/remote-edit"" rel=""nofollow noreferrer"">the remote-files plugin</a>.</li><li>For IntelliJ, <a href=""https://www.jetbrains.com/help/idea/2016.2/editing-individual-files-on-remote-hosts.html#d208495e42"" rel=""nofollow noreferrer"">editing files on remote hosts</a> is supported in the commercial edition.</li><li>For Eclipse, see the <a href=""http://tmober.blogspot.com/2006/11/remote-system-explorer-10-is-released.html"" rel=""nofollow noreferrer"">Remote System Explorer</a> from the <a href=""http://www.eclipse.org/tm/"" rel=""nofollow noreferrer"">Target Management project</a>.</li></ul><p>I'd suggest starting with the editor you prefer and evaluating options from there. If you set up your SSH session to be able to authenticate directly to root (<em>password</em> auth is best disabled for root, but if you have sudo you can install RSA keys), then you'll be able to specify root as a target user for any of the above.</p><hr><p>By contrast, if you <em>really</em> do need <code>sudo</code>, you still have options:</p><ul><li>See <a href=""https://www.emacswiki.org/emacs/TrampMode#toc20"" rel=""nofollow noreferrer"">Using <code>tramp</code> to open files sudoed to root</a> on the Emacs wiki. New versions also support a <code>ssh+sudo</code> transport, meaning this wiki entry may already be out-of-date.</li></ul>",14122,14122,2016-12-27T16:34:08.923,2016-12-27T16:34:08.923,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41348697
376,41386467,2,31329495,2016-12-29T20:04:16.787,10,"<p><strong>1)</strong> Configure your API Gateway resource to use <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html"" rel=""noreferrer"">Lambda Proxy Integration</a> by checking the checkbox labeled <strong>""Use Lambda Proxy integration""</strong> on the ""Integration Request"" screen of the API Gateway resource definition. (Or define it in your cloudformation/terraform/serverless/etc config)</p><p><strong>2)</strong> Change your lambda code in 2 ways</p><ul><li>Process the incoming <code>event</code> (1st function argument) appropriately. It is no longer just the bare payload, it represents the entire HTTP request including headers, query string, and body. Sample below. Key point is that JSON bodies will be strings requiring explicit <code>JSON.parse(event.body)</code> call (don't forget <code>try/catch</code> around that). Example is below.</li><li>Respond by calling the callback with null then a response object that provides the HTTP details including <code>statusCode</code>, <code>body</code>, and <code>headers</code>.<ul><li><code>body</code> should be a string, so do <code>JSON.stringify(payload)</code> as needed</li><li><code>statusCode</code> can be a number</li><li><code>headers</code> is an object of header names to values</li></ul></li></ul><h2>Sample Lambda Event Argument for Proxy Integration</h2><pre><code>{  ""resource"": ""/example-path"",  ""path"": ""/example-path"",  ""httpMethod"": ""POST"",  ""headers"": {  ""Accept"": ""*/*"",  ""Accept-Encoding"": ""gzip, deflate"",  ""CloudFront-Forwarded-Proto"": ""https"",  ""CloudFront-Is-Desktop-Viewer"": ""true"",  ""CloudFront-Is-Mobile-Viewer"": ""false"",  ""CloudFront-Is-SmartTV-Viewer"": ""false"",  ""CloudFront-Is-Tablet-Viewer"": ""false"",  ""CloudFront-Viewer-Country"": ""US"",  ""Content-Type"": ""application/json"",  ""Host"": ""exampleapiid.execute-api.us-west-2.amazonaws.com"",  ""User-Agent"": ""insomnia/4.0.12"",  ""Via"": ""1.1 9438b4fa578cbce283b48cf092373802.cloudfront.net (CloudFront)"",  ""X-Amz-Cf-Id"": ""oCflC0BzaPQpTF9qVddpN_-v0X57Dnu6oXTbzObgV-uU-PKP5egkFQ=="",  ""X-Forwarded-For"": ""73.217.16.234, 216.137.42.129"",  ""X-Forwarded-Port"": ""443"",  ""X-Forwarded-Proto"": ""https""  },  ""queryStringParameters"": {  ""bar"": ""BarValue"",  ""foo"": ""FooValue""  },  ""pathParameters"": null,  ""stageVariables"": null,  ""requestContext"": {  ""accountId"": ""666"",  ""resourceId"": ""xyz"",  ""stage"": ""dev"",  ""requestId"": ""5944789f-ce00-11e6-b2a2-dfdbdba4a4ee"",  ""identity"": {  ""cognitoIdentityPoolId"": null,  ""accountId"": null,  ""cognitoIdentityId"": null,  ""caller"": null,  ""apiKey"": null,  ""sourceIp"": ""73.217.16.234"",  ""accessKey"": null,  ""cognitoAuthenticationType"": null,  ""cognitoAuthenticationProvider"": null,  ""userArn"": null,  ""userAgent"": ""insomnia/4.0.12"",  ""user"": null  },  ""resourcePath"": ""/example-path"",  ""httpMethod"": ""POST"",  ""apiId"": ""exampleapiid""  },  ""body"": ""{\n  \""foo\"": \""FOO\"",\n  \""bar\"": \""BAR\"",\n  \""baz\"": \""BAZ\""\n}\n"",  ""isBase64Encoded"": false}</code></pre><h2>Sample Callback Response Shape</h2><pre><code>callback(null, {  statusCode: 409,  body: JSON.stringify(bodyObject),  headers: {  'Content-Type': 'application/json'  }})</code></pre><hr><p><strong>Notes</strong>- I believe the methods on <code>context</code> such as <code>context.succeed()</code> are deprecated. They are no longer documented although they do still seem to work. I think coding to the callback API is the correct thing going forward.</p>",266795,,,2016-12-29T20:04:16.787,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41386467
377,41506701,2,37542778,2017-01-06T13:23:19.463,0,"<p>I've got question: do you run your pipeline from web console or there is a program? The reason I'm asking, please check all fields are filled correctly. It could be you've missed region and it cant find method signature with empty param where supposed to be a <code>String (ex. eu-west-1).</code></p><p>From <a href=""https://github.com/awslabs/emr-dynamodb-connector/blob/master/emr-dynamodb-tools/src/main/java/org/apache/hadoop/dynamodb/tools/DynamoDBExport.java"" rel=""nofollow noreferrer"">https://github.com/awslabs/emr-dynamodb-connector/blob/master/emr-dynamodb-tools/src/main/java/org/apache/hadoop/dynamodb/tools/DynamoDBExport.java</a> you could chase your code flow. However keep in mind this class could be out of date so lines could be not matching. But it's giving you rough idea what happens there.</p>",1693021,,,2017-01-06T13:23:19.463,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41506701
378,41559293,2,41556358,2017-01-10T00:25:39.927,3,"<p>No - Amazon does not provide an API for updating your skill.  I believe that API.AI provides this and they have an integration with Alexa ASK but I've heard that it's not very good and I don't really expect it to get better now that Google has bought them.</p><p>In your first paragraph it sounds like you are talking about supporting a lot of different utterances (ways of saying the same thing), but after that it sounds like you are talking about a large number of 'slot values': the relevant user input extracted from the utterances.  Either way, the answer is still no - no API.</p><p>I believe there are popular features requests for both a general API for updating the skills, and support for dynamic slot lists, <a href=""https://forums.developer.amazon.com/content/idea/45116/dynamic-or-per-user-value-list-for-custom-slots.html"" rel=""nofollow noreferrer"">here</a>.  </p><p>And <a href=""https://developer.amazon.com/blogs/post/Tx3IHSFQSUF3RQP/why-a-custom-slot-is-the-literal-solution"" rel=""nofollow noreferrer"">here's</a> Amazon's defence of the custom slot and how it can replace the (deprecated) literal slot.</p>",150016,150016,2017-01-10T15:16:35.103,2017-01-10T15:16:35.103,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41559293
379,41656873,2,13274353,2017-01-15T01:24:12.790,13,"<p>There's a much easier method than making a web API call, the .NET SDK.</p><p>See the <a href=""https://docs.aws.amazon.com/sdkfornet/v3/apidocs/items/Util/TEC2InstanceMetadata.html"" rel=""noreferrer"">SDK documentation for EC2InstanceMetadata here</a></p><p>For example, if you need InstanceId you can use:</p><pre><code>Amazon.Util.EC2InstanceMetadata.InstanceId.ToString();</code></pre><p>All the other properties are available in a similar manner.</p><p>Note that the SDK used to have Amazon.EC2.Utils - this was deprecated in 2015 and moved to Amazon.Util namespace</p>",4757635,4757635,2019-02-06T17:54:12.070,2019-02-06T17:54:12.070,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/41656873
380,41741645,2,41741100,2017-01-19T12:18:59.270,1,"<p>When using NVidia please consider the information obsolete that is given in the related question you linked.</p><p>For about a year now the NVidia drivers support true headless operation without an X server running. See this exhaustive article given on the Nvidia developer blog: <a href=""https://devblogs.nvidia.com/parallelforall/egl-eye-opengl-visualization-without-x-server/"" rel=""nofollow noreferrer"">https://devblogs.nvidia.com/parallelforall/egl-eye-opengl-visualization-without-x-server/</a></p>",524368,,,2017-01-19T12:18:59.270,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41741645
381,41784099,2,27780356,2017-01-21T20:27:35.363,2,"<p>I'm not sure that the accepted answer is complete because it does not acknowledge use cases, and it does not address the question asked of ""what if I want to change the data structure."" Well, if you have outdated clients, and change the data structure of the documents in your nosql database, then those clients will not be able to access it. I don't believe DynamoDB offers a middleware platform to support this kind of old-to-new model adaptation. You'll have to force an update to your clients.</p><p>In fact, there are many operations beyond user-based permissions (which Cognito does do well) like this that you might need middleware for. Perhaps you want sorting logic to occur at request-time, and not maintain a copy of that sorting logic in every client application.</p><p>The question of ""is it worth it"" probably depends on the complexity of your application and your users' relationship with the data (ie. if the presentation layer basically just wrapped data -- then directly access DynamoDB. If you your presentation layer is not just wrapped data, then you should probably use custom middleware). In truth, I stumbled upon this question while running my own cost-benefit analysis, and am not sure which approach I will take. Another major factor of mine is that I might choose to switch database solutions in the future.. this will be more challenging to update on every client, if my clients are directly accessing the DB.</p><p>The one certain conclusion I've reached is that you should use middleware <em>somewhere</em> in your system, such that you can decouple your database vendor from either the client logic or the server logic as much as possible, eg. in a mobile app:  writeToDatabase(Data data){writeToDynamo(data);}</p><p>To achieve this, AWS suggests using Amazon Api Gateway as a proxy for AWS services, and even has premade configurations for Amazon API Gateway to behave as AWS service proxy.</p>",5666034,5666034,2017-01-21T20:54:19.890,2017-01-21T20:54:19.890,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41784099
382,41794686,2,29265270,2017-01-22T18:34:00.510,0,"<p>I think the way to do this is to write a custom provider: a class that implements the AWSIdentityProviderManager protocol: <a href=""http://docs.aws.amazon.com/AWSiOSSDK/latest/Protocols/AWSIdentityProviderManager.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AWSiOSSDK/latest/Protocols/AWSIdentityProviderManager.html</a>  </p><p>This protocol has basically one method, ""logins"" which you use to return your NSDictionary of credentials.  So instead of this line of code:</p><pre><code>let credentialsProvider : AWSCredentialsProvider = AWSServiceManager.defaultServiceManager().defaultServiceConfiguration.credentialsProvider</code></pre><p>you would design a custom class (most of this is straight from the AWS docs):</p><pre><code>class MyIdentityProvider: NSObject, AWSIdentityProviderManager {func logins() -&gt; AWSTask&lt;NSDictionary&gt; {  if let token = FBSDKAccessToken.current() {  return AWSTask(result: [AWSIdentityProviderFacebook:token])  }    return AWSTask(error:NSError(domain: ""Facebook Login"", code: -1 , userInfo: [""Facebook"" : ""No current Facebook access token""]))}</code></pre><p>then you instantiate your AWSService manager using your custom class:</p><pre><code>let identityProvider = MyIdentityProvider()let credentialsProvider = AWSCognitoCredentialsProvider(regionType: .usEast1,  identityPoolId: ""&lt;your poolId&gt;"",  identityProviderManager: identityProvider)let configuration = AWSServiceConfiguration(region:.usEast1, credentialsProvider:credentialsProvider)AWSServiceManager.default().defaultServiceConfiguration = configuration</code></pre><p>See an example in this SO answer (to a related question): <a href=""https://stackoverflow.com/questions/38311479/iosaws-cognito-logins-is-deprecated-use-awsidentityprovidermanager"">[iOS][AWS Cognito] &#39;logins&#39; is deprecated: Use &quot;AWSIdentityProviderManager&quot;</a></p>",5098848,-1,2017-05-23T12:24:41.687,2017-01-22T18:34:00.510,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41794686
383,41801876,2,12132350,2017-01-23T08:25:45.447,0,"<p>According <a href=""https://www.magentocommerce.com/magento-connect/pay-with-amazon-for-magento-1.html"" rel=""nofollow noreferrer"">Magentocommerce</a> the latest version is 1.4.0 </p><p>But this is outdated. You'll find a newer version (1.4.3 at this time) on <a href=""https://github.com/amzn/amazon-payments-magento-plugin"" rel=""nofollow noreferrer"">GitHub</a></p><p>So I recommend to check GitHub in future (too).</p>",2635490,,,2017-01-23T08:25:45.447,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41801876
384,41818987,2,41816583,2017-01-24T02:22:58.613,1,"<p>The policy provided looks correct and not out of date to me. Are you sure that the IAM user in question doesn't have any additional groups/policies applied beyond the <code>AWS::IAM::Group</code> specified that would be granting them the unexpected permissions?</p><p>One way to confirm this would be to create a new IAM user from scratch and attempt to reproduce the issue there.</p>",2518355,,,2017-01-24T02:22:58.613,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41818987
385,41832945,2,41832343,2017-01-24T16:10:38.507,1,"<p>Perhaps you need pass the <code>--httpinterface</code> flag to the mongodb container: </p><pre><code>docker run --name some-mongo -d mongo --httpinterface </code></pre><p>This flag are deprecated since version 3.2: HTTP interface for MongoDB</p><p>Check <a href=""https://docs.mongodb.com/manual/reference/program/mongod/#cmdoption--httpinterface"" rel=""nofollow noreferrer"">Mongodb Documentation</a></p>",5636405,,,2017-01-24T16:10:38.507,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41832945
386,41945844,2,20397132,2017-01-30T21:38:34.720,1,"<p><code>eb stop</code> is deprecated. I also had the same problem and the only solution I could come up with was to backup the environment and then restore it.</p><p>Here's a blog post in which I'm explaining it:<a href=""http://pminkov.github.io/blog/how-to-shut-down-and-restore-an-elastic-beanstalk-environment.html"" rel=""nofollow noreferrer"">http://pminkov.github.io/blog/how-to-shut-down-and-restore-an-elastic-beanstalk-environment.html</a></p>",375895,375895,2017-02-23T17:50:16.607,2017-02-23T17:50:16.607,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/41945844
387,42013451,2,41282207,2017-02-02T22:25:04.403,0,"<p>Sigv2 is deprecated in favor of Sigv4. The only service that still requires sigv2 is SDB, which you probably shouldn't be writing new code against. </p><p>Is this for SDB or another service?</p>",734691,,,2017-02-02T22:25:04.403,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42013451
388,42184058,2,42183433,2017-02-12T03:55:40.103,0,"<p>Look at this first line of code:</p><pre><code>client = AmazonEC2ClientBuilder.standard().withCredentials(credentials).build(); </code></pre><p>The variable <code>client</code> is an <code>AmazonEC2Client</code> object (you should have declared it as such higher up in your code). On the second line you are trying to call a method on the <code>AmazonEC2Client</code> object. You aren't using the <code>AmazonEC2ClientBuilder</code> anymore. You've already built your client object in the first line of code, and now you want to call a method on that object in the next line, like so:</p><pre><code>client.setRegion(""US-WEST2"");</code></pre><p>However that method is deprecated and it is recommended you call the <code>setRegion</code> method on the <code>AmazonEC2ClientBuilder</code> instead. So you would remove the second line entirely, and change the first line to this:</p><pre><code>client = AmazonEC2ClientBuilder.standard()  .withCredentials(credentials)  .withRegion(Regions.US_WEST_2)  .build();</code></pre>",13070,,,2017-02-12T03:55:40.103,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42184058
389,42217589,2,40849112,2017-02-14T03:53:18.340,4,"<p>After banging my head around on this for a day, it turns out the issue is caused by an out of date AWS CLI. <a href=""http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html#task-iam-roles-minimum-sdk"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html#task-iam-roles-minimum-sdk</a></p><p>In my case I was installing aws-cli with <code>apt-get install aws-cli</code> which installs version 1.4.2. This version does not handle the <code>AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</code> environment variable needed to get the correct IAM. So it defaults to getting the instance's IAM.</p><p>The solution was to install the AWS CLI through pip or the bundled installation to ensure I had the latest version. The same will apply for the AWS SDKs - required versions are described in the link above.</p>",773479,,,2017-02-14T03:53:18.340,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42217589
390,42298949,2,42288219,2017-02-17T13:16:14.020,2,"<p>It's a common misconception that you ""fetch"" pre-signed URLs from the service, but that isn't true.  </p><p>They're generated locally in your code (or locally in the SDK code).  Your secret key is used to generate an HMAC digest of a canonical representation of the request that the URL represents.  When the service receives the request (e.g., when the URL is clicked) the service canonicalizes the request, generates the hash from your secret (which is not in the URL, but is known to you and to the service), and if the results match, the request is considered authenticated and allowed to continue to the authorization stage (to ensure that the access key ID presented actually has permission to allow the request).  If the results don't match, then the key or secret are wrong, the signing code on your side has made an error, or the signed URL has been modified since it was signed.  Access denied.</p><p>There are, then, no service endpoints that return signed URLs, although you could relatively easily create one for your server to call, using Lambda and API gateway.  Lambda for Node, for example, already has the SDK installed, so you could pass the URL in and return the URL back out, over HTTPS.  Simple but a little bit silly.  You'd have to authenticate the requests from your app using api keys or secret headers, of course, since you wouldn't be able to use native auth, because that would require the SDK. :)</p><p>Generating your own code to create signed URLs is not that difficult.   The process is <a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html"" rel=""nofollow noreferrer"">fully documented</a> though you'll note on that page that they suggest you use the SDKs.  The reason for this, I suspect, is that even if you are not sufficiently skilled at coding to write your own implementation of the algorithm, it won't be a barrier to entry.  </p><p>I have written my own implementatiions of this... in fact, I've even written an implementation of the older <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html"" rel=""nofollow noreferrer"">V2 signing algorithm</a> (which still works in any region that has been online since before 2014) entirely in SQL, as a MySQL stored function (e.g. <code>SELECT get_signed_url('/bucket/key');</code> returns a signed url to <code>GET</code> an object.  The credentials are stored as variables inside the function)... and given that SQL is probably the least likely language you'd think of for such an operation, this works perfectly.  </p><p>So you can write it yourself.</p><p>But the objection to including the SDK for maintenance reasons is misplaced, I think.  </p><p>AWS doesn't make breaking changes to their service APIs. </p><p>They just don't.  When S3 introduced version 2 of the ListObjects action, V1 still remained available.  Using V2 is recommended, but V1 is <em>not</em> deprecated, nor subject to removal.  </p><p>When Signature Version 4 (mentioned above) was introduced, they added it to all the old regions, left Signature Version 2 in place where it was already available, and deployed new regions only with V4.  Sig V2 is also not deprecated, but V4 is recommended.  Object versioning in S3 was written in such a way that it is 100% backwards compatible with code that doesn't understand object versioning.  The list goes on.</p><p>Short of a bug that impacts you (unlikely), or something related to security (also unlikely) the current version of any SDK should not need to be replaced in your project unless you wanted to take advantage of new features, new services, or new AWS regions, which probably doesn't happen all that often in a stable project.</p>",1695906,,,2017-02-17T13:16:14.020,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42298949
391,42510741,2,34884802,2017-02-28T13:59:59.197,3,"<p>The above answer is outdated. AWS has added Cognito-Groups recently. That provides more flexibility</p><p>You can use technique described in the article to achieve that:<a href=""https://aws.amazon.com/blogs/aws/new-amazon-cognito-groups-and-fine-grained-role-based-access-control-2/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/aws/new-amazon-cognito-groups-and-fine-grained-role-based-access-control-2/</a></p>",4074545,,,2017-02-28T13:59:59.197,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42510741
392,42530951,2,42506880,2017-03-01T11:49:19.950,2,<p>The problem was that I was using <strong>ImageUtils</strong> instead of <strong>textureLoader</strong>. I think ImageUtils is deprecated...</p>,4165012,,,2017-03-01T11:49:19.950,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42530951
393,42564523,2,42563712,2017-03-02T19:42:31.047,2,"<p>I'd say pick anything which fit's your needs (see also <a href=""https://kubernetes.io/docs/getting-started-guides/#table-of-solutions"" rel=""nofollow noreferrer"">Picking the right solution</a>)...</p><p>Which could be:</p><ul><li>Speed of the cluster setup</li><li>Integration in your existing toolchain<ul><li>e.g. kops integrates with Terraform which might be a good fit for some prople</li></ul></li><li>Experience within your team/company/...<ul><li>e.g. how comfortable are you with the related Linux distribution</li></ul></li><li>Required maturity of the tool itself<ul><li>some tools are very alpha, are you willing to play to role of an early adaptor?</li></ul></li><li>Ability to upgrade between Kubernetes versions<ul><li>kubeadm has this on their agenda, some others prefer to throw away clusters instead of upgrading</li></ul></li><li>Required integration into external tools (monitoring, logging, auth, ...)</li><li>Supported cloud providers</li></ul><p>With your specific requirements I'd pick the Heptio or kubeadm approach.</p><ul><li><a href=""https://www.heptio.com/"" rel=""nofollow noreferrer"">Heptio</a> if you can live with the given constraints (e.g. predefined OS)</li><li><a href=""https://kubernetes.io/docs/getting-started-guides/kubeadm/"" rel=""nofollow noreferrer"">kubeadm</a> if you need more flexibility, everything done with kubeadm can be transferred to other cloud providers</li></ul><p>Other options for AWS lower on my list:</p><ul><li><a href=""https://github.com/kelseyhightower/kubernetes-the-hard-way"" rel=""nofollow noreferrer"">Kubernetes the hard way</a> - using this might be the only true way to setup a production cluster as this is the only way you can fully understand each moving part of the system. Lower on the list, because often the result from any of the tools might just be more than enough, even for production.</li><li><a href=""https://kubernetes.io/docs/getting-started-guides/aws/#kube-up-bash-script"" rel=""nofollow noreferrer"">kube-up.sh</a> - is deprecated by the community, so I'd not use it for new projects</li><li><a href=""https://github.com/kubernetes/kops"" rel=""nofollow noreferrer"">kops</a> - my team had some strange experiences with it which seemed due to our (custom) needs back then (existing VPC), that's why it's lower on my list - it would be #1 for an environment where Terraform is used too.</li><li><a href=""https://github.com/kubernetes-incubator/bootkube"" rel=""nofollow noreferrer"">bootkube</a> - lower on my list, because it's limitation to CoreOS</li><li><a href=""http://rancher.com/"" rel=""nofollow noreferrer"">Rancher</a> - interesting toolchain, seems to be too much for a single cluster</li></ul><hr><p>Offtopic: If you don't <em>have</em> to run on AWS, I'd also always consider to rather run on GCE for production workloads, as this is a well managed platform rather than something you've to build yourself.</p>",116932,116932,2017-03-02T22:14:30.263,2017-03-02T22:14:30.263,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42564523
394,42583411,2,28115250,2017-03-03T16:01:49.473,0,"<blockquote>  <p>macOS users: If you are using the Python 3.6 from the python.org  binary installer linked on this page, please carefully read the  Important Information displayed during installation; this information  is also available after installation by clicking on  /Applications/Python 3.6/ReadMe.rtf. There is important information  there about changes in the 3.6.0 installer-supplied Python,  particularly with regard to SSL certificate validation.</p></blockquote><p><a href=""https://www.python.org/downloads/release/python-360/"" rel=""nofollow noreferrer"">https://www.python.org/downloads/release/python-360/</a></p><p>From ReadMe.rtf at the time of this writing:</p><blockquote>  <p>Certificate verification and OpenSSL</p>   <p><strong>NEW</strong> This variant of Python 3.6 now includes its own private copy of OpenSSL 1.0.2.  Unlike previous releases, the deprecated  Apple-supplied OpenSSL libraries are no longer used.  This also means  that the trust certificates in system and user keychains managed by  the Keychain Access application and the security command line utility  are no longer used as defaults by the Python ssl module.  For 3.6.0, a  sample command script is included in /Applications/Python 3.6 to  install a curated bundle of default root certificates from the  third-party certifi package (<a href=""https://pypi.python.org/pypi/certifi"" rel=""nofollow noreferrer"">https://pypi.python.org/pypi/certifi</a>).   If you choose to use certifi, you should consider subscribing to the  project's email update service to be notified when the certificate  bundle is updated.</p>   <p>The bundled pip included with the Python 3.6 installer has its own  default certificate store for verifying download connections.</p></blockquote>",93345,,,2017-03-03T16:01:49.473,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42583411
395,42652927,2,42651235,2017-03-07T16:06:24.203,7,"<p>Spark will automatically copy your AWS Credentials to the s3n and s3a secrets. Apache Spark releases don't touch the s3:// URLs, as in Apache Hadoop, the s3:// schema is associated with the original, now deprecated s3 client, one which is incompatible with everything else. </p><p>On Amazon EMR, s3:// is bound to amazon EMR S3; EC2 VMs will provide the secrets for executors automatically. So I don't think it bothers with the env var propagation mechanism. It might also be that how it sets up the authentication chain, you can't override the EC2/IAM data.</p><p>If you are trying to talk to S3 and <em>you are not running in an EMR VM</em>, then presumably you are using Apache Spark with the Apache Hadoop JARs, not the EMR versions. In that world use URLs with s3a:// to get the latest S3 client library</p><p>If that doesn't work, look at the troubleshooting section of <a href=""https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md"" rel=""noreferrer"">the apache docs</a>. There's a section on ""403"" there including recommended steps for troubleshooting. It can be due to classpath/JVM version problems as well as credential issues, even clock-skew between client and AWS.</p>",2261274,2261274,2017-03-07T17:03:47.463,2017-03-07T17:03:47.463,7,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42652927
396,42661783,2,42659418,2017-03-08T02:16:02.720,3,"<p>In April, 2018, the original answer (and the question itself) were made obsolete...</p><blockquote>  <p>You can now specify tags for EBS snapshots as part of the API call that creates the resource or via the Amazon EC2 Console when creating an EBS snapshot.</p>   <p><a href=""https://aws.amazon.com/blogs/compute/tag-amazon-ebs-snapshots-on-creation-and-implement-stronger-security-policies/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/compute/tag-amazon-ebs-snapshots-on-creation-and-implement-stronger-security-policies/</a></p></blockquote><p>...unless you are using an older version of an SDK that does not implement the feature.</p><p>The same announcement extended resource-level permissions to snapshots.</p><p><strike>The underlying <a href=""http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateSnapshot.html"" rel=""nofollow noreferrer""><code>CreateSnapshot</code> action</a> in the EC2 API doesn't have any provision for adding tags simultaneously with the creation of the snapshot.  You have to go back and tag it after creating it.</strike></p>",1695906,1695906,2018-06-13T09:02:18.940,2018-06-13T09:02:18.940,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/42661783
397,42762861,2,42684034,2017-03-13T11:46:12.713,2,"<p>S3n doesn't support IAM roles, and 2.4 is a very outdated version anyway. Not as buggy as 2.5 when it comes to s3n, but still less than perfect.</p><p>If you want to use IAM roles, you are going to have to switch to S3a, and yes, for you, that does mean upgrading Hadoop. sorry.</p>",2261274,,,2017-03-13T11:46:12.713,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42762861
398,42786725,2,36495898,2017-03-14T13:00:15.720,11,"<p>DynamoDBMarshaller is deprecated. Use DynamoDBTypeConverter instead.</p><p>Example:</p><p>Enum class</p><pre><code>public static enum ValidationFailure {  FRAUD, GENERAL_ERROR}</code></pre><p>DynamoDBTable class</p><pre><code> @DynamoDBTable(tableName = ""receipt"")public class Receipt {  private Long id;  private List&lt;ValidationFailure&gt; validation;  @DynamoDBHashKey(attributeName = ""id"")  public Long getId() {  return id;  }  @DynamoDBTypeConverted(converter = ValidationConverter.class)  public List&lt;ValidationFailure&gt; getValidation() {  return validation;  }  public void setId(Long id) {  this.id = id;  }  public void setValidation(List&lt;ValidationFailure&gt; validation) {  this.validation = validation;  }}</code></pre><p>Convertor:</p><pre><code>public class ValidationConverter implements DynamoDBTypeConverter&lt;List&lt;String&gt;, List&lt;ValidationFailure&gt;&gt; {@Overridepublic List&lt;String&gt; convert(List&lt;ValidationFailure&gt; object) {  List&lt;String&gt; result = new ArrayList&lt;String&gt;();  if (object != null) {  object.stream().forEach(e -&gt; result.add(e.name()));  }  return result;}@Overridepublic List&lt;ValidationFailure&gt; unconvert(List&lt;String&gt; object) {  List&lt;ValidationFailure&gt; result = new ArrayList&lt;ValidationFailure&gt;();  if (object != null) {  object.stream().forEach(e -&gt; result.add(ValidationFailure.valueOf(e)));  }  return result;}}</code></pre>",2627749,5353128,2018-02-22T12:11:08.527,2018-02-22T12:11:08.527,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42786725
399,42834182,2,42822483,2017-03-16T12:30:44.627,17,"<p>What you are seeing is a problem with outputcommitter and s3.the commit job applies <code>fs.rename</code> on the _temporary folder and since S3 does not support rename it means that a single request is now copying and deleting all the files from _temporary to its final destination..</p><p><code>sc.hadoopConfiguration.set(""mapreduce.fileoutputcommitter.algorithm.version"", ""2"")</code> only works with hadoop version > 2.7. what it does is to copy each file from _temporary on commit task and not commit job so it is distributed and works pretty fast.</p><p>If you use older version of hadoop I would use Spark 1.6 and use:</p><pre><code>sc.hadoopConfiguration.set(""spark.sql.parquet.output.committer.class"",""org.apache.spark.sql.parquet.DirectParquetOutputCommitter"")</code></pre><p>*note that it does not work with specualtion turned on or writing in append mode</p><p>**also note that it is deprecated in Spark 2.0 (replaced by algorithm.version=2)</p><p>BTW in my team we actually write with Spark to HDFS and use DISTCP jobs (specifically s3-dist-cp) in production to copy the files to S3 but this is done for several other reasons (consistency, fault tolerance) so it is not necessary.. you can write to S3 pretty fast using what I suggested.</p>",3379058,512251,2018-09-07T16:03:02.467,2018-09-07T16:03:02.467,10,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/42834182
400,42854114,2,42733164,2017-03-17T09:46:46.893,0,"<p>Okay so I finally figured it out. The reason why it wasn't working was because my botocore was on version <code>1.4.62</code>. I only realized it because another script that ran fine on my colleague's machine was throwing exceptions on mine. We had the same boto3 version but different botocore versions. After I <code>pip install botocore==1.5.26</code> both the other script and my kinesis <code>put_record</code> started working.</p><p>tldr: <code>botocore 1.4.62</code> is horribly broken in many ways so upgrade NOW. I can't believe how much of my life is wasted by outdated broken libraries. I wonder if Amazon dev can unpublish broken versions of the client?</p>",429288,,,2017-03-17T09:46:46.893,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42854114
401,42901567,2,42891191,2017-03-20T10:45:38.260,1,"<p>It turned out it's related to a lot of obsolete stopped containers in in devicemapper folder. See details: <a href=""https://stackoverflow.com/questions/42901542/docker-ps-hangs-forever-after-server-restart/42901543#42901543"">&#39;docker ps&#39; hangs forever after server restart</a></p>",792313,-1,2017-05-23T12:32:10.313,2017-03-20T10:45:38.260,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42901567
402,42901673,2,16585898,2017-03-20T10:50:30.793,7,"<p>Accepted answer is quite outdated.</p><p>Now you can use <code>/opt/elasticbeanstalk/support/envvars</code> file which is already a shell script ready to be sourced:</p><pre><code>commands:  01_update_composer:  command: |  . /opt/elasticbeanstalk/support/envvars  /usr/bin/composer.phar self-updatecontainer_commands:  01_run_composer:  command: |  composer.phar install --no-scripts --no-dev  # already has user-specified env variables</code></pre><hr><p><strong>Update:</strong></p><p>After some deeper investigation turns out that <code>container_commands:</code> include your environment variables, but <code>commands:</code> not.</p>",2397150,2397150,2017-03-20T16:06:44.400,2017-03-20T16:06:44.400,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42901673
403,42906663,2,28399041,2017-03-20T14:47:07.853,1,"<p>The ec2-ami-tools package in the Ubuntu 14.04 repository is outdated. The version in the repository is only version 1.4.0. Frankfurt support was only added in <a href=""https://aws.amazon.com/items/368?externalID=368&amp;categoryID=88"" rel=""nofollow noreferrer"">version 1.5.6</a>. The newer version of the tool should work in the Frankfurt region.</p><p>This tool is only necessary if you plan on creating instance store-backed AMIs. If your instance is EBS-backed and you need to make an AMI from it, using the AWS CLI will be much easier.</p>",7740115,,,2017-03-20T14:47:07.853,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42906663
404,42974884,2,42972634,2017-03-23T11:35:54.080,1,"<p>First, freeSSHd is unreliable and obsolete server. I suggest you use a different one.</p><p>Actually, <a href=""https://serverfault.com/q/648855/168875"">Microsoft now provides a beta version of OpenSSH port for Windows</a>.</p><hr><p>Anyway, configure the accounts in freeSSHd settings. And then use the configured credentials in PuTTY.</p>",850848,-1,2017-04-13T12:13:41.357,2017-03-23T11:43:14.547,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42974884
405,42992543,2,42979358,2017-03-24T06:09:18.997,1,"<p>You are starting worker with <code>celery</code> command. Changing <code>/etc/default/celeryd</code> won't have any effect on celery command. Moreover celeryd is deprecated. </p><p>When a worker is started, celery launches a default process and n(concurrency) subprocesses. </p><p>You can start the worker with </p><pre><code>[program:celery]command=/opt/python/run/venv/bin/celery worker -c 1 -A foo""</code></pre><p>This will start a worker with concurrency of 1 and there will be 2 processes.  </p>",2698552,2698552,2017-03-24T16:07:44.070,2017-03-24T16:07:44.070,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/42992543
406,43029893,2,43028154,2017-03-26T14:20:25.673,3,"<p>From the S3 Developer Guide:</p><blockquote>  <p>Bucket ownership is not transferable; however, if a bucket is empty, you can delete it. After a bucket is deleted, the name becomes available to reuse, but the name might not be available for you to reuse for various reasons. For example, some other account could create a bucket with that name. <strong>Note, too, that it might take some time before the name can be reused.  So if you want to use the same bucket name, don't delete the bucket.</strong> <em>[emphasis added]</em></p>   <p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html</a></p></blockquote><p>There is your documentation of the expected delay, and a bonus nugget: you really should not be deleting and recreating buckets.  Since the bucket namespace is global, only one bucket of a given name can exist at any given time, and it can of course only be in one region.  Buckets are only intended to be deleted when you're never going to need them again -- otherwise, empty the bucket of objects, but keep the bucket.  Someone might snap the name up, if it's particularly non-obscure.  Also: </p><blockquote>  <p>The high-availability engineering of Amazon S3 is focused on get, put, list, and delete operations. Because bucket operations work against a centralized, global resource space, it is not appropriate to create or delete buckets on the high-availability code path of your application. It is better to create or delete buckets in a separate initialization or setup routine that you run less often.</p>   <p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html</a></p></blockquote><p>Although the details are not public, all S3 regions are in some sense aware of all buckets.   If you send an S3 request to the wrong region, that wrong region does know the correct region and will return an error that includes an (apparently undocumented) <a href=""https://stackoverflow.com/a/33289160/1695906""><code>x-amz-bucket-region:</code> response header</a>.</p><p>So when buckets are created -- regardless of region -- there is necessarily some internal communication with the S3 global database that occurs when creating a bucket.  A disruption here could presumably prevent the creation of a bucket in any region, even if that region was otherwise operating normally.</p><p>Note that -- as of this writing -- another section of the documentation, the S3 Getting Started Guide, provides essentially the same information; however, until at least late 2015, the information provided there was different.</p><blockquote>  <p>When you delete a bucket, there may be a delay of up to one hour before the bucket name is available for reuse in a new region or by a new bucket owner. If you re-create the bucket in the same region or with the same bucket owner, there is no delay.</p>   <p><a href=""https://web.archive.org/web/20150905132716/http://docs.aws.amazon.com/AmazonS3/latest/gsg/DeletingAnObjectandBucket.html"" rel=""nofollow noreferrer"">https://web.archive.org/web/20150905132716/http://docs.aws.amazon.com/AmazonS3/latest/gsg/DeletingAnObjectandBucket.html</a></p></blockquote><p>Intuitively, I suspect that the continued growth of S3 could have made this information obsolete prior to its removal, and prior this snapshot from 2015.</p>",1695906,-1,2017-05-23T12:02:42.557,2017-03-26T14:20:25.673,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43029893
407,43056995,2,43056007,2017-03-27T21:45:29.613,0,"<p>I read that the use of rc.local is getting deprecated. One thing to try is a line in /etc/crontab like this: </p><p>@reboot  full-path-of-script</p><p>If there's a specific user you want to run the script as, you can list it after @reboot.</p>",7724239,,,2017-03-27T21:45:29.613,10,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43056995
408,43207871,2,43207442,2017-04-04T12:54:37.510,2,"<p>The plugin you linked to looks very outdated, Last updated: <code>24 March 2010</code> and seems odd to depend on another plugin like quartz to use s3. </p><p>I would instead suggest Karman (maintained by the same person as asset-pipeline) and updated for Grails 2 &amp; 3 <a href=""https://grails.org/plugin/karman-aws?skipRedirect=true"" rel=""nofollow noreferrer"">https://grails.org/plugin/karman-aws?skipRedirect=true</a></p><p>Or you may just use the aws java sdk directly - it is straight forward api: <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadObjSingleOpJava.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadObjSingleOpJava.html</a></p>",2209885,,,2017-04-04T12:54:37.510,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43207871
409,43207907,2,43207442,2017-04-04T12:56:04.723,1,"<p><a href=""http://grails.org/plugin/amazon-s3"" rel=""nofollow noreferrer"">This plugin</a> is out of date.</p><p>I think it will be better to use <a href=""https://grails.org/plugin/aws-sdk"" rel=""nofollow noreferrer"">https://grails.org/plugin/aws-sdk</a>. This plugin supports S3 integration. You can find docs on github.</p>",3886830,,,2017-04-04T12:56:04.723,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43207907
410,43231324,2,43221191,2017-04-05T12:33:15.437,2,"<p>The problem is that you've included some classes from Kinesis, <em>not</em> Kinesis Firehose, Java package. For e.g., you've used:</p><pre><code>import com.amazonaws.services.kinesis.model.PutRecordRequest;</code></pre><p>Whereas, you should've used:</p><pre><code>import com.amazonaws.services.kinesisfirehose.model.PutRecordRequest;</code></pre><p>Kinesis, Kinesis Firehose and Kinesis Analytics are different services, even though they fall under one umbrella of streaming services on AWS. Consequently, they have different package namespaces in the Java SDK. If you start from the official documentation <a href=""http://docs.aws.amazon.com/firehose/latest/APIReference/Welcome.html"" rel=""nofollow noreferrer"">here</a>, you'll reach the correct Java SDK reference <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesisfirehose/model/PutRecordRequest.html"" rel=""nofollow noreferrer"">here</a>.</p><p><strong>EDIT</strong>: To answer your other question: yes, the following is deprecated:</p><pre><code>AmazonKinesisFirehoseClient firehoseClient = new AmazonKinesisFirehoseClient(credentials);</code></pre><p>You should instead use the following:</p><pre><code> AmazonKinesisFirehoseClient firehoseClient = AmazonKinesisFirehoseClientBuilder.standard().withCredentials(new AWSStaticCredentialsProvider(awsCredentials)).build();</code></pre><p>Refer to the official documentation <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesisfirehose/AmazonKinesisFirehoseClient.html"" rel=""nofollow noreferrer"">here</a> on how to correctly initialize AmazonKinesisFirehoseClient.</p>",1517410,1517410,2017-04-05T19:03:14.260,2017-04-05T19:03:14.260,6,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43231324
411,43240215,2,43239728,2017-04-05T19:40:45.553,1,"<p>Use the static builder() method instead of the deprecated constructors.</p><p>The javadoc documentation suggests this approach instead of using the deprecated constructor:</p><pre><code>AmazonKinesisFirehoseClient client = AmazonKinesisFirehoseClientBuilder.standard().withCredentials(new AWSStaticCredentialsProvider(awsCredentials)).build();// You can now use the client as you normally would...client.putRecord(xyz);</code></pre><p>See: <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesisfirehose/AmazonKinesisFirehoseClient.html#AmazonKinesisFirehoseClient-com.amazonaws.auth.AWSCredentials-"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesisfirehose/AmazonKinesisFirehoseClient.html#AmazonKinesisFirehoseClient-com.amazonaws.auth.AWSCredentials-</a></p>",910935,910935,2017-04-06T21:21:37.283,2017-04-06T21:21:37.283,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43240215
412,43285441,2,21917661,2017-04-07T19:08:13.557,27,"<p><strong>These answers are out of date.</strong></p><p>You can access elastic-cache outside of AWS by following these steps:</p><ol><li>Create a NAT instance in the same VPC as your cache cluster but in apublic subnet. </li><li>Create security group rules for the cache cluster andNAT instance. </li><li>Validate the rules. </li><li>Add an iptables rule to the NATinstance. </li><li>Confirm that the trusted client is able to connect to thecluster. </li><li>Save the iptables configuration.</li></ol><p>For a more detailed description see the aws guide:</p><p><a href=""https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/accessing-elasticache.html#access-from-outside-aws"" rel=""noreferrer"">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/accessing-elasticache.html#access-from-outside-aws</a></p>",2542922,1932097,2019-01-08T10:12:57.280,2019-01-08T10:12:57.280,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/43285441
413,43286947,2,42679445,2017-04-07T20:53:43.417,0,"<p>SOLVED: So it turns out that my friend had a different version of the pods and when I pulled his version I got the out of date ones too. We struggled for about a week before we found the solution, <code>pod update</code>. </p><p>TL/DR: <code>pod update</code>  </p>",6522822,,,2017-04-07T20:53:43.417,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43286947
414,43307654,2,43281729,2017-04-09T14:06:27.677,0,"<p>The AWS documentation is outdated.</p><p>You need to implement a different interface.</p><p>Passing a map:</p><pre><code>public class MyHandler implements RequestHandler&lt;Map&lt;String,Object&gt;,String&gt; </code></pre><p>Passing into a custom POJO object:</p><pre><code>public class MyHandler implements RequestHandler&lt;CustomRequest,CustomResponse&gt; {</code></pre><p>Consuming a stream:</p><pre><code>public class MyHandler implements RequestStreamHandler {</code></pre><p>Make sure to grab the right dependency:</p><pre><code>&lt;dependency&gt;  &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;  &lt;artifactId&gt;aws-lambda-java-core&lt;/artifactId&gt;  &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>I developed a <a href=""https://github.com/udoheld/aws-lambda-hello-log"" rel=""nofollow noreferrer"">sample application</a> implementing each of the interfaces. You can just clone it from <a href=""https://github.com/udoheld/aws-lambda-hello-log"" rel=""nofollow noreferrer"">github</a>. One of the handlers will print out all available environment variables as well.</p><p>Lambda expects JSON as input.</p>",1022141,,,2017-04-09T14:06:27.677,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43307654
415,43316867,2,28906981,2017-04-10T07:10:08.283,0,"<p>I had the same issue. The managed policies were correct in my case, but I had to update the trust relationships for both the DataPipelineDefaultRole and DataPipelineDefaultResourceRole roles using the documentation Gonfva linked to above as they were out of date.</p>",865309,,,2017-04-10T07:10:08.283,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43316867
416,43442679,2,26533245,2017-04-16T22:09:26.793,1,"<p>For Android SDK, setEndpoint solves the problem, although it's been deprecated. </p><pre><code>CognitoCachingCredentialsProvider credentialsProvider = new CognitoCachingCredentialsProvider(  context, ""identityPoolId"", Regions.US_EAST_1);AmazonS3 s3 = new AmazonS3Client(credentialsProvider);s3.setEndpoint(""s3.us-east-2.amazonaws.com"");</code></pre>",6664008,,,2017-04-16T22:09:26.793,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43442679
417,43450379,2,43449710,2017-04-17T11:17:19.660,4,"<p>Ah! That documented process won't work if the AMI used to originally launch the instance has been deprecated (expired).</p><p>For step 5, simply select your AMI from the <strong>AMIs</strong> section of the EC2 management console, then choose the <strong>Launch</strong> command from the Actions menu. This will let you launch a new machine using the AMI you created. Make sure you choose a new keypair for which you have the <code>.pem</code> file.</p><p>Then, just continue from step 6. The general steps are:</p><ul><li>Stop your original instance</li><li>Detach the boot disk (""Disk A"")</li><li>Launch another Windows instance (or use one you already have access to)</li><li>Attach Disk A to the 2nd instance</li><li>Update the <code>\Program Files\Amazon\Ec2ConfigService\Settings\config.xml</code> file on Disk A and update the <code>Ec2SetPassword</code> parameter to <code>Enabled</code> (see Step 9 on that documentation page)</li><li>Detach Disk A from the 2nd instance and reattach it to the original instance (from Step 5 on the documentation page)</li><li>Start the original instance and try to login</li></ul>",174777,,,2017-04-17T11:17:19.660,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43450379
418,43476581,2,43466637,2017-04-18T15:39:58.767,2,"<p>The <code>table_cache</code> system variable was deprecated and renamed <code>table_open_cache</code> back in MySQL 5.1, and was still called <code>table_open_cache</code> in <a href=""https://dev.mysql.com/doc/refman/5.6/en/table-cache.html"" rel=""nofollow noreferrer"">5.6</a>.</p><p>It's in the RDS parameter group.</p><p>However, it's very rare that this value is an appropriate value to tweak.  It has long been known to <a href=""https://www.percona.com/blog/2009/11/16/table_cache-negative-scalability/"" rel=""nofollow noreferrer"">scale negatively</a> -- the more ""optimum"" your configuration, the worse the server will perform.  </p><p>If you're using a tuning script, the odds are extremely high that you're operating on bad advice if changing that value has been recommended.  Tuning scripts in general are notorious for their well-intentioned, but ill-conceived, bad advice.</p>",1695906,,,2017-04-18T15:39:58.767,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43476581
419,43480255,2,43464820,2017-04-18T19:03:07.197,3,"<p>I found the solution! @SwarajGiri pointed in the right direction by suggesting aws-sdk version 2.2.15.</p><p>My problem was that I had a <a href=""https://docs.npmjs.com/cli/shrinkwrap"" rel=""nofollow noreferrer"">npm-shrinkwrap.json</a> in my folder and was not aware that it was overruling the package.json. The aws-sdk version inside the npm-shrinkwrap.json was 2.1.39 which does not include the documentClient(). <a href=""https://aws.amazon.com/de/blogs/developer/announcing-the-amazon-dynamodb-document-client-in-the-aws-sdk-for-javascript/"" rel=""nofollow noreferrer"">Document Client was introduced in version 2.2.0</a></p><p>Note that the <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/nodejs-dynamodb-tutorial.html"" rel=""nofollow noreferrer"">node.js Elastic Beantalk with DynamoDB tutorial</a> I followed includes this somewhat outdated npm-shrinkwrap.json!</p><p>I hope I do not break any rules by answering my own question.</p>",4475379,,,2017-04-18T19:03:07.197,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43480255
420,43543870,2,43542746,2017-04-21T13:21:28.747,1,"<p>I am not using the <a href=""https://github.com/awslabs/amazon-sqs-java-extended-client-lib"" rel=""nofollow noreferrer"">Amazon SQS Extended Client Library for Java</a> myself, so I can not really judge the details, but it looks like it has gotten out of sync with the AWS Java SDK.</p><p>See <a href=""https://github.com/awslabs/amazon-sqs-java-extended-client-lib/issues/12"" rel=""nofollow noreferrer"">Exeption on AmazonSQSExtendedClient.deleteMessage</a> for a (so far unresolved) issue on this, as well as <a href=""https://github.com/awslabs/amazon-sqs-java-extended-client-lib/issues/14"" rel=""nofollow noreferrer"">SQS Extended Client Lib not compatible with Core SDK 1.11.xx</a> for a more general one.</p><p>The latter points to the <em>eventually</em> helpful <a href=""https://github.com/awslabs/amazon-sqs-java-extended-client-lib/pull/11"" rel=""nofollow noreferrer"">support aws-sdk-java 1.11.8</a> pull request, though that one is pretty outdated by now as well.</p>",46149,,,2017-04-21T13:21:28.747,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43543870
421,43620365,2,17309559,2017-04-25T20:27:51.790,3,"<p>Since this post is so old and I believe streaming directly is now supported, I spent a lot of time reading out of date answers on this topic...</p><p>If it helps anyone I was able to stream from the client to s3 directly without the need for installing packages:</p><p><a href=""https://gist.github.com/mattlockyer/532291b6194f6d9ca40cb82564db9d2a"" rel=""nofollow noreferrer"">https://gist.github.com/mattlockyer/532291b6194f6d9ca40cb82564db9d2a</a></p><p>The server assumes <code>req</code> is a stream object, in my case a File object was used in xhr(send) which will send binary data in modern browsers.</p><pre><code>const fileUploadStream = (req, res) =&gt; {  //get ""body"" args from header  const { id, fn } = JSON.parse(req.get('body'));  const Key = id + '/' + fn; //upload to s3 folder ""id"" with filename === fn  const params = {  Key,  Bucket: bucketName, //set somewhere  Body: req, //req is a stream  };  s3.upload(params, (err, data) =&gt; {  if (err) {  res.send('Error Uploading Data: ' + JSON.stringify(err) + '\n' + JSON.stringify(err.stack));  } else {  res.send(Key);  }  });};</code></pre><p>Yes it breaks convention but if you look at the gist it's much cleaner than anything else I found relying on other packages.</p><p>+1 for pragmatism and thanks to @SalehenRahman for his help.</p>",1060487,,,2017-04-25T20:27:51.790,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43620365
422,43645031,2,32082115,2017-04-26T21:44:05.163,1,"<p>After many hours of searching through outdated SO answers, here's my working setup:</p><p>.ebextensions/01-migrate-db.config</p><pre><code>container_commands:  01_node_binary:  command: ""ln -sf `ls -td /opt/elasticbeanstalk/node-install/node-* | head -1`/bin/node /bin/node""  leader_only: true  02_npm_binary:  command: ""ln -sf `ls -td /opt/elasticbeanstalk/node-install/node-* | head -1`/bin/npm /bin/npm""  leader_only: true  03_migrate_db:  command: ""sudo DB_HOST=${DB_HOST} DB_PORT=${DB_PORT} DB_NAME=${DB_NAME} DB_USER=${DB_USER} DB_PASSWORD=${DB_PASSWORD} npm run db:migration:run""  leader_only: true</code></pre><p>package.json</p><pre><code>...scripts: {  ""db:migration:run"": ""knex migrate:latest""}</code></pre>",1249098,,,2017-04-26T21:44:05.163,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43645031
423,43793859,2,43793382,2017-05-04T22:41:45.410,0,<p>The issue was caused by my CLI being out of date.</p>,3166160,,,2017-05-04T22:41:45.410,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43793859
424,43794424,2,43793382,2017-05-04T23:51:33.390,0,"<p>Are you sure you found the root cause because of <code>out of date</code>?</p><p>The problem you reported is about <a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/bucket-policy-s3-sigv4-conditions.html"" rel=""nofollow noreferrer"">Amazon S3 Signature Version 4 Authentication Specific Policy Keys</a> </p><p>You should be fine to fix with the command</p><pre><code>aws configure set profile.jacoblambert.s3.signature_version s3v4</code></pre><p>or add below lines to that profile <code>[jacoblambert]</code> in ~/.aws/config</p><pre><code>s3 =  signature_version = s3v4</code></pre><p>Refer:</p><p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version"" rel=""nofollow noreferrer"">Specifying Signature Version in Request Authentication</a></p>",498256,,,2017-05-04T23:51:33.390,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43794424
425,43797138,2,43797035,2017-05-05T05:26:56.360,0,"<p>Found the error. The code in my question is from Amazon tutorials - <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadObjSingleOpJava.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadObjSingleOpJava.html</a></p><p>However I am sure it is incorrect or deprecated. </p><p>Pay attention to the below line</p><pre><code>s3client.putObject(new PutObjectRequest(bucketName, keyName, file));</code></pre><p>For this to work, instead of <code>keyName</code>, you have to pass the filename with extension. For an example <code>websitepage.html</code>.</p>",1379286,,,2017-05-05T05:26:56.360,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43797138
426,43819599,2,43565698,2017-05-06T10:56:55.857,1,"<p>I went through a similar instead some months ago, basically I have an ec2 instance with Kafka in N. Virginia and I configured topbeat on my local machine to send metrics to that ec2 instance. I was able to get it working by adding </p><pre><code>advertised.host.name=public-ip</code></pre><p>as configuration in the server.properties of kafka, but according to the <a href=""http://kafka.apache.org/documentation/#brokerconfigs"" rel=""nofollow noreferrer"">documentation</a> this properties is deprecated. </p><p>Reading further <a href=""http://kafka.apache.org/documentation/#brokerconfigs"" rel=""nofollow noreferrer"">documentation</a>, it is said that if you are in an IaaS environment, you have to configure the advertised.listeners different from the interface to which the broker binds. </p>",6115442,,,2017-05-06T10:56:55.857,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43819599
427,43867972,2,43451370,2017-05-09T10:57:11.333,0,"<p>Apparently the <code>AmazonS3Client</code> class is deprecated in the latest SDKs. Creating client in the following manner solves the problem as it also takes region into account and hence attaches proper signature versions :</p><pre><code>BasicAWSCredentials awsCreds = new BasicAWSCredentials(accessKeyId, secretAccessKey);  AmazonS3ClientBuilder builder = AmazonS3ClientBuilder.standard().withCredentials(new AWSStaticCredentialsProvider(awsCreds));  builder.setRegion(regionName);  AmazonS3 s3client = builder.build();</code></pre>",4962412,4962412,2018-12-06T01:10:02.060,2018-12-06T01:10:02.060,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/43867972
428,43888789,2,43882263,2017-05-10T09:41:44.100,2,"<p>The <code>InitializeDisks.ps1</code> script is part of EC2Launch.</p><blockquote>  <p>To accommodate the change from .NET Framework to .NET Core, the EC2Config service has been deprecated on Windows Server 2016 AMIs and replaced by EC2Launch. EC2Launch is a bundle of Windows PowerShell scripts that perform many of the tasks performed by the EC2Config service.</p></blockquote><p>This should be installed by default on the Windows 2016 AMI, but note that the C:\ProgramData\Amazon directory is hidden. </p><p>If for some reason it is not installed, you should be able to install it manually as follows:</p><blockquote>  <p><strong>To download and install the latest version of EC2Launch</strong></p>   <ol>  <li><p>If you have already installed and configured EC2Launch on an instance, make a backup of the EC2Launch configuration file. The  installation process does not preserve changes in this file. By  default, the file is located in the following directory:  C:\ProgramData\Amazon\EC2-Windows\Launch\Config.</p></li>  <li><p>Download EC2Launch.zip from the following location to a directory on the instance:</p></li>  </ol>   <p><a href=""https://s3.amazonaws.com/ec2-downloads-windows/EC2Launch/latest/EC2-Windows-Launch.zip"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/ec2-downloads-windows/EC2Launch/latest/EC2-Windows-Launch.zip</a></p>   <ol start=""3"">  <li>Download the Install.ps1 PowerShell script from the following location to the same directory where you downloaded EC2Launch.zip:</li>  </ol>   <p><a href=""https://s3.amazonaws.com/ec2-downloads-windows/EC2Launch/latest/install.ps1"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/ec2-downloads-windows/EC2Launch/latest/install.ps1</a></p>   <ol start=""4"">  <li><p>Run Install.ps1</p></li>  <li><p>Replace your backup of the EC2Launch configuration file in the C:\ProgramData\Amazon\EC2-Windows\Launch\Config directory.</p></li>  </ol></blockquote><p>Source: <a href=""http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2launch.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2launch.html</a></p><hr><p>As a test, I just deployed the Windows 2016 Base AMI template and can confirm that C:\ProgramData\Amazon does exist (ProgramData is a hidden directory, so go to View > Show Hidden Files to see it).</p><p>I also added a Cold Storage HDD and (as you noted) the following User Data (under the ""Advanced Details"" section of the ""Configure Instance Details"" page) to my instance on launch:</p><pre><code>&lt;powershell&gt;C:\ProgramData\Amazon\EC2-Windows\Launch\Scripts\InitializeDisks.ps1&lt;/powershell&gt;</code></pre><p>And can confirm that on boot of the VM the Cold HDD was correctly/automatically initialized and available as the D:\ drive.</p><p>If you didn't add the required User Data when you were first launching your instance, you can add it later by selecting your instance and go to Actions > Instance Settings > View/Change User Data.</p>",2796058,2796058,2017-05-10T09:47:37.320,2017-05-10T09:47:37.320,10,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43888789
429,43889585,2,43883077,2017-05-10T10:16:38.887,0,"<p>Not sure, how did you manage to upload .part files to s3 without authentication even if you have tweaked s3 policies. I guess you might have added aws keys in the system environment as properties or in conf files.In order to access aws resource, atleast its required to supply access key and secret key. Also, s3 scheme is deprecated now.Following code works with hadoop-aws-2.8.0.jar and spark 2.1.(note: I should have used s3a scheme as its the preferred over s3n (native scheme).</p><pre><code>val spark = SparkSession  .builder  .appName(""SparkS3Integration"")  .master(""local[*]"")  .getOrCreate()  spark.sparkContext.hadoopConfiguration.set(""fs.s3n.awsAccessKeyId"", awsAccessKey)  spark.sparkContext.hadoopConfiguration.set(""fs.s3n.awsSecretAccessKey"", awsSecretKey) val rdd = spark.sparkContext.parallelize(Seq(1,2,3,4)) rdd.saveAsTextFile(""s3n://&lt;bucket_name&gt;/&lt;path&gt;"")</code></pre>",6093375,,,2017-05-10T10:16:38.887,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43889585
430,43975312,2,33120348,2017-05-15T09:02:37.527,1,"<p>I need it too, however, there are no suitable codes for this purpose. I modify one by myself. Enjoy! May someone need it also.</p><p>Following code is modified from libcloud/contrib/scrape-ec2-prices.pyAnd this program will generate a dict about available instance types</p><pre><code>#!/usr/bin/env pythonimport osimport reimport jsonimport timefrom collections import defaultdict, OrderedDictimport requestsimport demjsonLINUX_PRICING_URLS = [  # Deprecated instances (JSON format)  'https://aws.amazon.com/ec2/pricing/json/linux-od.json',  # Previous generation instances (JavaScript file)  'https://a0.awsstatic.com/pricing/1/ec2/previous-generation/linux-od.min.js',  # New generation instances (JavaScript file)  'https://a0.awsstatic.com/pricing/1/ec2/linux-od.min.js']EC2_REGIONS = [  'us-east-1',  'us-east-2',  'us-west-1',  'us-west-2',  'us-gov-west-1',  'eu-west-1',  'eu-west-2',  'eu-central-1',  'ca-central-1',  'ap-southeast-1',  'ap-southeast-2',  'ap-northeast-1',  'ap-northeast-2',  'ap-south-1',  'sa-east-1',  'cn-north-1',]INSTANCE_SIZES = [  'micro',  'small',  'medium',  'large',  'xlarge',  'x-large',  'extra-large']RE_NUMERIC_OTHER = re.compile(r'(?:([0-9]+)|([-A-Z_a-z]+)|([^-0-9A-Z_a-z]+))')PRICING_FILE_PATH = './price.json'PRICING_FILE_PATH = os.path.abspath(PRICING_FILE_PATH)def scrape_ec2_pricing():  result = {}  result['regions'] = []  result['prices'] = defaultdict(OrderedDict)  result['models'] = defaultdict(OrderedDict)  for url in LINUX_PRICING_URLS:  response = requests.get(url)  if re.match('.*?\.json$', url):  data = response.json()  elif re.match('.*?\.js$', url):  data = response.content  match = re.match('^.*callback\((.*?)\);?$', data,  re.MULTILINE | re.DOTALL)  data = match.group(1)  # demjson supports non-strict mode and can parse unquoted objects  data = demjson.decode(data)  regions = data['config']['regions']  for region_data in regions:  region_name = region_data['region']  if region_name not in result['regions']:  result['regions'].append(region_name)  libcloud_region_name = region_name  instance_types = region_data['instanceTypes']  for instance_type in instance_types:  sizes = instance_type['sizes']  for size in sizes:  price = size['valueColumns'][0]['prices']['USD']  if str(price).lower() == 'n/a':  # Price not available  continue  if not result['models'][libcloud_region_name].has_key(size['size']):  result['models'][libcloud_region_name][size['size']] = {}  result['models'][libcloud_region_name][size['size']]['CPU'] = int(size['vCPU'])  if size['ECU'] == 'variable':  ecu = 0  else:  ecu = float(size['ECU'])  result['models'][libcloud_region_name][size['size']]['ECU'] = ecu  result['models'][libcloud_region_name][size['size']]['memoryGiB'] = float(size['memoryGiB'])  result['models'][libcloud_region_name][size['size']]['storageGB'] = size['storageGB']  result['prices'][libcloud_region_name][size['size']] = float(price)  return resultdef update_pricing_file(pricing_file_path, pricing_data):  ##  with open(pricing_file_path, 'r') as fp:  #  content = fp.read()  data = {'compute': {}}  # json.loads(content)  data['updated'] = int(time.time())  data['compute'].update(pricing_data)  # Always sort the pricing info  data = sort_nested_dict(data)  content = json.dumps(data, indent=4)  lines = content.splitlines()  lines = [line.rstrip() for line in lines]  content = '\n'.join(lines)  with open(pricing_file_path, 'w') as fp:  fp.write(content)def sort_nested_dict(value):  """"""  Recursively sort a nested dict.  """"""  result = OrderedDict()  for key, value in sorted(value.items(), key=sort_key_by_numeric_other):  if isinstance(value, (dict, OrderedDict)):  result[key] = sort_nested_dict(value)  else:  result[key] = value  return resultdef sort_key_by_numeric_other(key_value):  """"""  Split key into numeric, alpha and other part and sort accordingly.  """"""  return tuple((  int(numeric) if numeric else None,  INSTANCE_SIZES.index(alpha) if alpha in INSTANCE_SIZES else alpha,  other  ) for (numeric, alpha, other) in RE_NUMERIC_OTHER.findall(key_value[0]))def main():  print('Scraping EC2 pricing data')  pricing_data = scrape_ec2_pricing()  update_pricing_file(pricing_file_path=PRICING_FILE_PATH,  pricing_data=pricing_data)  print('Pricing data updated')if __name__ == '__main__':  main()</code></pre>",8013210,2550354,2017-05-16T05:58:58.633,2017-05-16T05:58:58.633,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/43975312
431,44163299,2,44159204,2017-05-24T15:57:39.990,6,"<p>According to the docs <a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlifecycle.html"" rel=""noreferrer"">here</a> you need to add Filter element, which is required as per Amazon API, and confusingly enough, not required by boto. I added the deprecated Prefix argument instead of the Filter and it seems to be working too.</p>",5581236,,,2017-05-24T15:57:39.990,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/44163299
432,44178910,2,44158930,2017-05-25T11:01:19.550,1,"<p>So, it seems like I was barking at the wrong tree here, trying to use Scan instead of Query : </p><blockquote>  <p>The customer should not use ScanFilter, which is deprecated. List/Map datatypes are better suited to the new FilterExpression syntax. I suspect the problem is with the multi-value attribute with ""N:"" in the name; this doesn't seem close to the wire format. To use the ""new"" filter expressions, they should create ExpressionAttributeNames and ExpressionAttributeValues to tell the AWS SDK about the attributes for the filter, addExpressionAttributeNamesEntry and addExpressionAttributeValuesEntry.<a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBScanExpression.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBScanExpression.html</a> Also, I would be remiss if I didn't point out that Scan is absolutely not the right API action for this query; Query is the correct action. testName should be used with Query with a KeyConditionExpression to narrow the search down to the correct partition, and revision should be added to the same with the BETWEEN syntax. Scan is intended for backups and background operations, and in this case the customer's index schema can be used with Query to narrow the search. Caution them about using Scan; this is a fundamental best practice:<a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/QueryAndScanGuidelines.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/QueryAndScanGuidelines.html</a></p></blockquote><p>So I've used Query instead :</p><pre><code>  Map&lt;String, AttributeValue&gt; eav = new HashMap&lt;String, AttributeValue&gt;();  eav.put("":v1"", new AttributeValue().withS(testName));  eav.put("":v2"", new AttributeValue().withS(version));  eav.put("":v3"", new AttributeValue().withN(String.valueOf(minRev)));  eav.put("":v4"", new AttributeValue().withN(String.valueOf(maxRev)));  final DynamoDBQueryExpression&lt;ValidationReport&gt; query = new DynamoDBQueryExpression&lt;&gt;();  query.withKeyConditionExpression(""testName = :v1"");  query.withFilterExpression(""buildVersion = :v2 and revision BETWEEN :v3 AND :v4"");  query.setConsistentRead(false);  query.withExpressionAttributeValues(eav);  return getMapper().query(ValidationReport.class, query);`</code></pre><p>Please note that when using the AttributeValue there's quite a difference between the following :</p><pre><code>new AttributeValue().withS(""SomeStringValue"")new AttributeValue().withN(""SomeNumber"")</code></pre><p>Which is what I was missing</p>",1249794,,,2017-05-25T11:01:19.550,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/44178910
433,44244284,2,34107898,2017-05-29T13:56:52.477,0,"<p>It seems the DynamoDB endpoint URL hasn't been specified yet. Please set the endpoint value in client object. Endpoint URL must be specified, even if we are using local DynamoDB (e.g. localhost:8000).</p><pre><code>AmazonDynamoDBClient client = new AmazonDynamoDBClient(credentials);client.setEndpoint(""http://localhost:8000"");client.setRegion(Region.getRegion(Regions.US_WEST_2));</code></pre><p>Additional note, it seems that you are using deprecated way of instantiating a DynamoDB client object. The following way is preferred, esp. if you are using the latest AWS DynamoDB SDK.</p><pre><code>AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard()  .withCredentials(new AWSStaticCredentialsProvider(  new BasicAWSCredentials(""*********"", ""************"")))  .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(  ""http://localhost:8000"",  Regions.US_WEST_2.getName()))  .build();</code></pre>",1527997,,,2017-05-29T13:56:52.477,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/44244284
434,44341051,2,44325418,2017-06-03T06:29:49.530,1,"<p>Signature Version 2 has been deprecated in favor of the more secure version 4. We don't support it, and the only reason it is supported in other sdks is for backwards compatibility. For Amazon S3, you can turn off body signing for v4. This is the default option for the S3 Client in the C++ SDK.</p>",734691,,,2017-06-03T06:29:49.530,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/44341051
435,44401684,2,44400227,2017-06-07T00:29:11.293,27,"<p>The SDKs do not contain a convenience method to do this. However, when you called PutObject, you provided the bucket and the object's key. You can simply combine those to make the URL of the object, for example:</p><ul><li><a href=""https://bucket.s3.amazonaws.com/key"" rel=""nofollow noreferrer"">https://bucket.s3.amazonaws.com/key</a></li></ul><p>So, for example, if your bucket is <em>pablo</em> and the object key is <em>dogs/toto.png</em>, use:</p><ul><li><a href=""https://pablo.s3.amazonaws.com/dogs/toto.png"" rel=""nofollow noreferrer"">https://pablo.s3.amazonaws.com/dogs/toto.png</a></li></ul><p>For region-specific buckets, see <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html"" rel=""nofollow noreferrer"">Working with Amazon S3 Buckets</a> and <a href=""http://www.wryway.com/blog/aws-s3-url-styles/"" rel=""nofollow noreferrer"">AWS S3 URL Styles</a>. Replace <code>s3</code> with <code>s3.&lt;region&gt;.amazonaws.com</code> or <code>s3-&lt;region&gt;.amazonaws.com</code> in the above URLs, for example:</p><ul><li><a href=""https://seamus.s3.eu-west-1.amazonaws.com/dogs/setter.png"" rel=""nofollow noreferrer"">https://seamus.s3.eu-west-1.amazonaws.com/dogs/setter.png</a></li><li><a href=""https://seamus.s3-eu-west-1.amazonaws.com/dogs/setter.png"" rel=""nofollow noreferrer"">https://seamus.s3-eu-west-1.amazonaws.com/dogs/setter.png</a></li></ul><p>If you are using IPv6, then the general URL form will be one of:</p><ul><li><a href=""https://BUCKET.s3.dualstack.REGION.amazonaws.com"" rel=""nofollow noreferrer"">https://BUCKET.s3.dualstack.REGION.amazonaws.com</a></li></ul><p>For some buckets, you may use the older path-style URLs. Path-style URLs are deprecated and <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/"" rel=""nofollow noreferrer"">only work with buckets created on or before September 30, 2020</a>. They are used like this:</p><ul><li><a href=""https://s3.amazonaws.com/bucket/key"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/bucket/key</a></li><li><a href=""https://s3.amazonaws.com/pablo/dogs/toto.png"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/pablo/dogs/toto.png</a> </li><li><a href=""https://s3.eu-west-1.amazonaws.com/seamus/dogs/setter.png"" rel=""nofollow noreferrer"">https://s3.eu-west-1.amazonaws.com/seamus/dogs/setter.png</a></li><li><a href=""https://s3.dualstack.REGION.amazonaws.com/BUCKET"" rel=""nofollow noreferrer"">https://s3.dualstack.REGION.amazonaws.com/BUCKET</a></li></ul><p>Currently there are TLS and SSL certificate issues that may require some buckets with dots (<code>.</code>) in their name to be accessed via path-style URLs. AWS plans to address this. See the <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/"" rel=""nofollow noreferrer"">AWS announcement</a>.</p><p><strong>Note:</strong> General guidance on <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-keys"" rel=""nofollow noreferrer"">object keys</a> where certain characters need special handling. For example space is encoded to + (plus sign) and plus sign is encoded to %2B. Also <a href=""https://stackoverflow.com/questions/7116450/what-are-valid-s3-key-names-that-can-be-accessed-via-the-s3-rest-api"">here</a>.</p>",271415,148146,2020-04-28T01:22:49.097,2020-04-28T01:22:49.097,5,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/44401684
436,44402449,2,44391817,2017-06-07T02:18:59.223,8,"<blockquote>  <p><strong>Edit: This answer is deprecated and is incorrect</strong>. There are several ways to list AWS resources (the AWS Tag Editor, etc.). Check  the other answers for more details.</p></blockquote><hr><p>No.</p><p>Each AWS Service (eg Amazon EC2, Amazon S3) have their own set of API calls. Also, each <strong>Region</strong> is independent.</p><p>To obtain a list of all resources, you would have to make API calls to every service in every region.</p><p>You might want to activate <a href=""http://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html"" rel=""nofollow noreferrer"">AWS Config</a>:</p><blockquote>  <p>AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.</p></blockquote><p>However, AWS Config only collects information about EC2/VPC-related resources, not everything in your AWS account.</p>",174777,2815219,2020-05-13T09:27:28.237,2020-05-13T09:27:28.237,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/44402449
437,44443872,2,44443417,2017-06-08T19:15:23.850,13,"<p>Unless you plan on maintaining it by yourself, no, you should not use TitanDB. Titan has not seen updates in almost 2 years. It is far out of date on its core dependencies.</p><p>You should use <a href=""https://janusgraph.org"" rel=""noreferrer"">JanusGraph</a>. It was established as a project at the Linux Foundation in January 2017, and 2 releases have already been made. Many individuals and companies are involved in the open community.</p><p>If you check out the <a href=""https://github.com/awslabs/dynamodb-titan-storage-backend/tree/master"" rel=""noreferrer"">AWS Labs DynamoDB Titan Storage adapter</a>, you'll see that <a href=""https://github.com/amcp"" rel=""noreferrer"">Alex Patrikalakis</a> has already integrated JanusGraph 0.1.1 on the master branch. Alex is also a committer on JanusGraph.</p>",5025129,,,2017-06-08T19:15:23.850,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/44443872
438,44452017,2,44430121,2017-06-09T07:38:59.963,0,"<p>I was using 3rd party library ""Simpl3r""  which uploads file to my bucket. Since i inserted this library quiet some time ago, so the aws-sdk-s3 version it was using was outdated.</p><p>Solution which works for me:</p><ol><li><p>open simpler gradle file.</p><p>removed line: compile fileTree(dir: 'libs', include: ['*.jar'])</p><p>added line : compile 'com.amazonaws:aws-android-sdk-core:2.4.4'</p></li><li><p>update aws sdk ver over there i.e 2.4.4 (latest at the moment)</p></li><li><p>integrated aws sdk ver 2.4.4 (similar with Simpl3r library version) in  build.gradle(app), to use this aws library function throughout my app module .  </p></li></ol><p>Atlast, my duplicate entry of classes.class (which builds at run time) issue gets resolved. </p><p>Tip: Try to make version of your sub-module(gradle files) similar to app gradle file.<br>Also, make sure that you use compile fileTree(dir:'libs', include: ['*jar']) in sub module gradle file only if , any particular jar file needs to be run with that particular module .</p>",7434126,,,2017-06-09T07:38:59.963,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/44452017
439,44487474,2,44482351,2017-06-11T18:47:22.543,2,<p>That cookbook (<code>python</code>) is deprecated and does not work with Chef 13. Use <code>poise-python</code> instead.</p>,78722,,,2017-06-11T18:47:22.543,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/44487474
440,44509566,2,44503347,2017-06-12T22:19:15.757,3,"<p>The article you cited is obsolete.  It was originally written in 2008, and apparently when updated in 2015, some of the outdated information was left in place.</p><blockquote><p>The version refers to the particular algorithm for signing the request. These AWS services have deprecated the older, less-secure methods (signature versions 0 and 1) and will no longer allow them after September 2009.</p></blockquote><p>Indeed, versions 0 and 1 are not supported.</p><blockquote><p>A few AWS services don't support signature version 2:</p><p>Amazon Simple Storage Service: You can still use HTTP with Amazon S3 and securely make authenticated requests. The service uses a different secure signing protocol.</p></blockquote><p>This is also inaccurate.  S3 supports signature version 2 in all regions where signature version 2 was deployed.  Regions launched in 2014 or later do not support V2 at all, they require Signature Version 4, and in those regions, S3 also requires Signature Version 4.</p><p>Importantly, though, <strong>none of this has anything at all to do with HTTPS.</strong></p><p>From the same document:</p><blockquote><p>Most AWS services accept HTTPS requests, including:</p><p>...</p><p>Amazon Simple Storage Service</p></blockquote><p>Okay, so, let's revisit this line:</p><blockquote><p>The service uses a different secure signing protocol.</p></blockquote><p>This statement is not about encryption, or security of the payload.  This is a statement about the secuity of the request authentication and authorization process -- its resistance to forgery and reverse-engineering -- whether or not the request is sent encrypted.</p><p><strong>HTTPS is supported by S3, to protect data in transit.</strong></p>",1695906,-1,2020-06-20T09:12:55.060,2017-06-12T22:19:15.757,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/44509566
441,44616825,2,44615176,2017-06-18T15:55:35.533,0,"<p>Amazon do not make the whole transcript available only the parts that match an intent and the extracted slots.</p><p>You used to be able to get this data but it was deprecated  for US users and never made available to users in other regions.</p>",504554,,,2017-06-18T15:55:35.533,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/44616825
442,44691960,2,44690702,2017-06-22T06:41:27.167,0,"<p>Which Version of JAVA AWS SDK? </p><p>From Aws SDK 1.11.113 setRegion is depreceated. </p><p><a href=""https://aws.amazon.com/blogs/developer/client-constructors-now-deprecated/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/developer/client-constructors-now-deprecated/</a></p><p>So you need to set region in AWS Credentials or you need to get region using Method getCurrentRegion()</p><p>ref: <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/regions/Regions.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/regions/Regions.html</a></p>",914107,,,2017-06-22T06:41:27.167,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/44691960
443,44832956,2,44678025,2017-06-29T18:56:38.540,0,"<p>Thanks for pointing that out. Can you give the links of the documentation that has deprecated classes? We will try to keep the latest ones and remove if there are any redundant deprecated references.</p><ol><li><p>load() API is used to retrieve an item: Using an object's primary key load the corresponding item from the database. Look for an example here: <a href=""http://docs.aws.amazon.com/mobile/sdkforios/developerguide/dynamodb-object-mapper.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/mobile/sdkforios/developerguide/dynamodb-object-mapper.html</a> under ""Retrieve an item"" section.query() API can be used to return any number of records that match the query. The query API enables you to query a table or a secondary index.To answer your question, if you know the primary key of the record that you are trying to retrieve, you can use load() API, otherwise use query() API.</p></li><li><p>DynamoDBStreams work well for your use case. Otherwise you can intgerate AWS Lambda with DynamoDB table to do polling which will be cleaner than a timer based approach. This question is partially answered here: <a href=""https://stackoverflow.com/questions/36636166/hooks-for-aws-dynamodb-streams"">Hooks for AWS DynamoDB streams</a></p></li></ol>",5169068,,,2017-06-29T18:56:38.540,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/44832956
444,45032391,2,45029553,2017-07-11T10:47:26.693,3,"<p>It is probably because of the outdated <code>sudo</code> usage from version <code>1.9</code>. From the official Ansible documentation.</p><blockquote>  <p>Before <code>1.9</code> Ansible mostly allowed the use of <code>sudo</code> and a limited use of <code>su</code> to allow a login/remote user to become a different user and execute tasks, create resources with the 2nd userŠ—Ès permissions. As of <code>1.9</code>, <code>become</code> supersedes the old <code>sudo/su</code>, while still being backwards compatible. </p></blockquote><p>You can remove it by using the <a href=""http://docs.ansible.com/ansible/become.html"" rel=""nofollow noreferrer""><code>become</code> module</a> which allows you to 'become' another user, different from the user that logged into the machine (remote user).  You need to set to <code>true</code> to activate privilege escalation. </p><pre><code>name: Installing Shiny Packages  shell: R -e ""install.packages('shiny', repos='https://cran.rstudio.com/')""become: true</code></pre>",5291015,5291015,2017-07-11T19:58:56.603,2017-07-11T19:58:56.603,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45032391
445,45037883,2,44738071,2017-07-11T14:47:44.793,0,"<p>Thanks for this solution!</p><p>I'm using lambci/docker-lambda with Docker to test my lambda functions, and just like real lambda, the botocore is currently outdated. To add botocore to your lambda project:</p><p><code>pip install botocore -t /your/project/dir</code> </p><p>In case you're working on Mac OSX and installed pip using brew, the <code>-t</code> won't work. Execute the following command where your lambda_function.py is located, and you're good to go. </p><p><code>docker run -v ""$PWD"":/localdir python:2.7-alpine pip install botocore -t /localdir</code></p>",8290239,,,2017-07-11T14:47:44.793,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45037883
446,45072950,2,45071052,2017-07-13T06:29:22.073,1,"<p>Yes, this is possible, if you look at the source code for the scrapy files pipeline <a href=""https://github.com/scrapy/scrapy/blob/master/scrapy/pipelines/files.py"" rel=""nofollow noreferrer"">here</a></p><p>you will see that it has methods which you can override, one of them is the <code>file_path</code> method, which you override by adding it to your pipeline class like so</p><pre><code>  def file_path(self, request, response=None, info=None):  # start of deprecation warning block (can be removed in the future)  def _warn():  from scrapy.exceptions import ScrapyDeprecationWarning  import warnings  warnings.warn('FilesPipeline.file_key(url) method is deprecated,\  please use file_path(request, response=None, info=None) instead',  category=ScrapyDeprecationWarning, stacklevel=1)  # check if called from file_key with url as first argument  if not isinstance(request, Request):  _warn()  url = request  else:  url = request.url  # detect if file_key() method has been overridden  if not hasattr(self.file_key, '_base'):  _warn()  return self.file_key(url)  # end of deprecation warning block  # Modify the file path HERE to your own custom path   filename = request.meta['filename']  media_ext = 'jpg'  return '%s/%s/%s.%s' % \  (request.meta['image_category'],  request.meta['image_month'],  filename, media_ext)</code></pre><p>The result of this would be a directory like:</p><pre><code>vehicles/june/toyota.jpgWhere vehicles is the image_category, june is image_month and toyota is the filename</code></pre><p>What this would do is if you look at the last few lines of the code [which is the only code I have added the rest is as the method is from the Scrapy source code]</p><pre><code>  # Modify the file path HERE to your own custom path   filename = request.meta['filename']  media_ext = 'jpg'  return '%s/%s/%s.%s' % \  (request.meta['image_category'],  request.meta['image_month'],  filename, media_ext)</code></pre><p>is return a custom pathnow this path depends on a few things, on the spider you can collect image meta fields like filename for image name, image category and anything else like date the image was take and use it in the pipeline to create a custom directory</p>",8269018,8269018,2017-07-13T06:35:39.483,2017-07-13T06:35:39.483,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45072950
447,45138234,2,45134811,2017-07-17T07:24:40.077,24,"<p><strong>[2020 May Update]</strong></p><p><a href=""https://spring.io/blog/2020/05/27/what-s-new-in-spring-data-elasticsearch-4-0"" rel=""nofollow noreferrer"">https://spring.io/blog/2020/05/27/what-s-new-in-spring-data-elasticsearch-4-0</a></p><p>As you can read Spring Data Elasticsearch 4.0:</p><blockquote>  <p>Spring Data Elasticsearch now uses Elasticsearch 7, 7.6.2 in particular. Elasticsearch clusters running on 6.x versions are not supported anymore. The ElasticsearchTemplate class is deprecated as it uses the TransportClient to access Elasticsearch, which itself is deprecated since Elasticsearch version 7.+ Users should switch to ElasticsearchRestTemplate or ReactiveElasticsearchTemplate.</p></blockquote><p><strong>[2019 February Update]</strong></p><p>A see now that 3.2.0 M1 Spring Data Elasticsearch supports the HTTP client (<a href=""https://docs.spring.io/spring-data/elasticsearch/docs/3.2.0.M1/reference/html/#reference"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-data/elasticsearch/docs/3.2.0.M1/reference/html/#reference</a>)</p><p>According to the documentation (it could of course change because it's not final version so I will put it here):</p><blockquote>  <p>The well known TransportClient is deprecated as of Elasticsearch 7.0.0 and is expected to be removed in Elasticsearch 8.0.</p>   <p>2.1. High Level REST Client</p>   <p>The Java High Level REST Client provides a straight forward replacement for the TransportClient as it accepts and returns the very same request/response objects and therefore depends on the Elasticsearch core project. Asynchronous calls are operated upon a client managed thread pool and require a callback to be notified when the request is done.</p></blockquote><p>Example 49. High Level REST Client</p><pre><code>static class Config {  @Bean  RestHighLevelClient client() {  ClientConfiguration clientConfiguration = ClientConfiguration.builder()   .connectedTo(""localhost:9200"", ""localhost:9201"")  .build();  return RestClients.create(clientConfiguration).rest();   }}// ...IndexRequest request = new IndexRequest(""spring-data"", ""elasticsearch"", randomID())  .source(singletonMap(""feature"", ""high-level-rest-client""))  .setRefreshPolicy(IMMEDIATE);IndexResponse response = client.index(request);</code></pre><hr><p><strong>[Original answer]</strong></p><p>Currently Spring Data Elasticsearch doesn't support the communication by the REST API. They are using the transport client.</p><p>There is separate fork of Spring Data Elasticsearch (the guy needed it for AWS the same as you) where the JEST library is used and communication is made by REST:</p><p><a href=""https://github.com/VanRoy/spring-data-jest"" rel=""nofollow noreferrer"">https://github.com/VanRoy/spring-data-jest</a></p><p>You will find the interesting discussion under the following ticked of Spring Data Elasticsearch:</p><p><a href=""https://jira.spring.io/browse/DATAES-220"" rel=""nofollow noreferrer"">https://jira.spring.io/browse/DATAES-220</a></p><p>I think the Spring Data Elasticseach will need to migrate to REST on the future according to the statements from Elasticsearch team that they are planning to support only HTTP communication for ES.</p><p>Hope it helps.</p>",328121,328121,2020-05-31T17:39:38.620,2020-05-31T17:39:38.620,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/45138234
448,45190588,2,45190364,2017-07-19T12:27:17.427,3,"<p>Deprecation warnings is not an error, it's just the compiler warning you that something has been deprecated and may be removed in the future - your code will still work even if you're using <code>new AmazonKinesisClient()</code>, until that constructor is removed from the SDK sometime in the future.</p><p>The new way of creating clients in the AWS SDK is to use the builder API like this:</p><pre><code>final AmazonKinesisClientBuilder builder = AmazonKinesisClient.builder();final AmazonKinesis client = builder.build();</code></pre><p>This way, you can use <code>builder</code> to customize the client, like setting region or using STS credentials.</p><p>If you just want to get an instance using the default settings you can do:</p><pre><code>final AmazonKinesis client = AmazonKinesisClient.builder().build();</code></pre>",3182188,,,2017-07-19T12:27:17.427,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45190588
449,45333388,2,45315607,2017-07-26T17:10:19.417,4,"<p>If you request ""the same"" certificate from Amazon Certificate Manager more than once -- whether in the same region or across regions -- you will not actually be issued the same <em>identical</em> certificate multiple times.  The multiple certificates will each have the same subjects and subject alternative names but they won't truly be ""the same"" certificate.  They'll have different private keys and ARNs.</p><p>There are no security implications for requesting certificates with the same subject (domain) across regions, because the two certs have nothing in common.</p><p>Note that if you are using <a href=""https://en.m.wikipedia.org/wiki/HTTP_Public_Key_Pinning"" rel=""nofollow noreferrer"">HPKP</a> then you'll need to account for the existence of multiple valid public keys.  Pinning ACM-issued certs <a href=""https://docs.aws.amazon.com/acm/latest/userguide/troubleshooting-pinning.html"" rel=""nofollow noreferrer"">is not recommended</a> and apparently, pinning is now deprecated at any rate.</p><p>Also, be sure to use DNS validation for your certificates whenever possible, whether you're using certs in multiple regions, or not.  Automatic annual renewal of the certificates may not work as anticipated if you use email validation, particularly when certificates for same domain(s) are created in multiple regions or the certificate is in a single region but is a cert for the wildcard domain, only.  You may have to manually acknowledge renewal emails in these and other cases, if you don't use DNS validation.  (This is not a limitation in the service <em>per se</em>.  Auto-renewal of email-validated certs requires that the service verify that the domain names listed on the cert are actually using the cert on the Internet, and ACM needs to validate this using no internal information.)</p><p>DNS validation was introduced after ACM became available, so if you have existing certs issued by ACM prior to the release of this feature, you should consider creating new certs with DNS validation, and switching over to them.</p>",1695906,1695906,2018-07-19T10:41:33.973,2018-07-19T10:41:33.973,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/45333388
450,45335927,2,44374705,2017-07-26T19:32:27.433,0,"<p>To expand a little on Muhammad Abdullah's answer to his own question.</p><p>Using an outdated SDK is indeed the issue. I went wrong on this when looking at the developer guide (<a href=""http://docs.aws.amazon.com/mobile/sdkforandroid/developerguide/setup.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/mobile/sdkforandroid/developerguide/setup.html</a>) which is not kept up to date on the SDKs. </p><p>On their github page (<a href=""https://github.com/aws/aws-sdk-android"" rel=""nofollow noreferrer"">https://github.com/aws/aws-sdk-android</a>) you can find the latest version of the SDKs without having to download them. Once you have found the current version number you can simply update the build.gradle file to reflect this newer version.</p><p>For example change <code>compile 'com.amazonaws:aws-android-sdk-core:2.2.+'</code> to <code>compile 'com.amazonaws:aws-android-sdk-core:2.4.+'</code> note that the third part of the version number is covered by the + so only the first two parts are relevant here.</p>",7868000,,,2017-07-26T19:32:27.433,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45335927
451,45355425,2,45352856,2017-07-27T15:44:03.863,1,"<p>You do not query <code>s3</code> information through the Spark API, but rather through the AWS S3 SDK. You can do it like this:</p><pre><code>import com.amazonaws.services.s3.AmazonS3Clientval lastModified = new AmazonS3Client().getObject(""myBucket"",""path/to/file"").getObjectMetadata.getLastModified</code></pre><p>Obviously, you will have to download the AWS S3 SDK through Maven and include the dependency. Also I think they might have deprecated the <code>AmazonS3Client</code> in newer versions of the SDK, so you may need to make slight changes depending on which version of the SDK you download:)</p>",5291448,,,2017-07-27T15:44:03.863,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45355425
452,45390471,2,44150156,2017-07-29T14:36:32.183,39,"<p>Long story short: Dynamo does not support this. It's not build for this use-case. It's intended for quick data access with low-latency. It simply does not support any aggregating functionality.</p><p>You have three main options:</p><ul><li><p>Export DynamoDB data to <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/RedshiftforDynamoDB.html"" rel=""noreferrer"">Redshift</a> or <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EMRforDynamoDB.Tutorial.html"" rel=""noreferrer"">EMR Hive</a>. Then you can execute SQL queries on a stale data. The benefit of this approach is that it consumes RCUs just once, but you will stick with outdated data.</p></li><li><p>Use <a href=""https://github.com/awslabs/emr-dynamodb-connector"" rel=""noreferrer"">DynamoDB connector</a> for Hive and directly query DynamoDB. Again you can write arbitrary SQL queries, but in this case it will access data in DynamoDB directly. The downside is that it will consume read capacity on every query you do.</p></li><li><p>Maintain aggregated data in a separate table using <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"" rel=""noreferrer"">DynamoDB streams</a>. For example you can have a table UserId as a partition key and a nested map with tags and counts as an attribute. On every update in your original data DynamoDB streams will execute a Lambda function or some code on your hosts to update aggregate table. This is the most cost efficient method, but you will need to implement additional code for each new query.</p></li></ul><p>Of course you can extract data at the application level and aggregate it there, but I would not recommend to do it. Unless you have a small table you will need to think about throttling, using just part of provisioned capacity (you want to consume, say, 20% of your RCUs for aggregation and not 100%), and how to distribute your work among multiple workers.</p><p>Both Redshift and Hive already know how to do this. Redshift relies on multiple worker nodes when it executes a query, while Hive is based on top of Map-Reduce. Also, both Redshift and Hive can use predefined percentage of your RCUs throughput.</p>",1112503,,,2017-07-29T14:36:32.183,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45390471
453,45420948,2,44027709,2017-07-31T16:25:28.073,3,"<p>This is due to outdated dependencies in the current version of the <code>awsebcli</code> tool. They pinned version ""docker-py (>=1.1.0,&lt;=1.7.2)"" which does not support the newer credential helper formats. The latest version of <code>docker-py</code> is the first one to properly support the latest credential helper format and until the AWS EB CLI developers update <code>docker-py</code> to use 2.4.0 (<a href=""https://github.com/docker/docker-py/releases/tag/2.4.0"" rel=""nofollow noreferrer"">https://github.com/docker/docker-py/releases/tag/2.4.0</a>) this will remain broken.</p>",1950432,,,2017-07-31T16:25:28.073,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45420948
454,45594391,2,32678325,2017-08-09T15:07:04.517,9,"<p>As a lowly dev without perms to create a Data Pipeline, I had to use this javascript.  Hassan Sidique's code was slightly out of date, but this worked for me:</p><p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false""><div class=""snippet-code""><pre class=""snippet-code-js lang-js prettyprint-override""><code>var fs = require('fs');var parse = require('csv-parse');var async = require('async');const AWS = require('aws-sdk');const dynamodbDocClient = new AWS.DynamoDB({ region: ""eu-west-1"" });var csv_filename = ""./CSV.csv"";rs = fs.createReadStream(csv_filename);parser = parse({  columns : true,  delimiter : ','}, function(err, data) {  var split_arrays = [], size = 25;  while (data.length &gt; 0) {  //split_arrays.push(data.splice(0, size));  let cur25 = data.splice(0, size)  let item_data = []  for (var i = cur25.length - 1; i &gt;= 0; i--) {  const this_item = {  ""PutRequest"" : {  ""Item"": {  // your column names here will vary, but you'll need do define the type  ""Title"": {  ""S"": cur25[i].Title  },  ""Col2"": {  ""N"": cur25[i].Col2  },  ""Col3"": {  ""N"": cur25[i].Col3  }  }  }  };  item_data.push(this_item)  }  split_arrays.push(item_data);  }  data_imported = false;  chunk_no = 1;  async.each(split_arrays, (item_data, callback) =&gt; {  const params = {  RequestItems: {  ""tagPerformance"" : item_data  }  }  dynamodbDocClient.batchWriteItem(params, function(err, res, cap) {  if (err === null) {  console.log('Success chunk #' + chunk_no);  data_imported = true;  } else {  console.log(err);  console.log('Fail chunk #' + chunk_no);  data_imported = false;  }  chunk_no++;  callback();  });  }, () =&gt; {  // run after loops  console.log('all data imported....');  });});rs.pipe(parser);</code></pre></div></div></p>",4515014,45525,2018-02-14T23:50:27.870,2018-02-14T23:50:27.870,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45594391
455,45696811,2,45657251,2017-08-15T16:04:19.753,0,"<p>Always use boto3. boto is deprecated. </p><ol><li><p>As long as you setup AWS CLI credential, you don't need to pass the hard-coded credential. Read <a href=""http://boto3.readthedocs.io/en/latest/guide/configuration.html"" rel=""nofollow noreferrer"">boto3 credential setup</a> throughly.</p></li><li><p>There is no reason to initiate boto3.session unless you are using different region and user profile. </p></li><li><p>Take your time and study difference between service client(boto3.client) vs service resources(boto3.resources).</p></li><li><p>Low level boto3.client is easier to use for experiments. Use high level boto3.resource if you need to pass around arbitrary object.</p></li></ol><p>Here is the simple code for <a href=""http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.download_file"" rel=""nofollow noreferrer"">boto3.client(""s3"").download_file</a>.</p><pre><code>import boto3 # initiate the proper AWS services client, i.e. S3 s3 = boto3.client(""s3"")s3.download_file('your_bucket_name', 'Date_Table.csv', '/your/local/path/and/filename')</code></pre>",6017840,,,2017-08-15T16:04:19.753,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45696811
456,45698759,2,45547556,2017-08-15T18:02:11.867,0,"<p>Typically for B/G testing you wouldn't use different dns for new functions, but define rules, such as every 100th user gets send to the new function or only ips from a certain region or office have access to the new functionality, etc. </p><p>Assuming you're using AWS, you should be able to create an ALB in front of the ELBs for context based routing in which you should be able define rules for your routing to either B or G. In this case you have to separate environments functioning independently (possibly using the same DB though).</p><p>For more complicated rules, you can use tools such as leanplum or omniture inside your spring boot application. With this approach you have one single environment hosting old and new functionality and later you'd remove the code that is outdated. </p>",3794466,,,2017-08-15T18:02:11.867,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45698759
457,45710238,2,45594813,2017-08-16T09:45:57.760,0,"<p>V2 has been deprecated. V3 is not backward compatible. </p><p>Updated reference material is here:</p><p><a href=""https://developer.amazon.com/docs/smarthome/smart-home-skill-api-message-reference.html"" rel=""nofollow noreferrer"">https://developer.amazon.com/docs/smarthome/smart-home-skill-api-message-reference.html</a></p><p>And a guide to v2/v3 migration is here:</p><p><a href=""https://developer.amazon.com/docs/smarthome/smart-home-skill-migration-guide.html"" rel=""nofollow noreferrer"">https://developer.amazon.com/docs/smarthome/smart-home-skill-migration-guide.html</a></p>",1469564,297526,2018-03-10T00:31:14.687,2018-03-10T00:31:14.687,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45710238
458,45731516,2,19088649,2017-08-17T09:36:05.123,2,"<p>If anyone is looking for this answer, its outdated and you can find the new documentation here: <a href=""https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/commands.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/commands.html</a></p><pre><code>use Aws\S3\S3Client;use Aws\CommandPool;// Create the client.$client = new S3Client([  'region'  =&gt; 'us-standard',  'version' =&gt; '2006-03-01']);$bucket = 'example';$commands = [  $client-&gt;getCommand('HeadObject', ['Bucket' =&gt; $bucket, 'Key' =&gt; 'a']),  $client-&gt;getCommand('HeadObject', ['Bucket' =&gt; $bucket, 'Key' =&gt; 'b']),  $client-&gt;getCommand('HeadObject', ['Bucket' =&gt; $bucket, 'Key' =&gt; 'c'])];$pool = new CommandPool($client, $commands);// Initiate the pool transfers$promise = $pool-&gt;promise();// Force the pool to complete synchronously$promise-&gt;wait();</code></pre><p>Same thing can be done for SES commands</p>",4722097,,,2017-08-17T09:36:05.123,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45731516
459,45746614,2,45734959,2017-08-18T00:01:59.347,2,"<p>This is not a reportable bug... it is a documented limitation in the REST interface of S3.</p><blockquote>  <p>User-defined metadata is a set of key-value pairs. Amazon S3 stores user-defined metadata keys in lowercase. <strong>Each key-value pair must conform to US-ASCII when using REST</strong> and UTF-8 when using SOAP or browser-based uploads via POST. (Emphasis added.)</p>   <p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-metadata"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-metadata</a></p></blockquote><p>SOAP is deprecated (not to mention terrible), and even though POST uploads will allow you to store UTF-8 characters in metadata, don't do it, because you may be unable to work with the object or read back the metadata.</p><p>In contrast with object metadata, object <em>tagging</em>   <strong>does</strong> support UTF-8.</p><blockquote>  <p>A tag key can be up to 128 Unicode characters in length and tag values can be up to 256 Unicode characters in length.</p>   <p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/object-tagging.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/dev/object-tagging.html</a></p></blockquote>",1695906,,,2017-08-18T00:01:59.347,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/45746614
460,46125574,2,46115279,2017-09-08T23:05:51.550,2,"<p>A search on the web shows the AMI to be:</p><pre><code>Windows_Server-2012-RTM-English-64Bit-SQL_2012_SP1_Web-2014.02.12 (ami-1523bd2f)</code></pre><p>Therefore, just do a search in the AMI section of the EC2 console for:</p><pre><code>Windows_Server-2012-RTM-English-64Bit-SQL_2012</code></pre><p>This will show available images.</p><p>It appears that the original image was Service Pack 1 (deprecated), while current images are listed as SP2 and SP3.</p>",174777,,,2017-09-08T23:05:51.550,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46125574
461,46173381,2,37868404,2017-09-12T09:57:21.223,2,"<p>Ideally you should use s3a rather than s3n, as s3n is deprecated.</p><p>With s3a, there is a parameter:</p><pre><code>&lt;property&gt;  &lt;name&gt;fs.s3a.buffer.dir&lt;/name&gt;  &lt;value&gt;${hadoop.tmp.dir}/s3a&lt;/value&gt;  &lt;description&gt;Comma separated list of directories that will be used to buffer fileuploads to. No effect if fs.s3a.fast.upload is true.&lt;/description&gt;&lt;/property&gt;</code></pre><p>When you are getting the local file error, it most likely because the buffer directory has no space.</p><p>While you can change this setting to point at a directory with more space, a better solution may be to set (again in S3a):</p><p>fs.s3a.fast.upload=true</p><p>This avoids buffering the data on local disk and should actually be faster too.</p><p>The S3n buffer directory parameter should be:</p><pre><code>fs.s3.buffer.dir</code></pre><p>So if you stick with s3n, ensure it has plenty of space and it should hopefully resolve this issue.</p>",88839,,,2017-09-12T09:57:21.223,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46173381
462,46180625,2,46178220,2017-09-12T15:43:38.777,2,"<p>Turns out that for some reason outdated AWS documentation and examples are among the first results when doing a search. A few google results pages later, a more up to date example came up. Basically the form I was using was wrong. The correct one is as follows: </p><pre><code>&lt;html&gt;  &lt;head&gt;  &lt;meta http-equiv=""Content-Type"" content=""text/html; charset=UTF-8"" /&gt;  &lt;/head&gt;  &lt;body&gt;  &lt;form action=""http://sigv4examplebucket.s3.amazonaws.com/"" method=""post"" enctype=""multipart/form-data""&gt;  Key to upload:   &lt;input type=""input""  name=""key"" value=""user/user1/${filename}"" /&gt;&lt;br /&gt;  &lt;input type=""hidden"" name=""acl"" value=""public-read"" /&gt;  &lt;input type=""hidden"" name=""success_action_redirect"" value=""http://sigv4examplebucket.s3.amazonaws.com/successful_upload.html"" /&gt;  Content-Type:   &lt;input type=""input""  name=""Content-Type"" value=""image/jpeg"" /&gt;&lt;br /&gt;  &lt;input type=""hidden"" name=""x-amz-meta-uuid"" value=""14365123651274"" /&gt;   &lt;input type=""hidden"" name=""x-amz-server-side-encryption"" value=""AES256"" /&gt;   &lt;input type=""text""   name=""X-Amz-Credential"" value=""AKIAIOSFODNN7EXAMPLE/20151229/us-east-1/s3/aws4_request"" /&gt;  &lt;input type=""text""   name=""X-Amz-Algorithm"" value=""AWS4-HMAC-SHA256"" /&gt;  &lt;input type=""text""   name=""X-Amz-Date"" value=""20151229T000000Z"" /&gt;  Tags for File:   &lt;input type=""input""  name=""x-amz-meta-tag"" value="""" /&gt;&lt;br /&gt;  &lt;input type=""hidden"" name=""Policy"" value='&lt;Base64-encoded policy string&gt;' /&gt;  &lt;input type=""hidden"" name=""X-Amz-Signature"" value=""&lt;signature-value&gt;"" /&gt;  File:   &lt;input type=""file""   name=""file"" /&gt; &lt;br /&gt;  &lt;!-- The elements after this will be ignored --&gt;  &lt;input type=""submit"" name=""submit"" value=""Upload to Amazon S3"" /&gt;  &lt;/form&gt;&lt;/html&gt;</code></pre><p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html"" rel=""nofollow noreferrer"">Here</a> is the link from where I got the form and where additional examples can be found.</p>",3424245,,,2017-09-12T15:43:38.777,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46180625
463,46208072,2,46205362,2017-09-13T23:02:21.537,1,"<p>Closest I could find is:</p><blockquote>  <p>ASCII is a deprecated leader nodeŠ—–only function.</p></blockquote><p>You could write a <a href=""http://docs.aws.amazon.com/redshift/latest/dg/udf-creating-a-scalar-udf.html"" rel=""nofollow noreferrer"">Python User-Defined Function</a> that returns a character given the ASCII value.</p>",174777,,,2017-09-13T23:02:21.537,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46208072
464,46243012,2,20822021,2017-09-15T15:29:15.360,1,"<p>I had the same problem, turns out another sysadmin decided to route outbound internet traffic through a proxy. I found this by noticing some wearied proxy env settings, dug a little deeper, and then noticed an entry in my /etc/yum.conf file. </p><p>Commented out the proxy= line and all worked again.</p><pre><code>[main]cachedir=/var/cache/yum/$basearch/$releaseverkeepcache=0debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1plugins=1installonly_limit=5bugtracker_url=http://bugs.centos.org/set_project.php?project_id=23&amp;ref=http://bugs.centos.org/bug_report_page.php?category=yumdistroverpkg=centos-release#proxy=http://pos-proxy-in-my-way-of-doing-actual-real-work:666</code></pre>",1104563,,,2017-09-15T15:29:15.360,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46243012
465,46245728,2,36071026,2017-09-15T18:42:13.153,4,"<p>Existing answer is outdated. Associating existing Elastic IPs is now possible thanks to this change: <a href=""https://github.com/hashicorp/terraform/pull/5236"" rel=""nofollow noreferrer"">https://github.com/hashicorp/terraform/pull/5236</a></p><p>Docs: <a href=""https://www.terraform.io/docs/providers/aws/r/eip_association.html"" rel=""nofollow noreferrer"">https://www.terraform.io/docs/providers/aws/r/eip_association.html</a></p><p>Excerpt:</p><blockquote>  <p>aws_eip_association </p>   <p>Provides an AWS EIP Association as a top level  resource, to associate and disassociate Elastic IPs from AWS Instances  and Network Interfaces.</p>   <p>NOTE: aws_eip_association is useful in scenarios where EIPs are either  pre-existing or distributed to customers or users and therefore cannot  be changed.</p></blockquote>",3728792,,,2017-09-15T18:42:13.153,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46245728
466,46266842,2,46260831,2017-09-17T17:22:16.207,1,"<p><strong>localhost</strong> inside a container doesn't point to real machine host. Literally when you use <strong>localhost</strong> inside any docker container it will be pointing to the container itself. </p><p>Docker provides two ways of handling this sitution:</p><p><strong>Links.</strong> This is deprecated way and <a href=""https://docs.docker.com/engine/userguide/networking/default_network/dockerlinks/"" rel=""nofollow noreferrer"">should be avoided</a> in new projects.</p><p><strong>Networks.</strong> Docker can establish a network where you can access your containers by DNS names (container ID, or container name).</p><p>Create a network in a <a href=""https://docs.docker.com/engine/userguide/networking/work-with-networks/"" rel=""nofollow noreferrer"">way described here</a>, replace <code>localhost</code> by the name of DB container and you'll be ready to go.</p>",2065796,,,2017-09-17T17:22:16.207,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46266842
467,46297214,2,46274499,2017-09-19T09:48:59.923,0,"<p>Generally, it's a bad idea. If somebody has access to port on server that is used for agent-server communication (8443 if I'm not mistaken), he can register as agent and get all your cluster configs&amp;passwords. Or classic man-in-the-middle attack would allow to do the same by reading your unencrypted traffic. A bit more difficult attack would allow to send commands to agents (probably with root permissions).</p><p>Your issue sounds like you reprovisioned your <code>ambari-server</code> host, and left old <code>ambari-agent</code> instances running, or maybe your certificates became outdated? At first connection to <code>ambari-server</code>, agents generate certificates and send to server. Server signs these certificates with it's own key, so now server-agent connection is encrypted. Did you try to remove old certificates and restart server&amp;agents as suggested <a href=""https://community.hortonworks.com/articles/68799/steps-to-fix-ambari-server-agent-expired-certs.html"" rel=""nofollow noreferrer"">here</a>?</p>",1421254,,,2017-09-19T09:48:59.923,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46297214
468,46350623,2,46094535,2017-09-21T18:11:19.427,0,"<p>I suggest you refer to the <a href=""https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html"" rel=""nofollow noreferrer"">Build Specification Reference</a>.</p><p>Specifically, you should remove <code>addons:</code> as well as <code>s3_region:</code> as neither of these are valid CodeBuild flags. You should also be using <code>version: 0.2</code>, as <code>version: 0.1</code> has been deprecated.</p><p>Here is what your <code>buildspec.yml</code> should look like:</p><pre><code>version: 0.2phases:  build:  commands:  - echo Nothing to do yetartifacts:  files:  - '**/*'</code></pre>",629493,,,2017-09-21T18:11:19.427,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46350623
469,46374359,2,46374358,2017-09-22T23:12:15.527,16,"<p>Here is my solution to get a GUI running on Amazon's AMI.  I used this <a href=""https://devopscube.com/setup-gui-for-amazon-ec2-linux/"" rel=""noreferrer"">post</a> as a starting point, but had to make many changes to get it working on Amazon's AMI.  I also added additional info to make this work in a reasonably automated way so an individual who needs to bring up this environment more than once could do it without too much hassle.</p><p>Note: I include a lot of commentary in this post.  I apologize in advance, but I thought it might be helpful to someone needing to make modfications if they could understand why made the various choices along the way.</p><p>The scripts included below install some files along the way.  See section 4 for a list of the files and the directory structure used by these scripts.</p><h2>Step 1. Install the Desktop</h2><p>After performing a 'yum update', most solutions include a line like</p><pre><code>sudo yum groupinstall -y ""Desktop""</code></pre><p>This deceivingly simple step requires significantly more effort on the Amazon AMI.  This group is not configured in the Amazon AMI (AAMI from here on out).  The AAMI has Amazon's own repositories installed and enabled by default.  Also installed is the epel repo, but it is disabled by default.  After enabling epel I found the Desktop group but it was not populated with packages.  I also found Xfce (another desktop alternative) which was populated.  Eventually I decided to install Xfce rather than Desktop.  Still, that was not straight forward, but it eventually led to the solution.</p><p>Here it's worth noting that the first thing I tried was to install the centos repository and install the Desktop group from there.  Initially this seemed promising.  The group was fully populated with packages.  However, after some effort I eventually decided there were simply too many version conflicts between the dependencies and packages that were already installed on the AAMI.</p><p>This led me choose Xfce from the epel repo.  Since the epel repo was already installed on AAMI I figured there would be better dependency version coordination with the Amazon repos.  This was generally true.  Many dependencies were found either in the epel repo or the Amazon repos.  For the ones that weren't, I was able to find them in the centos repo, and in most cases those were leaf dependencies.  So most of the trouble came from the few dependencies in the centos repo that had sub-dependencies which conflicted with the amazon or epel repo.  In the end a few hacks were required to bypass the dependency conflicts.  I tried to minimize those as much as possible.  Here is the script for installing Xfce</p><p>installGui.sh</p><pre><code>#!/bin/bash# echo each commandset -x# assumes RSRC_DIR and IS_EMR set by parent scriptYUM_RSRC_DIR=$RSRC_DIR/yumsudo yum -y update# Most info I've found on installing a GUI on AWS suggests to install using#&gt; sudo yum groupinstall -y ""Desktop""# This group is not available by default on the Amazon Linux AMI.  The group# is listed if the epel repo is enabled, but it is empty.  I tried installing# the centos repo, which does have support for this group, but it simply end# up having to many dependency version conflicts with packages already installed# by the Amazon repos.## I found the path of least resistance to be installing the group Xfce from# the epel repo. The epel repo is already included in amazon image, just not enabled.# So I'm guessing there was at least some consideration by Amazon to align# the dependency versions of this repo with the Amazon repos.## My general approach to this problem was to start with the last command:#&gt; sudo yum groupinstall -y Xfce# which will generate a list of missing dependencies.  The script below# essentially works backwards through that list to eliminate all the# missing dependencies.## In general, many of the dependencies required by Xfce are found in either# the epel repo or the Amazon repos.  Most of the remaining dependencies can be# found in the centos repo, and either don't have any further dependencies, or if they# do those dependencies are satisfied with the centos repo with no collisions# in the epel or amazon repo.  Then there are a couple of oddball dependencies# to clean up.# if yum-config-manager is not found then install yum-utils#&gt; sudo yum install yum-utilssudo yum-config-manager --enable epel# install centos repo# place the repo config @  /etc/yum.repos.d/centos.reposudo cp $YUM_RSRC_DIR/yum.repos.d/centos.repo /etc/yum.repos.d/# The config centos.repo specifies the key with a URL.  If for some reason the key# must be in a local file, it can be found here: https://www.centos.org/keys/RPM-GPG-KEY-CentOS-6# It can be installed to the right location in one step:#&gt; wget -O /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 https://www.centos.org/keys/RPM-GPG-KEY-CentOS-6# Note, a key file must also be installed in the system key ring.  The docs are a bit confusing# on this, I found that I needed to run both gpg AND then followed by rpm, eg:#&gt; sudo gpg --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6#&gt; sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6# I found there are a lot of version conflicts between the centos, Amazon and epel repos.# So I did not enable the centos repo generally.  Instead I used the --enablerepo switch# enable it explicitly for each yum command that required it.  This only works for yum.  If# rpm must be used, then yum-config-manager must be used to enable/disable repos as a# separate step.## Another problem I ran into was yum installing the 32-bit (*.i686) package rather than# the 64-bit (*.x86_64) verision of the package.  I never figured out why.  So I had# to specify the *.x86_64 package explicitly.  The search tools (eg. 'whatprovides')# did not list the 64 bit package either even though a manual search through the# package showed the 64 bit components were present.## Sometimes it is difficult to determine which package must be in installed to satisfy# a particular dependency.  'whatprovides' is a very useful tool for this#&gt; yum --enablerepo centos whatprovides libgdk_pixbuf-2.0.so.0#&gt; rpm -q --whatprovides libgdk_pixbufsudo yum --enablerepo centos install -y gdk-pixbuf2.x86_64sudo yum --enablerepo centos install -y gtk2.x86_64sudo yum --enablerepo centos install -y libnotify.x86_64sudo yum --enablerepo centos install -y gnome-icon-themesudo yum --enablerepo centos install -y redhat-menussudo yum --enablerepo centos install -y gstreamer-plugins-base.x86_64# problem when we get to libvte, installing libvte requires expat, which conflicts with amazon lib# the centos package version was older and did not install right lib version# but Š—_ the expat dependency was coming from a dependency on python-libs.# the easiest workaround was to install python using the amazon repo, that in turn# installs a version of python libs that is compatible with the version of libexpat on the system.sudo yum install -y pythonsudo yum --enablerepo centos install -y vte.x86_64sudo yum --enablerepo centos install -y libical.x86_64sudo yum --enablerepo centos install -y gnome-keyring.x86_64# another sticky point, xfdesktop requires desktop-backgrounds-basic, but Š—…whatprovidesŠ—È does not # provide any packages for this query (not sure why).  It turns out this is provided by the centos # repo, installing Š—…desktop-backgrounds-basicŠ—È will try to install the package redhat-logos, but # unfortunately this is obsoleted by AmazonŠ—Ès generic-logos package# The only way I could find to get around this was to erase the generic logos package.# This doesn't seem too risky since this is just images for the desktop and menus.#sudo yum erase -y generic-logos# Amazon repo must be disabled to prevent interference with the install# of redhat-logossudo yum --disablerepo amzn-main --enablerepo centos install -y redhat-logos# next problem is a dependency on dbus.  The dependency comes from dbus-x11 in # centos repo.  It requires dbus version 1.2.24, the amazon image already has# version 1.6.12 installed.  Since the dbus-x11 is only used by the GUI package,# easiest way around this is to install dbus-x11 with no dependency checks.# So it will use the newer version of dbus (should be OK).  The main thing that could be a problem# here is if it skips some other dependency.  When doing manually, its possible to run the install until# the only error left is the dbus dependency.  ItŠ—Ès a bit risky running in a script since, basically itŠ—Ès assuming# all the dependencies are already in place.yumdownloader --enablerepo centos dbus-x11.x86_64sudo rpm -ivh --nodeps dbus-x11-1.2.24-8.el6_6.x86_64.rpmrm dbus-x11-1.2.24-8.el6_6.x86_64.rpmsudo yum install -y xfdesktop.x86_64# We need the version of poppler-glib from centos repo, but it is found in several repos.# Disable the other repos for this step.# On EMR systems a newer version of poppler is already installed.  So move up 1 level# in dependency chain and force install of tumbler.if [ $IS_EMR -eq 1 ]then  yumdownloader --enablerepo centos tumbler.x86_64  sudo rpm -ivh --nodeps tumbler-0.1.21-1.el6.x86_64.rpmelse  sudo yum --disablerepo amzn-main --disablerepo amzn-updates --disablerepo epel --enablerepo centos install -y poppler-glibfisudo yum install  --enablerepo centos -y polkit-gnome.x86_64sudo yum install  --enablerepo centos  -y control-center-filesystem.x86_64sudo yum groupinstall -y Xfce</code></pre><p>Here are the contents for the centos repository config file:</p><p>centos.repo</p><pre><code>[centos]name=CentOS mirrorbaseurl=http://repo1.ash.innoscale.net/centos/6/os/x86_64/failovermethod=priorityenabled=0gpgcheck=1gpgkey=https://www.centos.org/keys/RPM-GPG-KEY-CentOS-6</code></pre><p>If all you needed was a recipe to get a desktop package installed on the Amazon AMI, then you're done.  The rest of this post covers how to configure VNC to access the desktop via an SSH tunnel, and how to package all of this so that the instance can be easily started from a script.</p><h2>Step 2. Install and Configure VNC</h2><p>Below is my top level script for installing the GUI.  After configuring a few variables the first thing it does is call the script from step 1 above. This script has some extra baggage since I've built it to work on a regular ec2 instance, or emr and as root or as ec2-user.  The essential steps are</p><ol><li>install libXfont</li><li>install tiger-vnc-server</li><li>install the VNC server config file </li><li>create a .vnc directory in the user home directory </li><li>install the xstartup file in the .vnc directory</li><li>install a dummy passwd file in the .vnc directory </li><li>start the VNC server</li></ol><p>A few key points to note:</p><p>This assumes you will access the VNC server through an SSH tunnel.  In the end this really seemed like the easiest and most reliably secure way to go.  Since you probably have a port for SSH open in your security group specification, you won't have to make any changes to it.  Also, the encryption config for VNC clients/servers is not straight forward.  It seemed easy to make a mistake and leave your communications unencrypted.  The settings for this are in the vncservers file.  The -localhost switch tells vnc only to accept local connections.  The '-nolisten tcp' tells associated xserver modules to also not accept connections from the network.  Finally the '-SecurityTypes None' switch allows you to open your VNC session without typing a passwd, since the only way into the machine is through ssh, the additional password check seems redundant.</p><p>The xstartup file determines what will start when your VNC session is initiated the first time.  I've noticed many posts on this subject skip this point.  If you don't tell it to start the Xfce desktop, you will just get a blank window when you start VNC.  The config I have here is very simple.  </p><p>Even though I mentioned above that the VNC server is configured to not prompt for a password, it nevertheless requires a passwd file in the .vnc directory in order for the server to start.  The first time you run the script it will fail when it tries to start the server.  Login to the machine via ssh and run 'vncpasswd'.  It will create a passwd file in the .vnc directory that you can save to use as part of these scripts during install.  Note, I've read that VNC does not do anything sophisticated to protect the passwd file.  So I would not recommend using a passwd that you use for other, more important accounts.</p><p>installGui.sh</p><pre><code>#!/bin/bash# echo each commandset -xBIN_DIR=""${BASH_SOURCE%/*}""ROOT_DIR=$(dirname $BIN_DIR)RSRC_DIR=$ROOT_DIR/rsrcVNC_DIR=$RSRC_DIR/vnc# Install user config files into ec2-user home directory# if it is available.  In practice, this should always# be trueif [ -d ""/home/ec2-user"" ]then  USER_ACCT=ec2-userelse  USER_ACCT=hadoopfiHOME_DIR=""/home""# Use existence of hadoop home directory as proxy to determine if# this is an EMR system.  Can be used later to differentiate# steps on EC2 system vs EMR.if [ -d ""/home/hadoop"" ]then  IS_EMR=1else  IS_EMR=0fi# execute Xfce desktop install. ""$BIN_DIR/installXfce.sh""# now roughly follow the following from step 3: https://devopscube.com/setup-gui-for-amazon-ec2-linux/sudo yum install -y pixman pixman-devel libXfontsudo yum -y install tigervnc-server# install the user account configuration file.# This setup assumes the user will always connect to the VNC server# through an SSH tunnel.  This is generally more secure, easier to# configure and easier to get correct than trying to allow direct# connections via TCP.# Therefore, config VNC server to only accept local connections, and# no password required.sudo cp $VNC_DIR/vncservers-$USER_ACCT /etc/sysconfig/vncservers# install the user account, vnc config filessudo mkdir $HOME_DIR/$USER_ACCT/.vncsudo chown $USER_ACCT:$USER_ACCT $HOME_DIR/$USER_ACCT/.vnc# need xstartup file to tell vncserver to start the window managersudo cp $VNC_DIR/xstartup $HOME_DIR/$USER_ACCT/.vnc/sudo chown $USER_ACCT:$USER_ACCT $HOME_DIR/$USER_ACCT/.vnc/xstartup# Even though the VNC server is config'd to not require a passwd, the# server still looks for the passwd file when it starts the session.# It will fail if the passwd file is not found.# The first time these scripts are run, the final step will fail.# Then manually run#&gt; vncpasswd# It will create the file ~/.vnc/passwd.  Then save this file to persistent# storage so that it can be installed to the user account during# server initialization.sudo cp $ROOT_DIR/home/user/.vnc/passwd $HOME_DIR/$USER_ACCT/.vnc/sudo chown $USER_ACCT:$USER_ACCT $HOME_DIR/$USER_ACCT/.vnc/passwd# This script will be running as root if called from the EC2 launch# command.  VNC server needs to be started as the user that# you will connect to the server as (eg. ec2-user, hadoop, etc.)sudo su -c ""sudo service vncserver start"" -s /bin/sh $USER_ACCT# how to stop vncserver# vncserver -kill :1# On the remote client# 1. start the ssh tunner#&gt; ssh -i ~/.ssh/&lt;YOUR_KEY_FILE&gt;.pem -L 5901:localhost:5901 -N ec2-user@&lt;YOUR_SERVER_PUBLIC_IP&gt;#  for debugging connection use -vvv switch# 2. connect to the vnc server using client on the remote machine.  When#  prompted for the IP address, use 'localhost:5901'#  This connects to port 5901 on your local machine, which is where the ssh#  tunnel is listening.</code></pre><p>vncservers</p><pre><code># The VNCSERVERS variable is a list of display:user pairs.## Uncomment the lines below to start a VNC server on display :2# as my 'myusername' (adjust this to your own).  You will also# need to set a VNC password; run 'man vncpasswd' to see how# to do that.  ## DO NOT RUN THIS SERVICE if your local area network is# untrusted!  For a secure way of using VNC, see this URL:# http://kbase.redhat.com/faq/docs/DOC-7028# Use ""-nolisten tcp"" to prevent X connections to your VNC server via TCP.# Use ""-localhost"" to prevent remote VNC clients connecting except when# doing so through a secure tunnel.  See the ""-via"" option in the# `man vncviewer' manual page.# Use ""-SecurityTypes None"" to allow session login without a password.# This should only be used in combination with ""-localhost""# Note: VNC server still looks for the passwd file in ~/.vnc directory# when the session starts regardless of whether the user is# required to enter a passwd.# VNCSERVERS=""2:myusername""# VNCSERVERARGS[2]=""-geometry 800x600 -nolisten tcp -localhost""VNCSERVERS=""1:ec2-user""VNCSERVERARGS[1]=""-geometry 1280x1024 -nolisten tcp -localhost -SecurityTypes None""</code></pre><p>xstartup</p><pre><code>#!/bin/shunset SESSION_MANAGERunset DBUS_SESSION_BUS_ADDRESS# exec /etc/X11/xinit/xinitrc/usr/share/vte/termcap/xterm &amp;/usr/bin/startxfce4 &amp;</code></pre><h2>Step 3. Connect to Your Instance</h2><p>Once you've got the VNC server running on EC2 you can try connecting to it.  First open an SSH tunnel to your instance.  5901 is the port where the VNC server listens for display 1 from the vncservers file.  It will listen for display 2 on port 5902, etc.  This command creates a tunnel from port 5901 on your local machine to port 5901 on the instance.</p><pre><code>ssh -i ~/.ssh/&lt;YOUR_KEY_FILE&gt;.pem -L 5901:localhost:5901 -N ec2-user@&lt;YOUR_SERVER_PUBLIC_IP&gt;</code></pre><p>Now open your preferred VNC client.  Where it prompts for the IP address of the server enter:</p><blockquote>  <p>localhost:5901</p></blockquote><p>If nothing happens at all, then either there was a problem starting the vnc server, or there is a connectivity problem preventing the client from reaching the server, or possibly a problem in vncservers config file</p><p>If a window comes up, but it is just blank then check that the Xfce install completed successfully and that the xstartup file is installed.</p><h2>Step 4. Simplify</h2><p>If you just need to do this once then sftp'ing the scripts over to your instance and running manually is fine.  Otherwise you're going to want to automate this as much as possible to make it faster and less error prone when you do need to fire up an instance with a GUI.</p><p>The first step to automating is to create an EFS volume containing the scripts and config files that can be mounted when the instance is started.  Amazon has plenty of <a href=""https://aws.amazon.com/getting-started/tutorials/create-network-file-system/"" rel=""noreferrer"">info</a> on creating a network file system.  A couple points to pay attention to when creating the volume.  If you don't want your volume to be open to the world you may want to create a custom security group to use for your EFS volume.  I created security group for my EFS volume (call it NFS_Mount) that only allows inbound TCP traffic on port 2049 coming from one of my other security groups, call it MasterVNC.  Then when you create an instance, make sure to associate the MasterVNC security group with that instance.  Otherwise the EFS volume won't allow your instance to connect with it.</p><p>Now mount the EFS volume:</p><pre><code>sudo mkdir /mnt/YOUR_MOUNT_POINT_DIRsudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 fs-YOUR_EFS_ID.efs.us-east-1.amazonaws.com:/ /mnt/YOUR_MOUNT_POINT_DIR</code></pre><p>Now populate /mnt/YOUR_MOUNT_POINT_DIR with the 6 files mentioned in steps 1 and 2 using the following directory structure.  Recall that you must create the passwd file the first time using the command 'vncpasswd'.  It will create the file at ~/.vnc/passwd.</p><blockquote>  <p>/mnt/YOUR_MOUNT_POINT_DIR/bin/installGui.sh  /mnt/YOUR_MOUNT_POINT_DIR/bin/installXfce.sh</p>   <p>/mnt/YOUR_MOUNT_POINT_DIR/rsrc/vnc/vncservers-ec2-user  /mnt/YOUR_MOUNT_POINT_DIR/rsrc/vnc/xstartup  /mnt/YOUR_MOUNT_POINT_DIR/rsrc/vnc/passwd</p>   <p>/mnt/YOUR_MOUNT_POINT_DIR/rsrc/yum/yum.repos.d/centos.repo</p></blockquote><p>At this point, setting up an instance with a GUI should be pretty easy.  Create your instance as you normally would (make sure to include the MasterVNC security group), ssh to the instance, mount the EFS volume, and run the installGui.sh script.  </p><h2>Step 5. Automate</h2><p>You can take things a step further and launch your instance in 1 step using the AWS CLI tools on your local machine.  To do this you will need to mount the EFS volume and run the installGui.sh script using arguments to the AWS CLI commands. This just requires creating a top level script and passing it to the CLI command.</p><p>Of course there are a couple complications.  EC2 and EMR use different switches and mechanisms to attach the script.  And furthermore, on EMR I only want the GUI to be installed on the master node (not the core or task nodes).</p><p>Launching an EC2 instance requires embedding the script in the command with the --user-data switch.  This is done easily by specifying the absolute path to the script file on your local machine.</p><pre><code>aws ec2 run-instances --user-data file:///PATH_TO_YOUR_SCRIPT/top.sh  ... other options</code></pre><p>The EMR launch does not support embedding scripts from a local file.  Instead you can specify an S3 URI in the bootstrap actions.</p><pre><code>aws emr create-cluster --bootstrap-actions '[{""Path"":""s3://YOUR_BUCKET/YOUR_DIR/top.sh"",""Name"":""Custom action""}]' ... other options</code></pre><p>Finally, you'll see in top.sh below most of the script is a function to determine if the machine is a basic EC2 instance or an EMR master.  If not for that the script could be 3 lines.  You may wonder why not just use the built in 'run-if' bootstrap action rather than writing my own function.  The built in 'run-if' script has a bug and does not properly run scripts located in S3.</p><p>Debugging things once you put them in the init sequence can be a challenge.  One thing that can help is the log file: /var/log/cloud-init-output.log. This captures all the console output from the scripts run during bootstrap initialization.</p><p>top.sh</p><pre><code>#!/bin/bash# note: conditional bootstrap function run-if has a bug, workaround ...# this function adapted from https://forums.aws.amazon.com/thread.jspa?threadID=222418# Determine if we are running on the master node.# 0 - running on master, or non EMR node# 1 - running on a task or core nodecheck_if_master_or_non_emr() {  python - &lt;&lt;'__SCRIPT__'import sysimport jsoninstance_file = ""/mnt/var/lib/info/instance.json""try:  with open(instance_file) as f:  props = json.load(f)  is_master_or_non_emr = props.get('isMaster', False)except IOError as ex:  is_master_or_non_emr = True   # file will not exist when testing on a non-emr machineif is_master_or_non_emr:  sys.exit(1)else:  sys.exit(0)__SCRIPT__}check_if_master_or_non_emrIS_MASTER_OR_NON_EMR=$?# If this machine is part of EMR cluster, then ONLY install on the MASTER nodeif [ $IS_MASTER_OR_NON_EMR -eq 1 ]then  sudo mkdir /mnt/YOUR_MOUNT_POINT_DIR  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 fs-YOUR_EFS_ID.efs.us-east-1.amazonaws.com:/ /mnt/YOUR_MOUNT_POINT_DIR  . /mnt/YOUR_MOUNT_POINT_DIR/bin/installGui.shfiexit 0</code></pre>",6471212,,,2017-09-22T23:12:15.527,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46374359
470,46423676,2,46423207,2017-09-26T10:14:34.630,1,"<p>Send from a cron script or other scheduled task that does not have a timeout - search on here for how to do that.</p><p>Send more efficiently - see <a href=""https://github.com/PHPMailer/PHPMailer/blob/master/examples/mailing_list.phps"" rel=""nofollow noreferrer"">the mailing list example provided with PHPMailer</a>.</p><p>Get your local mail server to work for you - submit messages to it (which will be very fast) and let it deal with slow deliveries - it's what mail servers are for.</p><p>I can see you've based your code on an obsolete example and are using an old version of PHPMailer, so <a href=""https://github.com/PHPMailer/PHPMailer"" rel=""nofollow noreferrer"">get the latest version</a>.</p>",333340,,,2017-09-26T10:14:34.630,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46423676
471,46455940,2,46455502,2017-09-27T19:37:39.700,1,"<p>Try this: </p><p><code>sudo -H pip install --upgrade awscli --ignore-installed six</code></p><p>This should work around the ""Uninstalling a distutils installed project (six) has been deprecated"" problem. It looks like the underlying issue has been fixed recently, as I haven't had to use the <code>--ignore-installed</code> option for the last few upgrades.</p>",4638378,,,2017-09-27T19:37:39.700,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46455940
472,46533878,2,46533670,2017-10-02T21:19:12.587,4,"<p>In general all of the old client constructors are deprecated in the newer AWS libraries.  You'll need to do something like:</p><pre><code>AWSCognitoIdentityProvider provider =   AWSCognitoIdentityProviderClientBuilder.standard().defaultClient();</code></pre><p>This is the bare bones version - if you need to pass a different credentials provider or region you'll need to add some more parameters.  See <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/cognitoidp/AWSCognitoIdentityProviderClientBuilder.html"" rel=""nofollow noreferrer"">AWSCognitoIdentityProviderClientBuilder</a> and <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/cognitoidp/AWSCognitoIdentityProvider.html"" rel=""nofollow noreferrer"">AWSCognitoIdentityProvider</a> for more details.</p>",2933977,,,2017-10-02T21:19:12.587,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46533878
473,46554671,2,35589641,2017-10-03T23:00:54.747,0,"<p>I was struggling with this issue for hours. I was using AmazonS3EncryptionClient and nothing I did helped. Then I noticed that the client is actually deprecated, so I thought I'd try switching to the builder model they have:</p><pre><code>var builder = AmazonS3EncryptionClientBuilder.standard()  .withEncryptionMaterials(new StaticEncryptionMaterialsProvider(encryptionMaterials))if (accessKey.nonEmpty &amp;&amp; secretKey.nonEmpty) builder = builder.withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey.get, secretKey.get)))builder.build()</code></pre><p>And... that solved it. Looks like Lambda has trouble injecting the credentials in the old model, but works well in the new one.</p>",3830413,,,2017-10-03T23:00:54.747,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46554671
474,46624848,2,46621362,2017-10-07T20:34:46.327,2,"<p>Please note that I don't have any clue regarding MySQL Connector or AWS. Having said this, I am making a guess:</p><p>From the error message, we eventually can conclude that the client and the server could not agree on a cipher suite. In case you didn't hear about that subject yet:</p><p>SSL is a protocol which can use a great number of different key exchange, encryption and MAC algorithms. A certain tuple of (key exchange, encryption, MAC) algorithms (methods) is called a <em>cipher suite</em>. When an SSL connection is being established, the server and the client need to agree on a certain cipher suite which will be used to exchange keys, encrypt and authenticate the connection.</p><p>The rationale behind this is that new, more secure ciphers can be easily introduced and used without changing the SSL specification itself, and that old, insecure, broken ciphers can be removed easily without changing the SSL specification itself.</p><p>This means that the owner of a server can say: ""I offer SSL to my clients, but only with cipher suites X, Y and Z"". If a client tries to connect, but only supports cipher suites A, B and C, the connection will fail.</p><p>Now, from time to time, a cipher suite becomes deprecated because it is considered unsafe. In such cases, that cipher suite often is silently disabled by security updates the O/S receives, and this is what might have happened to you:</p><p>When the server has been updated, one or more of the SSL cipher suites might have been disabled, either by updates to MySQL or by updates to the O/S itself or the libraries which provide cryptographic services to applications. This could have lead to a situation where you (the client) and the server do not have a common cipher suite any more.</p><p>It is also possible that somebody has disabled certain cipher suites on the server by intention (i.e. manually, i.e. not as ""collateral damage"" from installing updates).</p><p>Furthermore, it could be the opposite situation: If you have installed updates on <em>your</em> machine (client) or have disabled certain cipher suites, your client and the server might not being able to connect any more.</p><p>To determine how to debug this, we need to know a lot more about your configuration. One general tip, though:</p><p>During the SSL handshake, the client sends a hello to the server and vice versa. Client and server in this hello present a list of cipher suites they support. Since the client and server hello are not encrypted, you can easily sniff them using tools like Wireshark. Then you can compare the list of cipher suites the client and the server support and check if there is at least one common cipher suite.</p><p>If there isn't, you probably have found the problem. In that case, you have to change the configuration either at the client or the server side so that the client and the server have at least one common cipher suite again. Please come back if you need help in doing this, but please first analyze the situation like proposed above.</p>",3239842,3239842,2017-10-08T06:27:04.190,2017-10-08T06:27:04.190,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46624848
475,46625030,2,38956130,2017-10-07T20:56:03.547,4,"<p>Starting from the <a href=""http://citi.umich.edu/projects/nfsv4/windows/"" rel=""nofollow noreferrer"">NFSv4.1 client made by folks at CITI @ University of Michigan</a>, with a few relatively minor changes, you can get a working connection to an AWS EFS filesystem. </p><p>As @kafka points out: AWS EFS disallows / fails when any client specifies a share deny value other than <code>OPEN4_SHARE_DENY_NONE</code>. Luckily the CITI folks discovered this as a possible problem and added a definition that, when commented out, will only ever use <code>OPEN4_SHARE_DENY_NONE</code> for the share deny value. </p><p>Once this definition is commented out, then you need to recompile it for your system Š—– relatively trivial if you use the versions of Visual Studio and WDK that the readme specifies. One gotcha was that the self-signed certificate process needs to <em>not</em> use the outdated Root Agency certificate (since it's only 512-bit). Use <code>certreq</code> instead.</p><p>I'm working on collecting this knowledge into <a href=""https://github.com/contentfree/ms-nfs41-client"" rel=""nofollow noreferrer"">a fork of the CITI code at Github</a>. (I'm sure you either solved your problem or moved on, but good luck to those folks who landed here from Google!)</p>",137641,,,2017-10-07T20:56:03.547,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46625030
476,46691826,2,46690487,2017-10-11T15:11:09.047,0,"<p>According to php.net the function that is used is Deprecated, <a href=""http://php.net/manual/en/class.mongoclient.php"" rel=""nofollow noreferrer"">refer this</a>.</p><p>Can you try something like:</p><pre><code>&lt;?php$manager = new MongoDB\Driver\Manager(""mongodb://myusername:myp%40ss%3Aw%25rd@example.com/mydatabase"");?&gt;</code></pre><p><a href=""http://php.net/manual/en/mongodb-driver-manager.construct.php"" rel=""nofollow noreferrer"">Refer this</a></p>",6077057,,,2017-10-11T15:11:09.047,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46691826
477,46718013,2,46697687,2017-10-12T20:05:54.873,1,"<p>The issue is that when you use the old consumer (and use the <code>--zookeeper</code> argument) the ZooKeeper port should be provided (<code>2181</code>).</p><p>However, please note that the old consumer is now deprecated, and using the new consumer is highly recommended. Please see the answer by Mickael Maison for more info.</p>",3557290,,,2017-10-12T20:05:54.873,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46718013
478,46720353,2,46719462,2017-10-12T23:27:45.937,1,"<p>A typical mistake.</p><pre><code>ResultSet rs = ps.executeQuery(count);</code></pre><p>should be</p><pre><code>ResultSet rs = ps.executeQuery();</code></pre><p>as <code>ps</code> is a <code>PreparedStatement</code> extending <code>Statement</code>, and <code>executeQuery(String)</code> is an unusable Statement method.</p><p><em>Should one ever create a similar API, override <code>executeQuery(String)</code> and make it <code>@Deprecated</code> with a nice javadoc.</em></p>",984823,,,2017-10-12T23:27:45.937,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46720353
479,46770108,2,45758902,2017-10-16T12:14:59.497,0,"<p>You probably need to restart the MySQL workbench, not reboot the RDS.</p><p>As I described <a href=""https://stackoverflow.com/a/46770024/1285846"">here</a>, the database handles have changed, but the code doesn't notice. So the  MySQL workbench is using outdated handles that are being rejected.</p>",1285846,,,2017-10-16T12:14:59.497,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46770108
480,46797968,2,46797086,2017-10-17T19:41:53.523,0,"<p>When you call spark-submit it will add it's own python folder and the py4j library to PYTHONPATH for you.  They live in the folder spark is installed to, they are not directly installed into the site-packages of the python you are using.  For example, if I pyspark an print out the python path:</p><pre><code>$ pysparkPython 2.6.6 (r266:84292, Aug 18 2016, 15:13:37) [GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.17/10/17 15:34:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable17/10/17 15:34:04 WARN Utils: Your hostname, rwidmaier resolves to a loopback address: 127.0.0.1; using 10.4.248.126 instead (on interface em1)17/10/17 15:34:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address/home/rwidmaier/apps/spark/python/pyspark/context.py:195: UserWarning: Support for Python 2.6 is deprecated as of Spark 2.0.0  warnings.warn(""Support for Python 2.6 is deprecated as of Spark 2.0.0"")17/10/17 15:34:09 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.017/10/17 15:34:09 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException17/10/17 15:34:09 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectExceptionWelcome to  ____  __  / __/__  ___ _____/ /__  _\ \/ _ \/ _ `/ __/  '_/  /__ / .__/\_,_/_/ /_/\_\   version 2.2.0  /_/Using Python version 2.6.6 (r266:84292, Aug 18 2016 15:13:37)SparkSession available as 'spark'.&gt;&gt;&gt; import sys&gt;&gt;&gt; from pprint import pprint&gt;&gt;&gt; pprint(sys.path)['', u'/tmp/spark-0a08efa7-714a-498b-b5e9-bba48684d52d/userFiles-52012e25-d7af-4599-a214-9637141ed4ec', '/usr/lib64/python2.6/site-packages/deluge-1.3.11-py2.6-linux-x86_64.egg', '/home/ryan/apps/spark/python/lib/py4j-0.10.4-src.zip', '/home/ryan/apps/spark/python', '/usr/lib64/python26.zip', '/usr/lib64/python2.6', '/usr/lib64/python2.6/plat-linux2', '/usr/lib64/python2.6/lib-tk', '/usr/lib64/python2.6/lib-old', '/usr/lib64/python2.6/lib-dynload', '/usr/lib64/python2.6/site-packages', '/usr/lib64/python2.6/site-packages/gst-0.10', '/usr/lib64/python2.6/site-packages/gtk-2.0', '/usr/lib64/python2.6/site-packages/webkit-1.0', '/usr/lib/python2.6/site-packages']</code></pre><p>You can see in the list that it has manually added: </p><ul><li>/home/ryan/apps/spark/python/lib/py4j-0.10.4-src.zip</li><li>/home/ryan/apps/spark/python</li></ul><p>To ensure that the interpreter is always setup with the right libraries, you just need to set PYSPARK_PYTHON to point to the python install you would like to use, and then run ""pyspark"" instead of running python directly.  Or alternatively you can use spark-submit if you have want to provide a driver.</p>",1461187,,,2017-10-17T19:41:53.523,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46797968
481,46879241,2,46856731,2017-10-22T21:14:54.750,3,"<p>After a lot of trial and error, i've managed to get it working. I'm not sure if AWS docs are outdated but they are certainly over kill.</p><p>Here goes:</p><ol><li><p>From Apple Developer website visit navigate to Certificates, IDs &amp; Profiles > Identifiers > App IDs</p></li><li><p>Create a new App ID and enable the Push Notifications service.</p></li><li><p>Click Create a new Production SSL certificate, then create a new CSR file in the local keychain on Mac, upload it to Developer site (during the certificate generation process) then download the generated .cer file.</p></li><li><p>Double click the .cer file to add it to the Keychain on mac.</p></li><li><p>Open Keychain, select 'My Certificates' highlight the certificate that got added in step 3, probably starts with 'Apple Push Services'.</p></li><li><p>Right-click the cert and export it (extension is .p12). If it asks you to set a password you can leave blank.</p></li><li><p>In AWS SNS, navigate into Applications, click into your APNS application (or add a new application). Under 'platform application actions' click update credentials and upload your exported .p12 file.</p></li><li><p>Finally, click 'Load credentials from file' and update to exit the application settings.</p></li><li><p>Enjoy push notifications in your app.</p></li></ol><p>This worked for me, I'm not sure why AWS docs suggest the commands to convert files, it looks like they're not necessary and have caused great confusion.</p><p>Hope this else somebody.</p>",1042951,,,2017-10-22T21:14:54.750,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46879241
482,46961074,2,14969273,2017-10-26T18:09:15.890,4,"<p>The answer by <em>jamesis</em> is using <code>boto</code> which is an older version and will be deprecated. The current supported version is <code>boto3</code>.</p><p>The same expiration policy on the logs folder can be done as follows:</p><pre><code>import boto3from botocore.exceptions import ClientErrorclient = boto3.client('s3')try:  policy_status = client.put_bucket_lifecycle_configuration(  Bucket='boto-lifecycle-test',  LifecycleConfiguration={  'Rules':   [  {  'Expiration':  {  'Days': 30,  'ExpiredObjectDeleteMarker': True  },  'Prefix': 'logs/',  'Filter': {  'Prefix': 'logs/',  },  'Status': 'Enabled',  }  ]})except ClientError as e:  print(""Unable to apply bucket policy. \nReason:{0}"".format(e))</code></pre><p><strong>This will override any existing lifecycle configuration policy on <code>logs</code>.</strong></p><p>A good thing to do would be to check if the bucket exists and if you have the permissions to access it before applying the expiration configuration i.e. before the <code>try-except</code> </p><pre><code>bucket_exists = client.head_bucket(  Bucket='boto-lifecycle-test')</code></pre><p>Since the <code>logs</code> folder itself isn't a bucket but rather an object within the bucket <code>boto-lifecycletest</code>, the bucket itself can have a different expiration policy.You can check this from the result in <code>policy_exists</code> as below.</p><pre><code>policy_exists = client.get_bucket_lifecycle_configuration(  Bucket='boto-lifecycle-test')bucket_policy = policy_exists['Rules'][0]['Expiration']</code></pre><p>More information about setting the expiration policy can be checked at <a href=""https://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.put_bucket_lifecycle_configuration"" rel=""nofollow noreferrer"">Expiry policy</a></p>",1934182,,,2017-10-26T18:09:15.890,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46961074
483,46985780,2,46970210,2017-10-28T02:55:14.783,0,"<p>Make sure you are not using a deprecated constructor of the CognitoUserPool class. This worked for me:</p><pre><code>CognitoUserPool userPool = new CognitoUserPool(this, awsConfiguration);</code></pre>",5326518,,,2017-10-28T02:55:14.783,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46985780
484,46995002,2,46994914,2017-10-28T22:10:30.217,2,"<p>There will be a cost if it creates something other than a t2.micro instance. If it can use a t2.micro then you could shutdown your current t2.micro (if you have one running) while the CloudFormer server runs.</p><p>It needs to create an EC2 instance because all it is is a Ruby script that queries your AWS account. The last time I tried it it was very out of date and missing support for lots of AWS services. It was also slow and buggy and was a pain to get it to complete without errors. It's a shame they can't just release the script and you could run it however you saw fit. Or better yet they should build this feature directly into the CloudFormation console.</p>",13070,,,2017-10-28T22:10:30.217,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/46995002
485,47017469,2,47006358,2017-10-30T14:15:27.610,2,"<p>I cannot speak for EMR and its s3 connector. I can speak for Apache Hadoop itself, and the S3A connector</p><p>We need to buffer generated data before uploading to S3. You can't do a stream() followed by a close(), because for large files you need to break the upload into 4+GB files, and even for smaller uploads, you need to deal with the common condition of the app generating data faster than you can upload to S3.</p><p>Using transient local temp storage gives you the ability to generate data faster than your S3 upload bandwidth can handle, and cope with network errors by resending blocks.</p><p>Apache's original s3: and s3n: clients (and s3a prior to Hadoop 2.8) all wrote the entire file to HDD before starting upload. The storage you needed was the same as the #of bytes generated, and as it was only uploaded in close(), the time for that close call is data/bandwidth. </p><p>S3A in Hadoop 2.8+ supports fast upload (optional 2.8+, automatic in 3.0), where data is buffered to the size of a single block (5+ MB, default 64MB), with upload starting as soon as the block size is reached. This makes for faster writes, with enough bandwidth there's almost no close() delay (max: last-block-size/bandwidth). It still needs storage to cope with the mismatch between generation and upload rates, though you can instead configure it to use on heap byte arrays or off-heap byte buffers. Do that and you have to play very carefully with memory allocations, and queue sizes: you need to configure the client to block the writers when the queue of pending uploads is big enough.</p><p><strong>Update</strong> Johnathan Kelly @ AWS has confirmed that they do the same per block buffer and upload as the ASF S3A connector. This means that if your data generation rate in bytes/sec &lt;= upload bandwidth from the VM then the amount of local disk needed is minimal...if you generate data faster, then you'll need more (and eventually run out of disk or reach some queue limits to block the generator threads). I'm not going to quote any numbers on actual bandwidth as it invariably improves year-on-year and any statement will soon be obsolete. For that reason, look at the age of any post of benchmarks before believing. Do your own tests with your own workloads.</p>",2261274,2261274,2017-10-31T10:08:34.080,2017-10-31T10:08:34.080,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47017469
486,47077318,2,47067212,2017-11-02T14:08:30.293,5,"<p>I just tested this with an application running behind an Application Load Balancer. With gzip compression enabled on the server, the browser receives gzipped resources. The ALB correctly passes along the <code>content-encoding: gzip</code> HTTP header. Note that with ALB you also get HTTP/2 support which further reduces the time it takes for browsers to load your website's files.</p><p>I think that article you linked is incorrect, or out of date, or maybe it's an issue specific to Classic ELBs.</p>",13070,,,2017-11-02T14:08:30.293,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47077318
487,47084779,2,47073577,2017-11-02T21:14:53.307,1,"<p>You'll need to use a different parameter name. <code>select</code> now has a meaning to S3, so it is no longer quietly discarded.</p><p><strong>Update</strong>: The sudden appearance of the <code>?select</code> subresource appears to have been when AWS began deploying a new feature, <a href=""https://aws.amazon.com/blogs/aws/s3-glacier-select/"" rel=""nofollow noreferrer"">S3 Select</a>, which allows JSON and CSV objects to actually be queried for a subset of their content, using SQL expressions.  The feature was announced later the same month.</p><p>The original answer follows.</p><hr><p>For reasons that aren't readily explainable, <code>select=</code> in the query string causes S3 to interpret your request as... something different.  Exactly what it is, is not clear.</p><pre><code>&lt;ResourceType&gt;SELECT&lt;/ResourceType&gt;</code></pre><p>Interestingly, if you try a POST, you get an error message saying that <code>POST</code> is not allowed, either, but the <code>Allow: POST</code> is then no longer in the response headers.</p><p>The bucket logs show the request operation as <code>REST.GET.SELECT</code>, which doesn't seem to be documented, where a normal <code>GET</code> request is logged as <code>REST.GET.OBJECT</code>.</p><p>So you're triggering some unexpected behavior, and you'll need to use something different.</p><p>The fact that it previously worked tends to rule out my initial theory, that you were somehow prompting S3 to assume you wanted to make a deprecated SOAP request (which requires <code>POST</code>), but if it's really true that this was working all along, then I'm inclined instead to think instead that you may have inadvertently stumbled on a feature that has not yet been released. </p><p>Unofficially, S3 silently ignores most unexpected query string parameters.  Signature V2 also ignores them completely (and actually requires them <em>not</em> to be signed, if I remember my test results of that algorithm correctly).</p><p>Officially, it seems you should be using a query string parameter beginning with <code>x-</code> if you definitely don't want the service to interpret it.  This will also write the parameter to the logs, which might prove to be a useful side effect in the future for debugging purposes.</p><blockquote>  <p>You can include custom information to be stored in the access log record for a request by adding a custom query-string parameter to the URL for the request. <strong>Amazon S3 will ignore query-string parameters that begin with <code>x-</code></strong>, but will include those parameters in the access log record for the request, as part of the Request-URI field of the log record. <em>(emphasis added)</em></p>   <p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html</a></p></blockquote>",1695906,1695906,2017-11-29T17:41:16.063,2017-11-29T17:41:16.063,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47084779
488,47153880,2,30793481,2017-11-07T09:09:09.900,2,"<p>DynamoDBMarshaller is now deprecated but I get exactly the same problem with DynamoDBTypeConvertedJson. If you want to store a collection as JSON within a DynamoDBMapper class, use DynamoDBTypeConverted and write a custom converter class (do not use DynamoDBTypeConvertedJson which will not return your collection on unconvert).</p><p>Here is the solution using DynamoDBTypeConverted </p><pre><code>// Model.java@DynamoDBTable(tableName = ""..."")public class Model {  private String id;  private List&lt;MyObject&gt; objects;  public Model(String id, List&lt;MyObject&gt; objects) {  this.id = id;  this.objects = objects;  }  @DynamoDBHashKey(attributeName = ""id"")  public String getId() { return this.id; }  public void setId(String id) { this.id = id; }  @DynamoDBTypeConverted(converter = MyObjectConverter.class)  public List&lt;MyObject&gt; getObjects() { return this.objects; }  public void setObjects(List&lt;MyObject&gt; objects) { this.objects = objects; }}</code></pre><p>-</p><pre><code>public class MyObjectConverter implements DynamoDBTypeConverter&lt;String, List&lt;MyObject&gt;&gt; {  @Override  public String convert(List&lt;Object&gt; objects) {  //Jackson object mapper  ObjectMapper objectMapper = new ObjectMapper();  try {  String objectsString = objectMapper.writeValueAsString(objects);  return objectsString;  } catch (JsonProcessingException e) {  //do something  }  return null;  }  @Override  public List&lt;Object&gt; unconvert(String objectssString) {  ObjectMapper objectMapper = new ObjectMapper();  try {  List&lt;Object&gt; objects = objectMapper.readValue(objectsString, new TypeReference&lt;List&lt;Object&gt;&gt;(){});  return objects;  } catch (JsonParseException e) {  //do something  } catch (JsonMappingException e) {  //do something  } catch (IOException e) {  //do something  }  return null;  }}</code></pre>",4985580,,,2017-11-07T09:09:09.900,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47153880
489,47195647,2,24896743,2017-11-09T06:53:09.857,2,"<p>The accepted answer is now outdated. For future viewers, there is no need to include anything as extra header as now AWS includes a <code>Signature</code> field in every signed url which is different everytime you make a request.</p>",7739188,,,2017-11-09T06:53:09.857,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47195647
490,47202941,2,47142212,2017-11-09T13:21:09.463,0,"<p>Thank you for your answer ! So after trying the same code on Python and Java, I found out that the issue was related to the driver. the one I was using was apparently outdated. I installed the last driver, restarted Rstudio, now works perfectly ! Thanks ! </p>",8895384,,,2017-11-09T13:21:09.463,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47202941
491,47282176,2,47277347,2017-11-14T09:36:25.117,0,"<p>what are you using to deploy to production? </p><p>For example, if you use Heroku, you need to mantain separately the <code>env</code> variables in your production server. You can do this via dashboard or the terminal</p><p><a href=""https://stackoverflow.com/questions/10185353/what-are-the-other-heroku-environment-variables#28657555"">What are the other heroku environment variables?</a></p><p><a href=""https://devcenter.heroku.com/articles/config-vars"" rel=""nofollow noreferrer"">https://devcenter.heroku.com/articles/config-vars</a></p><p>If you use AWS then read here</p><p><a href=""http://docs.aws.amazon.com/lambda/latest/dg/env_variables.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/lambda/latest/dg/env_variables.html</a></p><p>For docker read here</p><p><a href=""https://docs.docker.com/compose/link-env-deprecated/"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/link-env-deprecated/</a></p><p>The env variables are defined in files like <code>~/.bash_profile</code> or <code>~/.bashrc</code> of every ubuntu server/container you are running </p><p>Read more about environment variable and how to configure them</p>",7295772,,,2017-11-14T09:36:25.117,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47282176
492,47324631,2,37685114,2017-11-16T08:14:39.157,1,"<p>From the description, it looks like what you are looking for is 1) Avro data write to S3</p><p>2) Data to be partitioned in S3</p><p>3) Exactly-once support while writing.</p><p>Qubole <a href=""https://github.com/qubole/streamx"" rel=""nofollow noreferrer"">StreamX</a> supports rich variety of format conversions, avro being one of them, along with data partition. And, exactly once is in our pipeline which will be out soon. </p><p>Whereas secor is getting deprecated(mentioned in one of their responses on google group) and it also do not support avro.</p><p>So you can use qubole streamx to start with.</p>",1526887,,,2017-11-16T08:14:39.157,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47324631
493,47345328,2,47329936,2017-11-17T07:24:06.427,1,"<p>As stated, your question is not very clear: ""What is the conflict resolution strategy for DynamoDB"" - what conflicts? Are you referring to potentially inconsistent reads?</p><p>DynamoDB, for GetItem queries, allows both eventual consistent and strongly consistent reads, configurable with a parameter on the request (as described in the docs here: <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a>). For strongly consistent reads the value returned is the most recent value at the time the query was executed. For eventual consistent reads it is possible to read a slightly out of date version of an item but there is no ""conflict resolution"" per se.</p><p>You may be thinking about conditional updates which allow for requests to fail if an expected condition is not met at the time the query is executed. </p>",63074,,,2017-11-17T07:24:06.427,10,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47345328
494,47438358,2,47438278,2017-11-22T15:26:28.970,1,"<p>You should use something like docker compose for that. Docker compose allows to specify parameters for you containers. In contrast Dockerfile configures your image. For example to limit resources you just create resources section in you compose.yml. Note that swappiness is an obsolete parameter.</p><p>See link: <a href=""https://docs.docker.com/compose/compose-file/#resources"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/compose-file/#resources</a></p><pre><code>version: '3'services:  redis:  image: redis:alpine  deploy:  resources:  limits:  cpus: '0.50'  memory: 50M  reservations:  cpus: '0.25'  memory: 20M</code></pre>",1201416,1201416,2017-11-22T15:29:19.900,2017-11-22T15:29:19.900,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47438358
495,47671516,2,47652281,2017-12-06T10:02:17.067,4,"<p>I'm going to have to correct the post by himanshuIIITian slightly, (sorry).</p><ol><li><p>Use the s3a connector, not the older, obsolete, unmaintained, s3n. S3A is: faster, works with the newer S3 clusters (Seoul, Frankfurt, London, ...), scales better. S3N has fundamental performance issues which have only been fixed in the latest version of Hadoop by deleting that connector entirely. Move on.</p></li><li><p>You cannot safely use s3 as a direct destination of a Spark query., not with the classic ""FileSystem"" committers available today. Write to your local file:// and then copy up the data afterwards, using the AWS CLI interface. You'll get better performance as well as the guarantees of reliable writing which you would normally expect from IO</p></li></ol>",2261274,,,2017-12-06T10:02:17.067,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47671516
496,47731473,2,47730479,2017-12-09T17:55:01.747,0,"<p>The ""Manage"" tab in the MTurk Requester Website is for managing Batches created with the MTurk Requester Website (using the Create) tab. If you need/want to view HITs that you create with the API, you can use the <a href=""http://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_ListHITsOperation.html"" rel=""nofollow noreferrer"">ListHITs API method</a> either with the API directly (using your Java code) or using the AWS Command Line Interface (CLI). </p><p>Here's a blog explaining how to do this with the AWS CLI: <a href=""https://blog.mturk.com/tutorial-managing-mturk-hits-with-the-aws-command-line-interface-56eaabb7fd4c"" rel=""nofollow noreferrer"">https://blog.mturk.com/tutorial-managing-mturk-hits-with-the-aws-command-line-interface-56eaabb7fd4c</a></p><p>The blog shows how to use aws-shell, which is a more interactive shell that sits atop the AWS CLI. It has autocomplete and shows you inline ""man"" pages on each command. I personally prefer this. </p><p>The CLI and aws-shell will also let you write filters and formatters for results. So you can do things like this:</p><pre><code>aws mturk list-hits --output table --query 'HITs[].{""1. HITId"": HITId, ""2. Title"": Title, ""3. Status"":HITStatus}' --endpoint-url https://mturk-requester-sandbox.us-east-1.amazonaws.com --max-results 5</code></pre><p>This calls <a href=""http://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_ListHITsOperation.html"" rel=""nofollow noreferrer"">ListHITs</a>, on the Sandbox (--endpoint), getting only 5 results (--max-results), formats the output as a table instead of the default JSON (--output) and filters that JSON for the HITs object (HITs[]) pulling down only the fields HITId, Title, and Status while also setting titles for those fields as ""1. HITId"", ""2. Title"", and ""3. Status"". </p><p>There used to be a link in the MTurk Requester Website for a GUI to manage HITs individually which would show HITs from the API, but it was deprecated this month. There's a brief thread on it here: <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=267769&amp;tstart=0"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=267769&amp;tstart=0</a></p>",404123,,,2017-12-09T17:55:01.747,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47731473
497,47734696,2,47734383,2017-12-10T00:33:11.907,2,"<p>Slowloris is a very simple Denial-of-Service attack. This is easy to detect and block.</p><p>Detecting and preventing DoS and DDos attacks are complex topics with many solutions. In your case you are making the situation worse by using outdated software and picking a low worker thread count so that the problem arises quickly.</p><p>A combination of services are available that would be used to manage Dos and DDos attacks.</p><p>The front-end of the total system would be protected by a firewall. Typically this firewall would include a Web Application Firewall to understand the nuances of HTTP protocols. In the AWS world, Amazon WAF and Shield are commonly used.</p><p>Another service that helps is a CDN. Amazon CloudFront uses Amazon Shield so it has good DDoS support.</p><p>The next step is to combine load balancers with auto scaling mechanisms. When the health checks start to fail (caused by Slowloris), the auto scaler will begin launching new instances and terminating failed instances. However, a sustained Slowloris attack will just hit the new servers. This is why the Web Application Firewall needs to detect the attack and start blocking it.</p><p>For your studies, take a look at <strong>mod_reqtimeout</strong>. This is an effective and tuneable solution for Apache for most Slowloris attacks.</p><p>[Update]</p><p>In the Amazon DDoS White Paper June 2015, Slowloris is specifically mentioned.</p><blockquote>  <p>On AWS, you can use Amazon CloudFront and AWS WAF to defend your  application against these attacks. Amazon CloudFront allows you to  cache static content and serve it from AWS Edge Locations that can  help reduce the load on your origin. <strong>Additionally, Amazon CloudFront  can automatically close connections from slow-reading or slow-writing  attackers (e.g., Slowloris)</strong>.</p></blockquote><p><a href=""https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf"" rel=""nofollow noreferrer"">Amazon DDoS White Paper June 2015</a></p>",8016720,8016720,2017-12-10T01:21:41.547,2017-12-10T01:21:41.547,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47734696
498,47772403,2,47765493,2017-12-12T12:15:37.277,1,"<ul><li>Nobody should be using S3n as the connector. It is obsolete and removed from Hadoop 3. If you have the Hadoop 2.7.x JARs on the classpath, use s3a</li><li>The issue with rename() is not just the consistency, but the bigger the file, the longer it takes.</li></ul><p>Really checkpointing to object stores needs to be done differently. If you look closely, there is no <code>rename()</code>, yet so much existing code expects it to be an O(1) atomic operation.</p>",2261274,,,2017-12-12T12:15:37.277,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47772403
499,47815678,2,47815526,2017-12-14T14:25:52.053,32,"<p>Bottom line: 1) Access Control Lists (ACLs) are legacy (but not deprecated), 2) bucket/IAM policies are recommended by AWS, and 3) ACLs give control over buckets AND objects, policies are only at the bucket level.</p><p>Decide which to use by considering the following: (As noted <a href=""https://stackoverflow.com/a/47818804/2121526"">below</a> by John Hanley, more than one type could apply and the <strong>most</strong> restrictive/least privilege permission will apply.)</p><p>Use S3 bucket policies if you want to:</p><ul><li>Control access in S3 environment</li><li>Know <em>who</em> can access a bucket</li><li>Stay under 20kb policy size max</li></ul><p>Use IAM policies if you want to:</p><ul><li>Control access in IAM environment, for potentially more than just buckets</li><li>Manage very large numbers of buckets</li><li>Know what a <em>user</em> can do in <em>AWS</em></li><li>Stay under 2-10kb policy size max, depending if user/group/role</li></ul><p>Use ACLs if you want to:</p><ul><li>Control access to buckets <em>and objects</em></li><li>Exceed 20kb policy size max</li><li>Continue using ACLs and you're happy with them</li></ul><p><a href=""https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/"" rel=""noreferrer"">https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/</a></p>",2121526,2121526,2018-06-04T21:00:08.983,2018-06-04T21:00:08.983,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/47815678
500,47855138,2,47845945,2017-12-17T12:33:57.403,1,"<p>Bear in mind that DyanmoDB now supports <a href=""https://aws.amazon.com/dynamodb/global-tables/"" rel=""nofollow noreferrer"">global tables</a> as <a href=""https://aws.amazon.com/about-aws/whats-new/2017/11/aws-launches-amazon-dynamodb-global-tables/"" rel=""nofollow noreferrer"">announced on 29 November 2017</a>. So you can probably replace your multitude of regional tables with global ones.</p><blockquote><p>Global Tables eliminates the difficult work of replicating databetween regions and resolving update conflicts, enabling you to focuson your applicationŠ—Ès business logic. In addition, Global Tablesenables your applications to stay highly available even in theunlikely event of isolation or degradation of an entire region.</p></blockquote><p>However to answer you queston directly, its not part of the AWS DynamoDB Java SDK. You have to use the dynamodb-cross-region-library from AWS labs.</p><blockquote><p>Cross-Region Replication</p><p>Important</p><p>AWS previously provided a cross-region replication solution based onAWS CloudFormation. This solution has now been deprecated in favor ofan open source command line tool. For more information, please referto the detailed instructions on</p><p>GitHub:</p></blockquote><p><a href=""https://github.com/awslabs/dynamodb-cross-region-library/blob/master/README.md"" rel=""nofollow noreferrer"">https://github.com/awslabs/dynamodb-cross-region-library/blob/master/README.md</a></p><blockquote><p>The DynamoDB cross-region replication solution uses the AmazonDynamoDB Cross-Region Replication Library. This library uses DynamoDBStreams to keep DynamoDB tables in sync across multiple regions innear real time. When you write to a DynamoDB table in one region,those changes are automatically propagated by the Cross-RegionReplication Library to your tables in other regions.</p><p>You can leverage the Cross-Region Replication Library in your ownapplications, to build your own replication solutions with DynamoDBStreams. For more information, and to download the source code, go tothe following GitHub repository:</p></blockquote><p><a href=""https://github.com/awslabs/dynamodb-cross-region-library"" rel=""nofollow noreferrer"">https://github.com/awslabs/dynamodb-cross-region-library</a></p>",4985580,-1,2020-06-20T09:12:55.060,2017-12-17T12:33:57.403,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47855138
501,47859094,2,47859052,2017-12-17T20:23:21.490,1,"<p>You're looking for <a href=""http://boto3.readthedocs.io/en/latest/reference/services/dynamodb.html#DynamoDB.Table.update_item"" rel=""nofollow noreferrer""><code>update_item()</code></a>. You should use <code>UpdateExpression</code> because <code>AttributeUpdates</code> is deprecated, but this simple example should get you started:</p><pre><code>response = table.update_item(  Key={  'Device': device,  },  AttributeUpdates={  'RequestList': {  'Alias': aliasInput,  'Date': date  },  'AvailableQuanity': 0,  'ReserveQuanity': 0,  },)</code></pre>",492773,,,2017-12-17T20:23:21.490,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47859094
502,47873654,2,47566748,2017-12-18T17:37:25.077,0,"<p>Personally I don't like the amazon documentation - I'm not sure if this is your case but, for me, most of the time the material there is outdated.</p><p>I'm using the aws client and the Lavelle community dynamo library to make some queries into DynamoDB via annotations / querydsl. </p><p>Below follows my pom.xml for a spring-boot/spring-data/dynamodb/swagger micro service.I hope it helps.</p><pre><code>&lt;projectxmlns=""http://maven.apache.org/POM/4.0.0""xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;sandbox-aws-DynamoDB&lt;/groupId&gt;&lt;artifactId&gt;sandbox-aws-DynamoDB&lt;/artifactId&gt;&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt;&lt;name&gt;sandbox-aws-DynamoDB&lt;/name&gt;&lt;url&gt;http://maven.apache.org&lt;/url&gt;&lt;properties&gt;  &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;  &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;  &lt;java.version&gt;1.8&lt;/java.version&gt;  &lt;spring-boot.version&gt;1.5.9.RELEASE&lt;/spring-boot.version&gt;  &lt;!--  Spring-boot 1.5.9.RELEASE DOES NOT WORK WITH spring-data-dynamodb 5.0.0  missing org.springframework.data.querydsl.QuerydslUtils  --&gt;  &lt;spring-data-dynamodb&gt;4.5.0&lt;/spring-data-dynamodb&gt;  &lt;aws-java-sdk-dynamodb&gt;1.11.136&lt;/aws-java-sdk-dynamodb&gt;  &lt;log4j.version&gt;1.3.8.RELEASE&lt;/log4j.version&gt;  &lt;swagger2.version&gt;2.7.0&lt;/swagger2.version&gt;  &lt;swagger.ui.version&gt;2.7.0&lt;/swagger.ui.version&gt;&lt;/properties&gt;&lt;dependencyManagement&gt;  &lt;dependencies&gt;  &lt;dependency&gt;  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt;  &lt;version&gt;${spring-boot.version}&lt;/version&gt;  &lt;type&gt;pom&lt;/type&gt;  &lt;scope&gt;import&lt;/scope&gt;  &lt;/dependency&gt;  &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;dependencies&gt;  &lt;!-- Spring Boot --&gt;  &lt;dependency&gt;  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;  &lt;/dependency&gt;  &lt;dependency&gt;  &lt;groupId&gt;de.codecentric&lt;/groupId&gt;  &lt;artifactId&gt;spring-boot-admin-starter-client&lt;/artifactId&gt;  &lt;version&gt;1.5.5&lt;/version&gt;  &lt;/dependency&gt;  &lt;!-- AWS DynamoDB --&gt;  &lt;dependency&gt;  &lt;groupId&gt;com.github.derjust&lt;/groupId&gt;  &lt;artifactId&gt;spring-data-dynamodb&lt;/artifactId&gt;  &lt;version&gt;${spring-data-dynamodb}&lt;/version&gt;  &lt;/dependency&gt;  &lt;dependency&gt;  &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;  &lt;artifactId&gt;aws-java-sdk-dynamodb&lt;/artifactId&gt;  &lt;version&gt;${aws-java-sdk-dynamodb}&lt;/version&gt;  &lt;/dependency&gt;  &lt;!-- Swagger --&gt;  &lt;dependency&gt;  &lt;groupId&gt;io.springfox&lt;/groupId&gt;  &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt;  &lt;version&gt;${swagger2.version}&lt;/version&gt;  &lt;/dependency&gt;  &lt;dependency&gt;  &lt;groupId&gt;io.springfox&lt;/groupId&gt;  &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt;  &lt;version&gt;${swagger.ui.version}&lt;/version&gt;  &lt;/dependency&gt;  &lt;!-- Commons --&gt;  &lt;dependency&gt;  &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;  &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;  &lt;version&gt;3.6&lt;/version&gt;  &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt;  &lt;finalName&gt;sandbox-aws-dynamodb&lt;/finalName&gt;  &lt;plugins&gt;  &lt;plugin&gt;  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;  &lt;executions&gt;  &lt;execution&gt;  &lt;goals&gt;  &lt;goal&gt;repackage&lt;/goal&gt;  &lt;/goals&gt;  &lt;/execution&gt;  &lt;/executions&gt;  &lt;/plugin&gt;  &lt;plugin&gt;  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;  &lt;configuration&gt;  &lt;delimiters&gt;  &lt;delimiter&gt;@&lt;/delimiter&gt;  &lt;/delimiters&gt;  &lt;useDefaultDelimiters&gt;false&lt;/useDefaultDelimiters&gt;  &lt;/configuration&gt;  &lt;/plugin&gt;  &lt;plugin&gt;  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;  &lt;configuration&gt;  &lt;source&gt;1.8&lt;/source&gt;  &lt;target&gt;1.8&lt;/target&gt;  &lt;/configuration&gt;  &lt;/plugin&gt;  &lt;plugin&gt;  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt;  &lt;version&gt;2.9&lt;/version&gt;  &lt;executions&gt;  &lt;execution&gt;  &lt;id&gt;attach-javadocs&lt;/id&gt;  &lt;goals&gt;  &lt;goal&gt;jar&lt;/goal&gt;  &lt;/goals&gt;  &lt;configuration&gt;  &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt;  &lt;failOnError&gt;false&lt;/failOnError&gt;  &lt;doclet&gt;org.umlgraph.doclet.UmlGraphDoc&lt;/doclet&gt;  &lt;docletPath&gt;/home/jorge/project/umlgraph-5.6.6.jar&lt;/docletPath&gt;  &lt;docletArtifact&gt;  &lt;groupId&gt;org.umlgraph&lt;/groupId&gt;  &lt;artifactId&gt;doclet&lt;/artifactId&gt;  &lt;version&gt;5.1&lt;/version&gt;  &lt;/docletArtifact&gt;  &lt;additionalparam&gt;-views&lt;/additionalparam&gt;  &lt;useStandardDocletOptions&gt;true&lt;/useStandardDocletOptions&gt;  &lt;/configuration&gt;  &lt;/execution&gt;  &lt;/executions&gt;  &lt;/plugin&gt;  &lt;plugin&gt;  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  &lt;artifactId&gt;maven-release-plugin&lt;/artifactId&gt;  &lt;version&gt;2.5.1&lt;/version&gt;  &lt;dependencies&gt;  &lt;dependency&gt;  &lt;groupId&gt;org.apache.maven.scm&lt;/groupId&gt;  &lt;artifactId&gt;maven-scm-provider-gitexe&lt;/artifactId&gt;  &lt;version&gt;1.9.2&lt;/version&gt;  &lt;/dependency&gt;  &lt;/dependencies&gt;  &lt;/plugin&gt;  &lt;/plugins&gt;  &lt;resources&gt;  &lt;resource&gt;  &lt;directory&gt;src/main/resources&lt;/directory&gt;  &lt;filtering&gt;true&lt;/filtering&gt;  &lt;/resource&gt;  &lt;/resources&gt;&lt;/build&gt;</code></pre><p></p>",958131,,,2017-12-18T17:37:25.077,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47873654
503,47897459,2,47897088,2017-12-20T01:11:09.353,1,"<p>Try creating the ApiKey without StageKeys at all. It isn't a required parameter, and per the <a href=""http://docs.aws.amazon.com/powershell/latest/reference/items/New-AGApiKey.html"" rel=""nofollow noreferrer"">New-AGApiKey documentation</a> this parameter has been deprecated in favor of <a href=""https://aws.amazon.com/blogs/aws/new-usage-plans-for-amazon-api-gateway/"" rel=""nofollow noreferrer"">usage plans</a>:</p><blockquote>  <p>-StageKey StageKey[]</p>   <p>DEPRECATED FOR USAGE PLANS - Specifies stages associated with the API key.<br>  Required? False<br>  Position? Named<br>  Accept pipeline input?  False</p></blockquote><p>This option has likewise been deprecated in the CLI and other SDKs.</p><p>If you still need to use StageKeys, the <a href=""http://docs.aws.amazon.com/sdkfornet/v3/apidocs/index.html?page=EC2/TEC2Filter.html&amp;tocid=Amazon_EC2_Model_Filter"" rel=""nofollow noreferrer"">Amazon.APIGateway.Model.StageKey type is defined here</a>, in the AWS SDK for .NET documentation. You can create a new instance of this type in powershell as described below, where a powershell with matching property names and values is used as input for the new object:</p><pre><code>$obj = New-Object Amazon.APIGateway.Model.StageKey -Property @{ RestApiId = ""myId""; StageName = ""myName"" }</code></pre><p>To verify the type is correct:</p><pre><code>$obj | get-member  TypeName: Amazon.APIGateway.Model.StageKeyName  MemberType Definition----  ---------- ----------Equals  Method   bool Equals(System.Object obj)GetHashCode Method   int GetHashCode()GetType   Method   type GetType()ToString  Method   string ToString()RestApiId   Property   string RestApiId {get;set;}StageName   Property   string StageName {get;set;}</code></pre>",775544,775544,2017-12-20T01:16:38.137,2017-12-20T01:16:38.137,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47897459
504,47901797,2,47839648,2017-12-20T08:36:45.673,1,"<p>The CertificateSource filed has been <a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CloudFront.html"" rel=""nofollow noreferrer"">deprecated</a> and instead we have to use one of the following:</p><p>ViewerCertificate$ACMCertificateArnViewerCertificate$IAMCertificateIdViewerCertificate$CloudFrontDefaultCertificate</p>",7753973,,,2017-12-20T08:36:45.673,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47901797
505,47945307,2,43994883,2017-12-22T17:35:08.107,1,"<p>A lot of the used API functions are deprecated and the AWS docu is ... I share a new implemented example. The Lambda function ""updateS3Chart"" calls another Lambda function ""AsyncUpdate"" asynchronous: </p><pre><code>public class LambdaInvoker {static final Logger logger = LogManager.getLogger(LambdaInvoker.class);static final String LambdaFunctionName = ""AsyncUpdate""; private class AsyncLambdaHandler implements AsyncHandler&lt;InvokeRequest, InvokeResult&gt;{  public void onSuccess(InvokeRequest req, InvokeResult res) {  logger.debug(""\nLambda function returned:"");  ByteBuffer response_payload = res.getPayload();  logger.debug(new String(response_payload.array()));  }  public void onError(Exception e) {  logger.debug(e.getMessage());  }}public void updateS3Chart(UpdateS3ChartRequest updateS3ChartRequest) {  Gson gson = new Gson();  try {  //issue: aws region is not set to debug-time. solution for eclipse:  //environment variable is set by lambda container or eclipse ide environment variables  //use instead for eclipse debugging: project -&gt; Run as -&gt; Run Configurations -&gt; Environment -&gt; Add variable: ""AWS_REGION"": ""eu-central-1""  AWSLambdaAsync lambda = AWSLambdaAsyncClientBuilder.defaultClient(); //Default async client using the DefaultAWSCredentialsProviderChain and DefaultAwsRegionProviderChain chain  InvokeRequest req = new InvokeRequest()  .withFunctionName(LambdaFunctionName)  .withPayload(gson.toJson(updateS3ChartRequest));  Future&lt;InvokeResult&gt; future_res = lambda.invokeAsync(req, new AsyncLambdaHandler());  logger.debug(""Waiting for async callback"");  while (!future_res.isDone() &amp;&amp; !future_res.isCancelled()) {  // perform some other tasks...  try {  Thread.sleep(1000);  }  catch (InterruptedException e) {  logger.debug(""Thread.sleep() was interrupted!"");  }  System.out.print(""."");  }  } catch (Exception e) {  logger.fatal(""Execute async lambda function: "" + LambdaFunctionName + "" failed: "" + e.getMessage());  }}}</code></pre><p>You have to set your AWS region as system property in your IDE for debugging (see comment in source code for Eclipse). UpdateS3ChartRequest is simple POJO with property set/get.</p>",9076928,9076928,2017-12-23T08:11:04.907,2017-12-23T08:11:04.907,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47945307
506,47999025,2,47998843,2017-12-27T21:46:37.663,1,"<p>The <a href=""http://docs.aws.amazon.com/athena/latest/ug/athena-sql-workbench.html"" rel=""nofollow noreferrer"">documentation at #1 is deprecated</a>, as you might gather from the different filenames in the two guides. Amazon hasn't fully cleaned up their docs, since switching from their homegrown JDBC driver to the driver they've OEMed from Simba.</p><p>You did not provide the JDBC URL you're using, so I cannot provide a specific correction, but the error message you got seems pretty clear -- you apparently didn't build your JDBC URL correctly.  It's missing the mandatory <code>AwsRegion</code> setting.</p><p>Note the URL syntax from the <a href=""https://s3.amazonaws.com/athena-downloads/drivers/JDBC/docs/Simba+Athena+JDBC+Driver+Install+and+Configuration+Guide.pdf"" rel=""nofollow noreferrer"">PDF guide for the JDBC driver</a> you're using --</p><p><code>jdbc:awsathena://AwsRegion=[Region];UID=[AccessKey];PWD=[SecretKey];S3OutputLocation=[Output];[Property1]=[Value1];[Property2]=[Value2];...</code></p>",241164,,,2017-12-27T21:46:37.663,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/47999025
507,48153828,2,48146202,2018-01-08T15:58:46.547,1,"<p>Once everything is provisioned, the CloudFormation part is done and the CloudFormation logs will only tell you about provisioning information. So the issue here is with the actual software and not with CloudFormation. </p><p>That being said, I happen to be quite experienced with Fleet and to get the logs for these services there's a couple things you can do.</p><p>First, try to query the logs using: </p><p><code>fleetctl journal httpd.service</code>, <code>fleetctl journal worker.1.service</code>, etc.</p><p>If that doesn't work, try to SSH into one host and use:<code>journalctl -u httpd.service</code></p><p>As final note, I don't know if you want to run this inside a production environment but I think you should know that <strong>fleetd</strong> is being deprecated.</p><blockquote>  <p>fleet is no longer developed or maintained by CoreOS. After February  1, 2018, a fleet container image will continue to be available from  the CoreOS Quay registry, but will not be shipped as part of Container  Linux. CoreOS instead recommends Kubernetes for all clustering needs.</p></blockquote><p>If I were you, I'd look for a similar solution running on either Kubernetes or AWS ECS.</p>",970247,,,2018-01-08T15:58:46.547,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48153828
508,48167392,2,35221631,2018-01-09T11:38:49.333,0,"<p>I needed to install all of the following packages to get <code>aclocal</code> running:</p><pre><code>apt install automakeapt install autoconfapt install m4apt install perlapt install libtool</code></pre><hr><p>This was the error message that I received when I tried to install ssdeep:</p><pre><code>WARNING: 'aclocal-1.13' is missing on your system.  You should only need it if you modified 'acinclude.m4' or  'configure.ac' or m4 files included by 'configure.ac'.  The 'aclocal' program is part of the GNU Automake package:  &lt;http://www.gnu.org/software/automake&gt;  It also requires GNU Autoconf, GNU m4 and Perl in order to run:  &lt;http://www.gnu.org/software/autoconf&gt;  &lt;http://www.gnu.org/software/m4/&gt;  &lt;http://www.perl.org/&gt;Makefile:426: recipe for target 'aclocal.m4' failedmake: *** [aclocal.m4] Error 127/bin/sh: 1: libtoolize: not found/usr/bin/m4:aclocal.m4:1069: cannot open `m4/libtool.m4': No such file or directory/usr/bin/m4:aclocal.m4:1070: cannot open `m4/ltoptions.m4': No such file or directory/usr/bin/m4:aclocal.m4:1071: cannot open `m4/ltsugar.m4': No such file or directory/usr/bin/m4:aclocal.m4:1072: cannot open `m4/ltversion.m4': No such file or directory/usr/bin/m4:aclocal.m4:1073: cannot open `m4/lt~obsolete.m4': No such file or directoryautom4te: /usr/bin/m4 failed with exit status: 1automake: error: autoconf failed with exit status: 1Failed while building ssdeep lib with configure and make.Retry with autoreconf ...Failed to reconfigure the project build.</code></pre>",3459910,,,2018-01-09T11:38:49.333,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48167392
509,48168516,2,48135471,2018-01-09T12:44:46.663,25,"<p>Update: this has been answered on the related <a href=""https://forums.aws.amazon.com/message.jspa?messageID=823536"" rel=""noreferrer"">aws forum post</a> which confirms that it's normal behaviour for network load balancers and cites their distributed nature as the reason. There is no way to configure a custom interval. At this moment, the docs are still out of date and specify otherwise.</p><hr><p>This is either a bug in NLB Target Groups, or normal behaviour with incorrect <a href=""https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html"" rel=""noreferrer"">documentation</a>. I've come to this conclusion because:</p><ul><li>I verified that the health checks are coming from the NLB</li><li>The configuration options are greyed out on the console<ul><li>inferring that AWS know about or imposed this limitation</li></ul></li><li>The same results are being observed by <a href=""https://stackoverflow.com/questions/46909388/aws-nlb-ignores-interval-setting"">others</a></li><li>The documentation is specifically for Network Load Balancers</li><li>AWS docs commonly lead you on a wild goose chase</li></ul><p>In this case I think it might be normal behaviour that's been documented incorrectly, but there's no way of verifying that unless someone from AWS can, and it's almost impossible to get an answer for an <a href=""https://forums.aws.amazon.com/message.jspa?messageID=823536"" rel=""noreferrer"">issue like this</a> on the aws forum.</p><p>It would be useful to be able to configure the setting, or at least have the docs updated.</p>",4206678,4206678,2018-02-08T13:06:09.030,2018-02-08T13:06:09.030,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48168516
510,48202632,2,48197756,2018-01-11T08:27:23.887,4,"<p>As your ELB will act as a proxy you'll have to configure Spring (and the server you're running on) to use the forward headers by setting <code>server.use-forward-headers</code> to <code>true</code> in your <code>application.properties</code>. See <a href=""https://docs.spring.io/spring-boot/docs/1.5.21.RELEASE/reference/html/howto-embedded-servlet-containers.html#howto-use-tomcat-behind-a-proxy-server"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-boot/docs/1.5.21.RELEASE/reference/html/howto-embedded-servlet-containers.html#howto-use-tomcat-behind-a-proxy-server</a> for more information. </p><p><strong>Edit:</strong> As of <strong>Spring Boot 2.2</strong> the <code>server.use-forward-headers</code> property has been deprecated in favor for <code>server.forward-headers-strategy</code>. See  <a href=""https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.2-Release-Notes#deprecations-in-spring-boot-22"" rel=""nofollow noreferrer"">https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.2-Release-Notes#deprecations-in-spring-boot-22</a></p>",1616589,1616589,2020-01-20T08:31:57.213,2020-01-20T08:31:57.213,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/48202632
511,48213414,2,44391817,2018-01-11T18:10:53.497,0,"<p><strong>EDIT: This answer is deprecated. Check the other answers.</strong></p><p>No, <br>There is no way to get all resources within your account in one go. Each region is independent and for some services like IAM concept of a region does not exist at all.Although there are API calls available to list down resources and services.<br><b>For example:</b><br><li>To get list of all available regions for your account:</p><pre><code>output, err := client.DescribeRegions(&amp;ec2.DescribeRegionsInput{})</code></pre><p><li>To get list of IAM users, roles or group you can use:</p><p><code>client.GetAccountAuthorizationDetails(&amp;iam.GetAccountAuthorizationDetailsInput{})</code></p><p>You can find more detail about API calls and their use at:<a href=""https://docs.aws.amazon.com/sdk-for-go/api/service/iam/#IAM.AttachGroupPolicy"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sdk-for-go/api/service/iam/</a></p><p>Above link is only for IAM. Similarly, you can find API for all other resources and services.</p>",6621961,2495341,2020-02-03T06:38:07.373,2020-02-03T06:38:07.373,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/48213414
512,48314242,2,27669927,2018-01-18T05:12:34.170,54,"<blockquote>  <p>Note: This may be obsolete for current versions of Amazon Linux 2 since late 2018 (see comments), you can now directly install it via <code>yum install python3</code>.</p></blockquote><p>In Amazon Linux <strong>2</strong>, there isn't a <code>python3[4-6]</code> in the default yum repos, instead there's the <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-linux-ami-basics.html#extras-library"" rel=""noreferrer"">Amazon Extras Library</a>. </p><pre><code>sudo amazon-linux-extras install python3</code></pre><p>If you want to set up isolated virtual environments with it; using <code>yum install</code>'d <code>virtualenv</code> tools don't seem to reliably work.</p><p><strike></p><pre><code>virtualenv --python=python3 my_venv</code></pre><p></strike></p><p>Calling the venv module/tool is less finicky, and you could double check it's what you want/expect with <code>python3 --version</code> beforehand.</p><pre><code>python3 -m venv my_venv</code></pre><hr><p>Other things it can install (versions as of 18 Jan 18):</p><pre><code>[ec2-user@x ~]$ amazon-linux-extras list  0  ansible2   disabled  [ =2.4.2 ]  1  emacs   disabled  [ =25.3 ]  2  memcached1.5   disabled  [ =1.5.1 ]  3  nginx1.12   disabled  [ =1.12.2 ]  4  postgresql9.6   disabled  [ =9.6.6 ]  5  python3=latest  enabled  [ =3.6.2 ]  6  redis4.0   disabled  [ =4.0.5 ]  7  R3.4   disabled  [ =3.4.3 ]  8  rust1   disabled  [ =1.22.1 ]  9  vim   disabled  [ =8.0 ] 10  golang1.9   disabled  [ =1.9.2 ] 11  ruby2.4   disabled  [ =2.4.2 ] 12  nano   disabled  [ =2.9.1 ] 13  php7.2   disabled  [ =7.2.0 ] 14  lamp-mariadb10.2-php7.2   disabled  [ =10.2.10_7.2.0 ]</code></pre>",194586,194586,2019-04-03T17:38:46.737,2019-04-03T17:38:46.737,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/48314242
513,48317094,2,48314880,2018-01-18T08:47:08.897,7,"<p>Parquet file written by <code>pyarrow</code> (long name: Apache Arrow) are compatible with Apache Spark. But you have to be careful which datatypes you write into the Parquet files as Apache Arrow supports a wider range of them then Apache Spark does. There is currently a flag <code>flavor=spark</code> in <code>pyarrow</code> that you can use to automatically set some compatibility options so that Spark can read these files in again. Sadly in the latest release, this option is not sufficient (expect to change with <code>pyarrow==0.9.0</code>). You should take care to write out timestamps using the deprecated INT96 type (<code>use_deprecated_int96_timestamps=True</code>) as well as avoiding unsigned integer columns. For the unsigned integer columns, convert them simply to a signed integer. Sadly Spark errors out if you have a unsigned type in your schema instead of just loading them as signed (they are actually always stored as signed, but only marked with a flag as unsigned). Respecting these two things, the files should be readable in Apache Spark and AWS Athena (which is just Presto under the hood).</p>",1689261,,,2018-01-18T08:47:08.897,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48317094
514,48378367,2,48378147,2018-01-22T09:32:40.703,1,"<p>File access for the RDS instance is forbidden.Access to the DATA_PUMP_DIR directory only through the db_link and use DBMS_FILE_TRANSFER package.</p><ul><li>Option 1</li></ul><p>You can do the export of data using the old exp utility on the EC2 instance, this utility also creates export files <code>.dmp</code>, but for a different format. The format is not compatible with <code>impdp expdp</code>.The <code>exp imp</code> utility can connect over the SQL*NET network to the target database as client-server. This utility is obsolete and has less performance. The <code>dmp</code> file is not created on the server, as when running the utility <code>expdp</code>. The <code>dmp</code> file is written on the side where the utility <code>exp</code> is run (server or client)</p><p><code>$ORACLE_HOME/bin/exp parfile=parfile_exp_full FILE=export.dmp LOG=export.log</code></p><p>And then do the data import using the <code>imp</code> to RDS instance.</p><pre><code>$ORACLE_HOME/bin/imp parfile=parfile_imp_full FILE=export.dmp LOG=import.log</code></pre><ul><li>Option 2</li></ul><p>You can export the data to an CSV file using the utility <code>$ORACLE_HOME/bin/sqlplus -s user/pass@ec2  @csv2.sql</code>.</p><pre><code>set heading offset termout OFFSET FEEDBACK OFFSET TAB OFFset pause offset verify offSET UNDERLINE OFFset trimspool onset echo offset linesize 1000set pagesize 0set wrap offspool test2.csvselect code||','||name||','||code_rail from alexs.all_station;spool offexit;</code></pre><p>And then make the data import to RDS instance using the utility <code>sqlldr</code>.</p>",7734732,7734732,2018-01-22T12:59:22.253,2018-01-22T12:59:22.253,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48378367
515,48381123,2,48379925,2018-01-22T11:59:34.310,3,"<p>This is the problem...</p><pre><code>// You're sending an asynchronous HTTP request here.request();// But you sent the response here without waiting for the above request to finish.context.succeed();</code></pre><p>Basically, you're executing <code>context.succeed()</code> before <code>request()</code> finishes. So you're basically ending your Lambda invocation without the response from that HTTP request.</p><p>To fix your code, put the <code>context.succeed()</code> <strong><em>inside</em></strong> the callback that you pass to the <code>request()</code> call.</p><p>P.S. </p><p>You should be using <code>callback</code> instead of the deprecated <code>context.succeed()</code>/<code>context.fail()</code> API.</p>",1252647,1252647,2018-01-22T15:59:33.130,2018-01-22T15:59:33.130,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48381123
516,48433056,2,48341064,2018-01-24T22:54:57.280,1,"<p>Use <code>callback()</code> instead of <code>context.succeed()</code> or <code>context.fail()</code>. Those context methods are deprecated.</p><p>According to <a href=""https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html#transition-to-new-nodejs-runtime"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html#transition-to-new-nodejs-runtime</a>,</p><blockquote>  <p>Node.js runtime v0.10.42 does not support the callback parameter for your Lambda function that runtimes v4.3 and v6.10 support. When using runtime v0.10.42, you use the following context object methods to properly terminate your Lambda function. The context object supports the done(), succeed(), and fail() methods that you can use to terminate your Lambda function. These methods are also present in runtimes v4.3 and v6.10 for backward compatibility.</p></blockquote><p>So, in your callback for <code>sendMail()</code>, it becomes...</p><pre><code>if(err) {  console.log(err);  callback(err);} else {  console.log(""===EMAIL SENT==="");  console.log(""EMAIL CODE END"");  console.log('EMAIL: ', email);  console.log(data);  callback(null, event);}</code></pre><p>Another thing, you don't need <code>context.callbackWaitsForEmptyEventLoop = false;</code>.</p><p>Also, since you connect to the database <code>inside</code> your handler, you also need to disconnect from it after you get your results.</p>",1252647,,,2018-01-24T22:54:57.280,6,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48433056
517,48469199,2,48397691,2018-01-26T20:39:30.397,0,"<p>What you could do is create a parameter that provides a list to pick from. </p><p>My below example allows you to pick 3 OS types within a specific region. I hard-coded the AMI identifiers so this may be out of date if AWS changes the AMI ID. The template uses a map to select the proper AMI ID from the value specified in the parameter and the region that the template is ran in. You could easily scale this up for many different regions or OS types.</p><pre><code>{  ""AWSTemplateFormatVersion"": ""2010-09-09"",  ""Parameters"": {  ""osType"": {  ""Description"": ""OS Type"",  ""Type"": ""String"",  ""AllowedValues"": [  ""Server2016"",  ""SUSE"",  ""RHEL""  ],  ""ConstraintDescription"": ""must be a prod or test""  }  },  ""Mappings"": {  ""RegionAndInstanceTypeToAMIID"": {  ""us-east-1"": {  ""Server2016"": ""ami-8ff710e2"",  ""SUSE"": ""ami-f5f41398"",  ""RHEL"": ""ami-26ebbc5c""  },  ""us-west-2"": {  ""Server2016"": ""ami-8ff710e2"",  ""SUSE"": ""ami-f5f41398"",  ""RHEL"": ""ami-26ebbc5c""  }  }  },  ""Resources"": {  ""testInstance"": {  ""Type"": ""AWS::EC2::Instance"",  ""Properties"": {  ""ImageId"": {  ""Fn::FindInMap"": [  ""RegionAndInstanceTypeToAMIID"",  {  ""Ref"": ""AWS::Region""  },  {  ""Ref"": ""osType""  }  ]  }  }  }  }}</code></pre>",9274337,,,2018-01-26T20:39:30.397,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48469199
518,48518874,2,48267479,2018-01-30T10:14:03.367,0,"<p>As far as I can tell your problem with <a href=""https://docs.rs/futures/0.1.18/futures/sync/mpsc/fn.channel.html"" rel=""nofollow noreferrer""><code>channel</code></a> is not that a single clone of the <a href=""https://docs.rs/futures/0.1.18/futures/sync/mpsc/struct.Sender.html"" rel=""nofollow noreferrer""><code>Sender</code></a> increase the capacity by one, it is that you clone the <code>Sender</code> for every item you're trying to send.</p><p>The error you're seeing without <code>clone</code> comes from your incorrect usage of the <code>Sink::send</code> interface.  With <code>clone</code> you actually should see the warning:</p><pre class=""lang-none prettyprint-override""><code>warning: unused `futures::sink::Send` which must be used: futures do nothing unless polled</code></pre><p>That is: your current code doesn't actually ever send anything!</p><p>In order to apply backpressure you need to chain those <code>send</code> calls; each one should wait until the previous one finished (and you need to wait for the last one too!); on success you'll get the <code>Sender</code> back.  The best way to do this is to generate a <code>Stream</code> from your iterator by using <a href=""https://docs.rs/futures/0.1.18/futures/stream/fn.iter_ok.html"" rel=""nofollow noreferrer""><code>iter_ok</code></a> and to pass it to <a href=""https://docs.rs/futures/0.1.18/futures/sink/trait.Sink.html#method.send_all"" rel=""nofollow noreferrer""><code>send_all</code></a>.</p><p>Now you got one future <code>SendAll</code> that you need to &quot;drive&quot;.  If you ignore the result and panic on error (<code>.then(|r| { r.unwrap(); Ok::&lt;(), ()&gt;(()) })</code>) you could spawn it as a separate task, but maybe you want to integrate it into your main application (i.e. return it in a <code>Box</code>).</p><pre class=""lang-rust prettyprint-override""><code>// this returns a `Box&lt;Future&lt;Item = (), Error = ()&gt;&gt;`. you may// want to use a different error typeBox::new(tx.send_all(iter_ok(data)).map(|_| ()).map_err(|_| ()))</code></pre><h2><code>RusotoFuture::sync</code> and <code>Future::wait</code></h2><p>Don't use <a href=""https://docs.rs/futures/0.1.18/futures/future/trait.Future.html#method.wait"" rel=""nofollow noreferrer""><code>Future::wait</code></a>: it is already deprecated in a branch, and it usually won't do what you actually are looking for.  I doubt <code>RusotoFuture</code> is aware of the problems, so I recommend avoiding <code>RusotoFuture::sync</code>.</p><h2>Cloning <code>Sender</code> increases channel capacity</h2><p>As you correctly stated cloning <code>Sender</code> increases the capacity by one.</p><p>This seems to be done to improve performance: A <code>Sender</code> starts in the unblocked (&quot;unparked&quot;) state; if a <code>Sender</code> isn't blocked it can send an item without blocking.  But if the number of items in the queue hits the configured limit when a <code>Sender</code> sends an item, the <code>Sender</code> becomes blocked (&quot;parked&quot;).  (Removing items from the queue will unblock the <code>Sender</code> at a certain time.)</p><p>This means that after the inner queue hits the limit each <code>Sender</code> still can send one item, which leads to the documented effect of increased capacity, but only if actually all the <code>Sender</code>s are sending items - unused <code>Sender</code>s don't increase the observed capacity.</p><p>The performance boost comes from the fact that as long as you don't hit the limit it doesn't need to park and notify tasks (which is quite heavy).</p><p>The private documentation at the top of the <a href=""https://docs.rs/futures/0.1.18/src/futures/sync/mpsc/mod.rs.html#30"" rel=""nofollow noreferrer""><code>mpsc</code></a> module describes more of the details.</p>",1478356,-1,2020-06-20T09:12:55.060,2018-01-30T10:20:11.243,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48518874
519,48587332,2,44975588,2018-02-02T16:50:42.577,1,"<p>You can do it by <code>client.setRegion(region)</code> something like:</p><p><code>AmazonS3Client client = new AmazonS3Client(new BasicAWSCredentials(accessKey, secretKey));client.setRegion(region);return client;</code></p><p>Alternatively, <code>new AmazonS3Client(new BasicAWSCredentials(accessKey, secretKey))</code> picks the Region information from the following, if you haven't provided (it tries searching from 1 to 3) :<p>1.) EnvironmentVariableCredentialsProvider()<p>2.) SystemPropertiesCredentialsProvider()<p>3.) EC2 instance</p><p>But this method is deprecated now, you should use now:</p><p><code>AmazonS3 amazonS3 = AmazonS3ClientBuilder.standard()  .withRegion(Regions.US_EAST_1)  .build();</code></p>",5860781,,,2018-02-02T16:50:42.577,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48587332
520,48589579,2,48588402,2018-02-02T19:25:09.197,2,"<p>The default constructors have fixed endpoints, which (at least for the services I've used) are in <code>us-east-1</code>. You can change to a different region by calling the <code>setRegion()</code> or <code>setEndpoint()</code> method.</p><p>However, the default constructors have been deprecated since version 1.11.11. You should now use a client builder (such as <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3ClientBuilder.html"" rel=""nofollow noreferrer"">AmazonS3ClientBuilder</a>), which will look for a configured region. Unless you have a good reason to do otherwise, call <code>AmazonS3ClientBuilder.defaultClient()</code>.</p>",9305910,,,2018-02-02T19:25:09.197,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48589579
521,48622583,2,31466916,2018-02-05T12:26:22.877,2,"<p>Some methods have been deprecated in AWS SDK. </p><pre><code>BasicAWSCredentials awsCredentials = new BasicAWSCredentials(""CLIENT-ID"", ""SECRET-KEY"");AmazonSNS snsClient = AmazonSNSClientBuilder.standard()  .withRegion(Regions.fromName(""YOUR_REGION""))  .withCredentials(new AWSStaticCredentialsProvider(awsCredentials)).build();Map&lt;String, MessageAttributeValue&gt; smsAttributes = new HashMap&lt;String, MessageAttributeValue&gt;();smsAttributes.put(""AWS.SNS.SMS.SenderID"",new MessageAttributeValue().withStringValue(""SENDER-ID"").withDataType(""String"");smsAttributes.put(""AWS.SNS.SMS.SMSType"",new MessageAttributeValue().withStringValue(""Transactional"").withDataType(""String""));PublishRequest request = new PublishRequest();request.withMessage(""YOUR MESSAGE"").withPhoneNumber(""E.164-PhoneNumber"").withMessageAttributes(smsAttributes);PublishResult result=snsClient.publish(request);</code></pre>",4356145,,,2018-02-05T12:26:22.877,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48622583
522,48709151,2,38578937,2018-02-09T15:32:52.413,8,"<p>The accepted answer is using a deprecated APIs. Here's an updated revision.</p><p>First, update your maven dependencies:</p><pre><code> &lt;dependency&gt;  &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;  &lt;artifactId&gt;aws-java-sdk&lt;/artifactId&gt;  &lt;version&gt;1.11.274&lt;/version&gt;  &lt;/dependency&gt;</code></pre><p>AWSConfiguration.java</p><pre><code>import com.amazonaws.auth.AWSCredentials;import com.amazonaws.auth.AWSStaticCredentialsProvider;import com.amazonaws.auth.BasicAWSCredentials;import com.amazonaws.services.s3.AmazonS3;import com.amazonaws.services.s3.AmazonS3ClientBuilder;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class AWSConfiguration {  @Value(""${cloud.aws.credentials.accessKey}"")  private String accessKey;  @Value(""${cloud.aws.credentials.secretKey}"")  private String secretKey;  @Value(""${cloud.aws.region}"")  private String region;  @Bean  public BasicAWSCredentials basicAWSCredentials() {  return new BasicAWSCredentials(accessKey, secretKey);  }  @Bean  public AmazonS3 amazonS3Client(AWSCredentials awsCredentials) {  AmazonS3ClientBuilder builder = AmazonS3ClientBuilder.standard();  builder.withCredentials(new AWSStaticCredentialsProvider(awsCredentials));  builder.setRegion(region);  AmazonS3 amazonS3 = builder.build();  return amazonS3;  }}</code></pre><p>S3Service.java</p><pre><code>import com.amazonaws.services.s3.AmazonS3;import com.amazonaws.services.s3.AmazonS3Client;import com.amazonaws.services.s3.model.*;import org.apache.commons.io.IOUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.http.HttpHeaders;import org.springframework.http.HttpStatus;import org.springframework.http.MediaType;import org.springframework.http.ResponseEntity;import org.springframework.stereotype.Service;import org.springframework.util.StringUtils;import org.springframework.web.multipart.MultipartFile;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.io.InputStream;import java.net.URLEncoder;import java.util.ArrayList;import java.util.Arrays;import java.util.List;@Servicepublic class S3Service {  @Autowired  private AmazonS3 amazonS3;  @Value(""${cloud.aws.s3.bucket}"")  private String bucket;  private PutObjectResult upload(String filePath, String uploadKey) throws FileNotFoundException {  return upload(new FileInputStream(filePath), uploadKey);  }  private PutObjectResult upload(InputStream inputStream, String uploadKey) {  PutObjectRequest putObjectRequest = new PutObjectRequest(bucket, uploadKey, inputStream, new ObjectMetadata());  putObjectRequest.setCannedAcl(CannedAccessControlList.PublicRead);  PutObjectResult putObjectResult = amazonS3.putObject(putObjectRequest);  IOUtils.closeQuietly(inputStream);  return putObjectResult;  }  public List&lt;PutObjectResult&gt; upload(MultipartFile[] multipartFiles) {  List&lt;PutObjectResult&gt; putObjectResults = new ArrayList&lt;&gt;();  Arrays.stream(multipartFiles)  .filter(multipartFile -&gt; !StringUtils.isEmpty(multipartFile.getOriginalFilename()))  .forEach(multipartFile -&gt; {  try {  putObjectResults.add(upload(multipartFile.getInputStream(), multipartFile.getOriginalFilename()));  } catch (IOException e) {  e.printStackTrace();  }  });  return putObjectResults;  }  public ResponseEntity&lt;byte[]&gt; download(String key) throws IOException {  GetObjectRequest getObjectRequest = new GetObjectRequest(bucket, key);  S3Object s3Object = amazonS3.getObject(getObjectRequest);  S3ObjectInputStream objectInputStream = s3Object.getObjectContent();  byte[] bytes = IOUtils.toByteArray(objectInputStream);  String fileName = URLEncoder.encode(key, ""UTF-8"").replaceAll(""\\+"", ""%20"");  HttpHeaders httpHeaders = new HttpHeaders();  httpHeaders.setContentType(MediaType.APPLICATION_OCTET_STREAM);  httpHeaders.setContentLength(bytes.length);  httpHeaders.setContentDispositionFormData(""attachment"", fileName);  return new ResponseEntity&lt;&gt;(bytes, httpHeaders, HttpStatus.OK);  }  public List&lt;S3ObjectSummary&gt; list() {  ObjectListing objectListing = amazonS3.listObjects(new ListObjectsRequest().withBucketName(bucket));  List&lt;S3ObjectSummary&gt; s3ObjectSummaries = objectListing.getObjectSummaries();  return s3ObjectSummaries;  }}</code></pre>",1019952,,,2018-02-09T15:32:52.413,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48709151
523,48715352,2,47398375,2018-02-09T23:00:10.543,1,"<p>Unless I am misunderstanding... Even though terraform refresh would update the state file it would not update your *.tf files which would still be out of date. (Which would break the concept of IAC).</p><p>On AWS you can use AWS Config to see any changes that have happened to your infrastructure after your initial deployment to reconcile the differences.</p><p>You can also use cloudformer to get a current cloudformation template of your infrastructure. <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html"" rel=""nofollow noreferrer"">CloudFormer</a></p>",4453351,,,2018-02-09T23:00:10.543,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48715352
524,48742904,2,48740949,2018-02-12T09:15:39.140,0,"<p>Firstly, I notice that your <code>IntegrationHttpMethod</code> is <code>ANY</code>. For Lambdas, unless you're using the <code>{proxy+}</code> configuration, try using <code>POST</code>. I'm pretty sure the CloudFormation docs are still out of date for this, but you'll find some useful info in <a href=""https://stackoverflow.com/a/39772260/4206678"">this answer</a>.</p><p>The second thing that I notice is the malformed proxy response, which in your case could just be a misconfiguration. Just to rule this out, dealing with a malformed proxy response is <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/malformed-502-api-gateway/"" rel=""nofollow noreferrer"">answered</a> on the AWS support centre. Basically, your lambda response should be in the following format, including not adding any extra keys.</p><pre><code>{  ""isBase64Encoded"": true|false,  ""statusCode"": httpStatusCode,  ""headers"": { ""headerName"": ""headerValue"", ... },  ""body"": ""...""}</code></pre><p>You could use the example function on the support doc in place of your regular function so that you can isolate the problem.</p>",4206678,,,2018-02-12T09:15:39.140,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48742904
525,48777760,2,48777467,2018-02-14T00:14:33.117,7,"<p>First of all, the devices aren't ""always <em>recording</em>"". They are ""always <em>listening</em>"". Anything that is stored in memory is for a very short period of time - just long enough to see if it is the triggering hotword and/or to forward anything else said once triggered.</p><p>There is no authorized way for you to tap into this mic.</p><p>Similarly, neither device gives you access to the audio recorded through a server - they just send you text transcripts. Google does let you listen to what was recorded on their servers, but you can't access it as a third-party.</p><p>On the Alexa, there probably isn't much you can do. It won't forward text that doesn't match one of the Intents, and open-ended text is deprecated and discouraged.</p><p>With the Assistant, using the Action SDK, you can get most of the text spoken. But there are a few caveats. Although you'll get text, you won't get who spoke the text, nor the timing of things (was it spoken slowly? quickly? was there a long pause between a question and response?). The Assistant will also send things in batches, either when it thinks you've said ""enough"" or when it thinks you've finished what you're saying, and your server then needs to send back a response (which could be as simple as a very short audio file), but while it is ""processing"", you'll miss everything that was said.</p><p>In short - while possible, the current hardware and system software is unsuited to this sort of application.</p>",1405634,1405634,2018-02-14T09:56:30.143,2018-02-14T09:56:30.143,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48777760
526,48801374,2,48794500,2018-02-15T06:48:51.307,2,"<p>There are several ways to solve this problem.<strong>First option.</strong></p><ol><li>Install a free database version of the Oracle XE version on EC2instance(It is very easy and fast)</li><li>Export a schema from the RDS instance to DATA_PUMP_DIRdirectory. Use <code>DBMS_DATAPUMP</code> package  or run <code>expdp user/pass@rds</code> on EC2 to create a dump file.</li><li>Create database link on RDS instance between RDS DB and Oracle XEDB.</li></ol><blockquote>  <p>If you are creating a database link between two DB instances inside  the same VPC or peered VPCs the two DB instances should have a valid  route between them.  <a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.CommonDBATasks.Database.html#Appendix.Oracle.CommonDBATasks.DBLinks"" rel=""nofollow noreferrer"">Adjusting Database Links for Use with DB Instances in a VPC</a></p></blockquote><ol start=""4""><li><p>Copy the dump files from RDS instance to Oracle XE DB on EC2 usesthe <code>DBMS_FILE_TRANSFER.PUT_FILE</code> via database link</p></li><li><p>Copy files from the <code>DATA_PUMP_DIR</code> directory Oracle XE on EC2 instance to the S3.</p></li></ol><p><strong>Second option.</strong>Use the obsolete utility <code>exp</code> to export. It has restrictions on the export of certain types of data and is slower. </p><ol><li>Run <code>exp user/password@rds</code> on EC2 instance.</li><li>Copy files from the directory Oracle XE on EC2 instance to the S3</li></ol><blockquote>  <p>Original export is desupported for general use as of Oracle Database  11g. The only supported use of Original Export in 11g is backward  migration of XMLType data to a database version 10g release 2 (10.2)  or earlier. Therefore, Oracle recommends that you use the new Data  Pump Export and Import utilities, except in the following situations  which require Original Export and Import:  <a href=""https://docs.oracle.com/cd/B28359_01/server.111/b28319/exp_imp.htm#g1070082"" rel=""nofollow noreferrer"">Original Export and Import</a></p></blockquote>",7734732,7734732,2018-02-15T07:50:28.207,2018-02-15T07:50:28.207,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48801374
527,48818173,2,42723189,2018-02-15T23:55:10.547,0,"<p>You can trigger the deployment directly in your user-data in the </p><pre><code>resource ""aws_launch_configuration"" ""my-application"" {  name   = ""my-application""  ...  user_data  = ""${data.template_file.node-init.rendered}""}data ""template_file"" ""node-init"" {  template = ""${file(""${path.module}/node-init.yaml"")}""}</code></pre><p>Content of my node-init.yaml, following recommendations of this documentation: <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/codedeploy-agent-launch-configuration/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/codedeploy-agent-launch-configuration/</a></p><pre><code>write_files: - path: /root/configure.shcontent: |  #!/usr/bin/env bash  REGION=$(curl 169.254.169.254/latest/meta-data/placement/availability-zone/ | sed 's/[a-z]$//')  yum update -y  yum install ruby wget -y  cd /home/ec2-user  wget https://aws-codedeploy-$REGION.s3.amazonaws.com/latest/install  chmod +x ./install  ./install auto  # Add the following line for your node to update itself  aws deploy create-deployment --application-name=&lt;my-application&gt; --region=ap-southeast-2 --deployment-group-name=&lt;my-deployment-group&gt; --update-outdated-instances-onlyruncmd:  - bash /root/configure.sh</code></pre><p>In this implementation the node is responsible for triggering the deployment itself. This is working perfectly so far for me but can result in deployment fails if the ASG is creating several instances at the same time (in that case the failed instances will be terminated quickly because not healthy).</p><p>Of course, you need to add the sufficient permissions to the role associated to your nodes to trigger the deployment.</p><p>This is still a workaround and if someone knows solution behaving the same way as cfn-init, I am interested.</p>",9091770,9091770,2018-02-16T00:00:29.183,2018-02-16T00:00:29.183,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48818173
528,48844091,2,48842054,2018-02-17T17:40:43.397,0,"<p>Its working now. The above issue is because I was using a deprecated Lambda API and there were few mistakes in usage of Lambda.</p><p>With below code I am able to save a JSON payload in to Atlas MongoDB instance. </p><p>If you run this inside an VPC, make sure necessary network policies are in place. Otherwise you will receive a Socket error in logs and below error in Eclipse console.</p><blockquote>  <p>==================== INVOCATION ERROR ==================== com.amazonaws.SdkClientException: Unable to execute HTTP request: Read  timed out</p></blockquote><p>My complete code, I am sure this will help others who are new to Lambda and MongoDB Atlas.</p><pre><code>package com.amazonaws.lambda.demo;import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;public class LambdaCreateUser implements RequestHandler&lt;RegistrationRequest, String&gt; {  public String handleRequest(RegistrationRequest input, Context context) {  context.getLogger().log(""Input: "" + input.getFirstName());  Boolean userStatus = false;  try {  CognitoSaveUser newUser = new CognitoSaveUser();  userStatus = newUser.saveNewUserInDB(input, context);  } catch (Exception e) {  // TODO Auto-generated catch block  e.printStackTrace();  }  return ""Hello from Lambda-""+userStatus.toString();  }} </code></pre><p>Don't forget to create the 2 constructors at the end of this bean class. I lost hours without those.package com.amazonaws.lambda.demo;</p><pre><code>public class RegistrationRequest {  String firstName;  String lastName;  String department;  String userName;  String userPassword;  String confirmPassword;  String email;  String contactNo;  public String getFirstName() {  return firstName;  }  public void setFirstName(String firstName) {  this.firstName = firstName;  }  public String getLastName() {  return lastName;  }  public void setLastName(String lastName) {  this.lastName = lastName;  }  public String getDepartment() {  return department;  }  public void setDepartment(String department) {  this.department = department;  }  public String getUserName() {  return userName;  }  public void setUserName(String userName) {  this.userName = userName;  }  public String getUserPassword() {  return userPassword;  }  public void setUserPassword(String userPassword) {  this.userPassword = userPassword;  }  public String getConfirmPassword() {  return confirmPassword;  }  public void setConfirmPassword(String confirmPassword) {  this.confirmPassword = confirmPassword;  }  public String getEmail() {  return email;  }  public void setEmail(String email) {  this.email = email;  }  public String getContactNo() {  return contactNo;  }  public void setContactNo(String contactNo) {  this.contactNo = contactNo;  }  public RegistrationRequest(String firstName, String lastName, String department, String userName, String userPassword, String confirmPassword, String email, String contactNo) {  this.firstName = firstName;  this.lastName = lastName;  this.department = department;  this.userName = userName;  this.userPassword = userPassword;  this.confirmPassword = confirmPassword;  this.email = email;  this.contactNo = contactNo;  }  public RegistrationRequest() {  }}</code></pre><p>The final class which will do all the magic. Don't forget to put the correct username, password and dbname for MongoDB URI</p><pre><code>package com.amazonaws.lambda.demo;import com.amazonaws.services.lambda.runtime.Context;import com.fasterxml.jackson.core.JsonFactory;import com.fasterxml.jackson.core.JsonParseException;import com.fasterxml.jackson.core.JsonParser;import com.fasterxml.jackson.core.JsonToken;import com.fasterxml.jackson.databind.JsonMappingException;import com.fasterxml.jackson.databind.ObjectMapper;import java.io.IOException;import org.bson.Document;import com.mongodb.MongoClient;import com.mongodb.MongoClientURI;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoDatabase;public class CognitoSaveUser {  public Boolean saveNewUserInDB (RegistrationRequest jsonUserDetails,Context context) {  Boolean processStatus = false;  context.getLogger().log(""Inside the method saveNewUserInDB"");  ObjectMapper mapper = new ObjectMapper();  try {  String jsonString = mapper.writeValueAsString(jsonUserDetails);  context.getLogger().log(""Converted JSOn String-&gt;""+jsonString);  JsonFactory factory = new JsonFactory();  JsonParser  parser  = factory.createParser(jsonString);  Document doc = new Document();  String docKey = """";  String docValue = """";  while(!parser.isClosed()){  JsonToken jsonToken = parser.nextToken();  if(JsonToken.FIELD_NAME.equals(jsonToken)){  docKey = parser.getCurrentName();  jsonToken = parser.nextToken();  docValue = parser.getValueAsString();  doc.append(docKey, docValue);  context.getLogger().log(""Key-&gt;""+docKey+""-value-&gt;""+docValue);  }   }  MongoClientURI uri = new MongoClientURI(  ""mongodb+srv://username:password@clusterdemo-07d17.mongodb.net/dbname"");  MongoClient mongoClient = new MongoClient(uri);  MongoDatabase database = mongoClient.getDatabase(""EclipseCognitoTest"");  MongoCollection&lt;Document&gt; collection = database.getCollection(""UserProfile"");  context.getLogger().log(""DB Connection is OK"");  collection.insertOne(doc);  processStatus = true;  context.getLogger().log(""processStatus=""+processStatus);  } catch (JsonParseException e) {  // TODO Auto-generated catch block  e.printStackTrace();  } catch (JsonMappingException e) {  // TODO Auto-generated catch block  e.printStackTrace();  } catch (IOException e) {  // TODO Auto-generated catch block  e.printStackTrace();  }catch (Exception e) {  e.printStackTrace();  }  return processStatus;  }}</code></pre>",5404024,,,2018-02-17T17:40:43.397,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48844091
529,48879926,2,22588733,2018-02-20T07:30:37.320,2,"<p>Since AmazonDynamoDBClient(credentials) is deprecated i use this.</p><pre><code>init {  val cp= AWSStaticCredentialsProvider(BasicAWSCredentials(ACCESS_KEY, SECRET_KEY))  val client = AmazonDynamoDBClientBuilder.standard().withCredentials(cp).withRegion(Regions.US_EAST_1).build()  dynamoDB = DynamoDB(client)  }</code></pre>",2102794,,,2018-02-20T07:30:37.320,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48879926
530,48882087,2,48881195,2018-02-20T09:41:58.447,1,"<p>A few things you might want to look into:</p><p>You'll need to make sure you're using either Node.js runtime v6.10 or v4.3. (Node v0.10.42 is currently marked as deprecated. AWS recommends migrating existing functions to the newer Node.js runtime versions as soon as possible)</p><p>The IAM role for your lambda function needs to have an <em>Allow</em> rule for the <code>sns:Publish</code> action.</p><p>AWS recommends that specify the phone number using the E.164 format. For example: +44xxxxxxxxxx. (<a href=""https://docs.aws.amazon.com/sns/latest/dg/sms_publish-to-phone.html"" rel=""nofollow noreferrer"">more info</a>)</p><p>Also, AWS <strong>strongly</strong> recommends updating any use of the <code>context</code> method and replacing it with the <code>callback</code> approach (<a href=""https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html"" rel=""nofollow noreferrer"">more info</a>). For example:</p><pre><code>const AWS = require(""aws-sdk"");const sns = new AWS.SNS({apiVersion: ""2010-03-31""});exports.handler = (event, context, callback) =&gt; {  const params = {  PhoneNumber: ""+44xxxxxxxxxx"", // E.164 format.  Message: ""STRING_VALUE"",  MessageStructure: ""STRING_VALUE""  }  sns.publish(params, (err, data) =&gt; {  if (err) {  console.error(`Error ${err.message}`);  callback(err);  } else {  console.log(""Success"");  callback(null, data); // callback instead of context.  }  }};</code></pre>",3770040,,,2018-02-20T09:41:58.447,5,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48882087
531,48913918,2,41935525,2018-02-21T19:25:25.827,0,"<p>I know that this is now somewhat out of date, but after hours of searching and having the same problem (Also, following the AWS Tutorials!), they didn't mention anything about this.</p><p>The package is there but named incorrectly, you can find it by using;</p><pre><code>yum search httpd</code></pre><p>For me this returned a package labeled as</p><pre><code>yum info httpd.x86_64</code></pre><p>I followed up using</p><pre><code>sudo yum install -y httpd.x86_64</code></pre><p>The package then successfully installed, next step is to set permissions and launch the service!</p><p>Hope this helps.</p><p>Ash</p>",4363004,,,2018-02-21T19:25:25.827,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48913918
532,48934983,2,48934511,2018-02-22T18:41:34.963,0,"<p>The <code>DynamoDB</code> class implements the <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.SDKs.Interfaces.Document.html"" rel=""nofollow noreferrer"">document interface</a>, which is a layer on top of the low-level interface (there's also the <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.SDKs.Interfaces.Mapper.html"" rel=""nofollow noreferrer"">object persistence interface</a>, which is another layer on top of the low-level interface).</p><p>You create a <code>DynamoDB</code> instance around an <code>AmazonDynamoDB</code> instance.</p><p>The latter is an interface, and there are two objects that implement this interface. The first is <code>AmazonDynamoDBClient</code>, which has been deprecated in favor of the client object returned by <code>AmazonDynamoDBBuilder</code>.</p><p>However, unless you need to configure your client object, it's better to call <code>AmazonDynamoDBBuilder.defaultClient()</code>, rather than construct an instance and call <code>build()</code>.</p>",9398157,,,2018-02-22T18:41:34.963,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/48934983
533,49000770,2,25721048,2018-02-27T03:45:10.347,0,"<p><code>[NSURL initWithScheme: host: path:]</code> is deprecated in iOS10.should use <code>NSURLComponents</code> instead.</p><p>my updated solution:</p><pre><code>SDWebImageManager.sharedManager.cacheKeyFilter = ^NSString *(NSURL *url) {  if([[url absoluteString] isEqualToString:@""""]){  return @"""";  }  NSURLComponents *urlComponents = [[NSURLComponents alloc] initWithURL:url resolvingAgainstBaseURL:NO];  urlComponents.query = nil;  return [[urlComponents URL] absoluteString];};</code></pre>",2609878,,,2018-02-27T03:45:10.347,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49000770
534,49014211,2,49011366,2018-02-27T17:07:49.740,2,<p>Looks like my S3 API was out of date. After updating to the latest version (1.11.257) I now have access to com.amazonaws.services.s3.model.SetBucketEncryptionRequest</p>,3727399,,,2018-02-27T17:07:49.740,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49014211
535,49128386,2,49116349,2018-03-06T10:17:16.517,0,"<p>I've finally solved the problem. ""It was easy"", of course!</p><p>Here is the code in lambda:</p><pre><code>S3.getObject(params).promise()  .then((configYaml) =&gt; {  // Get the content of the file as a string  // It works on any text file, should your config is in another format  const data = configYaml.Body.toString('utf-8');  console.log(data);  // Parse the string, which is a yaml in this case  // Please note that `YAML.parse()` is deprecated  return YAML.load(data);  })  .then((config) =&gt; {  // Do some wonderful staff with the config object  console.log(`Š—¢ Config: ${JSON.stringify(config)}`);  return null;  })  .then(callback)  .catch(callback);</code></pre><p>All I was asking for is: <code>YAML.load(configYaml.Body.toString('utf-8'))</code>.</p>",2107667,,,2018-03-06T10:17:16.517,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49128386
536,49181531,2,49180428,2018-03-08T20:01:51.150,2,"<p>You cannot use non-ASCII characters for S3 user-defined metadata when using either REST API or the AWS SDK (since AWS SDK is basically wrapper libraries that wrap the underlying Amazon S3 REST API):</p><blockquote>  <p>User-defined metadata is a set of key-value pairs. Amazon S3 stores  user-defined metadata keys in lowercase. Each key-value pair must  conform to <strong>US-ASCII when you are using REST</strong> and to UTF-8 when you are  using SOAP or browser-based uploads via POST.</p></blockquote><p>While UTF-8 is supported for both; SOAP and browser-based uploads using POST, AWS <a href=""https://docs.aws.amazon.com/AmazonS3/latest/API/APISoap.html"" rel=""nofollow noreferrer"">recommends</a> that you use either the REST API or the AWS SDKs saying that new features will not be supported for SOAP:</p><blockquote>  <p>SOAP support over HTTP is deprecated, but it is still available over  HTTPS. New Amazon S3 features will not be supported for SOAP. We  recommend that you use either the REST API or the AWS SDKs.</p></blockquote>",3770040,,,2018-03-08T20:01:51.150,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49181531
537,49284329,2,49279791,2018-03-14T17:36:05.417,0,"<p>I think I've found an answer here: <a href=""https://stackoverflow.com/questions/32695660/aws-ec2-new-instance-from-image-ami?rq=1"">AWS EC2 new instance from image AMI</a></p><blockquote>  <p>When launching an instance from an Amazon Machine Image (AMI), the disks will contain an exact copy of the disk at the time that the AMI was created.</p></blockquote><p>In other words, if I start a new instance, I'll lose my installed software. WRONG!</p><p>Launching != starting.  More editing to come once I get this completely figured out.</p><p>So, given that updated Windows images are created and deprecated all the time, and the Windows OS is constantly updated by Microsoft, one must wonder how it is a static Windows image can be used with other software?  It seems like far more trouble than it's worth, if you've got to constantly reinstall your software to keep your Windows system up to date.</p><p>Amazon recently came up with a solution for that, here: <a href=""https://aws.amazon.com/blogs/mt/windows-ami-patching-and-maintenance-with-amazon-ec2-systems-manager-2/"" rel=""nofollow noreferrer"">Patching Windows</a></p><p>I don't know how to do it yet, but this seems like exactly what I need in order to keep Windows up to date, and keep my installed software intact.</p>",2052219,2052219,2018-03-14T20:30:48.630,2018-03-14T20:30:48.630,4,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49284329
538,49285699,2,49259499,2018-03-14T18:54:48.763,1,"<p>Terraform Plugin released in JetBrains plugin community is used for HashiCorp Terraform / HCL language support for doing stuffs like:</p><ul><li>Syntax highlighting.</li><li>Code formatter with the 'Reformat code'.</li><li>Report usage of deprecated properties.</li></ul><p>And <strong>NOT</strong> as a substitute for executing terraform binary.</p><p>Reference: <a href=""https://plugins.jetbrains.com/plugin/7808-hashicorp-terraform--hcl-language-support"" rel=""nofollow noreferrer"">https://plugins.jetbrains.com/plugin/7808-hashicorp-terraform--hcl-language-support</a></p>",6684192,,,2018-03-14T18:54:48.763,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49285699
539,49291577,2,48964473,2018-03-15T04:26:38.997,1,"<p>I think the documentation may be out of date. At AWS re:Invent 2017, there was an excellent session called ""AWS CLI: 2017 and Beyond"" which is <a href=""https://www.youtube.com/watch?v=W8IyScUGuGI"" rel=""nofollow noreferrer"">currently available on YouTube</a>. The presentation goes into detail about some of the new features, including dynamic credentials for the AWS CLI, and there is a corresponding GitHub repository <a href=""https://github.com/awslabs/awsprocesscreds"" rel=""nofollow noreferrer"">awslabs/awsprocesscreds</a> which may have some useful examples.</p>",8972418,,,2018-03-15T04:26:38.997,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49291577
540,49307139,2,49306994,2018-03-15T18:51:00.073,0,"<p>Looking at the <a href=""https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-applicationautoscaling/src/main/java/com/amazonaws/services/applicationautoscaling/AWSApplicationAutoScalingClient.java#L198"" rel=""nofollow noreferrer"">aws-sdk</a> library source code, the call you're trying is deprecated, you'd want to use the following API</p><p><code>AWSApplicationAutoScalingClientBuilder#defaultClient()</code> </p><p>e.g.</p><pre><code>static AWSApplicationAutoScalingClient aaClient = AWSApplicationAutoScalingClientBuilder.defaultClient().build();</code></pre>",1054897,1054897,2018-03-15T18:59:51.963,2018-03-15T18:59:51.963,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49307139
541,49313550,2,49313351,2018-03-16T05:05:47.593,4,"<p>I believe you are receiving the ""No updates"" message because technically nothing is changing in your CloudFormation template. When attempting to build the changeset, the contents of the S3 file are not examined. It just sees that none of the CloudFormation resource properties have changed since the previous deployment.</p><p>Instead, you may use a local relative path for the CodeUri, and <code>aws cloudformation package</code> can upload the file to a unique S3 path for you. This ensures that the template changes each time and the new Lambda code is recognized/deployed. For example:</p><pre><code>aws cloudformation package --template-file samTemplate.yml --output-template-file sam-output-dev.yml --s3-bucket ""$CodePipelineS3Bucket"" --s3-prefix ""$CloudFormationPackageS3Prefix""</code></pre><p>This command can be put into the build step before your create/execute changeset steps.</p><p>To see an demonstration of this entire flow in practice, you can look at <a href=""https://github.com/lafiosca/lard-example-crud-api"" rel=""nofollow noreferrer"">this repository</a>, although I will warn that it's a bit outdated thanks to the new features released at the end of 2017. (For example, I was publishing Lambda aliases manually via extra steps because it was written pre-<code>AutoPublishAlias</code>.)</p>",8972418,,,2018-03-16T05:05:47.593,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49313550
542,49372527,2,49330745,2018-03-19T21:21:36.487,3,<p>This looks like you are mixing the 1.11.286 version of aws-java-sdk with an older version (1.11.149) of aws-java-sdk-core. The newer client is using a new field added to the core module but since your core module is out of date you are seeing the no such field error. Can you ensure all of your dependencies are in sync with one another?</p>,5932496,,,2018-03-19T21:21:36.487,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49372527
543,49401378,2,49306178,2018-03-21T08:28:36.950,1,"<p>Actually, I have two different aws cli's installed. And one was too old to support Fargate. I have updated cli installed on root level while jenkins user uses out of dated cli which does not support Fargate.</p>",2121089,,,2018-03-21T08:28:36.950,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49401378
544,49626907,2,49625833,2018-04-03T09:42:15.193,4,"<p>It totally comes down to the point, whether you want to use the REST API or the AWS SDKs to interact with S3.  </p><p>In both cases, you need to prove(authenticate/Sign-Request) your identity unless bucket is public.  </p><p><strong>a)</strong> If you are going with REST APIs, to prove identity, you need sign your request using ' <a href=""https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html"" rel=""nofollow noreferrer"">AWS Signature version 4</a> ' (deprecated ver 2 is also there), which includes three methods (one you have listed)  </p><ol><li>Authenticating Requests: Using the Authorization Header (AWS Signature Version 4)</li><li>Authenticating Requests: Using Query Parameters (AWS Signature Version 4)  </li><li>Authenticating Requests: Browser-Based Uploads Using POST (AWS Signature Version 4)  </li></ol><p><strong>b)</strong> If you are going to use AWS SDKs, you should let SDK do the signing ceremony(process). So the choice is straightforward to use SDK to sign the request  </p><blockquote>  <p><strong><em>(Part of the question) It seems also painless compared to browser upload since it doesn't require all the keys browser upload wants in the post form</em></strong>&lt;  </p></blockquote><p>For below code,  <code>s3Client</code> already has got your creds whether from AWS-CLI-Profile(if running local/laptop), IAM Role(in case of EC2, lambda, etc)</p><pre><code>string url = s3Client.GetPreSignedURL(request);</code></pre>",958616,,,2018-04-03T09:42:15.193,3,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49626907
545,49708118,2,42848346,2018-04-07T13:56:43.040,1,"<p>The question is a bit outdated, but I just released <a href=""https://github.com/fvinas/multi_sqs_listener"" rel=""nofollow noreferrer"" title=""Multi SQS listener"">multi_sqs_listener</a> that provides a high level, multi-threaded way to listen to multiple SQS queues from Python code.</p><pre class=""lang-python prettyprint-override""><code>import timefrom multi_sqs_listener import QueueConfig, EventBus, MultiSQSListenerclass MyListener(MultiSQSListener):  def low_priority_job(self, message):  print('Starting low priority, long job: {}'.format(message))  time.sleep(5)  print('Ended low priority job: {}'.format(message))  def high_priority_job(self, message):  print('Starting high priority, quick job: {}'.format(message))  time.sleep(.2)  print('Ended high priority job: {}'.format(message))  def handle_message(self, queue, bus, priority, message):  if bus == 'high-priority-bus':  self.high_priority_job(message.body)  else:  self.low_priority_job(message.body)low_priority_bus = EventBus('low-priority-bus', priority=1)high_priority_bus = EventBus('high-priority-bus', priority=5)EventBus.register_buses([low_priority_bus, high_priority_bus])low_priority_queue = QueueConfig('low-priority-queue', low_priority_bus)high_priority_queue = QueueConfig('high-priority-queue', high_priority_bus)my_listener = MyListener([low_priority_queue, high_priority_queue])my_listener.listen()</code></pre>",9611797,,,2018-04-07T13:56:43.040,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/49708118
546,49738501,2,49737238,2018-04-09T17:30:42.937,7,"<p>Well, I found the problem: Those two plugins are horribly old, obsolete and abandoned.</p><pre><code>&lt;extension&gt;  &lt;groupId&gt;com.allogy.maven.wagon&lt;/groupId&gt;  &lt;artifactId&gt;maven-s3-wagon&lt;/artifactId&gt;  &lt;version&gt;1.2.0&lt;/version&gt;&lt;/extension&gt; </code></pre><p>seems to work, but I'm not sure if that's the best one to use.</p>",6068,551406,2020-05-01T21:52:52.967,2020-05-01T21:52:52.967,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/49738501
547,49858046,2,48385176,2018-04-16T13:03:38.027,0,"<p><em>I was able to solve above problem.Simply we have to keep the files under resources directory.like in the below image</em> </p><p><a href=""https://i.stack.imgur.com/dK89t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dK89t.png"" alt=""enter image description here""></a></p><p><em>Now add the following dependency to the pom.xml :</em></p><pre><code>&lt;!-- https://mvnrepository.com/artifact/org.codehaus.jackson/jackson-mapper-asl --&gt;&lt;dependency&gt;  &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt;  &lt;artifactId&gt;jackson-mapper-asl&lt;/artifactId&gt;  &lt;version&gt;1.9.13&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>Use following code and you are ready to test it on AWS ;</p><pre><code> Test test = method.getAnnotation(Test.class);  URL resource = ClassLoader.getSystemResource(""test.json"");  InputStream stream = resource.openStream();  String data_all = IOUtils.toString(stream, ""UTF-8"");  ObjectMapper mapper = new ObjectMapper();  JsonNode rootNode = mapper.readTree(data_all);  JsonNode testNode = rootNode.get(test.testName());  if (!testNode.get(""Execute"").asText().equals(""Yes"")) {  throw new SkipException(""Skipping the test as per Configuration"");  }  if (testNode.get(""Obsolete"").asText().equals(""Yes"")) {  throw new SkipException(""Skipping the test as test is obsolete."");  }  userName = testNode.get(""testData"").get(""userName"").toString();  password = testNode.get(""testData"").get(""password"").toString();</code></pre><p>My test.json file is structured as below :</p><pre><code>{  ""test_case_name"": {  ""testName"": ""test_case_name"",  ""testDescription"": ""test description goes here"",  ""testDataType"": ""test data type"",  ""Execute"": ""Yes"",  ""Obsolete"": ""No"",  ""testData"": {  ""userName"": ""usernmae"",  ""password"": ""Password""  },  ""validations"": {  ""validation_value1"": ""NA"",  ""validation_value2"": ""NA""  }}</code></pre>",9252266,360274,2018-05-21T09:14:53.227,2018-05-21T09:14:53.227,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/49858046
548,50007649,2,50007220,2018-04-24T17:29:10.063,1,"<p>How about this?</p><pre><code>AmazonSQSAsyncClientBuilder.standard()  .withCredentials(getProfileCredentialsProvider())  .build();</code></pre><p>That is really mentioned in the JavaDocs:</p><pre><code>* @deprecated use {@link AmazonSQSAsyncClientBuilder#withCredentials(AWSCredentialsProvider)} */@Deprecatedpublic AmazonSQSAsyncClient(AWSCredentialsProvider awsCredentialsProvider) {</code></pre>",2756547,,,2018-04-24T17:29:10.063,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/50007649
549,50016580,2,50016185,2018-04-25T07:38:50.537,8,"<p>May i ask you why you are not using the built-in approach of <code>AWS CodeBuild</code>? You are able to get parameters out of <code>SSM</code> through the build spec of your <code>AWS CodeBuild</code> project. The additional call through the Java SDK is obsolete in this case.</p><pre><code>version: 0.2env:  parameter-store:  key: ""value""  key: ""value""phases:  build:  commands:  - command  - command</code></pre><blockquote>  <p>parameter-store: Required if env is specified, and you want to  retrieve custom environment variables stored in Amazon EC2 Systems  Manager Parameter Store. Contains a mapping of key/value scalars,  where each mapping represents a single custom environment variable  stored in Amazon EC2 Systems Manager Parameter Store. key is the name  you will use later in your build commands to refer to this custom  environment variable, and value is the name of the custom environment  variable stored in Amazon EC2 Systems Manager Parameter Store.</p></blockquote><p>For more informations please check the <a href=""https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html"" rel=""noreferrer"">Build Specification Reference for AWS CodeBuild</a></p>",3115607,,,2018-04-25T07:38:50.537,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/50016580
550,50026439,2,50016185,2018-04-25T15:48:10.817,0,"<p>Your AWS Java SDK is likely out of date. The minimum version for retrieving credentials in CodeBuild is 1.11.16.<a href=""https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html#troubleshooting-versions"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html#troubleshooting-versions</a></p>",5095975,,,2018-04-25T15:48:10.817,2,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/50026439
551,50032619,2,50032521,2018-04-25T23:16:15.143,3,"<blockquote>  <p>path must be absolute or specify root to res.sendFile</p></blockquote><p>You can use the <code>root</code> option for it:</p><pre><code>res.sendFile('index.html', { root: '.' })</code></pre><p>By the way, <code>sendfile</code> is deprecated, use <code>sendFile</code>.</p>",6401895,,,2018-04-25T23:16:15.143,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/50032619
552,50036442,2,50036271,2018-04-26T06:43:58.110,12,"<p>Basically, if you NEED to have the latest values, use a <strong>fully consistent read</strong>. You'll get the guaranteed current value.</p><p>If your app is okay with potentially outdated information (mere seconds or less out of date), then use <strong>eventually consistent reads</strong>.</p><p>Examples of fully-consistent:</p><ul><li>Bank balance (Want to know the latest amount)</li><li>Location of a locomotive on a train network (Need absolute certainty to guarantee safety)</li><li>Stock trading (Need to know the latest price)</li></ul><p>Use-cases for eventually consistent reads:</p><ul><li>Number of Facebook friends (Does it matter if another was added in the last few seconds?)</li><li>Number of commuters who used a particular turnstile in the past 5 minutes (Not important if it is out by a few people)</li><li>Stock research (Doesn't matter if it's out by a few seconds)</li></ul>",174777,,,2018-04-26T06:43:58.110,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/50036442
553,50050912,2,50048987,2018-04-26T19:54:43.457,0,"<p>You could use a dnslookup command. The following will give you the ip</p><pre><code>dig +nocmd your.cloud any +multiline +noall +answer</code></pre><p>Then you can use the following command to get the region:</p><pre><code>curl freegeoip.net/xml/xx.xxx.xxx.xxx</code></pre><p>this will get you the geolocation of the address. You can also use their <a href=""https://geoiptool.com/"" rel=""nofollow noreferrer"">website</a> if that helps </p><p>tested with: <code>curl freegeoip.net/xml/54.231.184.255</code></p><p>response: </p><p><code>&lt;Response&gt;  &lt;DeprecationMessage&gt;This API endpoint is deprecated and will stop working on July 1st, 2018. For more information please visit: https://github.com/apilayer/freegeoip#readme&lt;/DeprecationMessage&gt;  &lt;IP&gt;54.231.184.255&lt;/IP&gt;  &lt;CountryCode&gt;US&lt;/CountryCode&gt;  &lt;CountryName&gt;United States&lt;/CountryName&gt;  &lt;RegionCode&gt;OR&lt;/RegionCode&gt;  &lt;RegionName&gt;Oregon&lt;/RegionName&gt;  &lt;City&gt;Boardman&lt;/City&gt;  &lt;ZipCode&gt;97818&lt;/ZipCode&gt;  &lt;TimeZone&gt;America/Los_Angeles&lt;/TimeZone&gt;  &lt;Latitude&gt;45.7788&lt;/Latitude&gt;  &lt;Longitude&gt;-119.529&lt;/Longitude&gt;  &lt;MetroCode&gt;810&lt;/MetroCode&gt;&lt;/Response&gt;</code></p>",1546706,,,2018-04-26T19:54:43.457,1,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/50050912
554,50100401,2,50092851,2018-04-30T12:19:37.293,3,"<p>The answer posted above should work, per the documentation. But when using the Node.JS AWS DynamoDB SDK's <a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html#query-property"" rel=""nofollow noreferrer"">DocumentClient</a>, it doesn't. Particularly, I tried:</p><pre><code>  {  TableName: ""site"",  IndexName: ""orgId-lastCaptured-index"",  KeyConditionExpression: ""orgId = :orgId"",  FilterExpression: ""categories CONTAINS :categoriesValue"",  ExpressionAttributeValues: {  "":orgId"": orgId,  "":categoriesValue"": myVariable,  }  }</code></pre><p>This resulted in the following error: <code>{ ValidationException: Invalid FilterExpression: Syntax error; token: ""CONTAINS"", near: ""categories CONTAINS :categoriesValue""</code></p><p>I adjusted the query to the alternative query formatting as follows:</p><pre><code>  {  TableName: ""site"",  IndexName: ""orgId-lastCaptured-index"",  KeyConditions: {  orgId: {  ComparisonOperator: ""EQ"",  AttributeValueList: [orgId],  },  },  QueryFilter: {  categories: {  ComparisonOperator: ""CONTAINS"",  AttributeValueList: [myVariable],  }  }  }</code></pre><p>This worked as anticipated, filtering the returned results such that <code>categories</code> variable has an element that matches <code>myVariable</code>.</p><p>Update: You can now do <code>CONTAINS</code> operations without using the deprecated <code>QueryFilter</code> with this syntax: <code>FilterExpression: ""contains(categories, :categoriesValue)""</code></p>",3818342,3818342,2019-01-01T18:26:00.347,2019-01-01T18:26:00.347,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50100401
555,50112614,2,44823012,2018-05-01T07:07:23.587,0,"<p>The MobileHubHelper framework is deprecated and the AWS iOS SDK has the Auth SDK to manage authentication with AWS. It is recommended you use that. You can find the instructions here: <a href=""https://docs.aws.amazon.com/aws-mobile/latest/developerguide/add-aws-mobile-user-sign-in.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/aws-mobile/latest/developerguide/add-aws-mobile-user-sign-in.html</a></p><p>Thanks,Rohan</p>",5045692,,,2018-05-01T07:07:23.587,0,CC BY-SA 3.0,,,,https://stackoverflow.com/questions/50112614
556,50157158,2,48307251,2018-05-03T14:11:30.367,1,"<p>based on <a href=""https://github.com/RcppCore/RcppArmadillo/issues/200"" rel=""nofollow noreferrer"">https://github.com/RcppCore/RcppArmadillo/issues/200</a>, I think this issue is due to a g++ compatability issue. It might also explain why when I installed devtools it kept giving me [-Wdeprecated-declarations]so run:<code>sudo yum remove gcc72-c++.x86_64 libgcc72.x86_64</code></p>",9735906,,,2018-05-03T14:11:30.367,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50157158
557,50225318,2,50222478,2018-05-08T03:20:58.143,0,"<p>I think the stated focus on performance belies real trade-offs.</p><p>Consider that someone will have to maintain your code -- if you use an API, the test area is small, but AWS APIs might change or be deprecated; if you an SDK, next programmer will plug in new SDK version and hope that it works, but if it doesn't they'd be bogged down by sheer weight of the SDK.</p><p>Likewise, imagine someone needs to do a security review of this app, or to introduce something not yet covered by SDK (let's imagine propagating accounting group from caller role to underlying storage).</p><p>I don't think there is a clear answer.</p><p>Here are my <strong>suggestions</strong>:</p><ul><li>keep it consistent -- either API or SDK (within given app)</li><li>consider the bigger picture (how many apps do you plan to write?)</li><li>don't be afraid to switch to the other approach later</li></ul><p>I've had to decide on something similar <strong>in the past</strong>, with Docker (much nicer APIs and SDKs/libs). Here's how it played out:</p><p>For testing, we ended up using beta version of Docker Python bindings: prod version was not enough, and bindings (your SDK) were overall pretty good and clear.</p><p>For log scraping, I used HTTP calls (your API), ""because performance"", in reality comparative mental load using API vs SDK, and because bindings (SDK) did not support asyncio.</p>",705086,,,2018-05-08T03:20:58.143,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50225318
558,50236173,2,47939238,2018-05-08T14:35:48.390,1,"<p>This error for me was because I had previously logged in with a different user storing their JWT tokens in local storage, then logged in with a different user which confused things.</p><p>There seems to be a bug with the latest but now deprecated (non-AWSAmplify) version of the amazon-cognito-identity-js library where it doesn't clear the JWT tokens when you call its signout() method.  This can be fixed by manually clearing the keys from local storage.</p>",213719,,,2018-05-08T14:35:48.390,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50236173
559,50273450,2,50272459,2018-05-10T12:54:19.907,3,"<p>As <a href=""https://stackoverflow.com/users/1695906/michael-sqlbot"">@michael-sqlbot</a> pointed out you can get this behavior by using the <a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-lambda.html"" rel=""nofollow noreferrer"">X-Amz-Invocation-Type:Event</a> Header.  Getting this setup is a little screwy and the linked documentation is accurate but a little outdated (in my opinion).  </p><ol><li>The Path Override needs to be: <code>/2015-03-31/functions/&lt;ARN TO YOUR LAMBDA&gt;/invocations</code></li><li>The Execution Role needs to be able to invoke your lambda.</li><li>When initially setting up the Integration Request you cannot add headers so you have to save it, then come back and add headers.  From there you can hard code the X-Amz-Invocation-Type to Event by putting the string <code>'Event'</code> in the <code>Mapped from</code> field (as pictured below)</li></ol><p><a href=""https://i.stack.imgur.com/JYDWR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JYDWR.png"" alt=""Api Gateway Integration Request""></a></p><p>Or... you could also achieve this by having your lambda that's wired up to the API Gateway (we'll call it lambda A) invoke another lambda (lambda B) using the <code>Event</code> invocation type.  This way A doesn't care about the response of B and can return a successful response to the API Gateway within several hundred milliseconds (assuming you aren't doing too much else).  Then Lambda B can continue running for however long is necessary (as long as it's under the 5 minute lambda limitation or your configured timeout).</p>",1081990,1081990,2018-05-10T16:18:10.203,2018-05-10T16:18:10.203,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50273450
560,50374829,2,50340114,2018-05-16T15:29:34.243,1,"<p>It has to do with string interpolation and replacing the date variable in the <code>set_fact</code> directive. Here is an example, also I have used the <code>ec2_ami_facts</code> module instead of <code>ec2_ami_find</code> as <code>ec2_ami_find</code> would be deprecated soon.</p><pre><code>---- hosts: localhost  remote_user: me  gather_facts: no  connection: local  tasks:  - ec2_ami_facts:  owner: self  region: eu-central-1  register: ec2_ami  - set_fact:  filter_date: ""{{ lookup('pipe','date \""+%Y-%m-%d\"" -d \""180 day ago\""') }}""  - debug: var=filter_date  - set_fact:  filtered_ami: ""{{ ec2_ami | json_query(\""images[?creation_date&lt;=`\"" + filter_date + \""`]\"") }}""  - shell: echo ""{{ filtered_ami | length }} {{ ec2_ami.images | length }}""</code></pre>",2588830,1016425,2018-05-16T20:16:35.817,2018-05-16T20:16:35.817,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50374829
561,50420788,2,48770235,2018-05-19T00:47:00.157,2,"<p>If you are trying to use FreeTDS on ElasticBeanstalk, it's worth noting that the version on their yum repository is currently 0.91, which is ancient (from 2011).</p><p>Once I installed the latest version, all of my problems went away. I don't know if that would solve your performance issues or not, but if not, hopefully it helps someone else who comes along.</p><p>Here was the solution I came up with (which downloads and installs freetds directly from the source instead of the outdated yum repo):</p><ol><li>Create a file in your project called '/.ebextensions/001_install_freetds.config'</li><li>Paste in these contents:</li></ol><p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""false"" data-babel=""false""><div class=""snippet-code""><pre class=""snippet-code-html lang-html prettyprint-override""><code>commands:  000_download_freetds:  command: ""[ ! -e /home/ec2-user/freetds-1.00.86.tar.gz ] &amp;&amp; wget -nc ftp://ftp.freetds.org/pub/freetds/stable/freetds-1.00.86.tar.gz -O /home/ec2-user/freetds-1.00.86.tar.gz || true""  001_extract_freetds:  command: ""[ ! -e /home/ec2-user/freetds-1.00.86 ] &amp;&amp; tar -xvf /home/ec2-user/freetds-1.00.86.tar.gz -C /home/ec2-user/ || true""  002_configure_freetds:  command: ""[ ! -e /usr/local/etc/freetds.conf ] &amp;&amp; cd /home/ec2-user/freetds-1.00.86 &amp;&amp; sudo ./configure --prefix=/usr/local --with-tdsver=7.4 || true""  003_build_freetds_and_install:  command: ""[ ! -e /usr/local/etc/freetds.conf ] &amp;&amp; ( cd /home/ec2-user/freetds-1.00.86 &amp;&amp; sudo make &amp;&amp; sudo make install ) || true""</code></pre></div></div></p><ol start=""3""><li>Redeploy your app</li></ol><p>Note: You may want to change the various file names to reflect the latest stable version. This is just what is the most recent stable release, as of when I'm writing this post.</p>",1546785,,,2018-05-19T00:47:00.157,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50420788
562,50455363,2,46991117,2018-05-21T19:20:28.283,33,"<p>requests is not a standard library in AWS lambda.</p><p>So two ways to solve this:</p><p>1- Import it from the Botocore libraries stack as: (Option to be deprecated after the answer was given)</p><pre><code>from botocore.vendored import requests</code></pre><p><a href=""https://gist.github.com/gene1wood/4a052f39490fae00e0c3"" rel=""noreferrer"">Here</a> there is a list of all the available libraries to import in lambda</p><p>2- <a href=""https://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html"" rel=""noreferrer"">Create</a> a deployment package with virtualenv.</p>",4783390,4783390,2019-11-19T22:10:32.193,2019-11-19T22:10:32.193,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50455363
563,50470118,2,50469127,2018-05-22T14:23:44.327,3,"<p>AWS load balancers don't support ""backup"" nodes that only take traffic when the primary is down.</p><p>Beyond that, you are proposing a complicated scenario.</p><blockquote>  <p><em>was thinking I could setup a second EC2 server with a MySQL slave</em></p></blockquote><p>If you do that, you can only fail over once, then you can't fail back, because the master database will then be obsolete.  For a configuration like this to work and be useful, your two MySQL servers need to be configured with master/master (circular) replication, so that each is a replica of the other.  This is an advanced configuration that requires expertise and caution.</p><p>For the MySQL component, an RDS instance with multi-AZ enabled will provide you with hands-off fault tolerance of the database.</p><p>Of course, the client may be unwilling to pay for this as well.</p><p>A reasonable shortcut for small systems might be <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html"" rel=""nofollow noreferrer"">EC2 instance recovery</a> which will bring the site back up if the underlying hardware fails.  This feature replaces a failed instance with a new instance, reattaches the EBS volumes, and starts it back up.  If the system is stable and you have a solid backup strategy for all data, this might be sufficient.  Effective redundancy as a retrofit is non-trivial.</p>",1695906,,,2018-05-22T14:23:44.327,5,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50470118
564,50475782,2,50474710,2018-05-22T20:13:41.187,0,"<p>It looks pretty simple with <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html#changeObjectStorageClass-java.lang.String-java.lang.String-com.amazonaws.services.s3.model.StorageClass-"" rel=""nofollow noreferrer"">changeObjectStorageClass</a>:</p><pre><code>AmazonS3Client s3Client = (AmazonS3Client)AmazonS3ClientBuilder.standard()  .withRegion(clientRegion)  .withCredentials(new ProfileCredentialsProvider())  .build();PutObjectRequest request = new PutObjectRequest(bucketName,  fileObjKeyName, new File(fileName));ObjectMetadata metadata = new ObjectMetadata();metadata.setContentType(""binary/octet-stream"");request.setMetadata(metadata);s3Client.putObject(request);s3Client.changeObjectStorageClass(bucketName, fileObjKeyName,  StorageClass.ReducedRedundancy );</code></pre><p>The only strange part is that you need to use the <code>changeObjectStorageClass</code> on an instance of <code>AmazonS3Client</code> - the version on the interface <code>AmazonS3</code> is deprecated.</p>",2933977,,,2018-05-22T20:13:41.187,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50475782
565,50476245,2,50474710,2018-05-22T20:48:56.470,2,"<p>there is no need to use deprecated method in any case.in my case in need to update an existing object:</p><pre><code>   AmazonS3Client s3Client =  (AmazonS3Client)AmazonS3ClientBuilder.standard()  .withRegion(clientRegion)  .withCredentials(new ProfileCredentialsProvider())  .build();  CopyObjectRequest copyRequest = new CopyObjectRequest(sourceBucketName, sourceKey, destinationBucketName, destinationKey)  .withStorageClass(StorageClass.ReducedRedundancy);  s3Client.copyObject(copyRequest);</code></pre>",1181404,1181404,2018-05-23T14:53:39.597,2018-05-23T14:53:39.597,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50476245
566,50488215,2,50488089,2018-05-23T12:17:50.530,0,"<p>Either the method name createTags does not exist within the class AmazonEC2 or you pass a wrong parameter list to the method.</p><p>This can also happen if the client and server side version of the aws-sdk-java api differ from each other. Be sure that you use the correct client side API.</p><p>The AmazonE2Client is mostly deprecated: @see <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/ec2/AmazonEC2Client.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/ec2/AmazonEC2Client.html</a></p><p>The documentation says ""use AWSClientBuilder instead""</p>",2627868,2627868,2018-05-23T12:31:18.200,2018-05-23T12:31:18.200,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50488215
567,50518140,2,50516212,2018-05-24T21:04:03.440,0,"<p><strong>My Reality</strong></p><p>I have been developing an ORM for some time now, and there's a lot of the JDBC API offers to you in terms of metadata information about the database. It actually offers you the list of functions for numeric, varchar and dates values.</p><p>Unfortunately, the JDBC driver (and I imagine other drivers too) most of the time do not implement such part of the specification, and the call ends up returning an empty list.</p><p>In other cases, it actually returns a list of functions but it's blatantly outdated. Maybe the method was implemented 10 years ago, and no one updated it in any recent version of the driver or the database.</p><p>I would advise against relying on any database metadata that goes beyond the minimal set of operations. Yes, unfortunately that is my experience with 10 main stream databases so far, and I guess minor databases may be worse in this respect.</p><p><strong>A Workaround</strong></p><p>Now, if you really need to query that information at runtime, you can still assemble a SQL statement that uses the <em>candidate</em> function, run it, and see if it runs or crashes. Not the ideal option, but I guess this will clearly tell you if it's implemented or not. Running it for hundreds of functions should take no time.</p>",6436191,,,2018-05-24T21:04:03.440,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50518140
568,50590439,2,50590080,2018-05-29T18:09:26.843,23,"<p><strong>Mongoose 5.x supports</strong> following syntax for authorization and also make sure you have not used any special character in url like <code>@,-,+,&gt;</code></p><pre><code>mongoose.connect(MONGO_URL, {  auth: {  user: MONGO_DB_USER,  password: MONGO_DB_PASSWORD  }})</code></pre><p>Or if you want to remove deprication warning <strong>Avoid Š—“current URL string parser is deprecated""</strong></p><p>Add option <code>useNewUrlParser</code></p><pre><code>mongoose.connect(MONGO_URL, {  auth: {  user: MONGO_DB_USER,  password: MONGO_DB_PASSWORD  },  { useNewUrlParser: true }})</code></pre>",7510657,7510657,2018-10-14T18:36:25.813,2018-10-14T18:36:25.813,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50590439
569,50670252,2,35804042,2018-06-03T19:32:47.537,29,"<p>It seems AWS has simplified this process, so that many answers are outdated and/or overly complicated.</p><p>This is how I got Lambda to return an image through the API Gateway, as of June 2018:</p><p>1) In API Gateway, enable <code>Use Lambda Proxy integration</code> for your API. (This setting is located on the Integration Request section, where you had set the type to Lambda.)</p><p>2) In API Gateway, select your API and click <code>Settings</code>. In <code>Binary Media Types</code> add <code>*/*</code>. (Note: I tried adding simply 'image/jpeg', but it seems to require <code>*/*</code> to get all of this to work)</p><p>3) Be sure to deploy your API, otherwise your changes will not be live. (In API Gateway, select your API, then Actions > Deploy API).</p><p>4) In your Lambda code, return your image in Base64 encoding (this example is C# code):</p><pre><code>  // set the Content-type header  // set to whichever image type you're returning  var headersDic = new Dictionary&lt;string, string&gt;();  headersDic.Add(""Content-type"", ""image/jpeg"");  // return the response object that APIGateway requires  return new APIGatewayProxyResponse  {  StatusCode = 200,  Headers = headersDic,  // return the image in Base64 encoding  Body = Convert.ToBase64String(...your image data...),  IsBase64Encoded = true  };</code></pre><p>Done.</p><p>If you've setup your API to not require authentication, simply type your API link into your browser, and it will display the image. Or put the API link into an <code>IMG</code> tag. e.g. <code>&lt;img src=""https://asdf.execute-api.us-east-1.amazonaws.com/live/myapi"" /&gt;</code></p><p>Note: Even though in step 2 you set the <code>Binary Media Types</code> to <code>*/*</code>, API Gateway will still return text if that is what your Lambda is returning.</p>",1145177,1145177,2018-06-03T19:42:07.300,2018-06-03T19:42:07.300,8,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50670252
570,50857199,2,50856581,2018-06-14T12:07:20.170,2,"<p>You will need to check at the error you get while trying to execute that statement. Let's see the problems and possible problems:</p><h2>Rights</h2><p>You will need to make sure the MySQL user you try to execute the query with has the necessary rights to do so. Try to hard-code an <code>insert</code> statement along with all parameters. Are you able to do so? Or do you get an error that you do not have the rights to do so?</p><h2>Deprecation</h2><p><a href=""http://php.net/manual/ro/function.mysql-query.php"" rel=""nofollow noreferrer"">mysql_<em></a> functions are deprecated. You will need to use either <a href=""http://php.net/manual/ro/mysqli.query.php"" rel=""nofollow noreferrer"">mysqli_</em></a> functions or <a href=""http://php.net/manual/ro/book.pdo.php"" rel=""nofollow noreferrer"">PDO</a>.</p><h2>SQL Injection</h2><p>Your code has high risks of security due to possibility of <a href=""http://www.tizag.com/mysqlTutorial/mysql-php-sql-injection.php"" rel=""nofollow noreferrer"">SQL Injection</a>. You will need to escape your query via <a href=""http://php.net/manual/ro/mysqli.real-escape-string.php"" rel=""nofollow noreferrer"">mysqli_real_escape_string</a> or parameterize your query via PDO. If you do not do so, users will be able to damage your database if they want to hack your site, or even steal data.</p><h2>XSS Injection</h2><p>Your code has high risks of security due to possibility of <a href=""https://www.owasp.org/index.php/Cross-site_Scripting_(XSS)"" rel=""nofollow noreferrer"">XSS injection</a> as well. You will need to make sure no scripts will be injected into your fields unless you explicitly want to allow that. XSS injection is a possible means to steal data from other users.</p><h2>Is it message or messages</h2><p>Check what is inside your <code>$_POST[""messages""]</code>. Is it an array? If so, you try to use an array as a string and hence you get an exception.</p><h2>Check your logs</h2><p>You will need to check the server logs to find the exact problem you face. If server logging is not enabled, then you will need to enable it and run the PHP code again.</p>",436560,,,2018-06-14T12:07:20.170,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50857199
571,50926351,2,35753573,2018-06-19T10:37:13.067,1,"<p><a href=""https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html"" rel=""nofollow noreferrer"">AWS Lambda Best Practices notes</a> talks about DI:</p><blockquote>  <p><strong>Minimize the complexity of your dependencies.</strong> Prefer simpler  frameworks that load quickly on <a href=""https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html"" rel=""nofollow noreferrer"">Execution Context</a> startup. For  example, prefer simpler Java dependency injection (IoC) frameworks  like <a href=""http://square.github.io/dagger/"" rel=""nofollow noreferrer"">Dagger</a> or <a href=""https://github.com/google/guice"" rel=""nofollow noreferrer"">Guice</a>, over more complex ones like <a href=""https://github.com/spring-projects/spring-framework"" rel=""nofollow noreferrer"">Spring Framework</a>.</p></blockquote><p>So I'd like to suggest you to use <a href=""https://github.com/google/dagger"" rel=""nofollow noreferrer"">Dagger 2</a> (because Square's Dagger 1.x is already deprecated). It provides such benefits:</p><ul><li>light-weight framework with very few integrations, Java interface/annotation configuration and compile-time code generated bindings;</li><li>very small size;</li><li>fail as early as possible ( compile-time, not runtime);</li><li>performance - as fast as hand-written code and no coding around the framework.</li></ul>",1145792,,,2018-06-19T10:37:13.067,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50926351
572,50964107,2,50583520,2018-06-21T08:41:57.087,0,"<p>The problem here is that some dependency pulled in <code>akka-actor_2.11</code> version <code>2.2.20</code> which is very old and does not have the method mentioned in the exception.</p><p>Take a look at the output of <code>mvn dependency:analyze</code> and <code>mvn dependency:tree</code> and see which dependency brings in outdated Akka. Then update that dependency or force the usage of the latest Akka in your project by adding</p><pre><code>&lt;dependencyManagement&gt;  &lt;dependencies&gt;  &lt;dependency&gt;  &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt;  &lt;artifactId&gt;akka-actor_2.11&lt;/artifactId&gt;  &lt;version&gt;2.5.13&lt;/version&gt;  &lt;/dependency&gt;  &lt;/dependencies&gt;&lt;/dependencyManagement&gt;</code></pre><p>to your POM.</p>",77102,,,2018-06-21T08:41:57.087,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/50964107
573,51053361,2,50887599,2018-06-27T01:32:21.867,0,"<p>Use client builder instead of directly using client constructors since it has been deprecated by the latest SDK version.</p><pre><code>AmazonSNS sns = AmazonSNSClient.builder()  .withRegion(""&lt;your region&gt;"")  .withClientConfiguration(&lt;configuration&gt;)  .withCredentials(&lt;your credentials&gt;)  .build();</code></pre>",6284370,,,2018-06-27T01:32:21.867,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/51053361
574,51272109,2,36604024,2018-07-10T18:41:40.450,0,"<p>The AWSSecurityTokenServiceClient is deprecated.  The following code also works.</p><pre><code>BasicAWSCredentials theAWSCredentials= new BasicAWSCredentials("""","""");AWSCredentialsProvider theAWSCredentialsProvider = new AWSStaticCredentialsProvider(theAWSCredentials);AWSSecurityTokenService theSecurityTokenService = AWSSecurityTokenServiceClientBuilder.standard().withCredentials(theAWSCredentialsProvider).build();</code></pre>",10060983,,,2018-07-10T18:41:40.450,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/51272109
575,51395518,2,51275993,2018-07-18T07:01:06.893,1,"<p>Answer in two parts:</p><p>PART 1: Grant-EC2SecurityGroupIngress.Internally this command is modifying $IpPermission. If you try to change the $IpRange.CidrIp to a different address then you will see the error:</p><pre><code>Grant-EC2SecurityGroupIngress : Cannot set values for both Ipv4Ranges and IpRanges properties onthe IpPermission type which is part of the request. Consider using only Ipv4Ranges as IpRanges hasbeen marked obsolete.</code></pre><p>The solution is to reallocate $IpPermission.</p><pre><code>$IpRange.CidrIp = ""102.196.30.33/32""$IpRange.Description = ""FTP PASV""$IpPermission = New-Object Amazon.EC2.Model.IpPermission$IpPermission.IpProtocol = ""tcp""$IpPermission.FromPort = 2025$IpPermission.ToPort = 2030$IpPermission.Ipv4Ranges = $IpRangeGrant-EC2SecurityGroupIngress -GroupId $sg -IpPermission $IpPermission</code></pre><p>PART 2: For PowerShell (Windows) AWS CLI: you need to escape the double-quotes and surround the string with double-quotes. </p><p>Here is a working example:</p><p>aws ec2 authorize-security-group-ingress --group-id sg-12345678  --ip-permissions ""[{\""IpProtocol\"": \""tcp\"", \""FromPort\"": 2025, \""ToPort\"": 2030, \""IpRanges\"": [{\""CidrIp\"": \""102.196.30.33/32\"", \""Description\"": \""FTP PASV\""}]}]""</p>",8016720,8016720,2018-07-18T08:26:45.917,2018-07-18T08:26:45.917,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/51395518
576,51432753,2,28534695,2018-07-19T22:52:53.997,12,"<p>The accepted answer is out of date. It is now possible to create stacks across accounts and regions using <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started.html"" rel=""noreferrer"">CloudFormation StackSets</a>. </p>",7020467,,,2018-07-19T22:52:53.997,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/51432753
577,51555601,2,51554772,2018-07-27T10:05:01.867,0,"<p>sorry i didnt launch the VNC SERVER .... but now the connection is ok but nothing appear on my screen :</p><pre><code>  Fri Jul 27 09:57:29 2018 vncext:  VNC extension running! vncext:  Listening for VNC connections on local interface(s), port 5901 vncext:  created VNC server for screen 0/usr/bin/startxfce4: X server already running on display :1gpg-agent[862]: WARNING: ""--write-env-file"" is an obsolete option - it has no effectgpg-agent: a gpg-agent is already running - not starting a new one(xfce4-session:849): xfce4-session-WARNING **: gpg-agent returned no PID in the variables(xfce4-session:849): xfce4-session-WARNING **: xfsm_manager_load_session: Something wrong with /home/jamal/.cache/sessions/xfce4-session-kali:1, Does it exist? Permissions issue?(xfsettingsd:878): xfsettingsd-WARNING **: Failed to get the _NET_NUMBER_OF_DESKTOPS property.(xfwm4:866): xfwm4-WARNING **: Error opening /dev/dri/card0: Permission denied** (light-locker:875): ERROR **: Environment variable XDG_SESSION_PATH not set. Is LightDM running?Fri Jul 27 09:59:01 2018 Connections: accepted: [::1]::41716 SConnection: Client needs protocol version 3.8 SConnection: Client requests security type VncAuth(2)Fri Jul 27 09:59:05 2018 VNCSConnST:  Server default pixel format depth 24 (32bpp) little-endian rgb888 VNCSConnST:  Client pixel format depth 6 (8bpp) rgb222 VNCSConnST:  Client pixel format depth 24 (32bpp) little-endian rgb888</code></pre><p>above the vnc log.</p><p>my user has all admin privilege in sudoers files ( ALL:ALL:ALL ).</p>",9988527,,,2018-07-27T10:05:01.867,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/51555601
578,51662651,2,51662309,2018-08-02T21:54:49.730,0,"<p>Returns an empty function because it is just asynchronous, when await groupIds.forEach is resolved, it passes to the stack of the api of js, and the done (null, events); is resolved before the promise is completed, to avoid this problem you could do the following, that your arrangement of events, resolve an array of promises and this when it is resolved, just there send it to the callback.</p><p>You could do something like this.</p><pre><code>let arr=groupIds.map(groupId =&gt; docClient.query({  TableName: 'events',  IndexName: 'groupId-creationDate-index',  KeyConditionExpression: 'groupId = :g',  ExpressionAttributeValues: { ':g': groupId }  }))Promise.all(arr).then(events=&gt;done(null,events))</code></pre><p>I do not know the docClient API, but it is enough that it returns a promise (the callbacks are out of date) so that the variable arr has an array of Promises and these are resolved later.</p>",10173369,10173369,2018-08-02T22:00:15.763,2018-08-02T22:00:15.763,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/51662651
579,51806562,2,51801971,2018-08-12T07:08:29.840,0,"<p>I solved the problem by doing the following:</p><ul><li>using a p2.xlarge EC2 instance</li><li>selecting <a href=""https://aws.amazon.com/marketplace/pp/B077GCH38C"" rel=""nofollow noreferrer"">Deep Learning AMI (Ubuntu) v12.0</a> as starting AMI</li><li>use <code>conda env list</code> to see the list of available environments and then activating the one I needed with <code>source activate tensorflow_p36</code></li></ul><p>This last point was probably the thing I never realized to do in my previous tests.</p><p>After that, everything was working as expected</p><pre><code>&gt;&gt;&gt; from keras import backend as K/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.  from ._conv import register_converters as _register_convertersUsing TensorFlow backend.&gt;&gt;&gt; K.tensorflow_backend._get_available_gpus()['/job:localhost/replica:0/task:0/device:GPU:0']</code></pre><p>Also, running <code>nvidia-smi</code> was showing the use of gpu resources during model training, the same with <code>nvidia-smi -i 0 -q -d MEMORY,UTILIZATION,POWER</code>.</p><p>In my sample case, training a single epoch went from 42s to 13s.</p>",2702370,,,2018-08-12T07:08:29.840,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/51806562
580,51834988,2,51828454,2018-08-14T06:29:28.603,0,"<p>If you don't mind the data being slightly out of date, you could use <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html"" rel=""nofollow noreferrer"">Amazon S3 Inventory</a>, which can provide a daily CSV file listing all of your objects in the Amazon S3 bucket:</p><blockquote>  <p>Amazon S3 inventory provides comma-separated values (CSV) or Apache optimized row columnar (ORC) output files that list your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or a shared prefix (that is, objects that have names that begin with a common string).</p></blockquote><p>You could parse this file to obtain Keys and Last Modified dates, then sort by date.</p>",174777,,,2018-08-14T06:29:28.603,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/51834988
581,51892437,2,15050146,2018-08-17T09:33:24.090,1,"<p>If you want to use Java program to do it you can do:</p><pre><code>public  void uploadFolder(String bucket, String path, boolean includeSubDirectories) {  File dir = new File(path);  MultipleFileUpload upload = transferManager.uploadDirectory(bucket, """", dir, includeSubDirectories);  try {  upload.waitForCompletion();  } catch (InterruptedException e) {  e.printStackTrace();  }}</code></pre><hr><p>Creation of s3client and transfer manager to connect to local S3 if you wish to test is as below:</p><pre><code>  AWSCredentials credentials = new BasicAWSCredentials(accessKey, token);  s3Client = new AmazonS3Client(credentials); // This is deprecated but you can create using standard beans provided by spring/aws  s3Client.setEndpoint(""http://127.0.0.1:9000"");//If you wish to connect to local S3 using minio etc...  TransferManager transferManager = TransferManagerBuilder.standard().withS3Client(s3Client).build();</code></pre>",2207562,,,2018-08-17T09:33:24.090,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/51892437
582,51944162,2,51551067,2018-08-21T07:59:05.643,1,"<p>Don't try to install <code>google-cloud</code>, that package <a href=""https://pypi.org/project/google-cloud/"" rel=""nofollow noreferrer"">is deprecated</a>. </p><p>As you have tagged this as Storage also, I assume you intend to use GCS from Python: rather install <a href=""https://pypi.org/project/google-cloud-storage/"" rel=""nofollow noreferrer""><code>google-cloud-storage</code></a> with <code>pip install --upgrade google-cloud-storage</code>. It is also recommended to use <a href=""https://cloud.google.com/python/setup#linux"" rel=""nofollow noreferrer"">virtualenv</a>.</p>",3058302,,,2018-08-21T07:59:05.643,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/51944162
583,52014186,2,52007936,2018-08-25T05:01:56.730,1,"<p>You have to decide what is important to you and design your table(s) to match your use cases.</p><p>You say you want to query the last value for every sensor and that there are 2000+ sensors. What will you do with these 2000+ values? How often do you need these values and can the values be slightly out of date?</p><p>One solution would be to have two tables: one where you append historical values (time series data) and another table where you always update the most recent reading for each sensor. When you need the most recent sensor data, just scan this second table to get all your sensorsŠ—È most recent values. It's as efficient as it gets for reads. For writes, it means you have to write twice for each sensor update.</p><p>The other potential solution would be to write your time series data partitioned by time, as opposed to the sensor ids. Assuming all sensors are updated at each time point, with a single query you can get the value of all sensors. This works but only if you update the vales of all sensors every time, and only if you do it with regular cadence.</p><p>However, if you update all sensors at once, then further optimizations may be had by combining multiple sensor readings into a single item, therefore requiring less writes to update all 2000 of them.</p>",63074,63074,2018-08-25T07:19:14.460,2018-08-25T07:19:14.460,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52014186
584,52047518,2,17713387,2018-08-27T21:59:55.700,2,"<p>I had this issue too. For me, it was because I had outdated credentials in my ~/.aws/config file. Fixing that solved the problem.</p>",583366,,,2018-08-27T21:59:55.700,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52047518
585,52061386,2,41796355,2018-08-28T15:21:50.337,19,"<p>The top voted answer by Ryan put me on the right track, but since AmazonS3Client is now deprecated, this code has resolved the problem for me</p><pre><code>  AmazonS3 s3 = AmazonS3ClientBuilder.standard()  .withCredentials(DefaultAWSCredentialsProviderChain.getInstance())  .build();</code></pre><p>This code appears to correctly pick up the active IAM role, say in Lambda.</p>",10285578,,,2018-08-28T15:21:50.337,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52061386
586,52064938,2,52052946,2018-08-28T19:23:10.470,1,<p>Your C++ compiler crashed. Compare the compiler version on your working system with your failing system. Your solution might be an out of date compiler / tool chain.</p>,8016720,,,2018-08-28T19:23:10.470,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52064938
587,52146845,2,45468799,2018-09-03T09:27:08.253,1,"<p>The answer above is either wrong or outdated. I'm not sure. You can increase how long your credentials last. They can last up to 12 hours. The maximum is not 1 hour. Go to IAM > Roles > Your specified role > Edit the session duration.</p><p><a href=""https://i.stack.imgur.com/hItd5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hItd5.png"" alt=""enter image description here""></a></p><p>As per the <a href=""https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html"" rel=""nofollow noreferrer"">IAM documentation</a>, the maximum is defined by the Maximum CLI/API session duration, the max of which is 12 hours.</p><p>Editing might fix your issue, considering your operation may take more than 1 hour and less than 12. If more than 12, consider editing your script to refresh credentials. To be honest, I'm not sure how to do this or if it is possible, but this <a href=""https://stackoverflow.com/questions/36894947/boto3-uses-old-credentials"">SO answer</a> might help, as well as the <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html"" rel=""nofollow noreferrer"">docs</a>.</p>",8808193,,,2018-09-03T09:27:08.253,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52146845
588,52171144,2,46146268,2018-09-04T17:14:07.323,15,"<p>Ubuntu's main download repos are maintained by Canonical, and they are outdated for GoHugo latest releases. Hence, <code>sudo apt upgrade hugo</code> will not be much of help. Even now, hugo latest release is 0.48 dated August but the apt gives me only upto 0.40 outdated as of April.</p><blockquote><p>Always try to use <code>sudo snap install hugo</code> in Ubuntu for up-to-date experience with Go Hugo. Snaps are maintained and updated very well. So even if you miss manually to update your packages, it seamlessly updates in background within 12 hours of releases after arriving at Snap Stores.</p><p>Uninstall all the instances of Hugo and try fresh installation with snaps if possible because new versions always tend to leave support for some old configs. You can try verifying your needed hugo version using <code>sudo snap search hugo</code> which gives details of which latest version is available in snap store currently.</p><p>Or manually you could download the latest release <a href=""https://github.com/gohugoio/hugo/releases"" rel=""noreferrer"">link</a> and update your version and finally check with <code>sudo hugo version</code>. You can also confirm its primary location in system by <code>whereis hugo</code> and replacing it with newer versions.</p></blockquote>",8817876,-1,2020-06-20T09:12:55.060,2018-09-04T17:19:50.940,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52171144
589,52189107,2,40126851,2018-09-05T15:48:30.720,2,"<p>Restart ECS does not fix the issue.</p><p>In my case i check logs under</p><pre><code>/var/logs/ecstail -f ecs-agent.log.2018-09-05-15</code></pre><p>So I notice </p><pre><code>2018-09-05T15:26:22Z [ERROR] Could not register: NoCredentialProviders: no valid providers in chain. Deprecated.  For verbose messaging see aws.Config.CredentialsChainVerboseErrors</code></pre><p>Just Assign ECS role and instance registered with the cluster.</p><p>Maybe this answer helps some.</p>",3288890,,,2018-09-05T15:48:30.720,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52189107
590,52205456,2,52198433,2018-09-06T13:42:34.537,0,"<p>To register your service with application load balancer you need the role for the service itself, but some permission needs for ECS container to register with cluster.</p><blockquote>  <p>Amazon Elastic Container Service uses AWS Identity and Access  Management (IAM) service-linked roles. A service-linked role is a  unique type of IAM role that is linked directly to Amazon ECS.  Service-linked roles are predefined by Amazon ECS and include all the  permissions that the service requires to call other AWS services on  your behalf.</p>   <p>A service-linked role makes setting up Amazon ECS easier because you  donŠ—Èt have to manually add the necessary permissions. Amazon ECS  defines the permissions of its service-linked roles, and unless  defined otherwise, only Amazon ECS can assume its roles. The defined  permissions include the trust policy and the permissions policy, and  that permissions policy cannot be attached to any other IAM entity.</p></blockquote><p>So if you check this role this contain property </p><pre><code>""elasticloadbalancing:RegisterInstancesWithLoadBalancer"",</code></pre><p>To <strong>Debug</strong> remove the role to service , it will not register.</p><p>If you create service using console by default it attach the role.</p><blockquote>  <p>Amazon ECS needs permissions to register and deregister container  instances with your load balancer when tasks are created and stopped.</p>   <p>In most cases, the Amazon ECS service role is automatically created  for you in the Amazon ECS console first run experience. You can use  the following procedure to check and see if your account already has  an Amazon ECS service role.</p></blockquote><p>This managed polici by AWS having ARN </p><pre><code>arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceRolePolicy ARN</code></pre><blockquote>  <p>arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceRole   Description Default policy for Amazon ECS service role.</p></blockquote><pre><code>{  ""Version"": ""2012-10-17"",  ""Statement"": [  {  ""Effect"": ""Allow"",  ""Action"": [  ""ec2:AuthorizeSecurityGroupIngress"",  ""ec2:Describe*"",  ""elasticloadbalancing:DeregisterInstancesFromLoadBalancer"",  ""elasticloadbalancing:DeregisterTargets"",  ""elasticloadbalancing:Describe*"",  ""elasticloadbalancing:RegisterInstancesWithLoadBalancer"",  ""elasticloadbalancing:RegisterTargets""  ],  ""Resource"": ""*""  }  ]}</code></pre><p><a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/check-service-role.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/check-service-role.html</a></p><p>this is how its look like</p><p><a href=""https://i.stack.imgur.com/OO7jQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OO7jQ.png"" alt=""enter image description here""></a></p><p>If you do not assign a role to ECS container instance it will never show in you ECS cluster and it will throw an error</p><pre><code>2018-09-06T15:26:22Z [ERROR] Could not register: NoCredentialProviders: no valid providers in chain. Deprecated.  For verbose messaging see aws.Config.CredentialsChainVerboseErrors</code></pre><p>under <code>/var/logs/ecs</code></p><pre><code>tail -f ecs-agent.log.2018-09-06</code></pre><p><a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using-service-linked-roles.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using-service-linked-roles.html</a></p>",3288890,3288890,2018-09-06T14:23:50.990,2018-09-06T14:23:50.990,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52205456
591,52335121,2,27191347,2018-09-14T15:33:08.923,-1,"<p>For EC2 you should edit the <code>/etc/hosts</code> file to add:</p><pre><code>XXX.XXX.XXX.XXX ip-YYY-YYY-YYY-YYY</code></pre><p>where XXX... is your external IP and the ip-YYY-YYY-YYY-YYY is the string returned by the <code>hostname</code> command. You can use <code>127.0.0.1</code> instead of your external IP to communicate inside the server.</p><p><code>host.name</code> is <a href=""https://kafka.apache.org/documentation.html#brokerconfigs"" rel=""nofollow noreferrer"">deprecated</a> - as are <code>advertised.host.name</code> and <code>advertised.port</code></p>",281545,,,2018-09-14T15:33:08.923,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52335121
592,52340887,2,45311259,2018-09-15T01:30:39.677,0,"<p>It's not possible. The closest you can get there is by using the built-in slot type <a href=""https://developer.amazon.com/docs/custom-skills/literal-slot-type-reference.html"" rel=""nofollow noreferrer"">AMAZON.Literal</a> (US only and deprecated) or <a href=""https://developer.amazon.com/docs/custom-skills/slot-type-reference.html#amazonsearchquery"" rel=""nofollow noreferrer"">AMAZON.SearchQuery</a> (available in all locales). I say ""closest"" because SearchQuery for example requires a carrier phrase in the utterance besides the slot (it can't be there alone capturing everything).</p><p>Note that the free form capture provided by these types is less accurate than if you define a custom slot type (i.e. you know more or less what you want to capture)</p>",267265,,,2018-09-15T01:30:39.677,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52340887
593,52376521,2,52370736,2018-09-17T22:33:50.727,1,"<p>So a couple of things:</p><ol><li>OpsWorks Stacks is dangerously out of date and using it should be considered highly suspect.</li><li>I don't actually recognize that <code>define</code> block thing in there, maybe that's an older OpsWorks syntax?</li><li>You can definitely run an Ansible playbook from Chef code, but I would probably go a little simpler than you have there. Probably just run <code>ansible-playbook</code> locally and aim it at localhost.</li></ol>",78722,,,2018-09-17T22:33:50.727,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52376521
594,52395119,2,52395028,2018-09-18T21:54:20.443,1,"<p><a href=""https://dbfiddle.uk/?rdbms=postgres_9.6&amp;fiddle=be5f9e22d0cf6b8eea374a0bfcdf3b30"" rel=""nofollow noreferrer"">demo:db&lt;>fiddle</a><hr><strong>Postgres</strong>:</p><pre><code>SELECT   ascii(upper(t.letter)) - 64FROM  table t</code></pre><p>Explanation:</p><ul><li><code>upper()</code> makes the input to capital letters (to handle the different ascii value for capital and non-capital letters)</li><li><code>ascii()</code> converts the letters to <a href=""https://en.wikipedia.org/wiki/ASCII#Printable_characters"" rel=""nofollow noreferrer"">ASCII code</a>. The capital letters begin at number 65.</li><li>decrease the input by 64 to shift from ASCII starting point == 65 downto 1</li></ul><hr><p><strong>Redshift</strong>:</p><p>The <code>ascii()</code> function is marked as deprecated on Redshift (<a href=""https://docs.aws.amazon.com/redshift/latest/dg/c_SQL_functions_leader_node_only.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/redshift/latest/dg/c_SQL_functions_leader_node_only.html</a>)</p><p>So one possible (and more pragmatic) solution is to get a fixed alphabet string and give out the index for a given letter:</p><pre><code>SELECT   letter,   strpos('ABCDEFGHIJKLMNOPQRSTUVWXYZ', upper(t.letter))FROM   table t</code></pre>",3984221,3984221,2018-09-18T22:33:59.853,2018-09-18T22:33:59.853,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52395119
595,52422874,2,52402376,2018-09-20T10:23:02.573,0,"<p>Setting <code>global.proxy.includeIPRanges</code> is deprecated and should not work. There was a <a href=""https://github.com/istio/istio/issues/6146"" rel=""nofollow noreferrer"">discussion</a> on Git about this. The new closest thing is <code>includeOutboundIpRanges</code> in pod's sidecar-injector Config-Map or <code>traffic.sidecar.istio.io/includeOutboundIPRanges</code> pod annotation. Annotation looks easier. For now, it is not clear in the official documentation.</p><p>You could add the annotation to your deployment:</p><pre><code>apiVersion: extensions/v1beta1kind: Deploymentmetadata:  annotations:  traffic.sidecar.istio.io/includeOutboundIPRanges: ""172.20.0.0/16""   name: service-onespec:  replicas: 1  template:  metadata:  labels:  app: service-one  spec:  containers:  - name: app  image: gcr.io/google_containers/echoserver:1.4  ports:  - containerPort: 8080</code></pre><p>And the same for second deployment.</p>",9485673,9485673,2018-09-20T12:03:52.957,2018-09-20T12:03:52.957,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52422874
596,52462657,2,52462585,2018-09-23T03:16:29.210,0,"<p>DynamoDB Streams enables a read-only view of the actions that have been performed against DynamoDB tables. Think of streams as analogous to a Kinesis stream that provides an audit of all writes to the tables. Streams does not provide the ability to enforce atomicity on writes to tables, but could be used for monitoring after the fact that the sources are out of sync. The best I can see for transactional storage within DynamoDB is in the <a href=""https://aws.amazon.com/blogs/developer/performing-conditional-writes-using-the-amazon-dynamodb-transaction-library/"" rel=""nofollow noreferrer"">AWS Developer Blog</a>, but it is from 2014 so it may be outdated.</p><p><a href=""https://aws.amazon.com/about-aws/whats-new/2014/11/10/introducing-dynamodb-streams/"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aws/whats-new/2014/11/10/introducing-dynamodb-streams/</a><a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>",1546662,,,2018-09-23T03:16:29.210,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52462657
597,52482519,2,46454876,2018-09-24T15:20:03.400,-1,"<p>Check out <a href=""https://github.com/EddyVerbruggen/nativescript-nodeify"" rel=""nofollow noreferrer"">nativescript-nodeify</a> for instructions. The cognito-specific stuff is at the very bottom. The nativescript-aws-sdk has now been deprecated so that's not a good option anymore</p>",6238533,,,2018-09-24T15:20:03.400,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52482519
598,52494343,2,52484539,2018-09-25T09:01:44.813,1,"<p>You can't really implement 'SQL MERGE' method in s3 since it's not possible to update existing data objects. </p><p>A workaround is to load existing rows in a Glue job, merge it with incoming dataset, drop obsolete records and overwrite all objects on s3. If you have a lot of data it would be more efficient to partition it by some columns and then override those partitions that should contain new data only.</p><p>If you goal is preventing duplicates then you can do similar: load existing, drop those records from incoming dataset that already exist in s3 (loaded on previous step) and then write to s3 new records only.</p>",1791510,,,2018-09-25T09:01:44.813,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52494343
599,52548475,2,52548330,2018-09-28T04:45:33.950,0,"<p>Read the following PHP documentation linked below, particularly the backward incompatibility changes and deprecated features.  If you use any of those, you need to change your code. If you don't, then you should be fine.  You need to test it.</p><p><a href=""http://php.net/manual/en/migration56.php"" rel=""nofollow noreferrer"">http://php.net/manual/en/migration56.php</a></p>",3720435,,,2018-09-28T04:45:33.950,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52548475
600,52556597,2,12871033,2018-09-28T13:35:09.380,-1,"<p>something is obsoleted</p><pre><code>List&lt;Reservation&gt; listReservations = ec2Client.DescribeInstances(requestInstances).Reservations;foreach (Reservation reservation_item in listReservations){  foreach (Instance instance_item in reservation_item.Instances)  {  WriteLine(instance_item.InstanceId);  WriteLine(instance_item.InstanceType);  }}</code></pre>",10429577,1988508,2018-09-28T13:38:21.370,2018-09-28T13:38:21.370,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52556597
601,52563919,2,52495435,2018-09-29T00:06:16.753,2,"<p>The error message refers to <code>part_number</code> which is the name of a CSV column (called ""flat files"" in MWS). In XML you need to specify a <code>StandardProductID</code> instead, like this:</p><pre><code>  &lt;StandardProductID&gt;  &lt;Type&gt;EAN&lt;/Type&gt;  &lt;Value&gt;1234567890123&lt;/Value&gt;  &lt;/StandardProductID&gt;  </code></pre><p>Please also note, that my <code>Beauty.xsd</code> does not have <code>HairCareProduct</code>. I had to change this to <code>BeautyMisc</code> to validate the feed - but that may be due to an outdated set of XSDs.</p>",2097290,,,2018-09-29T00:06:16.753,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52563919
602,52617621,2,52600867,2018-10-02T23:11:43.827,0,"<p>Out of curiosity, have you read <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">How Amazon DynamoDB adaptive capacity accommodates uneven data access patterns (or, why what you know about DynamoDB might be outdated)</a> yet?</p>",10448632,,,2018-10-02T23:11:43.827,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52617621
603,52631083,2,52628056,2018-10-03T15:51:13.923,3,"<p>According to the <a href=""https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html#RESTBucketGET-responses"" rel=""nofollow noreferrer"">documentation</a> for NextMarker:</p><blockquote>  <p>Note</p>   <p>This element is returned only if you specify a delimiter request parameter. If the  response does not include the NextMarker and it is truncated, you can use the value of the last Key in the response as the marker in the subsequent request to get the next set of object keys.</p></blockquote><p>Your code should therefore be:</p><pre><code>if(!isDone) {  objects_request.SetMarker(outcome.GetResult().GetContents().back().GetKey());}</code></pre><p>Also note that the V1 ListObjects method is deprecated, you should be using <a href=""https://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_s3_1_1_s3_client.html#ae6bcac3c32efa6939df8b507112ba2ce"" rel=""nofollow noreferrer"">ListObjectsV2</a> which uses explicit continuation tokens which are a bit easier to use.</p>",5494370,,,2018-10-03T15:51:13.923,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52631083
604,52640164,2,52639945,2018-10-04T06:15:16.910,0,"<p>The problem is that you are using EB CLI version 2.x. This is deprecated and no longer supported. The base set of commands changed with version 3.x.</p><p><a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3.html"" rel=""nofollow noreferrer"">The Elastic Beanstalk Command Line Interface (EB CLI)</a></p><p>The solution is to install the latest version.</p><p><a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install.html"" rel=""nofollow noreferrer"">Install the Elastic Beanstalk Command Line Interface (EB CLI)</a></p>",8016720,,,2018-10-04T06:15:16.910,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52640164
605,52708413,2,52707801,2018-10-08T18:50:25.260,1,"<p>The <code>cafile</code> option was added later, and is not available in my version of npm. It only looks at the <code>ca</code> option, which is set to this by default:</p><pre><code>npm config get ca[ '-----BEGIN CERTIFICATE-----\nMIIChzCCAfACCQDauvz/KHp8ejANBgkqhkiG9w0BAQUFADCBhzELMAkGA1UEBhMC\nVVMxCzAJBgNVBAgTAkNBMRAwDgYDVQQHEwdPYWtsYW5kMQwwCgYDVQQKEwNucG0x\nIjAgBgNVBAsTGW5wbSBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkxDjAMBgNVBAMTBW5w\nbUNBMRcwFQYJKoZIhvcNAQkBFghpQGl6cy5tZTAeFw0xMTA5MDUwMTQ3MTdaFw0y\nMTA5MDIwMTQ3MTdaMIGHMQswCQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExEDAOBgNV\nBAcTB09ha2xhbmQxDDAKBgNVBAoTA25wbTEiMCAGA1UECxMZbnBtIENlcnRpZmlj\nYXRlIEF1dGhvcml0eTEOMAwGA1UEAxMFbnBtQ0ExFzAVBgkqhkiG9w0BCQEWCGlA\naXpzLm1lMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDLI4tIqPpRW+ACw9GE\nOgBlJZwK5f8nnKCLK629Pv5yJpQKs3DENExAyOgDcyaF0HD0zk8zTp+ZsLaNdKOz\nGn2U181KGprGKAXP6DU6ByOJDWmTlY6+Ad1laYT0m64fERSpHw/hjD3D+iX4aMOl\ny0HdbT5m1ZGh6SJz3ZqxavhHLQIDAQABMA0GCSqGSIb3DQEBBQUAA4GBAC4ySDbC\nl7W1WpLmtLGEQ/yuMLUf6Jy/vr+CRp4h+UzL+IQpCv8FfxsYE7dhf/bmWTEupBkv\nyNL18lipt2jSvR3v6oAHAReotvdjqhxddpe5Holns6EQd1/xEZ7sB1YhQKJtvUrl\nZNufy1Jf1r0ldEGeA+0ISck7s+xSh9rQD2Op\n-----END CERTIFICATE-----\n',  '-----BEGIN CERTIFICATE-----\nMIIDdTCCAl2gAwIBAgILBAAAAAABFUtaw5QwDQYJKoZIhvcNAQEFBQAwVzELMAkGA1UEBhMCQkUx\nGTAXBgNVBAoTEEdsb2JhbFNpZ24gbnYtc2ExEDAOBgNVBAsTB1Jvb3QgQ0ExGzAZBgNVBAMTEkds\nb2JhbFNpZ24gUm9vdCBDQTAeFw05ODA5MDExMjAwMDBaFw0yODAxMjgxMjAwMDBaMFcxCzAJBgNV\nBAYTAkJFMRkwFwYDVQQKExBHbG9iYWxTaWduIG52LXNhMRAwDgYDVQQLEwdSb290IENBMRswGQYD\nVQQDExJHbG9iYWxTaWduIFJvb3QgQ0EwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDa\nDuaZjc6j40+Kfvvxi4Mla+pIH/EqsLmVEQS98GPR4mdmzxzdzxtIK+6NiY6arymAZavpxy0Sy6sc\nTHAHoT0KMM0VjU/43dSMUBUc71DuxC73/OlS8pF94G3VNTCOXkNz8kHp1Wrjsok6Vjk4bwY8iGlb\nKk3Fp1S4bInMm/k8yuX9ifUSPJJ4ltbcdG6TRGHRjcdGsnUOhugZitVtbNV4FpWi6cgKOOvyJBNP\nc1STE4U6G7weNLWLBYy5d4ux2x8gkasJU26Qzns3dLlwR5EiUWMWea6xrkEmCMgZK9FGqkjWZCrX\ngzT/LCrBbBlDSgeF59N89iFo7+ryUp9/k5DPAgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNV\nHRMBAf8EBTADAQH/MB0GA1UdDgQWBBRge2YaRQ2XyolQL30EzTSo//z9SzANBgkqhkiG9w0BAQUF\nAAOCAQEA1nPnfE920I2/7LqivjTFKDK1fPxsnCwrvQmeU79rXqoRSLblCKOzyj1hTdNGCbM+w6Dj\nY1Ub8rrvrTnhQ7k4o+YviiY776BQVvnGCv04zcQLcFGUl5gE38NflNUVyRRBnMRddWQVDf9VMOyG\nj/8N7yy5Y0b2qvzfvGn9LhJIZJrglfCm7ymPAbEVtQwdpf5pLGkkeB6zpxxxYu7KyJesF12KwvhH\nhm4qxFYxldBniYUr+WymXUadDKqC5JlR3XC321Y9YeRq4VzW9v493kHMB65jUr9TU/Qr6cf9tveC\nX4XSQRjbgbMEHMUfpIBvFSDJ3gyICh3WZlXi/EjJKSZp4A==\n-----END CERTIFICATE-----\n',  '-----BEGIN CERTIFICATE-----\nMIIDujCCAqKgAwIBAgILBAAAAAABD4Ym5g0wDQYJKoZIhvcNAQEFBQAwTDEgMB4GA1UECxMXR2xv\nYmFsU2lnbiBSb290IENBIC0gUjIxEzARBgNVBAoTCkdsb2JhbFNpZ24xEzARBgNVBAMTCkdsb2Jh\nbFNpZ24wHhcNMDYxMjE1MDgwMDAwWhcNMjExMjE1MDgwMDAwWjBMMSAwHgYDVQQLExdHbG9iYWxT\naWduIFJvb3QgQ0EgLSBSMjETMBEGA1UEChMKR2xvYmFsU2lnbjETMBEGA1UEAxMKR2xvYmFsU2ln\nbjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKbPJA6+Lm8omUVCxKs+IVSbC9N/hHD6\nErPLv4dfxn+G07IwXNb9rfF73OX4YJYJkhD10FPe+3t+c4isUoh7SqbKSaZeqKeMWhG8eoLrvozp\ns6yWJQeXSpkqBy+0Hne/ig+1AnwblrjFuTosvNYSuetZfeLQBoZfXklqtTleiDTsvHgMCJiEbKjN\nS7SgfQx5TfC4LcshytVsW33hoCmEofnTlEnLJGKRILzdC9XZzPnqJworc5HGnRusyMvo4KD0L5CL\nTfuwNhv2GXqF4G3yYROIXJ/gkwpRl4pazq+r1feqCapgvdzZX99yqWATXgAByUr6P6TqBwMhAo6C\nygPCm48CAwEAAaOBnDCBmTAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4E\nFgQUm+IHV2ccHsBqBt5ZtJot39wZhi4wNgYDVR0fBC8wLTAroCmgJ4YlaHR0cDovL2NybC5nbG9i\nYWxzaWduLm5ldC9yb290LXIyLmNybDAfBgNVHSMEGDAWgBSb4gdXZxwewGoG3lm0mi3f3BmGLjAN\nBgkqhkiG9w0BAQUFAAOCAQEAmYFThxxol4aR7OBKuEQLq4GsJ0/WwbgcQ3izDJr86iw8bmEbTUsp\n9Z8FHSbBuOmDAGJFtqkIk7mpM0sYmsL4h4hO291xNBrBVNpGP+DTKqttVCL1OmLNIG+6KYnX3ZHu\n01yiPqFbQfXf5WRDLenVOavSot+3i9DAgBkcRcAtjOj4LaR0VknFBbVPFd5uRHg5h6h+u/N5GJG7\n9G+dwfCMNYxdAfvDbbnvRG15RjF+Cv6pgsH/76tuIMRQyV+dTZsXjAzlAcmgQWpzU/qlULRuJQ/7\nTBj0/VLZjmmx6BEP3ojY+x1J96relc8geMJgEtslQIxq/H5COEBkEveegeGTLg==\n-----END CERTIFICATE-----\n',  '-----BEGIN CERTIFICATE-----\nMIIEYDCCA0igAwIBAgILBAAAAAABL07hRQwwDQYJKoZIhvcNAQEFBQAwVzELMAkG\nA1UEBhMCQkUxGTAXBgNVBAoTEEdsb2JhbFNpZ24gbnYtc2ExEDAOBgNVBAsTB1Jv\nb3QgQ0ExGzAZBgNVBAMTEkdsb2JhbFNpZ24gUm9vdCBDQTAeFw0xMTA0MTMxMDAw\nMDBaFw0yMjA0MTMxMDAwMDBaMF0xCzAJBgNVBAYTAkJFMRkwFwYDVQQKExBHbG9i\nYWxTaWduIG52LXNhMTMwMQYDVQQDEypHbG9iYWxTaWduIE9yZ2FuaXphdGlvbiBW\nYWxpZGF0aW9uIENBIC0gRzIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIB\nAQDdNR3yIFQmGtDvpW+Bdllw3Of01AMkHyQOnSKf1Ccyeit87ovjYWI4F6+0S3qf\nZyEcLZVUunm6tsTyDSF0F2d04rFkCJlgePtnwkv3J41vNnbPMYzl8QbX3FcOW6zu\nzi2rqqlwLwKGyLHQCAeV6irs0Z7kNlw7pja1Q4ur944+ABv/hVlrYgGNguhKujiz\n4MP0bRmn6gXdhGfCZsckAnNate6kGdn8AM62pI3ffr1fsjqdhDFPyGMM5NgNUqN+\nARvUZ6UYKOsBp4I82Y4d5UcNuotZFKMfH0vq4idGhs6dOcRmQafiFSNrVkfB7cVT\n5NSAH2v6gEaYsgmmD5W+ZoiTAgMBAAGjggElMIIBITAOBgNVHQ8BAf8EBAMCAQYw\nEgYDVR0TAQH/BAgwBgEB/wIBADAdBgNVHQ4EFgQUXUayjcRLdBy77fVztjq3OI91\nnn4wRwYDVR0gBEAwPjA8BgRVHSAAMDQwMgYIKwYBBQUHAgEWJmh0dHBzOi8vd3d3\nLmdsb2JhbHNpZ24uY29tL3JlcG9zaXRvcnkvMDMGA1UdHwQsMCowKKAmoCSGImh0\ndHA6Ly9jcmwuZ2xvYmFsc2lnbi5uZXQvcm9vdC5jcmwwPQYIKwYBBQUHAQEEMTAv\nMC0GCCsGAQUFBzABhiFodHRwOi8vb2NzcC5nbG9iYWxzaWduLmNvbS9yb290cjEw\nHwYDVR0jBBgwFoAUYHtmGkUNl8qJUC99BM00qP/8/UswDQYJKoZIhvcNAQEFBQAD\nggEBABvgiADHBREc/6stSEJSzSBo53xBjcEnxSxZZ6CaNduzUKcbYumlO/q2IQen\nfPMOK25+Lk2TnLryhj5jiBDYW2FQEtuHrhm70t8ylgCoXtwtI7yw07VKoI5lkS/Z\n9oL2dLLffCbvGSuXL+Ch7rkXIkg/pfcNYNUNUUflWP63n41edTzGQfDPgVRJEcYX\npOBWYdw9P91nbHZF2krqrhqkYE/Ho9aqp9nNgSvBZnWygI/1h01fwlr1kMbawb30\nhag8IyrhFHvBN91i0ZJsumB9iOQct+R2UTjEqUdOqCsukNK1OFHrwZyKarXMsh3o\nwFZUTKiL8IkyhtyTMr5NGvo1dbU=\n-----END CERTIFICATE-----\n' ]</code></pre><p>I didn't check where this default came from, but I'm guessing it's hardcoded and rather outdated.</p><p>Okay, so let's just convert our system-wide CA bundle into a setting in <a href=""https://docs.npmjs.com/misc/config#ca"" rel=""nofollow noreferrer"">the format that <code>ca</code> expects</a>:</p><pre><code>cat /etc/pki/tls/certs/ca-bundle.crt | \  awk 'cert { cert = cert ""\\n"" $0 }  /-----BEGIN CERTIFICATE-----/ { cert = $0 }  /-----END CERTIFICATE-----/ {  printf ""ca[] = \""%s\""\n"", cert;  cert = """"  }'  &gt; ~/.npmrc</code></pre><p>If it's stupid but it works, it ain't stupid.</p>",14637,14637,2018-10-08T18:56:05.093,2018-10-08T18:56:05.093,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52708413
606,52729688,2,51106504,2018-10-09T21:37:37.247,0,"<p>For any interested parties, <code>cs-import-documents</code> no longer exists. This was part of an older version of the AWC CLI, but has since been deprecated and not replaced. </p>",4043845,,,2018-10-09T21:37:37.247,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52729688
607,52732126,2,52731573,2018-10-10T03:21:16.923,2,"<p>Here are the steps at a high level to setup a fresh application with new image.</p><blockquote><ol><li><p>Do a git clone of the repository.</p></li><li><p>Modify the public/index.html locally.</p></li><li><p>Do a Docker build using <code>docker build ....</code></p></li><li><p>Push the image to a registry (<a href=""https://hub.docker.com/"" rel=""nofollow noreferrer"">https://hub.docker.com/</a> or <a href=""https://cloud.google.com/container-registry/"" rel=""nofollow noreferrer"">https://cloud.google.com/container-registry/</a> or some where else). The command depends on the registry. Also, make sure that the image is public.</p></li><li><p>Update the image appropriately in guestbook-controller.json.</p></li><li><p>Follow the steps as mentioned in the README.md.</p></li></ol></blockquote><p>If you want to update the image in an existing K8S application already running, then a rolling-update has to be done as mentioned <a href=""https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#updating-the-container-image"" rel=""nofollow noreferrer"">here</a>.</p><p>FYI ..... Without creating the image, the index.html can also be modified by copying the new index.html to all the running Pods as mentioned <a href=""https://stackoverflow.com/questions/52677618/how-to-use-kubectl-cp-to-copy-files-automatically-from-a-local-system-to-kuberne/52678961#52678961"">here</a>.</p><p>FYI ..... the example uses <a href=""https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/"" rel=""nofollow noreferrer"">ReplicationController</a> which is outdated (mentioned in guestbook-controller.json), a <a href=""https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"" rel=""nofollow noreferrer"">Deployment</a> is the recommended way.</p>",614157,-1,2020-06-20T09:12:55.060,2018-10-11T01:17:50.110,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52732126
608,52775445,2,47446171,2018-10-12T08:33:56.780,22,"<p>DynamoDb will store the boolean value as 0 or 1 by default. </p><p>Use the following decorators to save the attribute as <code>false</code> or <code>true</code> respectively.</p><pre><code>@DynamoDBTyped(DynamoDBAttributeType.BOOL)@DynamoDBAttributeprivate boolean notificationFlag;</code></pre><p>Note: <code>@DynamoDBNativeBoolean</code> which used to do this is <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBNativeBoolean.html"" rel=""noreferrer"">deprecated</a></p>",1921287,,,2018-10-12T08:33:56.780,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52775445
609,52835056,2,50722876,2018-10-16T12:04:38.230,0,"<p>The unofficial backport should soon be obsolete. Apache httpd team planned to release 2.4.36 with openssl 1.1.1 and tlsv1.3 support.Due to an issue with h2 this has been stopped, you can read more about it here:<a href=""https://lists.apache.org/thread.html/1ebfadad8f34cdf8520e2b9771cd8b8ca1a11b8d6d416b41e73c2687@%3Cdev.httpd.apache.org%3E"" rel=""nofollow noreferrer"">lists.apache.org/</a></p><p>If you still want to compile by yourself, you can</p><ul><li>either check out the source via <a href=""https://github.com/apache/httpd/tree/2.4.36"" rel=""nofollow noreferrer"">github</a> (tag: 2.4.36)</li><li>or download the <a href=""https://dist.apache.org/repos/dist/dev/httpd/"" rel=""nofollow noreferrer"">source</a> directly from apache dist server (content not static, will no longer be present as soon as an update was uploaded)</li></ul><p>Maybe you want to apply the h2 fix from <a href=""http://svn.apache.org/viewvc?view=revision&amp;revision=1843954"" rel=""nofollow noreferrer"">rev 1843954</a></p>",10074916,,,2018-10-16T12:04:38.230,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52835056
610,52899085,2,52824961,2018-10-19T19:53:51.697,3,"<p>ElastiCache docs are way out of date; new announcements change what's available even when the three-year old docs remain unchanged. Redis on ElastiCache introduced support for online resizing in 2017. From <a href=""https://aws.amazon.com/blogs/aws/amazon-elasticache-update-online-resizing-for-redis-clusters/"" rel=""nofollow noreferrer"">the announcement</a>:</p><blockquote>  <p>You can now adjust the number of shards in a running ElastiCache for  Redis cluster while the cluster remains online and responding to  requests. This gives you the power to respond to changes in traffic  and data volume without having to take the cluster offline or to start  with an empty cache. You can also rebalance a running cluster to  uniformly redistribute slot space without changing the number of  shards.</p></blockquote><p>I wish they'd update their 2015 (!) docs, but at any rate, this is the latest we have on the subject. As of October 19, 2018, on a cluster with cluster mode enabled:</p><p>You can:</p><ul><li>Scale out (add shards)</li><li>Scale in (remove shards)</li><li>Rebalance (move keys among shards)</li><li>Online resharding and shard balancing</li></ul><p>You cannot:</p><ul><li>Scale up/down (change node type)</li><li>Upgrade your engine</li><li>Configure shards independently</li></ul><p>Source: <a href=""https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/redis-cluster-resharding-online.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/redis-cluster-resharding-online.html</a></p>",40352,40352,2018-10-19T20:13:57.350,2018-10-19T20:13:57.350,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52899085
611,52959606,2,52947430,2018-10-24T00:42:34.160,3,"<p>I think that in this case, the simplest method will be the best: you could just keep a pointer to a <code>Transform</code> component in components that read/write it.</p><p>I don't think that using events (or some other indirection, like observers) solves any real problem in here.</p><ol><li><p><code>Transform</code> component is very simple - it's not something that will be changed during development. Abstracting access to it will actually make code more complex and harder to maintain.</p></li><li><p><code>Transform</code> is a component that will be frequently changed for many objects, maybe even most of your objects will update it each frame. Sending events each time there's a change has a cost - probably much higher than simply copying matrix/vector/quaternion from one location to another.</p></li><li><p>I think that using events, or some other abstraction, won't solve other problems, like multiple components updating the same <code>Transform</code> component, or components using outdated transform data.</p></li><li><p>Typically, renderers just copy all matrices of rendered objects each frame. There's no point in caching them in a rendering system.</p></li></ol><p>Components like <code>Transform</code> are used often. Making them overly complex might be a problem in many different parts of an engine, while using the simplest solution, a pointer, will give you greater freedom.</p><hr><p>BTW, there's also very simple way to make sure that <code>RenderComponent</code> will read transform <em>after</em> it has been updated (e. g. by <code>PhysicsComponent</code>) - you can split work into two steps:</p><ol><li><p><code>Update()</code> in which systems may modify components, and</p></li><li><p><code>PostUpdate()</code> in which systems can only read data from components</p></li></ol><p>For example <code>PhysicsSystem::Update()</code> might copy transform data to respective <code>TransformComponent</code> components, and then <code>RenderSystem::PostUpdate()</code> could just read from <code>TransformComponent</code>, without the risk of using outdated data.</p>",8964493,,,2018-10-24T00:42:34.160,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/52959606
612,53062912,2,33455244,2018-10-30T11:03:50.570,0,"<p>Upgrading a MySQL DB instance can be tricky but easy to achieve through the following steps.</p><p><strong><em>1. Ensure that running an updated version of MySQL Engine(not deprecated).</em></strong>Trying to upgrade a deprecated MySQL Version via the AWS Console(UI) results in an error message.""</p><blockquote>  <p>""Cannot find version 5.... for mysql (Service: AmazonRDS; Status Code:  400; Error Code: InvalidParameterCombination; Request ID:........""</p></blockquote><p><strong>Even Snapshot Restore is most likely to run for several mins/hours without any success.</strong> </p><p>2.<strong>Use AWS CLI</strong> </p><p>Use 'modify-db-instance' command to scale the storage size[1] and applied the version upgrade on your DB instance. Here's the example command:</p><pre><code>aws rds modify-db-instance \   --db-instance-identifier &lt;RDS_identifier&gt; \   --allocated-storage &lt;storage_size&gt; \   --apply-immediately </code></pre><p>You may also refer to this guide on to install AWS CLI toll: Installing the AWS Command Line Interface - [<a href=""https://docs.aws.amazon.com/cli/latest/userguide/installing.html][1]"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cli/latest/userguide/installing.html][1]</a></p><p>3.After the successful upgrade, optionally modify/upgrade your MySQL version to a non depreciated Version via the AWS Console(UI).</p>",2841570,,,2018-10-30T11:03:50.570,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53062912
613,53080135,2,53073711,2018-10-31T09:27:13.497,1,"<p>It looks like that documentation is out of date with the library; it is written for the SDK v1 but the latest version is v2.</p><p>You have a few options:</p><ol><li>Use v1 of the library and follow that tutorial2. </li><li>Follow these steps for migrating the various classes used in the tutorial (<a href=""https://github.com/alexa/alexa-skills-kit-sdk-for-java/blob/2.0.x/docs/en/Migrating-To-ASK-SDK-v2-For-Java.rst#request-handlers"" rel=""nofollow noreferrer"">https://github.com/alexa/alexa-skills-kit-sdk-for-java/blob/2.0.x/docs/en/Migrating-To-ASK-SDK-v2-For-Java.rst#request-handlers</a>)</li><li>Use the v2 version of the hello-world example (<a href=""https://github.com/alexa/alexa-skills-kit-sdk-for-java/tree/2.0.x/samples/helloworld"" rel=""nofollow noreferrer"">https://github.com/alexa/alexa-skills-kit-sdk-for-java/tree/2.0.x/samples/helloworld</a>)</li></ol>",2531132,,,2018-10-31T09:27:13.497,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53080135
614,53259973,2,53257307,2018-11-12T10:14:13.097,0,"<p>Try using the ""preference"" parameter while querying the data. Something like this:</p><pre><code>es.search(index=""myindex"",  body={""query"": {""match"": {""text_field"": ""search_term""}}},  preference=""_primary_first"")</code></pre><p>Update:Some possible values like ""_primary_first"" have been deprecated as of Elasticsearch 6.x and will be completely removed in Elasticsearch 7.0</p>",7076459,7076459,2018-11-13T07:10:33.580,2018-11-13T07:10:33.580,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53259973
615,53315348,2,53228612,2018-11-15T08:40:29.743,0,"<p>The problem is that (<a href=""https://www.postgresql.org/docs/9.1/sql-vacuum.html"" rel=""nofollow noreferrer"">source</a>):</p><blockquote>  <p>""In normal PostgreSQL operation, tuples that are deleted or obsoleted by an update are not physically removed from their table""</p></blockquote><p>Furthermore, we did not always close the cursor which also increased database size while running.</p><p>One last problem is that we were running one huge query, not allowing the system to autovacuum properly. This problem is described in more detail <a href=""http://blog.lerner.co.il/in-postgresql-as-in-life-dont-wait-too-long-to-commit/"" rel=""nofollow noreferrer"">here</a></p><p>Our solution was to re-approach the problem such that the rows did not have to be updated. Other solutions that we could think of but have not tried is to stop the process every once in a while allowing the autovacuum to work correctly.</p>",6766457,6766457,2018-11-16T08:24:37.213,2018-11-16T08:24:37.213,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53315348
616,53330262,2,52715441,2018-11-16T01:39:10.943,1,"<p>Most likely you are trying to rotate the secret too quickly. Secrets Manager will automatically clean up <a href=""https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_delete-restore-secret.html#proc-delete-version"" rel=""nofollow noreferrer"">deprecated secrets</a> asynchronously, but this takes time. If you create them faster than they can be deleted, you will run into limits.</p>",5844292,,,2018-11-16T01:39:10.943,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53330262
617,53334157,2,37249475,2018-11-16T08:39:05.367,6,"<p><strong>Important: AMAZON.LITERAL is deprecated as of October 22, 2018. Older skills built with AMAZON.LITERAL do continue to work, but you must migrate away from AMAZON.LITERAL when you update those older skills, and for all new skills.</strong></p><p>Instead of using AMAZON.LITERAL, you can use a custom slot to trick alexa into passing the free flow text into the backend.</p><p>You can use this configuration to do it:</p><pre><code>{  ""interactionModel"": {  ""languageModel"": {  ""invocationName"": ""siri"",  ""intents"": [  {  ""name"": ""SaveIntent"",  ""slots"": [  {  ""name"": ""text"",  ""type"": ""catchAll""  }  ],  ""samples"": [  ""{text}""  ]  }  ],  ""types"": [  {  ""name"": ""catchAll"",  ""values"": [  {  ""name"": {  ""value"": ""allonymous isoelectrically salubrity apositia phantomize Sangraal externomedian phylloidal""  }  },  {  ""name"": {  ""value"": ""imbreviate Bertie arithmetical undramatically braccianite eightling imagerially leadoff""  }  },  {  ""name"": {  ""value"": ""mistakenness preinspire tourbillion caraguata chloremia unsupportedness squatarole licitation""  }  },  {  ""name"": {  ""value"": ""Cimbric sigillarid deconsecrate acceptableness balsamine anostosis disjunctively chafflike""  }  },  {  ""name"": {  ""value"": ""earsplitting mesoblastema outglow predeclare theriomorphism prereligious unarousing""  }  },  {  ""name"": {  ""value"": ""ravinement pentameter proboscidate unexigent ringbone unnormal Entomophila perfectibilism""  }  },  {  ""name"": {  ""value"": ""defyingly amoralist toadship psoatic boyology unpartizan merlin nonskid""  }  },  {  ""name"": {  ""value"": ""broadax lifeboat progenitive betel ashkoko cleronomy unpresaging pneumonectomy""  }  },  {  ""name"": {  ""value"": ""overharshness filtrability visual predonate colisepsis unoccurring turbanlike flyboy""  }  },  {  ""name"": {  ""value"": ""kilp Callicarpa unforsaken undergarment maxim cosenator archmugwump fitted""  }  },  {  ""name"": {  ""value"": ""ungutted pontificially Oudenodon fossiled chess Unitarian bicone justice""  }  },  {  ""name"": {  ""value"": ""compartmentalize prenotice achromat suitability molt stethograph Ricciaceae ultrafidianism""  }  },  {  ""name"": {  ""value"": ""slotter archae contrastimulant sopper Serranus remarry pterygial atactic""  }  },  {  ""name"": {  ""value"": ""superstrata shucking Umbrian hepatophlebotomy undreaded introspect doxographer tractility""  }  },  {  ""name"": {  ""value"": ""obstructionist undethroned unlockable Lincolniana haggaday vindicatively tithebook""  }  },  {  ""name"": {  ""value"": ""unsole relatively Atrebates Paramecium vestryish stockfish subpreceptor""  }  },  {  ""name"": {  ""value"": ""babied vagueness elabrate graphophonic kalidium oligocholia floccus strang""  }  },  {  ""name"": {  ""value"": ""undersight monotriglyphic uneffete trachycarpous albeit pardonableness Wade""  }  },  {  ""name"": {  ""value"": ""minacious peroratory filibeg Kabirpanthi cyphella cattalo chaffy savanilla""  }  },  {  ""name"": {  ""value"": ""Polyborinae Shakerlike checkerwork pentadecylic shopgirl herbary disanagrammatize shoad""  }  }  ]  }  ]  }  }}</code></pre>",6741167,,,2018-11-16T08:39:05.367,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53334157
618,53349294,2,53228612,2018-11-17T07:48:02.293,2,"<p>If you have not changed that then your <a href=""https://www.postgresql.org/docs/11/sql-createtable.html#id-1.9.3.85.6.3.2"" rel=""nofollow noreferrer""><code>fillfactor</code></a> is at 100% on the table since that is the default.</p><p>This means that every change in your table will mark the changed row as obsolete and will recreate the updated row. The issue could be even worse if you have indices on your table since those should be updated on every row change too. As you could imagine this hurts the <code>UPDATE</code> performance too.</p><p>So technically if you would read the whole table and update even the smallest column after reading the rows then it would double the table size when your <code>fillfactor</code> is 100.</p><p>What you can do is to <code>ALTER</code> your table lower the <code>fillfactor</code> on it, then <code>VACUUM</code> it:</p><pre><code>ALTER TABLE your_table SET (fillfactor = 90);VACUUM FULL your_table;</code></pre><p>Of course with this step your table will be about <code>10%</code> bigger but Postgres will spare some space for your updates and it won't change its size with your process.</p><p>The reason why autovacuum helps is because it cleans the obsoleted rows periodically and therefore it will keep your table at the same size. But it puts a lot of pressure on your database. If you happen to know that you'll do operations like you described in the opening question then I would recommend tuning the <code>fillfactor</code> for your needs.</p>",221213,221213,2018-11-17T10:25:56.970,2018-11-17T10:25:56.970,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53349294
619,53427302,2,49553573,2018-11-22T09:08:58.530,0,"<p>The problem is in the amazon's outdated SNS documentation.</p><p>I have changed the payload format and it works for me.</p><pre><code>{""GCM"": ""{ \""notification\"": { \""body\"": \""hello....\"", \""title\"": \""title123\"" } }""}</code></pre>",97651,,,2018-11-22T09:08:58.530,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53427302
620,53455439,2,53455415,2018-11-24T05:27:13.707,0,"<p>The below answer is basically obsolete, but I'm leaving it for posterity. DynamoDB <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html"" rel=""nofollow noreferrer"">now supports transactions</a> which allow you to provide a condition expression for a different key value in the same or another table.</p><hr><p>No, a ConditionExpression cannot look at any items other that the current item which is being saved/updated/deleted.</p><p><strong>Update</strong></p><p>If possible, can you design your data in such a way that things cannot be deleted? (Maybe you can mark it as obsolete instead.) That eliminates the problem entirely because once something exists, it will always exist. </p><p>If thatŠ—Ès not possible, you might want to look at <a href=""https://github.com/awslabs/dynamodb-lock-client"" rel=""nofollow noreferrer"">DynamoDBLockClient</a>. It provides a general-purpose distributed locking API thatŠ—Ès backed by DynamoDB. </p><p>Your new pseudo code would be</p><pre><code>a. if parent identifier is setb.   Acquire lock for parent identifierc.   get parent resource from DynamoDBd.   if parent not founde.   Release lockf.   return 404g. persist resource in DynamoDBh. Release lock</code></pre><p>Also, in any operation where you are deleting something that is a parent identifier, you should acquire a lock on that parent identifier before deleting it. If you do this, you can ensure that the parent does not get deleted in the midst of a child being created. </p>",5563569,5563569,2019-07-20T05:30:47.383,2019-07-20T05:30:47.383,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53455439
621,53463672,2,53463247,2018-11-25T00:34:14.337,1,"<p>The question seems premised on obsolete information.</p><p><a href=""https://web.archive.org/web/20140105032747/https://aws.amazon.com/dynamodb/pricing/"" rel=""nofollow noreferrer"">Previously</a>, DynamoDB capacity was sold in blocks of 10 WCU and 50 RCU, each of which was $0.0065.  (It may have been $0.01 several years ago, since AWS does sometimes decrease pricing when new technologies or efficiency gains permit it or this might be the result of rounding.)  The 4 is 200 ’à 50.</p>",1695906,,,2018-11-25T00:34:14.337,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53463672
622,53491672,2,53491598,2018-11-27T01:55:37.023,1,"<p>The only way I can see the same query returning two different result sets is if, somehow, the underlying data is being changed.  The obvious cause might be if data is being added/removed in between when you run the same query twice.</p><p>However, in the case of AWS, there is another explanation.  If your database instance is logically replicated across RDS, then it might be possible that the first query hits the database before newly added/removed data has been replicated across RDS.  That is, your query could actually be executing against a slightly out of date database.</p>",1863229,,,2018-11-27T01:55:37.023,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53491672
623,53589551,2,53586562,2018-12-03T07:53:11.870,0,"<p>Depends what image you use. Lightsail is just a type of instance, which is kind of isolated from aws ecosystem(I would go for a ec2 t2 instance). </p><p>When you start an instance you select an image with an operating system. It doesn't mean it comes with the nodejs. You have to install it after you start the image(or to look for an image which has nodejs preinstalled).</p><p>If it has node installed, usually, in linux it's an obsolete version. Better to install nvm and then to select the desired node version. </p>",268856,,,2018-12-03T07:53:11.870,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53589551
624,53726450,2,53726267,2018-12-11T14:38:48.687,2,"<p>Yeah, but I've managed to find it:</p><pre><code>// Item itemInternalUtils.toAttributeValues(item)</code></pre><p>However, above API is deprecated in newer DynamoDB library which basically delegates the call to <code>ItemUtils</code> which is not deprecated fortunately. So I ended up using this:</p><pre><code>ItemUtils.toAttributeValues(item)</code></pre><p>Hope this will help others in future!</p>",1162233,1162233,2018-12-11T14:43:42.173,2018-12-11T14:43:42.173,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53726450
625,53737797,2,43000359,2018-12-12T07:12:31.353,0,"<p>@DonMag solution seem to be outdated but was of big help !!</p><p>Here is the updated one : </p><pre><code>com.amazon.mobile.shopping://www.amazon.com/products/B00KWFCV32/</code></pre>",4046561,,,2018-12-12T07:12:31.353,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53737797
626,53749018,2,53748832,2018-12-12T18:14:15.720,1,"<p>There is no way of doing that yet, officially supported, at least.</p><p><a href=""https://github.com/Azure/AKS/issues/371"" rel=""nofollow noreferrer"">https://github.com/Azure/AKS/issues/371</a></p><p>EDIT: this answer is outdated, this is now possible</p>",6067741,6067741,2019-02-25T21:01:21.313,2019-02-25T21:01:21.313,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53749018
627,53772908,2,48340877,2018-12-14T02:56:14.347,0,"<p>After reading your final comment on the boot sequence and answering that question instead, I solved this (even in non-AWS) using the docker-compose <code>depends</code>.</p><p>Simple e.g.</p><pre><code>services:  web:  depends_on:  - ""web_db""  web_db:  image: mongo:3.6  container_name: my_mongodb</code></pre><p>You should be able to remove the deprecated <code>links</code> and just use the hostnames  that docker creates from the service container names.  e.g. above the website would connect to the hostname: ""my_mongodb"".</p>",209288,,,2018-12-14T02:56:14.347,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53772908
628,53789287,2,34065157,2018-12-15T03:30:29.617,11,"<p>The previous answers are now out of date. The newer AWS RDS Aurora does support autoscaling. Aurora Auto Scaling is available for both Aurora MySQL and Aurora PostgreSQL.</p><blockquote>  <p><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html"" rel=""noreferrer"">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html</a></p></blockquote>",102675,,,2018-12-15T03:30:29.617,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53789287
629,53893955,2,53893575,2018-12-22T07:50:58.410,0,"<p>Which AWS services are supported by <code>boto3</code> isn't directly defined in <a href=""https://github.com/boto/boto3/"" rel=""nofollow noreferrer""><code>boto3</code></a>, but in <a href=""https://github.com/boto/botocore"" rel=""nofollow noreferrer""><code>botocore</code></a>. For Quicksight support, you need at least version <code>1.12.49</code> of <code>botocore</code>, which is the version where Quicksight support got added (although the changelog erroneously talks about <a href=""https://github.com/boto/botocore/blob/develop/CHANGELOG.rst#11249"" rel=""nofollow noreferrer"">updated Quicksight support</a>).</p><p>When using AWS Lambda you can either use an AWS-provided version of <code>boto3</code> or bundle your own. As you're using the AWS-provided versions, your code currently runs with outdated versions of <a href=""https://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html"" rel=""nofollow noreferrer""><code>botocore</code> (1.10.74) and <code>boto3</code> (1.7.74)</a>, as AWS hasn't updated them in a while. These old versions don't support Quicksight yet.</p><p>We can only speculate why AWS stopped updating <code>botocore</code> and <code>boto3</code> for the AWS Lambda environment, but it might have to do with some backwards-incompatible changes introduced with <a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/index.html#upgrading-to-1-12-0"" rel=""nofollow noreferrer""><code>botocore</code> 1.12.0</a> and <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/upgrading.html#id1"" rel=""nofollow noreferrer""><code>boto3</code> 1.9.0</a>.</p><p>To solve your problem you can add recent versions of <code>botocore</code> and <code>boto3</code>, which do support Quicksight, to your <a href=""https://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html"" rel=""nofollow noreferrer"">deployment package</a>, to use them, instead of the AWS-provided ones.</p>",4779904,,,2018-12-22T07:50:58.410,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53893955
630,53966864,2,53959664,2018-12-29T05:07:02.047,0,"<p>Michael's description is good. Amazon has also stated (link below) ""Signature Version 2 is being deprecated, and the final support for Signature Version 2 will end on June 24, 2019.""</p><p><a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/auth-request-sig-v2.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonS3/latest/dev/auth-request-sig-v2.html</a></p>",3743602,,,2018-12-29T05:07:02.047,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/53966864
631,54007663,2,22968730,2019-01-02T13:59:46.513,0,"<p>The AWS SQS river plugin for Elasticsearch does not support the <code>_parent</code>, neither does it support the new <code>join</code> datatype. The plugin was meant to have pretty basic functionality but unfortunately it's now <strong>deprecated</strong>. This is because rivers have been <a href=""https://www.elastic.co/blog/deprecating_rivers"" rel=""nofollow noreferrer"">deprecated since Elasticsearch 1.5</a>.</p><p>If you still wish to implement the <code>_parent</code> field and you're still using the plugin with an older version of ES, feel free to contact me. I wrote the plugin and I should be able to help you out.</p>",108758,,,2019-01-02T13:59:46.513,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54007663
632,54012207,2,18683206,2019-01-02T19:39:22.933,0,"<p>""S3Client::factory is deprecated in SDK 3.x, otherwise the solution is valid"" said by RADU</p><p>Here is the updated solution to help others who come across this answer:</p><pre><code># composer dependenciesrequire '/vendor/aws-autoloader.php';//AWS access info  DEFINE command makes your Key and Secret more secureif (!defined('awsAccessKey')) define('awsAccessKey', 'ACCESS_KEY_HERE');///  &lt;- put in your key instead of ACCESS_KEY_HEREif (!defined('awsSecretKey')) define('awsSecretKey', 'SECRET_KEY_HERE');///  &lt;- put in your secret instead of SECRET_KEY_HEREuse Aws\S3\S3Client;$config = [  's3-access' =&gt; [  'key' =&gt; awsAccessKey,  'secret' =&gt; awsSecretKey,  'bucket' =&gt; 'bucket',  'region' =&gt; 'us-east-1', // 'US East (N. Virginia)' is 'us-east-1', research this because if you use the wrong one it won't work!  'version' =&gt; 'latest',  'acl' =&gt; 'public-read',  'private-acl' =&gt; 'private'  ]];# initializing s3$s3 = Aws\S3\S3Client::factory([  'credentials' =&gt; [  'key' =&gt; $config['s3-access']['key'],  'secret' =&gt; $config['s3-access']['secret']  ],  'version' =&gt; $config['s3-access']['version'],  'region' =&gt; $config['s3-access']['region']]);$bucket = 'bucket';$objects = $s3-&gt;getIterator('ListObjects', array(  ""Bucket"" =&gt; $bucket,  ""Prefix"" =&gt; 'filename' //must have the trailing forward slash for folders ""folder/"" or just type the beginning of a filename ""pict"" to list all of them like pict1, pict2, etc.));foreach ($objects as $object) {  echo $object['Key'] . ""&lt;br&gt;"";}</code></pre>",7480839,,,2019-01-02T19:39:22.933,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54012207
633,54013726,2,53278110,2019-01-02T21:59:58.820,1,"<p>This is one of the top google results but there is some outdated information here.</p><p>You can enable sql native backup / restore for sql server in RDS, backup a single database to S3 and then restore that database to the same RDS instance with a new database name.</p><p>Steps for <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/native-backup-rds-sql-server/"" rel=""nofollow noreferrer"">environment setup and performing the backup/restore</a>:</p><ol><li>Create an S3 bucket to store your native backups.</li><li>Enable Sql native backup by configuring a new option group for your RDS instance and adding the ""SQLSERVER_BACKUP_RESTORE"" option.</li><li>Executing a series of procedures in SSMS to run the backup and restore tasks:</li></ol><hr><pre><code>exec msdb.dbo.rds_backup_database @source_db_name='database_name', @s3_arn_to_backup_to='arn:aws:s3:::bucket_name/file_name_and_extension', @overwrite_S3_backup_file=1;exec msdb.dbo.rds_restore_database @restore_db_name='database_name', @s3_arn_to_restore_from='arn:aws:s3:::bucket_name/file_name_and_extension';exec msdb.dbo.rds_task_status @db_name='database_name'</code></pre><hr><p>Note: I cannot find any announcement about removing this prior limitation:</p><blockquote>  <p>You can't restore a backup file to the same DB instance that was used to create the backup file. Instead, restore the backup file to a new DB instance.</p></blockquote><p>However, as of today, I can confirm restoring a native backup to the same instance works as you would expect.</p>",185200,,,2019-01-02T21:59:58.820,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54013726
634,54045954,2,52758171,2019-01-04T20:57:11.637,5,"<p>Make sure Eclipse is running with Java 8.  AWS Toolkit requires JAXB for the upload to AWS S3, but JAXB was deprecated in Java 9 &amp; 10, and removed from Java 11.  If you're running Eclipse under Java 9, 10, or 11, Eclipse/AWS Toolkit won't find JAXB and you'll get this error.</p><p>You can resolve the problem by specifying the JVM Eclipse should use on startup.  Instructions for various platforms found here: <a href=""https://wiki.eclipse.org/Eclipse.ini"" rel=""noreferrer"">https://wiki.eclipse.org/Eclipse.ini</a></p>",1836871,,,2019-01-04T20:57:11.637,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54045954
635,54078758,2,53992133,2019-01-07T17:07:30.727,0,"<p>The documentation was outdated. To be able to use your own cloudfront resource, you need to deploy the stack without the demo ui.</p>",2344682,,,2019-01-07T17:07:30.727,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54078758
636,54184839,2,54184769,2019-01-14T15:53:42.670,1,"<p>According to <a href=""https://www.terraform.io/docs/providers/aws/r/iam_instance_profile.html"" rel=""nofollow noreferrer"">Terraform Docs</a> an instance can have 1 profile linking 1 role to the instance (up to 20 policies -> 1 profile -> 1 roles -> many instances).  So, if there are too many policies you can follow on of these paths:</p><ul><li><p>split the responsibilities across different group of instances with a different role assigned to each group.  This will allow 20 policies attached to group A's instance role and 20 other policies attached to group B's instance role. </p></li><li><p>consolidate the many smaller policies into a fewer larger policies which are then attached to a role which becomes the instance role via a profile.</p></li></ul><blockquote>  <p>roles - (Deprecated) A list of role names to include in the profile. The current default is 1. If you see an error message similar to Cannot exceed quota for InstanceSessionsPerInstanceProfile: 1, then you must contact AWS support and ask for a limit increase. WARNING: This is deprecated since version 0.9.3 (April 12, 2017), as >= 2 roles are not possible. See issue #11575.</p></blockquote>",9259570,9259570,2019-01-16T02:04:33.260,2019-01-16T02:04:33.260,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54184839
637,54194074,2,54191023,2019-01-15T07:03:42.907,0,"<p>With Lambda's Node 6/8 runtimes, it's easier if you use Promises (or even the <code>async/await</code> syntax).</p><p>The old <code>context.succeed()</code>/<code>context.fail()</code> syntax was for the old Node versions and those have been deprecated.</p><pre><code>// Leave this outside your handler so you don't instantiate in every invocation.const codepipeline = new aws.CodePipeline();exports.handler = event =&gt; {  if (!('CodePipeline.job' in event)) {  return Promise.resolve();  }  const jobId = event['CodePipeline.job'].id;  const environment = event['CodePipeline.job'].data.actionConfiguration.configuration.UserParameters;  return doStuff  .then(() =&gt; putJobSuccess(message, jobId));}function putJobSuccess(message, jobId) {  console.log('Post Job Success For JobID: '.concat(jobId));  const params = {  jobId: jobId  };  return codepipeline.putJobSuccessResult(params).promise()  .then(() =&gt; {  console.log(`Post Job Success Succeeded Message: ${message}`);  return message;  })  .catch((err) =&gt; {  console.log(`Post Job Failure Message: ${message}`);  throw err;  });}</code></pre>",1252647,,,2019-01-15T07:03:42.907,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54194074
638,54204722,2,54204287,2019-01-15T18:18:06.297,1,"<p>If you use express.js for the Webapplication, you first could use <code>pm2</code> (<a href=""https://www.npmjs.com/package/pm2"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/pm2</a>) to create a ""container"" for your application. If you want to Host your application, I recommend using Nginx with a reverse proxy.Here are some Links: </p><ol><li><a href=""https://serverfault.com/questions/601332/how-to-configure-nginx-so-it-works-with-express"">https://serverfault.com/questions/601332/how-to-configure-nginx-so-it-works-with-express</a></li><li><a href=""https://stackoverflow.com/questions/53807393/nginx-reverse-proxy-expressjs-angular-ssl-configuration-issues"">Nginx Reverse Proxy + ExpressJS + Angular + SSL configuration issues</a></li></ol><p>I hope that helps you. And if you want to install your application, just run <code>npm install</code> in the directory where the <code>package.json</code> is. Maybe you should update node.js because version 4.x is absolutely outdated.</p>",10058437,,,2019-01-15T18:18:06.297,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54204722
639,54209272,2,54203838,2019-01-16T01:35:30.483,1,"<p>It looks like you missed a step.  [cloud-configs have been deprecated for quite some time now.  You correctly converted that cloud-config into a <a href=""https://coreos.com/os/docs/latest/migrating-to-clcs.html"" rel=""nofollow noreferrer"">container linux config</a> (CLC) file, but missed using <a href=""https://coreos.com/os/docs/latest/overview-of-ct.html"" rel=""nofollow noreferrer"">config transpiler</a> (CT) to then render an ignition sequence.  You can check this by running your config through the <a href=""https://coreos.com/validate/"" rel=""nofollow noreferrer"">online validator</a>.  After running that CLC config through the config transpiler I get the following, which validates correctly:</p><pre><code>{  ""ignition"": {  ""config"": {},  ""timeouts"": {},  ""version"": ""2.1.0""  },  ""networkd"": {},  ""passwd"": {},  ""storage"": {  ""filesystems"": [  {  ""mount"": {  ""device"": ""/dev/xvdb"",  ""format"": ""ext4"",  ""wipeFilesystem"": true  },  ""name"": ""ephemeral1""  }  ]  },  ""systemd"": {  ""units"": [  {  ""contents"": ""[Unit]\nBefore=local-fs.target\n[Mount]\nWhat=/dev/xvdb\nWhere=/media/ephemeral\nType=ext4\n[Install]\nWantedBy=local-fs.target\n"",  ""enable"": true,  ""name"": ""media-ephemeral.mount""  },  {  ""contents"": ""[Unit]\nDescription=Mount ephemeral to /var/lib/docker\nBefore=local-fs.target\n[Mount]\nWhat=/dev/xvdb\nWhere=/var/lib/docker\nType=ext4\n[Install]\nWantedBy=local-fs.target\n"",  ""enable"": true,  ""name"": ""var-lib-docker.mount""  },  {  ""dropins"": [  {  ""contents"": ""[Unit]\nAfter=var-lib-docker.mount\nRequires=var-lib-docker.mount\n"",  ""name"": ""10-wait-docker.conf""  }  ],  ""name"": ""docker.service""  }  ]  }}</code></pre><p>Additionally, it's important to note that <a href=""https://coreos.com/ignition/docs/latest/what-is-ignition.html#ignition-vs-coreos-cloudinit"" rel=""nofollow noreferrer"">there are other differences as well</a> between <code>ignition</code> and <code>coreos-cloud-init</code>.  The most important of which is that <em>ignition only runs once</em>.  Thus, for things like wiping the contents of that ephemeral disk, you should not expect <code>wipe_filesystem: true</code> to be run every single boot.</p><p>Try booting the machine with this config instead.  You should get the expected results.</p>",2780658,,,2019-01-16T01:35:30.483,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54209272
640,54216158,2,54186070,2019-01-16T11:30:40.943,0,"<p>As pointed out by @John Rotenstein , all bucket name must be DNS-compliant, so a direct slash is not allowed for bucket name. </p><blockquote>  <p>In fact the 2nd piece of code above does work, when I added this: for  key in bucket.get_all_keys(prefix='s-DI-S/', delimiter='/') and took  what was really the prefix off the Bucketname.</p></blockquote><p>From the above OP comment,  it seems OP maybe misunderstood S3 object store with a file directory system.  Each s3 object are never store in a hierarchy format. The so call ""prefix""  is simply an text filter that retrieve object name that contains the the similar ""prefix"" in the object name. </p><p>In OP, there is a bucket name call  <code>b-datasci</code>, while there are many object with the prefix <code>x-DI-S</code> store under the bucket.</p><p>In addition, the usage of <code>delimiter</code> under <code>get_all_keys</code> is not what OP think. If you check out AWS documentation on <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/ListingKeysHierarchy.html"" rel=""nofollow noreferrer"">Listing Keys Hierarchically Using a Prefix and Delimiter</a>, you will learn that it is just a secondary filter.  i.e. </p><blockquote>  <p>The prefix and delimiter parameters limit the kind of results returned by a list operation. Prefix limits results to only those keys that begin with the specified prefix, and delimiter causes list to roll up all keys that share a common prefix into a single summary list result.</p></blockquote><p>p/s: OP should use boto3 because boto is deprecated by AWS for more than 2 years. </p>",6017840,,,2019-01-16T11:30:40.943,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54216158
641,54264937,2,52108481,2019-01-19T07:19:15.633,2,"<p>When I first asked this question, AWS had the ability to <strong>record</strong> metrics with the Swift SDK, but <strong>not</strong> view them in the Pinpoint API, which is absurd, because then you can only record metrics. What's the point? I asked in the AWS forums, and a couple months later, they responded something along the lines of ""Please wait - coming soon.""</p><p>This feature is now available, whereas before it simply wasn't.</p><p>Go to Pinpoint, your project, then click the Analytics drop-down menu, then click events. You can see that you can sort by metric. If you look at my outdated screenshot above, you'll see that this was <strong>not</strong> an option.</p><p><a href=""https://i.stack.imgur.com/DBTLt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DBTLt.png"" alt=""Image of Pinpoint console""></a></p>",8808193,,,2019-01-19T07:19:15.633,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54264937
642,54295705,2,9648255,2019-01-21T18:17:09.660,0,"<p>Previous answers are obsolete now<a href=""https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/</a>""This S3 request rate performance increase removes any previous guidance to randomize object prefixes to achieve faster performance. That means you can now use logical or sequential naming patterns in S3 object naming without any performance implications. ""</p>",6700830,,,2019-01-21T18:17:09.660,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54295705
643,54329221,2,54184611,2019-01-23T14:15:25.297,0,"<p>The TransferManager component in the AWS Android SDK has been deprecated in favor of the TransferUtility component. The TransferUtility component allows you to pause and resume transfers. It also has support for network monitoring and will automatically pause and resume transfers when the network goes down and comes back up. Here is the link to the TransferUtility documentation - <a href=""https://aws-amplify.github.io/docs/android/storage"" rel=""nofollow noreferrer"">https://aws-amplify.github.io/docs/android/storage</a></p>",9282303,,,2019-01-23T14:15:25.297,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54329221
644,54432392,2,29755014,2019-01-30T02:08:22.947,6,"<p>Like <a href=""https://stackoverflow.com/users/3692910"">otognan</a>, I would also recommend doing #2, but it seems that his answer is outdated.</p><p>First of all, use the <code>jenkins/jenkins:lts</code> image, as the <code>jenkins</code> image is deprecated (see <a href=""https://hub.docker.com/_/jenkins/"" rel=""nofollow noreferrer"">https://hub.docker.com/_/jenkins/</a> )</p><p>Now, lets set it up.  You'll need to stop your current jenkins container to free up the ports.</p><p>First, you'll need a certificate keystore.  If you don't have one, you could create a self-signed one with </p><pre><code>keytool -genkey -keyalg RSA -alias selfsigned -keystore jenkins_keystore.jks -storepass mypassword -keysize 4096</code></pre><p>Next, let's pass the SSL arguments into the jenkins container.  This is the script I use to do so:</p><pre><code>read -s -p ""Keystore Password:"" passwordechosudo cp jenkins_keystore.jks /var/lib/docker/volumes/jenkins_home/_datadocker run -d -v jenkins_home:/var/jenkins_home -v $(which docker):/usr/bin/docker -v /var/run/docker.sock:/var/run/docker.sock -p 443:8443 -p 50000:50000 jenkins/jenkins:lts --httpPort=-1 --httpsPort=8443 --httpsKeyStore=/var/jenkins_home/jenkins_keystore.jks --httpsKeyStorePassword=$password</code></pre><ul><li>this script prompts the user for the keystore password</li><li><code>-v jenkins_home:/var/jenkins_home</code> creates a named volume called <code>jenkins_home</code>, which happens to exist at <code>/var/lib/docker/volumes/jenkins_home/_data</code> by convention.<ul><li>if the directory at <code>/var/lib/docker/volumes/jenkins_home/_data</code> does not exist yet, you will need to create the named volume using <code>docker volume</code> before copying the keystore.</li></ul></li><li><code>-p 443:8443</code> maps 8443 jenkins port in the container to the 443 port of the host</li><li><code>--httpPort=-1 --httpsPort=8443</code> blocks http and exposes https on port 8443 inside the container (port 443 outside the container).  </li><li><code>--httpsKeyStore=/var/jenkins_home/jenkins_keystore.jks --httpsKeyStorePassword=$password</code> provides your keystore, which exists at <code>/var/jenkins_home/jenkins_keystore.jks</code> inside the container ( <code>/var/lib/docker/volumes/jenkins_home/_data/jenkins_keystore.jks</code> outside the container).</li><li><code>-v /var/run/docker.sock:/var/run/docker.sock</code> is optional, but is the recommended way to allow your jenkins instance to spin up other docker containers.  <ul><li>WARNING:  By giving the container access to <code>/var/run/docker.sock</code>, it is easy to break out of the containment provided by the container, and gain access to the host machine.  This is obviously a potential security risk.</li></ul></li><li><code>-v $(which docker):/usr/bin/docker</code> is also optional, but allows your jenkins container to be able to run the docker binary. <ul><li>Be aware that, because docker is now dynamically linked, it no longer comes with dependencies, so you may be required to install dependencies in the container.  </li><li>The alternative is to omit <code>-v $(which docker):/usr/bin/docker</code> and install docker within the jenkins container.  You'll need to ensure that the inner container docker and the outer host docker are the same version, so that communication over <code>/var/run/docker.sock</code> is supported.</li><li>In either case, you may want to use a Dockerfile to create a new Jenkins docker image. </li><li>Another alternative is to include <code>-v $(which docker):/usr/bin/docker</code>, but install a statically-linked binary of docker on the host machine.</li></ul></li></ul><p>You should now be able to access the jenkins webportal via https with no port specifier (since port 443 is default for https)</p><p>Thanks to <a href=""https://stackoverflow.com/users/3692910"">otognan</a> for getting me part of the way here.</p>",1698736,1698736,2020-06-16T23:56:55.807,2020-06-16T23:56:55.807,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54432392
645,54459928,2,54439440,2019-01-31T11:47:50.640,16,"<p>My experience has been it does not obsolete the need for a users table, for a couple of reasons.</p><ol><li>I would routinely run into AWS throttling errors when invoking the Cognito getUser/listUser methods while using Cognito as the primary user data store. AWS support would increase the API limits on our account but I was always worried they would reappear. </li><li>You are essentially limited to querying users by username/email/phone. The listUser method is very limited for searching</li><li>If you want to store other user data then you have to put them in custom Cognito attributes and managing those got tiresome quickly</li></ol><p>The design I ended up on was to set a post-confirmation trigger on the Cognito user pool and have it copy all the user data into a relational database or DynamoDB when a user signed up. The primary key of the users table would be the Cognito username so if necessary I could lookup the user in both the database and Cognito. Basically I just use Cognito for authentication and not as a primary user database</p>",6234168,,,2019-01-31T11:47:50.640,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54459928
646,54499820,2,54496131,2019-02-03T03:54:19.950,4,"<p>So, I was able to successfully complete the AWS Amplify / iOS SDK Getting Started guide after leveraging the Apple iOS Swift Getting Started guide to create the necessary view elements required by AWS. What that means is this:</p><p>Two text fields: 'name' and 'description'; a label; and a button. Here are my outlet properties:</p><pre><code>//MARK: Properties@IBOutlet weak var nameTextField: UITextField!@IBOutlet weak var descTextField: UITextField!@IBOutlet weak var todoItemLabel: UILabel!</code></pre><p>My viewDidLoad():</p><pre><code>override func viewDidLoad() {  super.viewDidLoad()  // Handle the text fieldŠ—Ès user input through delegate callbacks.  nameTextField.delegate = self  descTextField.delegate = self  let appDelegate = UIApplication.shared.delegate as! AppDelegate  appSyncClient = appDelegate.appSyncClient}</code></pre><p>My button action which calls runMutation():</p><pre><code>//MARK: Actions@IBAction func addToDoItem(_ sender: UIButton) {  runMutation()}</code></pre><p>And changes to runMutation() to update DynamoDB with the entered values:</p><pre><code>let mutationInput = CreateTodoInput(name: nameTextField.text ?? ""No Entry"", description: descTextField.text)</code></pre><p>If you have followed Steps 1 - 4 of the <a href=""https://aws-amplify.github.io/docs/ios/start"" rel=""nofollow noreferrer"">AWS Amplify / iOS SDK Getting Started guide</a> and added the necessary UI elements then the code above will seal the deal.</p><p>Also note that <a href=""https://aws-amplify.github.io/docs/ios/api"" rel=""nofollow noreferrer"">the API reference</a> pointed at by @dennis-w in the comments above takes care of those deprecated references in AppDelegate from the Getting Started guide.</p>",5477981,5477981,2019-02-03T16:03:31.263,2019-02-03T16:03:31.263,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54499820
647,54526914,2,54452410,2019-02-05T02:09:15.880,0,"<p>Jenkins is hopelessly out of date and unmaintained. I added the Post Build Task plugin, <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install.html"" rel=""nofollow noreferrer"">installed <code>eb</code> tool</a> as <code>jenkins</code> user, ran <code>eb init</code> in the job directory, edited <code>.elasticbeanstalk/config.yml</code> to add the lines</p><pre><code>deploy:  artifact: target/AppName-Sprint5-SNAPSHOT-bin.zip</code></pre><p>Then entered in the shell command to deploy the build.</p><pre><code>/var/lib/jenkins/.local/bin/eb deploy -l sprint5-${BUILD_NUMBER}</code></pre><p><a href=""https://i.stack.imgur.com/S2wcG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S2wcG.png"" alt=""enter image description here""></a></p>",148844,,,2019-02-05T02:09:15.880,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54526914
648,54528108,2,54472827,2019-02-05T05:23:28.053,0,"<p>Finally issue RESOLVED I could make work fine the NGINX and also I had to change another things: </p><p>I've passed from an Application Load Balancer to a Classic Load Balancer. The final scheme is like I've explained in the UPDATE of this topic, I mean:</p><p>User connects via HTTP or HTTPS through Classic LB and then it goes to EC2 NGINX listening on port 80.</p><p>Then from NGINX to WebApp I've used a proxy_pass in this way:</p><pre><code>  location / {  proxy_pass  http://172.x.y.z:8080;  }</code></pre><p>And finally an HTTP forward in NGINX to use HTTPS exclusively:</p><pre><code>  proxy_set_header X-Forwarded-Proto $scheme;  if ( $http_x_forwarded_proto != 'https' )   {  return 301 https://$host$request_uri;  }</code></pre><p>Lijo Abraham, your answer helped me to have a clear direction and this post shows the exactly solution applied (thats why I will green tick this post).</p><p>Many thanks and regards.</p><p>**UPDATE 1 - 10 February 2019 17:21 (GMT-3) ** Finally I've remade all again using Application ELB this time instead of Classic ELB (the latter deprecated) and everything works as expected, don't know why in the beginning ELB Classic didn't work (probably some error in security groups rules configuration or something kind of that).</p>",7723350,7723350,2019-02-10T20:26:39.937,2019-02-10T20:26:39.937,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54528108
649,54538441,2,54538239,2019-02-05T16:04:25.497,1,"<p>You do not want to be using <code>django-storages-redux</code>, the warning is correct in telling you that you should replace it with <code>django-storages</code>. Your error was caused by having both <code>django-storages</code> and <code>django-storages-redux</code> installed at the same time. You should remove the <code>-redux</code> package, and repair your setup by re-installing <code>django-storages</code>, alone.</p><p>The warning is raised because you have version 1.3.3 of the deprecated <a href=""https://pypi.org/project/django-storages-redux/"" rel=""nofollow noreferrer""><code>django-storages-redux</code> project</a> installed; this is the <a href=""https://github.com/jschneier/django-storages/commit/4503027d1333288a844a754cbe92ce9f03bbc133#diff-7d7e96974dff6c5efdca1620cfdaba7c"" rel=""nofollow noreferrer"">only release with that warning</a>. Version 1.3.3 is quite old, it was released in 2017.</p><p>You need to <em>uninstall</em> that project, and then re-install <code>django-storages</code>:</p><pre><code>pip uninstall django-storages-reduxpip install --force-reinstall django-storages</code></pre><p>Make sure that <code>django-storages-redux</code> is not listed in your requirements.txt file or as a dependency anywhere.</p><p>The <code>django-storages-redux</code> and <code>django-storages</code> projects write to the same package, and they clashed, creating a broken package. The <code>storage.utils</code> module is <a href=""https://github.com/jschneier/django-storages/blob/4503027d1333288a844a754cbe92ce9f03bbc133/storages/utils.py"" rel=""nofollow noreferrer"">the version from <code>django-storages-redux</code></a>, while the <code>storage.backends.s3boto3</code> module is <a href=""https://github.com/jschneier/django-storages/blob/1.5.0/CHANGELOG.rst"" rel=""nofollow noreferrer"">new in version 1.5.0 of `django-storage</a>.</p>",100297,100297,2019-02-05T16:13:52.310,2019-02-05T16:13:52.310,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54538441
650,54540168,2,54539708,2019-02-05T17:41:52.630,4,"<p>GraphQL designers have a strong opinion against versioning of schemas. </p><p>Versioning was used in the past, or is used with REST API, to avoid introducing breaking changes. </p><p>GraphQL only returns the data that's explicitly requested, so new capabilities can be added via new types and new fields on those types without creating a breaking change. Deprecated fields can just be marked as deprecated (see <a href=""https://facebook.github.io/graphql/draft/#sec--deprecated"" rel=""nofollow noreferrer"">https://facebook.github.io/graphql/draft/#sec--deprecated</a>)</p><p>This has led to a common practice of always avoiding breaking changes and serving a versionless API.</p>",663360,,,2019-02-05T17:41:52.630,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54540168
651,54690020,2,40348696,2019-02-14T12:04:37.523,7,"<p><code>AWSS3TransferManager</code> is deprecated. Use <code>AWSS3TransferUtility</code> instead.</p><blockquote>  <p>The transfer utility provides methods for both single-part and  multipart uploads. When a transfer uses multipart upload, the data is  chunked into a number of 5 MB parts which are transferred in parallel  for increased speed.</p></blockquote><pre><code> func uploadFile(withImage image: UIImage) {  let access = ""YOUR ACCESS KEY""  let secret = ""YOUR SECRET KEY""  let credentials = AWSStaticCredentialsProvider(accessKey: access, secretKey: secret)  let configuration = AWSServiceConfiguration(region: AWSRegionType.USEast1, credentialsProvider: credentials)  AWSServiceManager.default().defaultServiceConfiguration = configuration  let s3BucketName = ""YOUR BUCKET NAME""  let compressedImage = image.resizedImage(newSize: CGSize(width: 80, height: 80))  let data: Data = compressedImage.pngData()!  let remoteName = generateRandomStringWithLength(length: 12)+"".""+data.format  print(""REMOTE NAME : "",remoteName)  let expression = AWSS3TransferUtilityUploadExpression()  expression.progressBlock = { (task, progress) in  DispatchQueue.main.async(execute: {  // Update a progress bar  })  }  var completionHandler: AWSS3TransferUtilityUploadCompletionHandlerBlock?  completionHandler = { (task, error) -&gt; Void in  DispatchQueue.main.async(execute: {  // Do something e.g. Alert a user for transfer completion.  // On failed uploads, `error` contains the error object.  })  }  let transferUtility = AWSS3TransferUtility.default()  transferUtility.uploadData(data, bucket: s3BucketName, key: remoteName, contentType: ""image/""+data.format, expression: expression, completionHandler: completionHandler).continueWith { (task) -&gt; Any? in  if let error = task.error {  print(""Error : \(error.localizedDescription)"")  }  if task.result != nil {  let url = AWSS3.default().configuration.endpoint.url  let publicURL = url?.appendingPathComponent(S3BucketName).appendingPathComponent(remoteName)  if let absoluteString = publicURL?.absoluteString {  // Set image with URL  print(""Image URL : "",absoluteString)  }  }  return nil  }}</code></pre><p>For generating random strings for remote name.</p><pre><code>func generateRandomStringWithLength(length: Int) -&gt; String {  let randomString: NSMutableString = NSMutableString(capacity: length)  let letters: NSMutableString = ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789""  var i: Int = 0  while i &lt; length {  let randomIndex: Int = Int(arc4random_uniform(UInt32(letters.length)))  randomString.append(""\(Character( UnicodeScalar( letters.character(at: randomIndex))!))"")  i += 1  }  return String(randomString)}</code></pre><p>For resizing the image and data formatting. Use below Image and Data extensions.</p><pre><code>extension UIImage {  func resizedImage(newSize: CGSize) -&gt; UIImage {  guard self.size != newSize else { return self }  UIGraphicsBeginImageContextWithOptions(newSize, false, 0.0);  self.draw(in: CGRect(x: 0, y: 0, width: newSize.width, height: newSize.height))  let newImage: UIImage = UIGraphicsGetImageFromCurrentImageContext()!  UIGraphicsEndImageContext()  return newImage  } }extension Data {  var format: String {  let array = [UInt8](self)  let ext: String  switch (array[0]) {  case 0xFF:  ext = ""jpg""  case 0x89:  ext = ""png""  case 0x47:  ext = ""gif""  case 0x49, 0x4D :  ext = ""tiff""  default:  ext = ""unknown""  }  return ext  }}</code></pre>",4608334,4608334,2019-02-14T12:32:00.557,2019-02-14T12:32:00.557,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54690020
652,54784930,2,54784929,2019-02-20T11:10:46.270,4,"<p><strong>Update 2020-03</strong></p><p>A lot has changed since my post below and Hibernation is now a piece of cake. Encryption can be done in the normal ec2 creation flow in AWS console, Amazon Linux 2 is also supported.</p><p>All you need to be aware of:</p><ul><li>not all instance types are supported, see <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html#hibernating-prerequisites"" rel=""nofollow noreferrer"">here</a> for an up to date list</li><li>RAM > 150GB is not supported</li><li>your disk space needs to be bigger than your RAM (as RAM will be written on disk when hibernating)</li></ul><p><strong>Original post</strong>:</p><p>In the end I was stuck at <em>many stages</em>, the official documentation is somehow lacking so I thought I'd document my findings:</p><p>In order to hibernate an instance it needs to be of type <strong>C3, C4, C5, M3, M4, M5, R3, R4 or R5</strong>. Plus it needs to be a <strong>Amazon Linux 1</strong> AMI (this may be outdated, see <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html#hibernating-prerequisites"" rel=""nofollow noreferrer"">here</a> for an up to date list). I started with a Linux 2 AMI and didn't get any warning until it just didn't work in the end (only showing <code>Suspend key pressed.</code>, <code>Requested operation not supported, ignoring.</code> in /var/log/messages), even <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html#hibernation-enabled-AMI"" rel=""nofollow noreferrer"">their suggested steps to enable hibernation</a> didn't work, as <code>ec2-hibinit-agent</code> is not an available packet on Amazon Linux 2.</p><p>Hibernation requires the boot volume to be encrypted. In order to achieve that, you need to encrypt the AWS Linux 1 AMI <strong>prior to starting the launch wizard</strong>:</p><ol><li>create a KMS key using AWS console</li><li>find the ami id if AWS Linux 1 (not 2!) of your region e.g. using the ec2 launch wizard. In my case (eu-central-1) this was <code>ami-0cfbf4f6db41068ac</code></li><li>create your own, encrypted ami (replace <code>eu-central-1</code>, the <code>ami</code> and the <code>kms</code> key with your own</li></ol><p>&nbsp;</p><pre><code>aws ec2 copy-image --source-region eu-central-1 --source-image-id ami-0cfbf4f6db41068ac \--region eu-central-1 --name ""LinuxAMIEncrypted"" --encrypted \--kms-key-id arn:aws:kms:eu-central-1:123412341234:key/aaaaaaaa-1234-abce-abcd-1234abcdef01</code></pre><p>Wait a few minutes until you see the new AMI showing up in EC2 -> AMIs</p><ol><li>From <code>AMIs</code> (in EC2 AWS console) launch your new AMI</li><li>choose C3, C4, C5, M3, M4, M5, R3, R4 or R5</li><li>click <code>Enable hibernation as an additional stop behavior</code></li></ol><p>To test it:</p><ul><li>run <code>tail -f /var/log/messages</code> on the ec2 instance to check for errors</li><li>run <code>aws ec2 stop-instances --instance-ids ""i-Š—_"" --hibernate</code> to hibernate via terminal, or alternatively over aws console</li></ul><p>Additionally I saw this error in <code>/var/log/messages</code>:</p><p><code>Agent hibernate - AccessDeniedException: User: arn:Š—_ is not authorized to perform: ssm:UpdateInstanceInformation on resource: ssm:UpdateInstanceInformation</code>. I needed to attach <code>AmazonEC2RoleforSSM</code> and <code>AmazonSSMAutomationRole</code> to make these errors go away.</p>",119861,119861,2020-03-29T10:38:44.727,2020-03-29T10:38:44.727,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54784930
653,54819944,2,54818630,2019-02-22T04:09:24.793,2,"<p>Burst capacity does not cause your table to create a new partition. <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html"" rel=""nofollow noreferrer"">The docs</a> are pretty clear that new partitions are caused by the size of your data and the amount of <em>provisioned</em> capacity.</p><blockquote><p>DynamoDB allocates additional partitions to a table in the following situations:</p><p>Š—¢   If you increase the table's provisioned throughput settings beyond what the existing partitions can support.</p><p>Š—¢   If an existing partition fills to capacity and more storage space is required.</p></blockquote><p>ThereŠ—Ès no way for you to view to number of partitions in your table. You <em>might</em> be able to find out by contacting AWS support.</p><p>That being said recent improvements to DynamoDB mean that you shouldnŠ—Èt really have to worry about the number of partitions you have. Hot partitions are not a concern like they used to be because <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">DynamoDB adaptive capacity accommodates uneven data access patterns</a>.</p>",5563569,-1,2020-06-20T09:12:55.060,2019-02-22T04:09:24.793,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54819944
654,54853027,2,52882777,2019-02-24T14:45:24.993,0,"<p>Based on the documentation <a href=""https://docs.ansible.com/ansible/latest/modules/ec2_remote_facts_module.html"" rel=""nofollow noreferrer"">ec2_remote_facts</a> is marked as DEPRECATED from ansible version 2.8 in favor of using <a href=""https://docs.ansible.com/ansible/latest/modules/ec2_instance_facts_module.html#ec2-instance-facts-module"" rel=""nofollow noreferrer"">ec2_instance_facts</a>.</p><p>This is working good for me:</p><pre><code>- name: Get instances list  ec2_instance_facts:  region: ""{{ region }}""  filters:  ""tag:Name"": ""{{ myname }}""  register: ec2_list- debug: msg=""{{ ec2_metadata.instances }}""</code></pre><p>Maybe the filte is not being applied? Can you go through the results in the object?</p>",2840214,,,2019-02-24T14:45:24.993,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54853027
655,54854789,2,54846513,2019-02-24T17:53:52.233,15,"<p>Back in the original Lambda runtime environment for Node.js 0.10, Lambda provided helper functions in the <code>context</code> object: <code>context.done(err, res)</code> <code>context.succeed(res)</code> and <code>context.fail(err)</code>.</p><p>This was formerly documented, but has been removed. </p><p><a href=""https://web.archive.org/web/20161216092320/https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html"" rel=""noreferrer"">Using the Earlier Node.js Runtime v0.10.42</a> is an archived copy of a page that no longer exists in the Lambda documentation, that explains how these methods were used.</p><p>When the Node.js 4.3 runtime for Lambda was launched, these remained for backwards compatibility (and remain available but undocumented), and <code>callback(err, res)</code> was introduced.</p><p>Here's the nature of your problem, and why the two solutions you found actually seem to solve it.</p><blockquote>  <p>Context.succeed, context.done, and context.fail however, are more than just bookkeeping Š—– they cause the request to return after the current task completes and freeze the process immediately, even if other tasks remain in the Node.js event loop. Generally thatŠ—Ès not what you want if those tasks represent incomplete callbacks.</p>   <p><a href=""https://aws.amazon.com/blogs/compute/node-js-4-3-2-runtime-now-available-on-lambda/"" rel=""noreferrer"">https://aws.amazon.com/blogs/compute/node-js-4-3-2-runtime-now-available-on-lambda/</a></p></blockquote><p>So with <code>callback</code>, Lambda functions now behave in a more paradigmatically correct way, but this is a problem if you intend for certain objects to remain on the event loop during the freeze that occurs between invocations -- unlike the old (deprecated) <code>done</code> <code>fail</code> <code>succeed</code> methods, using the callback doesn't suspend things immediately.  Instead, it waits for the event loop to be empty.</p><p><a href=""https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-context.html"" rel=""noreferrer""><code>context.callbackWaitsForEmptyEventLoop</code></a> -- default <code>true</code> -- was introduced so that you can set it to <code>false</code> for those cases where you want the Lambda function to return immediately after you call the callback, regardless of what's happening in the event loop.  The default is <code>true</code> because <code>false</code> can mask bugs in your function and can cause very erratic/unexpected behavior if you fail to consider the implications of container reuse -- so you shouldn't set this to <code>false</code> unless and until you understand why it is needed.</p><p>A common reason <code>false</code> is needed would be a database connection made by your function.  If you create a database connection object in a global variable, it will have an open socket, and potentially other things like timers, sitting on the event loop.  This prevents the callback from causing Lambda to return a response, until these operations are also finished or the invocation timeout timer fires.</p><p>Identify why you need to set this to <code>false</code>, and if it's a valid reason, then it is correct to use it.  </p><p>Otherwise, your code may have a bug that you need to understand and fix, such as leaving requests in flight or other work unfinished, when calling the callback.</p><p>So, how do we parse the Cognito error?  At first, it seemed pretty unusual, but now it's clear that it is not.</p><p>When executing a function, Lambda will throw an error that the tasked timed out after the configured number of seconds.  You should find this to be what happens when you test your function in the Lambda console.</p><p>Unfortunately, Cognito appears to have taken an internal design shortcut when invoking a Lambda function, and instead of waiting for Lambda to timeout the invocarion (which could tie up resources inside Cognito) or imposing its own explicit timer on the maximum duration Cognito will wait for a Lambda response, it's relying on a lower layer socket timer to constrain this wait... thus an ""unexpected"" error is thrown while invoking the timeout.</p><p>Further complicating interpreting the error message, there are missing quotes in the error, where the lower layer exception is interpolated.</p><p>To me, the problem would be much more clear if the error read like this:</p><pre><code>'arn:aws:lambda:...' failed with error 'Socket timeout' while invoking Lambda function</code></pre><p>This format would more clearly indicate that while <em>Cognito</em> was invoking the function, it threw an internal <code>Socket timeout</code> error (as opposed to <em>Lambda</em> encountering an unexpected internal error, which was my original -- and incorrect -- assumption). </p><p>It's quite reasonable for Cognito to impose some kind of response time limit on the Lambda function, but I don't see this documented.  I suspect a short timeout on your Lambda function itself (making it fail more promptly) would cause Cognito to throw a somewhat more useful error, but in my mind, Cognito should have been designed to include logic to make this an expected, defined error, rather than categorizing it as ""unexpected.""</p>",1695906,,,2019-02-24T17:53:52.233,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54854789
656,54876742,2,54876596,2019-02-26T00:21:57.833,2,"<p>In general, you no longer need to worry about hot partitions in DynamoDB, especially if the partition keys which are being requested the most remain relatively constant.</p><p>More Info: <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/</a></p>",6918960,,,2019-02-26T00:21:57.833,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54876742
657,54877580,2,54877345,2019-02-26T02:30:15.387,1,"<p>You forwarded port 49001 to the Docker container's port 8080. That's how you access Jenkins. It looks like you got the right response for <code>curl localhost:49001</code>. It's asking you to login.</p><p>Also note the <code>jenkins</code> image is deprecated. You should use <code>jenkins/jenkins:lts</code> instead.</p>",492773,,,2019-02-26T02:30:15.387,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54877580
658,54891138,2,54887416,2019-02-26T17:33:48.310,3,"<p>Either option would work but since your using AutoScaling I would recommend creating a custom AMI.</p><p><strong>Pros:</strong></p><ol><li>You will have the same image every time you scale up. Installing from a script could provide new instances that have different software versions.  That can turn into a troubleshooting nightmare.</li><li>It's faster to have your software pre-loaded.</li><li>Allows for a structured Blue/Green Deployment</li></ol><p><strong>Cons:</strong></p><ol><li>Static images are outdated as soon as they are created.</li></ol>",1723857,,,2019-02-26T17:33:48.310,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54891138
659,54899355,2,40348696,2019-02-27T06:29:21.063,1,"<p>We should use <strong>AWSS3TransferUtility</strong> now because AWSS3TransferManagerUploadRequest is <strong>deprecated</strong>, here is the jpeg upload function in <strong>Swift 4.2</strong> but it can be easily changed for any data type:</p><pre><code>func uploadS3(image: UIImage,  name: String,  progressHandler: @escaping (Progress) -&gt; Void,  completionHandler: @escaping (Error?) -&gt; Void) {  guard let data = UIImageJPEGRepresentation(image, Constants.uploadImageQuality) else {  DispatchQueue.main.async {  completionHandler(NetErrors.imageFormatError) // Replace your error  }  return  }  let credentialsProvider = AWSStaticCredentialsProvider(accessKey: Constants.accessKeyS3, secretKey: Constants.secretKeyS3)  let configuration = AWSServiceConfiguration(region: Constants.regionS3, credentialsProvider: credentialsProvider)  AWSServiceManager.default().defaultServiceConfiguration = configuration  let expression = AWSS3TransferUtilityUploadExpression()  expression.progressBlock = { task, progress in  DispatchQueue.main.async {  progressHandler(progress)  }  }  AWSS3TransferUtility.default().uploadData(  data,  bucket: Constants.bucketS3,  key: name,  contentType: ""image/jpg"",  expression: expression) { task, error in  DispatchQueue.main.async {  completionHandler(error)  }  print(""Success"")  }.continueWith { task -&gt; AnyObject? in  if let error = task.error {  DispatchQueue.main.async {  completionHandler(error)  }  }  return nil  }}</code></pre><p>Do not forget to define or change <strong>Constants</strong> in the code. If you don't want to give public access, you should also define a user in IAM, and put this code in your bucket policy:</p><pre><code>{  ""Version"": ""2012-10-17"",  ""Id"": ""S3AccessPolicy"",  ""Statement"": [  {  ""Sid"": ""GiveAppAccess"",  ""Effect"": ""Allow"",  ""Principal"": {  ""AWS"": ""arn:aws:iam::123456789012:user/YOUR_USER""  },  ""Action"": [  ""s3:GetObject"",  ""s3:PutObject""  ],  ""Resource"": ""arn:aws:s3:::YOUR_BUCKET/*""  }  ]}</code></pre>",1375876,,,2019-02-27T06:29:21.063,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54899355
660,54899560,2,25369602,2019-02-27T06:45:30.570,0,"<p>With <strong>AWSS3TransferUtility</strong> you can upload any data type, also AWSS3TransferManagerUploadRequest is <strong>deprecated</strong> now, here is the code sample to upload jpeg but can be converted for any data type:</p><p><a href=""https://stackoverflow.com/questions/35240317/swift-aws-s3-upload-image-from-photo-library-and-download-it/54899370#54899370"">Code sample</a></p>",1375876,,,2019-02-27T06:45:30.570,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54899560
661,54955564,2,54953491,2019-03-02T05:36:13.450,0,"<p>One answer to this question is found in this tutorial by following step 3 and modifying the information.</p><p>I changed the ""Id"" to ""id"" and changed exports.writeMovie to exports.handler</p><p><a href=""https://hackernoon.com/create-a-serverless-rest-api-with-node-js-aws-lambda-dynamodb-api-gateway-f08e7111fd16"" rel=""nofollow noreferrer"">https://hackernoon.com/create-a-serverless-rest-api-with-node-js-aws-lambda-dynamodb-api-gateway-f08e7111fd16</a></p><p>It doesn't use the async and await and is a touch outdated but it works.</p>",10469460,10469460,2019-03-02T05:52:44.293,2019-03-02T05:52:44.293,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54955564
662,54978632,2,54961986,2019-03-04T07:41:28.333,2,<p>The failure to install is due to an outdated version of <code>setuptools</code>. Upgrade <code>pip</code> to version 19+ and setuptools to version 40+.</p>,1998200,,,2019-03-04T07:41:28.333,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/54978632
663,55181231,2,55180146,2019-03-15T11:06:53.370,1,"<p>You have most likely installed the wrong/outdated version of awscli.</p><p>It is <a href=""https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html#install-tool-pip"" rel=""nofollow noreferrer"">recommended</a> to install awscli via pip.</p><pre><code>pip3 install awscli --upgrade --user</code></pre>",1054268,,,2019-03-15T11:06:53.370,6,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55181231
664,55228196,2,44907295,2019-03-18T18:50:37.117,5,"<p>Unsigned's answer is good but is a tad outdated. As of February 2019, CodeBuild allows both caching in an S3 bucket and allows the user to cache locally. You can now specify cache at 3 different layers of a build:</p><ul><li>Docker Layer Caching</li><li>Git Layer Cahing (cache the last build and then only build from <code>git diff</code>)</li><li>Custom caching - specified within the <code>cache:</code> portion of your buildspec.yml file. Personally, I cache my node_modules/ here and then cache at the Git Layer.</li></ul><p>Source: <a href=""https://aws.amazon.com/blogs/devops/improve-build-performance-and-save-time-using-local-caching-in-aws-codebuild/"" rel=""noreferrer"">https://aws.amazon.com/blogs/devops/improve-build-performance-and-save-time-using-local-caching-in-aws-codebuild/</a></p>",6891245,,,2019-03-18T18:50:37.117,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55228196
665,55232970,2,46932607,2019-03-19T02:54:59.977,0,"<p>You can try this answer: <a href=""https://stackoverflow.com/a/42300647/2727462"">https://stackoverflow.com/a/42300647/2727462</a></p><blockquote>  <p>Solution If you're DNS is configured to hit directly on the ELB -> you  should reduce the TTL of the association (IP,DNS). The IP can change  at any time with the ELB so you can have serious damage on your  traffic.</p>   <p>The client keep Some IP from the ELB in cache so you can have those  can of trouble.</p>   <p>Scaling Elastic Load Balancers Once you create an elastic load  balancer, you must configure it to accept incoming traffic and route  requests to your EC2 instances. These configuration parameters are  stored by the controller, and the controller ensures that all of the  load balancers are operating with the correct configuration. The  controller will also monitor the load balancers and manage the  capacity that is used to handle the client requests. It increases  capacity by utilizing either larger resources (resources with higher  performance characteristics) or more individual resources. The Elastic  Load Balancing service will update the Domain Name System (DNS) record  of the load balancer when it scales so that the new resources have  their respective IP addresses registered in DNS. The DNS record that  is created includes a Time-to-Live (TTL) setting of 60 seconds, with  the expectation that clients will re-lookup the DNS at least every 60  seconds. By default, Elastic Load Balancing will return multiple IP  addresses when clients perform a DNS resolution, with the records  being randomly ordered on each DNS resolution request. As the traffic  profile changes, the controller service will scale the load balancers  to handle more requests, scaling equally in all Availability Zones.</p></blockquote><p>In my case the problem was in TTL. Issue can be tracked by a command like <code>wget https://your-url</code>. The comand output will show you the IP address to which it tries to connect. And when connection hangs you can figure out a wrong outdated IP address. If it happen - check your DNS settings and update TTL.</p>",2727462,,,2019-03-19T02:54:59.977,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55232970
666,55238531,2,55127124,2019-03-19T10:16:53.383,1,"<ol><li><p>Nobody needs to set the fs.s3X.impl for any value of X. It's predefined in the core-default.xml file in hadoop-common. This rule ""you have to declare it"" is just some superstition passed down by people who have never tried anything different.</p></li><li><p>S3n was already obsolete by the time hadoop-2.7 shipped (usually the version bundled in Spark), and has had no maintenance whatsoever since hadoop-2.6 was released. It's been removed from the most recent versions as everyone is expected to have migrated.</p></li><li><p>It has fundamental limitations, including</p><ul><li>atrocious handling of <code>seek()</code> operations on large files (the underlying library now reads to the end of the file on every backwards seek. This kills performance on columnar ORC/Parquet data.</li><li>no support for the v4 authentication protocol, so no support for AWS frankfurt, Seoul, London &amp;c.</li><li>no support for encryption</li><li>no support for recovery of transient failures</li><li>no support for parallelized and incremental uploads of large files. S3A writes files a block at a time while the output is still being generated</li></ul></li></ol><p>Because of these issues and the fact it obsolete, all bugs filed related to s3n on the ASF JIRA are closed as WONTFIX. </p><p>Just step from s3n, move to s3a. Your life will be better.</p><p>Do know that as you use s3a, the version of the AWS libraries and all hadoop-* JARs must be in sync. the hadoop-* JARs rely on each other being synchronized, and the aws-* JARs are brittle enough over time that hadoop needs serious retesting on every upgrade, often changes in the APIs, bugs filed against the AWS SDK team, etc. Mixing things will only create unusual stack traces, which are closed as INVALID: ""don't do that then"".</p><p>Now, if you really want to test s3n support, </p><ol><li>check out the <a href=""https://github.com/apache/hadoop"" rel=""nofollow noreferrer"">hadoop source tree</a> for the version of hadoop you want spark to run with</li><li>Follow the <a href=""https://hadoop.apache.org/docs/r2.9.1/hadoop-aws/tools/hadoop-aws/testing.html"" rel=""nofollow noreferrer"">testing instructions</a> for testing s3a &amp; s3n against your endpoint.</li><li>Especially the bit for <a href=""https://hadoop.apache.org/docs/r2.9.1/hadoop-aws/tools/hadoop-aws/testing.html#Testing_against_non_AWS_S3_endpoints."" rel=""nofollow noreferrer"">testing against non-AWS S3 services</a>. </li><li>For the best s3a testing, check out hadoop trunk and run those tests too.</li></ol><p>As noted, problems with s3n won't be fixed. Your homework. S3A ones, maybe, but you'll have to show it is a fault on the s3a libraries itself.</p>",2261274,2261274,2019-03-19T12:37:20.670,2019-03-19T12:37:20.670,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55238531
667,55249677,2,52860945,2019-03-19T20:43:58.103,0,"<p>tyron's comment on your question is spot on. Check permissions of the user executing the CloudFormation. If you're running commands directly, this is usually pretty easy to check. In some cases, you may be working with a more complicated environment with automation.</p><p>I find the best way to troubleshoot permissions in an automated world is via CloudTrail. After any API call has failed, whether from the CLI, CloudFormation, or another source, you can look up the call in CloudTrail.</p><p>In this case, searching for ""Event Name"" = ""CreateQueue"" in the time range of the failure will turn up a result with details like the following:</p><ul><li><strong>Source IP Address</strong>; this field may say something like cloudformation.amazonaws.com, or the IP of your machine/office. Helpful when you need to filter events based on the source.</li><li><strong>User name</strong>; In my case, this was the EC2 instance ID of the agent running the CFN template.</li><li><strong>Access Key ID</strong>; For EC2 instances, this is likely a set of temporary access credentials, but for a real user, it will show you what key was used.</li><li><strong>Actual event data</strong>; Especially helpful for non-permissions errors, the actual event may show you errors in the request itself.</li></ul><p>In my case, the specific EC2 instance that ran automation was out of date and needed to be updated to use the correct IAM Role/Instance Profile. CloudTrail helped me track that down.</p>",3570747,,,2019-03-19T20:43:58.103,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55249677
668,55261792,2,13681213,2019-03-20T13:24:48.017,33,"<p>The answers on this thread are a little bit outdated, so I've decided to add my two cents to it:</p><p>You can see <strong>SNS</strong> as a traditional topic which you can have multiple Subscribers. You can have heterogeneous subscribers for one given SNS topic, including Lambda and SQS, for example. You can also send SMS messages or even e-mails out of the box using SNS. One thing to consider in SNS is only one message (notification) is received at once, so you cannot take advantage from batching.</p><p><strong>SQS</strong>, on the other hand, is nothing but a Queue, where you store messages and subscribe one consumer (yes, you can have N consumers to one SQS queue, but it would get messy very quickly and way harder to manage considering all consumers would need to read the message at least once, so one is better off with SNS combined with SQS for this use case, where SNS would push notifications to N SQS queues and every queue would have one subscriber, only) to process these messages. As of Jun 28, 2018, <a href=""https://aws.amazon.com/blogs/aws/aws-lambda-adds-amazon-simple-queue-service-to-supported-event-sources/"" rel=""noreferrer"">AWS Supports Lambda Triggers for SQS</a>, meaning you don't have to <strong>poll</strong> for messages anymore. Furthermore, you can configure a DLQ on your source SQS queue to send messages to in case of failure. In case of success, messages are automatically deleted (this is another great improvement), so you don't have to worry about the already processed messages being read again in case you forgot to delete them manually.  I suggest taking a look at <a href=""https://docs.aws.amazon.com/lambda/latest/dg/retries-on-errors.html"" rel=""noreferrer"">Lambda Retry Behaviour</a> to better understand how it works. One great benefit of using SQS is that it enables batch processing. Each batch can contain up to 10 messages, so if 100 messages arrive at once in your SQS queue, then 10 Lambda functions will spin up (considering the default auto-scaling behaviour for Lambda) and they'll process these 100 messages (keep in mind this is the happy path as in practice, a few more Lambda functions could spin up reading less than the 10 messages in the batch, but you get the idea). If you posted these same 100 messages to SNS, however, 100 Lambda functions would spin up, unnecessarily increasing costs and using up your Lambda concurrency.However, if you are still running traditional servers (like EC2 instances), you will still need to poll for messages and manage them manually. </p><p>You also have <strong>FIFO SQS queues</strong>, which guarantee the delivery order of the messages. This is not a supported trigger by Lambda, thus when choosing this type of Queue, keep in mind that polling is still necessary as well as having to delete the messages manually.</p><p>Even though there's some overlap in their use cases, both SQS and SNS have their own spotlight.</p><p>Use <strong>SNS</strong> if:</p><ul><li>multiple subscribers is a requirement</li><li>sending SMS/E-mail out of the box is handy</li></ul><p>Use <strong>SQS</strong> if:</p><ul><li>only one subscriber is needed</li><li>batching is important</li></ul>",10950867,10950867,2019-03-20T13:40:44.563,2019-03-20T13:40:44.563,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55261792
669,55284035,2,55283800,2019-03-21T15:33:28.717,5,"<p>Since you're already using Node 8, you don't need to use the outdated, confusing <code>callback</code> approach anymore. Use <code>await</code> instead</p><pre><code>exports.handler = async (event) =&gt; {  try {  await somePromise1   await somePromise2  await somePromise3   console.log(""Success"", resp);  response = {  statusCode: 200,  body: JSON.stringify('Ok!'),  };  return response;  } catch (err) {  console.log(""Error"", err);  response = {  statusCode: 500,  body: JSON.stringify(err),  };  return response;  }};</code></pre><p>where somePromise1, somePromise2 and somePromise3 are your promisified callbacks.</p><p>More on async/await <a href=""https://javascript.info/async-await"" rel=""nofollow noreferrer"">here</a>.</p>",10950867,8411036,2019-10-22T18:30:31.723,2019-10-22T18:30:31.723,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55284035
670,55403239,2,55317415,2019-03-28T17:04:09.093,0,"<p>So this is the OOM issue in existing Alpakka framework and get resolved in 1.0-RC2 - <a href=""https://github.com/akka/alpakka/milestone/27"" rel=""nofollow noreferrer"">https://github.com/akka/alpakka/milestone/27</a></p><p>However as an alternate <a href=""https://github.com/s12v/akka-stream-sqs"" rel=""nofollow noreferrer"">https://github.com/s12v/akka-stream-sqs</a> work as charm ( though it is deprecated in favor of Alpakka Sqs)</p>",412957,,,2019-03-28T17:04:09.093,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55403239
671,55436250,2,55435954,2019-03-30T22:32:59.570,3,"<p><code>saveData()</code> is <code>async</code>, therefore it returns a promise. You need to <code>await</code> on it. Also, I don't understand why you gave up on the .promise() call. Using callbacks is outdated and much more convoluted.</p><p>Your code should look like this:</p><pre><code>module.exports.saveData = async () =&gt; {  let params = {  TableName: ""SynData"",  Item:{  ""synId"": 66,  ""synValue"": 1579.21,  ""synTime"": ""2019-01-01""  }  };  return await documentClient.put(params).promise();};</code></pre><p>On your client, just await on saveData():</p><pre><code>for(let i = 0; i &lt; allValues.length; i++){  await db.saveData(i, allValues[i], allStart);}</code></pre>",10950867,,,2019-03-30T22:32:59.570,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55436250
672,55477120,2,55476533,2019-04-02T14:20:36.243,2,"<p>CloudTrail logs can sometimes be slow, as mentioned <a href=""https://aws.amazon.com/cloudtrail/faqs/"" rel=""nofollow noreferrer"">in the CloudTrail FAQ under ""How long does it take CloudTrail to deliver an event for an API call?""</a>:</p><blockquote>  <p>Typically, CloudTrail delivers an event within 15 minutes of the API call.</p></blockquote><p>Concerning the event rule for a specific sub-folder, it is currently (and someone update me if I'm wrong, might be outdated) possible only through an S3 Lambda trigger. The only thing you can do though through CloudWatch event rules is add a specific key to the event rule by following <a href=""https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source-console.html"" rel=""nofollow noreferrer"">this guide</a>, it just won't act as a prefix but would be a specific key that triggers the event, which might be helpful.</p><p>If a specific key doesn't suffice and you still need a prefix/suffix in your S3 trigger definition, then consider adding a Lambda that executes your step function, which is often used anyways because it allows you to customize the event sent to the step function. You can use the <a href=""https://docs.aws.amazon.com/step-functions/latest/apireference/API_StartExecution.html"" rel=""nofollow noreferrer"">StartExecution</a> API call from inside your Lambda, and setup an S3 Lambda trigger with a prefix, <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html#notification-how-to-event-types-and-destinations"" rel=""nofollow noreferrer"">which you can find information on here.</a></p>",7207514,,,2019-04-02T14:20:36.243,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55477120
673,55485222,2,55477419,2019-04-03T00:09:21.797,0,"<p>Figured I'd take a look at the actual code (go figure, right?) and found that <code>apigateway.x.API</code> is now under <code>awsx.apigateway.API</code>. Was wondering if <code>awsx</code> was related. Feel better now.</p><p><a href=""https://github.com/pulumi/pulumi-aws/pull/508"" rel=""nofollow noreferrer"">https://github.com/pulumi/pulumi-aws/pull/508</a></p><blockquote>  <p>The deprecated api aws.apigateway.x.API has been removed. Its replacement is in the @pulumi/awsx package with the name awsx.apigateway.API.</p></blockquote><p><code>pulumi-awsx</code> is described as ""AWS infrastructure best practices in component form.</p><p><a href=""https://github.com/pulumi/pulumi-awsx"" rel=""nofollow noreferrer"">https://github.com/pulumi/pulumi-awsx</a></p>",124563,,,2019-04-03T00:09:21.797,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55485222
674,55490217,2,53302266,2019-04-03T08:15:37.833,4,"<p>EDIT: Apparently my answer is now outdated. For newer versions of servicestack, see the other answers. I don't know which answer is best/most up-to-date, but if someone lets me know I'll change which answer is accepted to that one.</p><p>I eventually got it to work, so here's how I set up my autherizer's serverless.yml:</p><pre><code>service: user-admin-authorizercustom:  region: ${file(serverless.env.yml):${opt:stage}.REGION}provider:  name: aws  region: ${self:custom.region}functions:  authorizer:  handler: src/authorizer.handler  runtime: nodejs8.10resources:  Resources:  Authorizer:  Type: AWS::ApiGateway::Authorizer  Properties:  Name: Authorizer  Type: REQUEST  AuthorizerUri:  Fn::Join: [ """",  [  ""arn:aws:apigateway:"",  ""${self:custom.region}"",  "":lambda:path/"",  ""2015-03-31/functions/"",  Fn::GetAtt: [""AuthorizerLambdaFunction"", ""Arn"" ],  ""/invocations""  ]]  RestApiId:  Fn::ImportValue: api-gateway:${opt:stage}:rest-api-id  apiGatewayLambdaPermissions:  Type: AWS::Lambda::Permission  Properties:  FunctionName:  Fn::GetAtt: [ AuthorizerLambdaFunction, Arn]  Action: lambda:InvokeFunction  Principal:  Fn::Join: [ """",  [  ""apigateway."",  Ref: AWS::URLSuffix  ]]  Outputs:  AuthorizerRef:  Value:  Ref: Authorizer  Export:  Name: authorizer-ref:${opt:stage}</code></pre><p>Things to note: Even though the authorizer function is called ""authorizer"", you need to capitalize the first letter and append ""LambdaFunction"" to its name when using it with GetAtt, so ""authorizer"" becomes ""AuthorizerLambdaFunction"" for some reason. I also had to add the lambda permission resource.</p><p>The API gateway resource also needs two outputs, its API ID and its API root resource ID. Here's how my API gateway's serverless.yml is set up:</p><pre><code>resources:  Resources:  ApiGateway:  Type: AWS::ApiGateway::RestApi  Properties:  Name: ApiGateway  Outputs:  ApiGatewayRestApiId:  Value:  Ref: ApiGateway  Export:  Name: api-gateway:${opt:stage}:rest-api-id  ApiGatewayRestApiRootResourceId:  Value:  Fn::GetAtt:  - ApiGateway  - RootResourceId  Export:  Name: api-gateway:${opt:stage}:root-resource-id</code></pre><p>Now you just need to specify to your other services that they should use this API gateway (the imported values are the outputs of the API gateway):</p><pre><code>provider:  name: aws  apiGateway:  restApiId:  Fn::ImportValue: api-gateway:${opt:stage}:rest-api-id  restApiRootResourceId:  Fn::ImportValue: api-gateway:${opt:stage}:root-resource-id</code></pre><p>After that, the authorizer can be added to individual functions in this service like so:</p><pre><code>  authorizer:  type: CUSTOM  authorizerId:  Fn::ImportValue: authorizer-ref:${opt:stage}</code></pre>",9879658,9879658,2019-09-23T07:01:06.363,2019-09-23T07:01:06.363,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55490217
675,55524598,2,55514647,2019-04-04T20:42:35.990,1,"<p>get rid of that line about fs.s3a.impl. All it does is change the default mapping of ""s3a"" to ""the modern, supported, maintained S3A connector"" to the ""old, obsolete, unsupported S3N connector""</p><p>you do not need that line. The fact that people writing spark apps always do this is just superstition. Hadoop-common knows which filesystem class handles s3a URLs the same way it knows who handles ""file"" and ""hdfs""</p>",2261274,,,2019-04-04T20:42:35.990,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55524598
676,55653384,2,55633029,2019-04-12T14:00:22.330,2,"<p>After @a_horse_with_no_name pointed me in the right direction and chatting with AWS I am able to answer my own question, at least if you are using AWS Database Migration Service (DMS).</p><p>The problem is, DMS only focuses on the data itself and not really the schema (which to me seems like a major oversight, especially if your using the same database technology but that is another issue).So the schema itself is not migrated. The documentation does not really make this clear.</p><p>To fix this issue:</p><ol><li>Stop (if it still exists) the existing AWS DMS migration</li><li>Drop the existing migrated database, and create a new empty schema to use</li><li>Follow the steps here <a href=""https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Installing.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Installing.html</a> to install and setup the Amazon Schema Conversation Tool (SCT)</li><li>Once you are connected to both databases, follow the steps here <a href=""https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Converting.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Converting.html</a> to ""convert"" your schema (I did the entire ""public"" schema for this database to ensure everything is covered</li><li>Create or modify your AWS DMS Migration, ensuring Target Table Preparation Mode = ""TRUNCATE"" and disable foreign keys on the target database. If modifying, make sure when asked you ""RESTART"" not resume</li></ol><p>What I have not yet tested is how to handle the fact that I am migrating a live database. So the sequences may be out of date on the target database when the migration is done. I believe I can just later go into SCT and only migrate the sequences but I have not tested this yet.</p>",1955996,,,2019-04-12T14:00:22.330,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55653384
677,55657852,2,55657701,2019-04-12T18:47:46.707,1,"<p>Security group rules can reference CIDR blocks, prefix list IDs (for VPC endpoints) or other security groups.</p><p>This allows you to say that everything that has a security group of <code>foo</code> is allowed to communicate with everything with a security group of <code>bar</code>.</p><p>The bit specifically about whether it can be IDs (of the form <code>sg-123456</code>) or the names is down to a quirk in the AWS APIs and support for the long deprecated EC2 classic accounts. In general you want to use the IDs to reference security groups.</p>",2291321,,,2019-04-12T18:47:46.707,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55657852
678,55761393,2,55295305,2019-04-19T11:50:23.273,0,"<p>This what The python 3.7 AWS lambda environment looks like at time of writing:</p><pre><code>python: 3.7.2 (default, Mar 1 2019, 11:28:42)[GCC 4.8.3 20140911 (Red Hat 4.8.3-9)], boto3: 1.9.42, botocore: 1.12.42</code></pre><p>By comparing botocore 1.12.42 (error) with 1.12.133 (working ok) I found that an outdated botocore in AWS Lambda is the culprit. One solution could be to include the latest botocore in your lambda package. For example using the the python requirements plugin:</p><pre><code>serverless plugin install -n serverless-python-requirements</code></pre><p>And creating a <code>requirements.txt</code> file containing <code>botocore==1.12.133</code></p><p>(instead of 1.12.133 you might want to use the latest version at the time you read this)</p>",923557,,,2019-04-19T11:50:23.273,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55761393
679,55884418,2,55873006,2019-04-27T20:01:34.523,7,"<p>There are two problems here.  The efficiency of the the scalar subquery above <code>select max(partition_id) from table</code>, and the one @PiotrFindeisen pointed out around dynamic filtering.</p><p>The the first problem is that queries over the partition keys of a Hive table are a lot more complex than they appear.  Most folks would think that if you want the max value of a partition key, you can simply execute a query over the partition keys, but that doesn't work because Hive allows partitions to be empty (and it also allows non-empty files that contain no rows).  Specifically, the scalar subquery above <code>select max(partition_id) from table</code> requires Presto to find the max partition containing at least one row.  The ideal solution would be to have perfect stats in Hive, but short of that the engine would need to have custom logic for hive that open files of the partitions until it found a non empty one.</p><p>If you are are sure that your warehouse does not contain empty partitions (or if you are ok with the implications of that), you can replace the scalar sub query with one over the hidden <code>$partitions</code> table""</p><pre><code>select * from table where column1 = 'val' and   partition_id = (select max(partition_id) from ""table$partitions"");</code></pre><p>The second problem is the one @PiotrFindeisen pointed out, and has to do with the way that queries are planned an executed.  Most people would look at the above query, see that the engine should obviously figure out the value of <code>select max(partition_id) from ""table$partitions""</code> during planning, inline that into the plan, and then continue with optimization.  Unfortunately, that is a pretty complex decision to make generically, so the engine instead simply models this as a broadcast join, where one part of the execution figures out that value, and broadcasts the value to the rest of the workers.  The problem is the rest of the execution has no way to add this new information into the existing processing, so it simply scans all of the data and then filters out the values you are trying to skip.  There is a project in progress to add this <a href=""https://github.com/prestosql/presto/issues/52"" rel=""noreferrer""><em>dynamic filtering</em></a>, but it is not complete yet. </p><p>This means the best you can do today, is to run two separate queries: one to get the max partition_id and a second one with the inlined value.</p><p>BTW, the hidden ""$partitions"" table was added in Presto <a href=""https://prestosql.io/docs/current/release/release-0.199.html"" rel=""noreferrer"">0.199</a>, and we fixed some minor bugs in <a href=""https://prestosql.io/docs/current/release/release-0.201.html"" rel=""noreferrer"">0.201</a>.  I'm not sure which version Athena is based on, but I believe it is is pretty far out of date (the current release at the time I'm writing this answer is <a href=""https://prestosql.io/docs/current/release/release-309.html"" rel=""noreferrer"">309</a>.</p>",2958751,2958751,2019-04-27T21:31:57.113,2019-04-27T21:31:57.113,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55884418
680,55891586,2,55890992,2019-04-28T15:06:30.677,0,"<p>You are making a mistake trying to move the database <em>and</em> change the engine from MySQL to Aurora at the same time.</p><p>Migrate the MySQL 5.7 system now, and convert to Aurora later.  You do not need to ask for trouble, and doing both at the same time is exactly that.</p><p>It is not possible to ""quickly"" migrate a primary database over distance, but it is possible to make the amount of setup time irrelevant, and activation time near zero.</p><p>Instead of trying to do a copy, create an RDS cross-region replica of your data, and at the last moment, promote that replica to master.</p><blockquote>  <p><strong>Creating a Read Replica in a Different AWS Region</strong></p>   <p>With Amazon RDS, you can create a MariaDB, MySQL, or PostgreSQL Read Replica in a different AWS Region than the source DB instance. You create a Read Replica to do the following:</p>   <ul>  <li><p>Improve your disaster recovery capabilities.</p></li>  <li><p>Scale read operations into an AWS Region closer to your users.</p></li>  <li><p>Make it easier to migrate from a data center in one AWS Region to a data center in another AWS Region.</p></li>  </ul>   <p><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.XRgn"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.XRgn</a></p></blockquote><p>It doesn't matter how long it takes for RDS to copy the data and set up the replica, because as soon as it is copied, it starts replicating everything that changed on the master server since the process began.</p><p>Once you have verified that everything is correct and consistent, then you <a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.Promote"" rel=""nofollow noreferrer"">promote a replica</a>.  It is permanently and irrevocably detached from its original upstream instance, and becomes writable.  This is the last thing you do and after the application starts writing to this new database, your original system in S’£o Paulo is obsolete because changes to it will no longer replicate to the new system -- they're permanently isolated.</p><p>This arrangement does not require you to establish any networking or make the databases publicly accessible.</p><p>And, you can create and destroy multiple replicas to test this process, without disturbing production.</p>",1695906,,,2019-04-28T15:06:30.677,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55891586
681,55952526,2,55918088,2019-05-02T12:31:22.990,0,"<p>Hi I am providing my own answer here (thanks my colleagues Kannan for the insight)</p><p><code>#1</code> above is what AWS called <code>Class Tag Editor</code>.  If you click on the Question mark on the Web UI (upper right corner), you will be taken to a page that says:</p><blockquote>  <p>This documentation is for classic Tag Editor, which has been  deprecated</p></blockquote><p>So <code>#2</code> is the version that AWS want us to use.</p><p>Below I will called <code>#1</code> <strong>Old</strong> and <code>#2</code> <strong>New</strong></p><p>I compared the example outputs from our environment (about 50 resources).  The two outputs differ in these respects:</p><ol><li><strong>New</strong> seems to retain past resources for a longer time.  For example, if an EC2 instance has been terminated, it may take alonger time to be removed from the listing of <strong>New</strong></li><li><strong>New</strong> seems to include resources for DynamoDB but <strong>Old</strong> does not</li><li><strong>Old</strong> seems to include resources for Route 53 Hosted Zones but <strong>New</strong> does not.</li><li>Both <strong>New</strong> and <strong>Old</strong> show Security Groups, but the ID strings are rendered slightly differently.  <ul><li><strong>New</strong> renders an ID as <code>sg-xxxxxxxxxxxxxxxxxxxxxx</code></li><li><strong>Old</strong> renders an ID as <code>someName (sg-xxxxxxxxxxxxxxxxx)</code></li></ul></li></ol>",1168041,1168041,2019-05-02T12:45:20.287,2019-05-02T12:45:20.287,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55952526
682,55967936,2,55878755,2019-05-03T10:27:22.380,0,"<p>As I mentioned, it was connecting problem in my case.</p><p>In addition <code>template_file</code> was deprecated so I change the code  to:</p><pre><code>resource ""aws_instance"" ""secondary_zone"" {  instance_type = ""${var.ec2_instance_type}""  ami   = ""${data.aws_ami.latest-ubuntu.id}""  key_name = ""${aws_key_pair.deployer.key_name}""  subnet_id = ""${aws_subnet.secondary.id}""  vpc_security_group_ids =  [""${aws_security_group.server.id}""]  associate_public_ip_address = true  connection {  type   = ""ssh""  user = ""ubuntu""  private_key = ""${file(""~/.ssh/id_rsa"")}""  timeout = ""2m""  }  provisioner ""file"" {  source  = ""/server/script.sh""  destination = ""/tmp/script.sh""  }    provisioner ""remote-exec"" {  inline = [  ""chmod +x /tmp/script.sh"",  ""/tmp/script.sh args"",  ]  }}</code></pre><p>Also, I learned that the scrip.sh have to be formatted as <code>LR</code></p>",7807419,7807419,2019-05-03T10:39:59.993,2019-05-03T10:39:59.993,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/55967936
683,56003144,2,55918088,2019-05-06T10:16:55.057,3,"<p>I am part of the team building the new Tag Editor. Yes, you are correct: Classic Tag Editor is deprecated, and will be shut down soon entirely. We are working on full feature parity between the two Editors, so you will very soon find everything you can do in the old one as well in the new one.</p><p>To add some more context on your different items below:</p><p>1) Both old and new Tag Editor use the same underlying tagging infrastructure, so this should never happen. Maybe there is some browser issue involved here? Feel free to open a support issue so we can look deeper into it, if this continues the case.</p><p>2) Yes, the new one also includes Lambda, and will very soon add more resource types. The same by the way for regions: The old Tag Editor supports not all regions, for example <code>eu-north-1</code> or <code>eu-west-3</code>.</p><p>3) No, Route53 Hosted Zones are supported in both Editors. Route53 resources only exists in the <code>us-east-1</code> region, so maybe you used the Tag Editor in another region?</p><p>4) Both Editors show the same data. The old editor merged what you used as Name Tag and the ID in the same field - in the new one, you see only the ID in the column ID, and the Name Tag is displayed in the column <code>Tag: Name</code>.</p><p>Searching across regions is something the new Editor soon will support, too, and the same applies for the filter you mention. For showing resources without a specific tag, there is a workaround you already can do: Click on the settings icon in the top right of the table, and enable the tag you are interested in as a column. You then can sort this column so that all untagged ones show up on top.</p><p>If you have any other ideas or requests for the Tag Editor, please let us know. The fastest and most reliable way is to just use the 'Feedback' Button in the console in the bottom left.</p><p>Cheers,Florian </p>",972592,,,2019-05-06T10:16:55.057,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56003144
684,56038511,2,56034053,2019-05-08T10:23:02.063,6,"<p>Put <code>async</code> back to its place as using callbacks is outdated and much more error prone. Use the built-in <code>promise()</code> methods available on the node.js aws-sdk and just <code>await</code> on these promises. If you want to deal with errors, just surround your code with a <code>try/catch</code> block.</p><pre><code>const AWS = require('aws-sdk')const dynamodb = new AWS.DynamoDB.DocumentClient({region: 'ap-southeast-2'});exports.handler = async (event) =&gt; {  const params = {  TableName: 'people-dev',  Item: {  id: '1',  name: 'person',  email: 'person@example.com'  }  };  await dynamodb.put(params).promise()  return {  statusCode: 200,  body: JSON.stringify({message: 'Success'})  }};</code></pre><p>More on <a href=""https://javascript.info/async-await"" rel=""noreferrer"">async/await</a></p>",10950867,,,2019-05-08T10:23:02.063,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56038511
685,56053053,2,55982706,2019-05-09T06:00:31.453,0,"<p>From S3 perspective you can't check free space.</p><p>Ceph has implemented expiration mechanism, which will delete outdated objects from your object storage.Check the doc: <a href=""http://docs.ceph.com/docs/master/radosgw/s3/#features-support"" rel=""nofollow noreferrer"">http://docs.ceph.com/docs/master/radosgw/s3/#features-support</a></p>",1881757,,,2019-05-09T06:00:31.453,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56053053
686,56087567,2,56085647,2019-05-11T05:45:00.143,0,"<p>You should not need to have to create each and every type, what youwould need to do is make a generic tag handling routine, that looks atthe type of node the tag is on (mapping, sequence, scalar), thencreates such a node as a Ruby type on which the tag can be attached. </p><p>I don't know how to do that with <code>Psych</code> and <code>Ruby</code>, but you indicatedneither is a strict requirement, and most of the hard workfor this kind of round-tripping in <code>ruamel.yaml</code> for <code>Python</code>(disclaimer: I am the author of that package). </p><p>If this is your inputfile <code>input.yaml</code>:</p><pre><code>Foo: !Bar bazN1:  - !mytaggedmaptype  parm1: 3  parm3: 4  - !mytaggedseqtype  - 8  - 9N2: &amp;someanchor1  a: ""some stuff""  b: 0.2e+1  f: |  within a literal scalar newlines  are preservedN3: &amp;someanchor2  c: 0x3  b: 4  # this value is not taken, as the first entry found is taken   ['the', 'answer']: still unknown  {version: 28}: tested!N4:  d: 5.000  &lt;&lt;: [*someanchor1, *someanchor2]</code></pre><p>Then this Python (3) program:</p><pre><code>import sysfrom pathlib import Pathimport ruamel.yamlyaml_in = Path('input.yaml')yaml_out = Path('output.yaml')yaml = ruamel.yaml.YAML()yaml.preserve_quotes = True# uncomment next line if your YAML is the outdated version 1.1 YAML but has no tag# yaml.version = (1, 1) data = yaml.load(yaml_in)# do your updating heredata['Foo'].value = 'hello world!'  # see the first of the notesdata['N1'][0]['parm3'] = 4444data['N1'][0].insert(1, 'parm2', 222)data['N1'][1][1] = 9999data['N3'][('the', 'answer')] = 42# and dump to fileyaml.dump(data, yaml_out)</code></pre><p>creates <code>output.yaml</code>:</p><pre><code>Foo: !Bar hello world!N1:- !mytaggedmaptype  parm1: 3  parm2: 222  parm3: 4444- !mytaggedseqtype  - 8  - 9999N2: &amp;someanchor1  a: ""some stuff""  b: 0.2e+1  f: |  within a literal scalar newlines  are preservedN3: &amp;someanchor2  c: 0x3  b: 4   # this value is not taken, as the first entry found is taken   ['the', 'answer']: 42  {version: 28}: tested!N4:  d: 5.000  &lt;&lt;: [*someanchor1, *someanchor2]</code></pre><p>Please note:</p><ul><li><p>you can update tagged scalars while preserving the tag on thescalar, but since you replace such a scalar with its assignment(instead of updating a value as with lists (sequences/arrays) ordicts (mappings/hashes), you cannot just assign the new value oryou'll lose the tagging information, you have to update the <code>.value</code> attribute.</p></li><li><p>things like anchors, merges, comments, quotes are preserved, as arespecial forms of integers (hex, octal, etc) and floats.</p></li><li><p>For YAML sequences that are mapping keys, you need to use a tuple(<code>('the', 'answer')</code>) instead of a sequence (<code>['the', 'answer']</code>),as Python doesn't allow mutable keys in mappings. And for YAMmappings that are mapping keys you would need to use the immutable<code>Mapping</code> from<a href=""https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping"" rel=""nofollow noreferrer""><code>collections.abc</code></a>.(I am not sure if Psych supports these kind of valid YAML keys)</p></li><li><p>See <a href=""https://stackoverflow.com/a/55717146/1307905"">this</a> if you need to update anchored/aliased scalars</p></li></ul>",1307905,,,2019-05-11T05:45:00.143,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56087567
687,56212216,2,56212071,2019-05-19T21:38:01.500,1,"<p>PHP 7.0 is obsolete -- <a href=""https://php.net/eol.php"" rel=""nofollow noreferrer"">it reached end-of-life in January 2019</a> -- and it is not available in Ubuntu 18.04. The appropriate version for this release is PHP 7.2 (e.g. <a href=""https://packages.ubuntu.com/bionic/php7.2"" rel=""nofollow noreferrer""><code>php7.2</code></a>, <a href=""https://packages.ubuntu.com/bionic/php7.2-mysql"" rel=""nofollow noreferrer""><code>php7.2-mysql</code></a>, etc); you will need to change your install commands accordingly.</p><p>The mcrypt extension <a href=""https://www.php.net/manual/en/migration71.deprecated.php"" rel=""nofollow noreferrer"">was deprecated in PHP 7.1</a> and subsequently <a href=""https://www.php.net/manual/en/migration72.other-changes.php"" rel=""nofollow noreferrer"">removed in PHP 7.2</a>. If you are using this extension in your application, <a href=""https://stackoverflow.com/questions/41272257/mcrypt-is-deprecated-what-is-the-alternative"">you will need to rework those parts of your code</a>.</p>",149341,,,2019-05-19T21:38:01.500,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56212216
688,56234110,2,37385411,2019-05-21T08:24:36.103,0,"<p>As the error message rather clearly tells you, you somehow ended up passing in <code>169.254.169.254http</code> as the host name, and <code>169.254.169.254http:80</code> as the host.  Just to spell this out completely, you probably wanted the host to be <code>169.254.169.254</code>. You need to figure out why your request was botched like this, and correct the code or your configuration files so you send what you wanted to send.</p><p><code>ENOTFOUND</code> in response to <code>getaddrinfo</code> simply means that you wanted to obtain the address of something which doesn't exist or is unknown. Very often this means that you have a typo, or that the information you used to configure your service is obsolete or otherwise out of whack (attempting to reach a private corporate server when you are outside the  corporate firewall, for example).</p>",874188,,,2019-05-21T08:24:36.103,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56234110
689,56251044,2,56248883,2019-05-22T07:02:09.373,0,"<p><a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBMarshalling.html"" rel=""nofollow noreferrer"">DynamoDBMarshalling</a> is deprecated, so I suggest using the newer <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBTypeConverted.html"" rel=""nofollow noreferrer"">DynamoDBTypeConverted</a> annotation. </p><p>There are some useful notes on <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.ArbitraryDataMapping.html"" rel=""nofollow noreferrer"">Mapping Arbitrary Data</a>.</p><p>You can also see an example of mine in <a href=""https://stackoverflow.com/questions/30793481/dynamodb-jsonmarshaller-cannot-deserialize-list-of-object/47153880#47153880"">this answer</a></p><p>In summary, you create an <code>AllCities</code> plain java object. You then write a simple converter class which tells DynamoDB how to turn your <code>AllCities</code> object into a string to get into DynamoDB. Similarly, the converter class tells your application how to turn the string back into a Java object.</p>",4985580,,,2019-05-22T07:02:09.373,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56251044
690,56299937,2,53919763,2019-05-24T21:56:33.307,-1,"<p>The problem seems to be with Athena, it only seems to support int96 and when you create a timestamp in pandas it is an int64</p><p>my dataframe column that contains a string date is ""sdate""  I first convert to timestamp</p><pre class=""lang-py prettyprint-override""><code># add a new column w/ timestampdf[""ndate""] = pandas.to_datetime[""sdate""]# convert the timestamp to microsecondsdf[""ndate""] = pandas.to_datetime([""ndate""], unit='us')# Then I convert my dataframe to pyarrowtable = pyarrow.Table.from_pandas(df, preserve_index=False)# After that when writing to parquet add the coerce_timestamps and # use_deprecated_int96_timstamps. (Also writing to S3 directly)OUTBUCKET=""my_s3_bucket""pyarrow.parquet.write_to_dataset(table, root_path='s3://{0}/logs'.format(OUTBUCKET), partition_cols=['date'], filesystem=s3, coerce_timestamps='us', use_deprecated_int96_timestamps=True)</code></pre>",3230003,,,2019-05-24T21:56:33.307,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56299937
691,56376017,2,56368882,2019-05-30T10:04:58.430,0,"<p>Bitnami Engineer here.</p><p>Let's try these changes to fix the errors you are running into:</p><ul><li>Remove the following block</li></ul><pre><code>  &lt;Directory /&gt;  Options -Indexes  AllowOverride All  &lt;/Directory&gt;</code></pre><p>You only need the one that sets the configuration for the folder where you have your app's files.</p><ul><li>You will need to substitute these lines because they are not supported in the new Apache's version </li></ul><pre><code>Order allow,denyallow from all</code></pre><p>is deprecated. You need to use <code>Require all granted</code> when using Apache 2.4</p><p><a href=""https://httpd.apache.org/docs/2.4/mod/mod_authz_core.html#require"" rel=""nofollow noreferrer"">https://httpd.apache.org/docs/2.4/mod/mod_authz_core.html#require</a></p><ul><li>Set the permissions of your app's folder properly</li></ul><pre><code>sudo chown -R bitnami:daemon /opt/bitnami/apache2/site1.com/htdocssudo chmod -R g+w /opt/bitnami/apache2/site1.com/htdocs</code></pre>",4709625,,,2019-05-30T10:04:58.430,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56376017
692,56391585,2,56224913,2019-05-31T08:43:21.403,1,"<p>You should use DynamoDB because it has a lot of useful features such as <a href=""https://aws.amazon.com/blogs/aws/new-amazon-dynamodb-continuous-backups-and-point-in-time-recovery-pitr/"" rel=""nofollow noreferrer"">Point in Time Recovery</a>, <a href=""https://aws.amazon.com/about-aws/whats-new/2018/11/announcing-amazon-dynamodb-support-for-transactions/"" rel=""nofollow noreferrer"">transactions</a>, <a href=""https://aws.amazon.com/blogs/security/now-available-encryption-at-rest-for-amazon-dynamodb/"" rel=""nofollow noreferrer"">encryption-at-rest</a>, and <a href=""https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/"" rel=""nofollow noreferrer"">activity streams</a> that SimpleDB does not have.</p><p>If you're operating on a small scale, DynamoDB has the advantage that it allows you to set a maximum capacity for your table, which means you can make sure you stay in the free tier.</p><p>If you're operating at a larger scale, DynamoDB automatically handles all of the partitioning of your data (and has, for all practical purposes, limitless capacity), whereas <a href=""https://docs.aws.amazon.com/AmazonSimpleDB/latest/DeveloperGuide/SDBLimits.html"" rel=""nofollow noreferrer"">SimpleDB has a limit of 10 GB</a> per domain (aka ""table"") and you are required to manage any horizontal partitioning across domains that you might need.</p><p>Finally, there are signs that SimpleDB is already on a deprecation path. For example, if you look at the <a href=""https://aws.amazon.com/releasenotes/?tag=releasenotes%23keywords%23amazon-simpledb"" rel=""nofollow noreferrer"">SimpleDB release notes</a>, you will see that the last update was in 2011, whereas DynamoDB had several new features announced at the last re:Invent conference. Also, there are a number of reddit posts (such as <a href=""https://www.reddit.com/r/aws/comments/a3280t/no_amazon_simpledb/"" rel=""nofollow noreferrer"">here</a>, <a href=""https://www.reddit.com/r/aws/comments/8rwov0/whats_wrong_with_simpledb_and_should_i_use_it/"" rel=""nofollow noreferrer"">here</a>, and <a href=""https://www.reddit.com/r/aws/comments/2iuw11/cant_find_simpledb/"" rel=""nofollow noreferrer"">here</a>) where the general consensus is that SimpleDB is already deprecated, and in some of the threads, <a href=""https://aws.amazon.com/blogs/aws/author/jbarr/"" rel=""nofollow noreferrer"">Jeff Barr</a> even commented and did not contradict any of the assertions that SimpleDB is deprecated.</p><hr><p>That being said, in DynamoDB, you can support your desired queries. You will need two <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html"" rel=""nofollow noreferrer"">Global Secondary Indexes</a>, which use a <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-sort-keys.html"" rel=""nofollow noreferrer"">composite sort key</a>. Your queries can be supported with the following schema:</p><ul><li><code>ID</code> Š—” hash key of your table</li><li><code>Author</code> Š—” hash key of the <code>Author-Status-CreatedDateTime-index</code></li><li><code>Category</code> Š—” hash key of the <code>Category-Status-CreatedDateTime-index</code></li><li><code>Status</code></li><li><code>CreatedDateTime</code></li><li><code>UpdatedDateTime</code></li><li><code>Status-CreatedDateTime</code> Š—” sort key of <code>Author-Status-CreatedDateTime-index</code> and <code>Category-Status-CreatedDateTime-index</code>. This is a composite attribute that exists to enable some of your queries. It is simply the value of <code>Status</code> with a separator character (I'll assume it's <code>#</code> for the rest of this answer), and <code>CreatedDateTime</code> appended to the end. (Personal opinion here: use <a href=""https://en.wikipedia.org/wiki/ISO_8601"" rel=""nofollow noreferrer"">ISO-8601</a> timestamps instead of unix timestamps. It will make troubleshooting a lot easier.)</li></ul><p>Using this schema, you can satisfy all of your queries.</p><p><strong>query by ID:</strong>Simply perform a <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_GetItem.html"" rel=""nofollow noreferrer""><code>GetItem</code></a> request on the main table using the blog post Id.</p><p><strong>query by Author, paginated:</strong>Perform a <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html"" rel=""nofollow noreferrer""><code>Query</code></a> on the <code>Author-Status-CreatedDateTime-index</code> with a key condition expression of <code>Author = :author</code>.</p><p><strong>query by (Author, Status), sorted by CreatedDateTime, paginated:</strong> Perform a <code>Query</code> on the <code>Author-Status-CreatedDateTime-index</code> with a key condition expression of <code>Author = :author and begins_with(Status-CreatedDateTime, :status)</code>. The results will be returned in order of ascending <code>CreatedDateTime</code>.</p><p><strong>query by (Category, Status), sorted by CreatedDateTime, paginated:</strong> Perform a <code>Query</code> on the <code>Category-Status-CreatedDateTime-index</code> with a key condition expression of <code>Author = :author and begins_with(Status-CreatedDateTime, :status)</code>. The results will be returned in order of ascending <code>CreatedDateTime</code>. (Additionally, if you wanted to get all the blog posts in the ""technology"" category that have the status <code>published</code> and were created in 2019, you could use a key condition expression of <code>Category = ""technology"" and begins_with(Status-CreatedDateTime, ""published#2019"")</code>.</p><p>The sort order of the results can be controlled using the <code>ScanIndexForward</code> field of the Query request. The default is <code>true</code> (sort ascending); but by setting it to <code>false</code> DynamoDB will return results in descending order.</p><p>DynamoDB has built in support for paginating the results of a Query operation. Basically, any time that there are more results that were not returned, the query response will contain a <code>lastEvaluatedKey</code> which you can pass into your next query request to pick up where you left off. (See <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html#Query.Pagination"" rel=""nofollow noreferrer"">Query Pagination</a> for more details about how it works.)</p><hr><p>On the other hand, if you're already familiar with SQL, and you want to make this as easy for yourself as possible, consider just using the <a href=""https://aws.amazon.com/blogs/aws/new-data-api-for-amazon-aurora-serverless/"" rel=""nofollow noreferrer"">Aurora Serverless Data API</a>.</p>",5563569,,,2019-05-31T08:43:21.403,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56391585
693,56401067,2,56392645,2019-05-31T19:58:44.883,1,"<p>""Good choices"" depend on context, and here the contexts differ. A Device Shadow is meant to track and accurately reflect the state of the device. The page you linked does not discuss having multiple shadows for one device, and may not have been written with multiple receivers in mind. Still, its simple topic architecture would work in a system that had multiple shadows per device because a <em>shadow</em> should receive all responses from its device. Any of those responses could reveal a change in device state, and any shadow whose client does not receive the response will be out of date, and out of sync with the other shadows.</p><p><em>Your masters do not sound like shadows.</em> Perhaps they independently report their results to a data store that functions as the shadow. Perhaps nothing in the responses represents a state change worth tracking beyond the request. Either way, the documentation does not sound relevant to your goal. </p><p>I support your preference for the first choice, especially as the number of nodes grows. The main drawback is the extra work keeping track of the requesting master ID. It's easy for the masters (each can just subscribe to <code>mySystem/+/res/masterId</code>, and in a system with topic access control you could even isolate them). If the request body is otherwise simple (not already multiple attributes to parse), you might consider having the masters post to <code>mysystem/slaveId/req/masterId</code> (slave could subscribe to <code>mysystem/slaveId/req/+</code>).</p><p>The biggest example to take from AWS might be the clear sense of hierarchy in the topics. If the parsing convenience of having the masterId at the end of the topic is not your top concern, it might make more sense to order as: <code>mysystem/masterId/slaveId/req</code> (or <code>res</code>). Very dependent on your system and goals.</p>",3960558,,,2019-05-31T19:58:44.883,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56401067
694,56411234,2,56405090,2019-06-01T23:52:37.257,3,"<p>You can point to an Amazon S3 bucket using the <code>s3.amazonaws.com/bucketname</code> method, however:</p><ul><li>This does not enable the <strong>Static Website Hosting</strong> features (such as using default <code>index</code> and <code>error</code> pages)</li><li><strong>It is being deprecated</strong>. See: <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/"" rel=""nofollow noreferrer"">Amazon S3 Path Deprecation Plan Š—– The Rest of the Story | AWS News Blog</a></li></ul>",174777,,,2019-06-01T23:52:37.257,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56411234
695,56420710,2,49918991,2019-06-03T03:11:13.870,0,"<p>Note: so far as I can tell, adding 'replica' servers does nothing unless you are using the db_* functional calls (which are deprecated), or if you manually instantiate the database.replica connection in any of your custom queriese.g. <code>/** @var \Drupal\Core\Database\Connection $database_replica */$database_replica = \Drupal::service('database.replica');$query = $database_replica-&gt;select('node', 'n');....</code></p>",11591707,,,2019-06-03T03:11:13.870,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56420710
696,56511181,2,56509693,2019-06-09T00:50:27.133,3,"<p><a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html"" rel=""nofollow noreferrer"">CloudFormer</a> has <a href=""https://forums.aws.amazon.com/ann.jspa?annID=1048"" rel=""nofollow noreferrer"">been in beta since 2011</a>. It does not appear to have been maintained much lately, so it might be deprecated in future.</p><p>So, it looks like you'll need to add the User Data section manually.</p>",174777,,,2019-06-09T00:50:27.133,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56511181
697,56511427,2,56510763,2019-06-09T01:59:02.603,3,"<p>As far as I know, this is legacy functionality, no longer serving any useful purpose.  Many years ago, X.509 certificates were used to sign requests for AWS SOAP APIs.  </p><p>All of these are likely deprecated if not gone entirely.</p><p>In some cases -- like EC2 -- the SOAP functionality was deprecated and later <a href=""https://docs.aws.amazon.com/AWSEC2/latest/APIReference/using-soap-api.html"" rel=""nofollow noreferrer"">completely removed from the API and SDKs in late 2015</a>.</p><p>So this IAM feature would only be used for legacy systems using other AWS services that still support the old SOAP APIs that expect X.509-based authentication, assuming there are any.  If there are, the documentation has long since been deleted or is buried, as is the case for <a href=""https://docs.aws.amazon.com/AmazonS3/latest/API/APISoap.html"" rel=""nofollow noreferrer"">S3</a>, which only supports SOAP over HTTPS these days, and appears to expect Access Keys rather than certificates.  </p><p>Perhaps SimpleDB still supports the old SOAP API.  SimpleDB?  One of the original Amazon Web Services like SQS and S3, SimpleDB was -- sort of, kind of, in a manner of speaking, loosely -- a predecessor to DynamoDB... it was quite an innovation back in its day, and it is still alive though perhaps not well, and you will be hard-pressed to find anyone talk about it, or find much evidence of it in the AWS documentation, though it's there <a href=""https://aws.amazon.com/simpledb/"" rel=""nofollow noreferrer"">if you know where to look</a>.  SimpleDB hasn't been deployed in any AWS region that launched after about 2013.</p>",1695906,,,2019-06-09T01:59:02.603,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56511427
698,56703700,2,56698407,2019-06-21T12:42:51.123,0,"<p><code>AWSIotDataClient</code> is not deprecated, just the constructors are deprecated, as are the constructors of all <code>AWSClient</code> implementations in favor of builders. You should use <code>AwsClientBuilder.build()</code> to obtain an instance of <code>AWSIotDataClient</code>. Then you can call the <code>publish()</code> method on the <code>AWSIotDataClient</code> instance to publish to your IoT topic.</p>",13070,,,2019-06-21T12:42:51.123,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56703700
699,56744352,2,56742001,2019-06-24T21:48:27.977,1,"<p>If you drop the tables you will lose assigned permissions to these tables. If you have views for these tables they will be obsolete.</p><p>Truncate is a better option, truncate does not require vacuum or analyze, it is built for use cases like this. </p><p>For further info <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwi8kpTVi4PjAhVRG80KHYgnBWIQFjABegQIChAE&amp;url=https%3A%2F%2Fdocs.aws.amazon.com%2Fredshift%2Flatest%2Fdg%2Fr_TRUNCATE.html&amp;usg=AOvVaw0y8mi4pI66UU9tNhAw1xr1"" rel=""nofollow noreferrer"">Redshift Truncate documentation</a></p>",1628812,,,2019-06-24T21:48:27.977,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56744352
700,56745301,2,54011315,2019-06-25T00:10:29.290,0,"<h1>Update Your Runtime</h1><p>The <code>transactWrite</code> function was added in a later version of the AWS SDK. To access the updated AWS SDK, switch your Lambda function to a newer runtime.</p><p>AWS does not update the AWS SDK version for old runtimes. For example, if you are using the outdated <code>nodejs8.10</code> runtime, you only have access to version <code>2.290.0</code> of the AWS SDK which does not support <code>transactWrite</code>. If you switch your runtime to <code>nodejs10.x</code>, you get version <code>2.437.0</code> (at the time of this post) which does support DynamoDB transactions.</p><p>View the complete list of SDK versions available in each runtime here:<a href=""https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html"" rel=""nofollow noreferrer"">AWS Lambda Runtimes</a></p><h2>How To Update Your Runtime</h2><p>To update your runtime, click the ""Runtime"" dropdown in the ""Function Code"" section of your Lambda function and select an updated runtime.</p><p><a href=""https://i.stack.imgur.com/ZS8sA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZS8sA.png"" alt=""Update Lambda runtime""></a></p>",3305145,,,2019-06-25T00:10:29.290,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56745301
701,56820259,2,56813475,2019-06-29T19:19:28.850,2,"<p>I've successfully completed compiling the AWS C++ SDK on a stock install of Solaris 11.4, and found several issues that could cause the problems noted.</p><p>Start with a clean source tree.</p><p><strong>Remove <code>-Werror</code></strong></p><p>The first thing do to is remove the <code>-Werror</code> compiler options.  The version of OpenSSL installed by default on Solaris 11.4 has quite a few deprecated functions, and the <code>-Werror</code> option causes the build to fail when it runs into those deprecations.  I used this <code>find</code> command run from the topmost directory of the AWS SDK source tree to remove all the <code>-Werror</code> options:</p><pre><code>vi `find . | xargs grep -l Werror`</code></pre><p>You'll get about three or four files, only two of which are actually setting the <code>-Werror</code> as a compiler option.  Just remove the <code>""-Werror""</code> strings from those files.</p><p><strong>Fix the POSIX defines</strong></p><p>Then run <code>cmake .</code> in the topmost directory.  It will fail because the cmake files that it downloads will have improper POSIX command-line options - <code>-D_POSIX_C_SOURCE=200809L -D_XOPEN_SOURCE=500</code>.  That <code>500</code> is wrong.  <code>_POSIX_C_SOURCE=200809L</code> corresponds to <code>_XOPEN_SOURCE=700</code>.  <code>_XOPEN_SOURCE=500</code> is <a href=""https://en.wikipedia.org/wiki/Single_UNIX_Specification#1997:_Single_UNIX_Specification_version_2"" rel=""nofollow noreferrer"">SUSv2, circa 1997</a>.  It's not proper to compile a SUSv2 application with C99.</p><p>Per <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap02.html#tag_02_02_01"" rel=""nofollow noreferrer""><strong>2.2.1 Strictly Conforming POSIX Application</strong>, paragraph 8</a>:</p><blockquote>  <ol start=""8"">  <li>For the C programming language, shall define <code>_POSIX_C_SOURCE</code> to be <code>200809L</code> before any header is included</li>  </ol></blockquote><p>and <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap02.html#tag_02_02_04"" rel=""nofollow noreferrer""><strong>2.2.4 Strictly Conforming XSI Application</strong>, paragraph 8</a>:</p><blockquote>  <ol start=""8"">  <li>For the C programming language, shall define <code>_XOPEN_SOURCE</code> to be <code>700</code> before any header is included</li>  </ol></blockquote><p>Per <a href=""https://github.com/illumos/illumos-gate/blob/4e0c5eff9af325c80994e9527b7cb8b3a1ffd1d4/usr/src/uts/common/sys/feature_tests.h#L264"" rel=""nofollow noreferrer"">the Illumos <code>sys/feature_tests.h</code> file</a> (based on OpenSolaris, which was also the basis for Solaris 11):</p><pre><code> * Feature Test Macro  Specification * ------------------------------------------------  ------------- * _XOPEN_SOURCE   XPG3 * _XOPEN_SOURCE &amp;&amp; _XOPEN_VERSION = 4   XPG4 * _XOPEN_SOURCE &amp;&amp; _XOPEN_SOURCE_EXTENDED = 1   XPG4v2 * _XOPEN_SOURCE = 500   XPG5 * _XOPEN_SOURCE = 600  (or POSIX_C_SOURCE=200112L)  XPG6 * _XOPEN_SOURCE = 700  (or POSIX_C_SOURCE=200809L)  XPG7</code></pre><p>The files <code>cmake</code> downloads via <code>git</code> need to be edited:</p><pre><code>vi `find .deps | xargs grep -l XOPEN_SOURCE`</code></pre><p>Change any <code>-D_XOPEN_SOURCE=500</code> to <code>-D_XOPEN_SOURCE=700</code> and rerun <code>cmake .</code>.  It should complete successfully this time.</p><p>Then run <code>gmake</code>. (I find <code>gmake</code> works much better on Solaris for just about all open source projects, as many open source projects use GNU-specific <code>make</code> extensions.)</p><p>Now you get to fix any broken source code you run into.</p><p><strong>Fix broken source code</strong></p><p><strong>1</strong> </p><p>The <a href=""https://github.com/aws/aws-sdk-cpp/blob/797a5c5890e0c1772ac8de02cc6116d832dd7806/aws-cpp-sdk-core/source/platform/linux-shared/OSVersionInfo.cpp#L54"" rel=""nofollow noreferrer"">file <code>aws-sdk-cpp/aws-cpp-sdk-core/source/platform/linux-shared/OSVersionInfo.cpp</code></a> has the following wrong code:</p><pre><code>Aws::String ComputeOSVersionString(){  utsname name;  int32_t success = uname(&amp;name);</code></pre><p><a href=""http://pubs.opengroup.org/onlinepubs/9699919799/functions/uname.html"" rel=""nofollow noreferrer"">Per POSIX</a>, the correct type is <code>struct utsname</code>, not just <code>utsname</code>:</p><pre><code> int uname(struct utsname *name);</code></pre><p>The AWS code needs to be:</p><pre><code>Aws::String ComputeOSVersionString(){  struct utsname name;  int success = uname(&amp;name);</code></pre><p>And no, I'm most certainly not impressed with the quality of the AWS code, given <a href=""https://github.com/aws/aws-sdk-cpp/blob/797a5c5890e0c1772ac8de02cc6116d832dd7806/aws-cpp-sdk-core/source/platform/linux-shared/OSVersionInfo.cpp#L37"" rel=""nofollow noreferrer"">this, umm, laugher</a>:</p><pre><code> while (!feof(outputStream))</code></pre><p>Yes, an actual <code>while (!feof())</code> loop...</p><p><strong>2</strong> </p><p>The file <a href=""https://github.com/aws/aws-sdk-cpp/blob/797a5c5890e0c1772ac8de02cc6116d832dd7806/aws-cpp-sdk-mediaconvert/include/aws/mediaconvert/model/M2tsSegmentationMarkers.h#L33"" rel=""nofollow noreferrer"">aws-sdk-cpp/aws-cpp-sdk-mediaconvert/include/aws/mediaconvert/model/M2tsSegmentationMarkers.h</a> uses an enumeration with the value <code>EBP</code>, which conflicts with <a href=""https://github.com/illumos/illumos-gate/blob/4e0c5eff9af325c80994e9527b7cb8b3a1ffd1d4/usr/src/uts/intel/sys/regset.h#L108"" rel=""nofollow noreferrer"">the <code>EBP</code> register <code>#define</code> in <code>/usr/include/sys/regset.h</code></a>.</p><p>I just changed it to <code>EBP_HASH</code> as that seems to match the code somewhat:</p><pre><code>vi `find . | xargs grep -l EBP`</code></pre><p><strong>3</strong></p><p>The file <a href=""https://github.com/aws/aws-sdk-cpp/blob/797a5c5890e0c1772ac8de02cc6116d832dd7806/aws-cpp-sdk-route53domains/include/aws/route53domains/model/CountryCode.h#L92"" rel=""nofollow noreferrer"">aws-sdk-cpp/aws-cpp-sdk-route53domains/include/aws/route53domains/model/CountryCode.h</a> creates an enumeration value <code>ES</code> that conflicts with <a href=""https://github.com/illumos/illumos-gate/blob/4e0c5eff9af325c80994e9527b7cb8b3a1ffd1d4/usr/src/uts/intel/sys/regset.h#L112"" rel=""nofollow noreferrer"">the <code>ES</code> register <code>#define</code> in <code>/usr/include/sys/regset.h</code></a>.  I just added</p><pre><code>#ifdef ES#undef ES#endif </code></pre><p>and the compile continued.  I don't know if that <code>#undef</code> could have broken anything.</p><p><strong>4</strong></p><p>The file <a href=""https://github.com/aws/aws-sdk-cpp/blob/797a5c5890e0c1772ac8de02cc6116d832dd7806/aws-cpp-sdk-waf/include/aws/waf/model/GeoMatchConstraintValue.h#L235"" rel=""nofollow noreferrer"">aws-sdk-cpp/aws-cpp-sdk-waf/include/aws/waf/model/GeoMatchConstraintValue.h</a> has <code>ES</code>, <code>GS</code>, and <code>SS</code> enumeration value that conflict with <a href=""https://github.com/illumos/illumos-gate/blob/4e0c5eff9af325c80994e9527b7cb8b3a1ffd1d4/usr/src/uts/intel/sys/regset.h#L114"" rel=""nofollow noreferrer"">the <code>ES</code>, <code>GS</code>, and <code>SS</code> register <code>#define</code>'s in <code>/usr/include/sys/regset.h</code></a>.</p><p>Again, I just added a few more <code>#undef</code>'s:</p><pre><code>#ifdef ES#undef ES#endif#ifdef GS#undef GS#endif#ifdef SS#undef SS#endif</code></pre><p>I'm really wondering why <code>sys/regset.h</code> is being <code>#include</code>'d in just about everything in the AWS SDK.</p><p><strong>5</strong></p><p>Same problem in <a href=""https://github.com/aws/aws-sdk-cpp/blob/797a5c5890e0c1772ac8de02cc6116d832dd7806/aws-cpp-sdk-waf-regional/include/aws/waf-regional/model/GeoMatchConstraintValue.h#L235"" rel=""nofollow noreferrer"">aws-sdk-cpp/aws-cpp-sdk-waf-regional/include/aws/waf-regional/model/GeoMatchConstraintValue.h</a>.  Same fix, add:</p><pre><code>#ifdef ES#undef ES#endif#ifdef GS#undef GS#endif#ifdef SS#undef SS#endif</code></pre><p>Note that compiling on SPARC hardware means the <code>#define</code> value from <code>sys/regset.h</code> will be completely different, and any errors will be completely different.</p><p><strong>6</strong></p><p>The file <a href=""https://github.com/aws/aws-sdk-cpp/blob/797a5c5890e0c1772ac8de02cc6116d832dd7806/aws-cpp-sdk-core-tests/utils/FileSystemUtilsTest.cpp#L284"" rel=""nofollow noreferrer"">aws-sdk-cpp/aws-cpp-sdk-core-tests/utils/FileSystemUtilsTest.cpp</a> incorrectly assumes the POSIX <code>NAME_MAX</code> value is defined.  Per <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/limits.h.html#tag_13_23_03_02"" rel=""nofollow noreferrer"">the POSIX <strong>Pathname Variable Values</strong> standard</a> (bolding mine):</p><blockquote>  <p><strong>Pathname Variable Values</strong></p>   <p>The values in the following list may be constants within an  implementation or may vary from one pathname to another. For example,  file systems or directories may have different characteristics.</p>   <p>A definition of one of the symbolic constants in the following list  <strong>shall be omitted</strong> from the <code>&lt;limits.h&gt;</code> header on specific  implementations where the corresponding value is equal to or greater  than the stated minimum, but where the value can vary depending on the  file to which it is applied. The actual value supported for a specific  pathname shall be provided by the <code>pathconf()</code> function.</p></blockquote><p>Again:  the ""definition ... <strong>shall be omitted</strong> ... where the value can vary"".</p><p>The AWS code wrongly assumes <code>NAME_MAX</code> <strong>must</strong> be <code>#define</code>'d.</p><p>I just hardcoded a value of <code>255</code> to get past this point, although using something like <code>_POSIX_NAME_MAX</code> or <code>_XOPEN_NAME_MAX</code> is probably better.</p><p><strong>7</strong></p><p>File <a href=""https://github.com/aws/aws-sdk-cpp/blob/797a5c5890e0c1772ac8de02cc6116d832dd7806/aws-cpp-sdk-core-tests/http/HttpClientTest.cpp#L31"" rel=""nofollow noreferrer"">aws-sdk-cpp/ws-cpp-sdk-core-tests/http/HttpClientTest.cpp</a> seems to be incorrectly assuming a <code>std::shared_ptr</code> will be 8 bytes.  <a href=""https://stackoverflow.com/questions/29082958/what-is-the-size-of-an-auto-ptr"">This question and answer</a> provides a good example of how that's wrong. </p><p>I just ignored this error as it's just a test and continued with <code>gmake -i</code>, which completed successfully outside of this one error.</p>",4756299,4756299,2019-07-03T12:26:14.753,2019-07-03T12:26:14.753,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56820259
702,56826677,2,56822716,2019-06-30T17:17:14.547,0,"<p>IŠ—Èm unable replicate the issue from my side. Maybe your lambda container is out of date. Have you tried forcing a new lambda container to start, or even deleting the whole lambda and starting again might work? See <a href=""https://stackoverflow.com/questions/47445815/force-discard-aws-lambda-container"">Force Discard AWS Lambda Container</a>. I tried something like this and it worked for me (IŠ—Èm using Python 3.7):</p><pre><code>import jsondef lambda_handler(event, context):  print(event)  # print(""got event: "" + json.dumps(event))  get_event(event, context)  return {  'statusCode': 200,  'body': json.dumps('Hello from Lambda!')  }def get_event(event, context):  print(event)</code></pre>",5009139,5009139,2019-06-30T17:45:51.780,2019-06-30T17:45:51.780,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56826677
703,56830225,2,56829651,2019-07-01T05:06:45.940,1,"<p>You donŠ—Èt need UUIDs or any pseudo-random ID. </p><p>It was once possible that you could have a hot partition if one user is particularly active, but hot partitions are <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">basically a non-issue</a> now because of DynamoDBŠ—Ès adaptive capacity. Furthermore, you should probably be limiting how fast users can create comments/posts, which would prevent hot partitions even if adaptive capacity didnŠ—Èt exist.</p><p>(Why should you limit the rate a user can post? You donŠ—Èt want a malicious actor to be able to create a new post every few millisecondsŠ—”you should have some sort of rate limit as a protection against denial of service attacks.)</p>",5563569,,,2019-07-01T05:06:45.940,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56830225
704,56837581,2,54410850,2019-07-01T14:34:09.787,1,"<p>You DON'T NEED Username and Password in your Python Script to authenticate!What you need is <strong>CLIENT_ID, SCOPE</strong> and <strong>REDIRECT_URI</strong> and three requests:</p><ol><li><p>Get authorization code:</p><p><code>GET https://www.amazon.com/ap/oa?client_id={{CLIENT_ID}}&amp;scope={{SCOPE}}&amp;response_type=code&amp;redirect_uri={{REDIRECT_URI}}</code></p></li></ol><p>This will open the 'Login with Amazon' Consent Page, where you (or your customer) log into your Amazon Seller Central account and grant access to the Console APP with API access rights.</p><ol start=""2""><li><p>Request tokens</p><p><code>POST https://api.amazon.com/auth/o2/token</code></p><p>with headers:</p><p><code>Content-Type:application/x-www-form-urlencoded</code></p><p>with body data:</p><pre><code>grant_type:authorization_codecode:{{AUTH_CODE}}  &lt;----- returned from step 1client_id:{{CLIENT_ID}}client_secret:{{CLIENT_SECRET}}redirect_uri:{{REDIRECT_URI}}</code></pre></li><li><p>Get/Refresh <strong>access token</strong> (every time it is outdated):</p><p><code>POST https://api.amazon.com/auth/o2/token</code></p><p>with headers:</p><pre><code>Content-Type:application/x-www-form-urlencodedcharset:UTF-8</code></pre><p>with body data:</p><pre><code>grant_type:refresh_tokenrefresh_token:{{REFRESH_TOKEN}}   &lt;------ returned from step 2client_id:{{CLIENT_ID}}client_secret:{{CLIENT_SECRET}}</code></pre></li></ol><hr><ol start=""4""><li><p>With the CLIENT_ID and (fresh) access token you can now request every service from the API. For excample listCampaigns:</p><p><code>GET https://advertising-api.amazon.com/v2/sp/campaigns</code></p><p>Headers:</p><pre><code>Content-Type:application/jsonAmazon-Advertising-API-ClientId:{{CLIENT_ID}}Amazon-Advertising-API-Scope:{{PROFILE_ID}}Authorization:Bearer {{ACCESS_TOKEN}}   &lt;----- returned from step 3</code></pre></li></ol>",850547,,,2019-07-01T14:34:09.787,5,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56837581
705,56837783,2,56796437,2019-07-01T14:49:29.670,0,"<p>The Amazon Advertising API v2 was released late 2018, so it's pretty new but has some missing information in the official documentation. But to get started please follow the official documentation guide:</p><p><a href=""https://advertising.amazon.com/API/docs/v2/guides/get_started"" rel=""nofollow noreferrer"">https://advertising.amazon.com/API/docs/v2/guides/get_started</a></p><p>A lot of other sources are outdated, so I suggest to stick to this guide.</p><p>The logic to handle multiple Amazon Seller Accounts you have to handle yourself.But every Seller Account has to go through the consent workflow to give your console app the rights to get access to the shop data via API. Therefore you need to implement a <strong>'Login with Amazon'</strong> button - That's the part that is not mentioned in the documentation. See <a href=""https://login.amazon.com/"" rel=""nofollow noreferrer"">https://login.amazon.com/</a></p>",850547,8677101,2019-07-01T18:33:03.987,2019-07-01T18:33:03.987,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56837783
706,56840962,2,20433238,2019-07-01T18:50:52.817,12,"<p>Several of the paths in the answers to this question are out of date as of July 2019.  There is no longer any C:\Program Files\Amazon\Ec2ConfigService nor C:\CFN (for me atleast but I'm not using CFN to provision if that matters)</p><p>At first I thought the C:\ProgramData\Amazon\ location was also out of date but I forgot that ProgramData is <strong>hidden</strong> on windows and by default hidden files are not shown.  Remember how every time you install windows you have to set the selection to ""show hidden files""?  I forgot about that here.</p><p>So the windows user data logs <strong>are</strong> in <code>C:\ProgramData\Amazon\EC2-Windows\Launch\Log\UserdataExecution.log</code></p><p>Also if it helps, the Userdata script itself (post-any-provisioning-templating, as presented to the instance) is in <code>C:\Windows\Temp\UserScript.ps1</code> </p><p>But I'd like to second tarvinder91's recommendation of using the powershell function ""Start-Transcript"" to trivially create your own logs.You can set a custom path and -Append like <code>Start-Transcript -Path ""C:\UserData.log"" -Append</code> at the beginning of your script.  This way you can control where the log goes and not worry about how the AMI is configured to store logs.</p>",1169420,1169420,2019-10-11T15:39:54.563,2019-10-11T15:39:54.563,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56840962
707,56913209,2,37365009,2019-07-06T10:25:53.253,0,"<p>Approach given in the accepted answer is now deprecated. Answer given by the user @dassum is the approach to be followed, but the answer lacks a bit of explanation.</p><p>When creating the InvokeRequest, set the InvocationType as ""Event"" for asynchronous invocation and ""RequestResponse"" for synchronous invocation. </p><pre><code>AWSLambda lambda = //create your lambda client herelambda.invoke(new InvokeRequest()  .withFunctionName(LAMBDA_FUNCTION_NAME)  .withInvocationType(InvocationType.Event) //asynchronous  .withPayload(payload))</code></pre><p>Reference to docs:</p><p><a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/lambda/AWSLambda.html#invoke-com.amazonaws.services.lambda.model.InvokeRequest-"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/lambda/AWSLambda.html#invoke-com.amazonaws.services.lambda.model.InvokeRequest-</a></p>",1800900,,,2019-07-06T10:25:53.253,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56913209
708,56945609,2,54671944,2019-07-09T04:59:48.273,2,"<p>If you can use the Pyarrow library, load the parquet tables and then <a href=""https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html"" rel=""nofollow noreferrer"">write them back out in Parquet format</a> using the <code>use_deprecated_int96_timestamps</code> parameter. Redshift will correctly recognize those. I haven't had any luck getting it to properly recognize any other timestamp formats when loading Parquet.</p>",646628,,,2019-07-09T04:59:48.273,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/56945609
709,57001791,2,57000602,2019-07-12T07:06:48.000,0,"<p>Use <code>S3TransferUtility</code> instead of the deprecated TransferManager. Sample code is available here <a href=""https://github.com/awslabs/aws-sdk-ios-samples/tree/master/S3TransferUtility-Sample/Swift"" rel=""nofollow noreferrer"">https://github.com/awslabs/aws-sdk-ios-samples/tree/master/S3TransferUtility-Sample/Swift</a> </p>",663360,,,2019-07-12T07:06:48.000,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57001791
710,57062004,2,53631825,2019-07-16T16:45:41.483,2,"<p>As of 5/2019 the answer to this question has changed.  I'd like to preface my answer by saying I have not validated this against a production workload.  Also my answer assumes you, like the OP, are using <a href=""https://aws.amazon.com/dynamodb/pricing/on-demand/"" rel=""nofollow noreferrer"">on-demand pricing</a>.</p><p>First a general understanding of how DynamoDB (DDB) adaptive capacity works can be gleamed by reading <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">this</a>. Adaptive capacity is <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html#bp-partition-key-partitions-adaptive"" rel=""nofollow noreferrer"">on by default</a>.  In short, when a hot partition exceeds its throughput capacity, DDB ""moves the rudder"" and instantly increases throughput capacity on the partition.</p><p>Before 5/2019 you'd 300 seconds of instant <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html#bp-partition-key-throughput-bursting"" rel=""nofollow noreferrer"">burst capacity</a>, then you'd be throttled until adaptive capacity fully <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">""kicked in"" (5-30 minutes)</a>.</p><p>On 5/23/2019 AWS announced that adaptive capacity is <a href=""https://aws.amazon.com/about-aws/whats-new/2019/05/amazon-dynamodb-adaptive-capacity-is-now-instant/"" rel=""nofollow noreferrer"">now instant</a>.  This means no more 5-30 minute wait.</p><p>Does this mean if you use DDB on-demand pricing, your hot partition problems go away? Yes and no.</p><p>Yes, in that you should not get throttled.</p><p>No, in that your bank account will take the hit.  In order to not get throttled, DDB will scale up on-demand (now instantly).  You will pay for the RCUs and WCUs needed to handle the throughput. </p><p>So the moral is, you still have to think about hot partitions and design your application to avoid them as much as possible.  You won't be paying for it in downtime/unhappy customers, you'll be paying for it out of profits.</p>",563420,,,2019-07-16T16:45:41.483,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57062004
711,57076984,2,57076595,2019-07-17T13:25:45.673,6,"<p>Your Lambda function is <code>async</code> but your code uses callbacks. By the time the callback is reached, your function has already been terminated because it ran asychronously.</p><p>Rather than using the outdated, confusing callback approach, just stick with <a href=""https://javascript.info/async-await"" rel=""noreferrer"">async/await</a>. The AWS SDK for Node.js already provides a <code>.promise()</code> method which you can call on asynchronous methods that use a callback, making it seamless to <code>await</code> on them.</p><p>That said, change your code to:</p><pre><code>const dynamodb = new AWS.DynamoDB.DocumentClient({apiVersion: '2012-08-10', region: 'eu-west-1'});exports.handler = async (event) =&gt; {  const params = {  TableName : ""DontMissAPlaceTable"",  KeyConditionExpression: ""Partition_Key = :id"",  ExpressionAttributeValues: {  "":id"": ""media_001""  }  };  return {  statusCode: 200,  body: JSON.stringify(await dynamodb.query(params).promise())  }};</code></pre><p>Since you are already using the DocumentClient API, you don't need to specify the types (""S"", ""N"", etc) when querying.</p><p>Also, make sure that <code>Partition_Key</code> really is <em>your</em> partition key. You use the name you defined in your table, no <code>HashKey</code> nor <code>PartitionKey</code>, meaning if you called your Hash Key <code>id</code>, then <code>id</code> should be used.</p>",10950867,10950867,2019-07-17T13:32:07.567,2019-07-17T13:32:07.567,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57076984
712,57100724,2,57100171,2019-07-18T18:33:34.780,0,"<p>From PHP documentation <code>mysql</code> class is deprecated since version 5.5 and removed in version 7. that's why you're receiving the class not found error.</p><p>Try using <code>mysqli</code>. The PHP documentation has good examples on this. Look at example 2. I've pasted part of the example in this post for you.<a href=""https://www.php.net/manual/en/function.mysql-connect.php"" rel=""nofollow noreferrer"">https://www.php.net/manual/en/function.mysql-connect.php</a></p><pre><code>&lt;?php$mysqli = new mysqli(""example.com"", ""user"", ""password"", ""database"");if ($mysqli-&gt;connect_errno) {  echo ""Failed to connect to MySQL: "" . $mysqli-&gt;connect_error;}$res = $mysqli-&gt;query(""SELECT 'choices to please everybody.' AS _msg FROM DUAL"");$row = $res-&gt;fetch_assoc();echo $row['_msg'];?&gt;</code></pre>",864538,,,2019-07-18T18:33:34.780,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57100724
713,57136371,2,42892451,2019-07-21T19:37:46.667,1,"<p>I had the same issue. And the problem was that my ssh client was outdated.</p><p>You can solve by downloading the latest binaries: <a href=""https://github.com/PowerShell/Win32-OpenSSH/releases"" rel=""nofollow noreferrer"">OpenSSH Binaries Windows</a></p><p>Just extract the zip file and add the folder into your environment variables and you won't see that problem anymore :)</p>",4051258,,,2019-07-21T19:37:46.667,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57136371
714,57271489,2,56966682,2019-07-30T12:17:20.970,0,"<p>I worked around this issue by changing logical ids of lambdas that I was updating. This way, CloudFormation deletes old lambdas and creates brand new lambdas with new runtime. Unfortunately, this seems to be the only way for now as updates to any lambda pointing to 2.0 runtime is deprecated (event if the update is upgrading the runtime).</p><p>Not an ideal way to do it, but does the job.  </p>",2717391,,,2019-07-30T12:17:20.970,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57271489
715,57287826,2,39603852,2019-07-31T09:44:41.993,0,"<pre><code>import loggingimport boto3from botocore.exceptions import ClientErrorfrom botocore.client import Config# python &gt; 3 should be installed# pip install boto3# s3v4# (Default) Signature Version 4# v4 algorithm starts with X-Amz-Algorithm## s3# (Deprecated) Signature Version 2,  this only works in some regions new regions not supported# if you have to generate signed url that has &gt; 7 days expiry then use version 2 if your region supports it. below code illustration of thiss3_signature ={  'v4':'s3v4',  'v2':'s3'}def create_presigned_url(bucket_name, bucket_key, expiration=3600):  """"""Generate a presigned URL to share an S3 object  :param bucket_name: string  :param bucket_key: string  :param expiration: Time in seconds for the presigned URL to remain valid  :return: Presigned URL as string. If error, returns None.  """"""  # Generate a presigned URL for the S3 object  s3_client = boto3.client('s3',  aws_access_key_id='your_access_key_here',  aws_secret_access_key='your_secret_key_here',  config=Config(signature_version=s3_signature['v2']),  region_name='us-east-1'  )  try:  response = s3_client.generate_presigned_url('get_object',  Params={'Bucket': bucket_name,  'Key': bucket_key},  ExpiresIn=expiration)  except ClientError as e:  logging.error(e)  return None  # The response contains the presigned URL  return responseweeks = 8seven_days_as_seconds = 604800signed_url = create_presigned_url('your_bucket_here', 'your_key/file_name.xls', (seven_days_as_seconds*weeks))print(signed_url)</code></pre>",4509876,,,2019-07-31T09:44:41.993,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57287826
716,57341974,2,57281604,2019-08-03T20:46:10.050,0,"<p>Okay, here's what I've learned.</p><p>As of version 1.18.0, the call to <code>getCredentials</code> does <em>NOT</em> consider id token expiration.  It <em>only</em> checks if the access token is expired, and if it is, it will then refresh the <code>id_token</code> and <code>access token</code>.  Unfortunately the <code>access token</code> expiry is locked in at 24 hours unless you do additional work.</p><p>Make sure you have <code>setOIDCCompliant</code> to <code>true</code> when you create your Auth0Account instance, or else the call to renew will hit the <code>/delegation</code> endpoint which is now deprecated and only works if your client id is setup to support non oidc compliant calls.</p><p>One other thing to be aware of that's somewhat off topic.  The <code>SecureCredentialsManager</code> clears out credentials if anything appears to go wrong.  This isn't acceptable for me, as simply being offline and being unable to make the call causes the credentials to be cleared.</p>",2080104,,,2019-08-03T20:46:10.050,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57341974
717,57367607,2,57359926,2019-08-06T00:51:56.710,0,"<p>Here's a script that can find instances using AMIs where the Owner is not <code>amazon</code>:</p><pre class=""lang-py prettyprint-override""><code>import boto3ec2_client = boto3.client('ec2', region_name='ap-southeast-2')instances = ec2_client.describe_instances()# Get a set of AMIs used on all the instancesimages = set(i['ImageId'] for r in instances['Reservations'] for i in r['Instances'])# Find which of these are owned by Amazonamis = ec2_client.describe_images(ImageIds=list(images), Owners=['amazon'])amazon_amis = [i['ImageId'] for i in amis['Images']]# Which instances are not using Amazon images?non_amazon_instances = [(i['InstanceId'], i['ImageId']) for r in instances['Reservations'] for i in r['Instances'] if i['ImageId'] not in amazon_amis]for i in non_amazon_instances:  print(f""{i[0]} uses {i[1]}"")</code></pre><p>A few things to note:</p><ul><li>Deprecated AMIs might not have accessible information, so might be marked a non-Amazon.</li><li>This script, as written, only works on one region. You could change it to loop through regions.</li><li>This script, as written, only works on one account. You would need a way to loop through credentials for other accounts.</li></ul>",174777,,,2019-08-06T00:51:56.710,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57367607
718,57385215,2,57380939,2019-08-06T23:32:54.420,2,"<p>Bucket names are in the format:</p><pre><code>http://BUCKETNAME.s3-REGION.amazonaws.com</code></pre><p>Examples:</p><pre><code>http://mybucket.s3-ap-southeast-2.amazonaws.comhttp://my.bucket.name.has.dots.s3-us-west-2.amazonaws.comhttp://invoices.s3.amazonaws.com   (Defaults to us-east-1, can involve redirects to other regions)</code></pre><p>Note the period after the bucket name and the dash in the <code>s3-REGION</code> portion.</p><p>There is also an older format:</p><pre><code>http://s3-ap-southeast-2.amazonaws.com/mybucket</code></pre><p>However, that format is being deprecated. See: <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/"" rel=""nofollow noreferrer"">Amazon S3 Path Deprecation Plan Š—– The Rest of the Story | AWS News Blog</a></p>",174777,,,2019-08-06T23:32:54.420,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57385215
719,57434304,2,57343678,2019-08-09T16:51:34.660,1,"<h2><strong>Flushing Buffers to Netty</strong></h2><p>In the picture above, the credit-based flow control mechanics actually sit inside the Š—“Netty ServerŠ— (and Š—“Netty ClientŠ—) components and the buffer the RecordWriter is writing to is always added to the result subpartition in an empty state and then gradually filled with (serialised) records. But when does Netty actually get the buffer? Obviously, it cannot take bytes whenever they become available since that would not only add substantial costs due to cross-thread communication and synchronisation, but also make the whole buffering obsolete.</p><p>In Flink, there are three situations that make a buffer available for consumption by the Netty server:</p><p>a buffer becomes full when writing a record to it, orthe buffer timeout hits, ora special event such as a checkpoint barrier is sent.</p><h2><strong>Flush after Buffer Full</strong></h2><p>The RecordWriter works with a local serialisation buffer for the current record and will gradually write these bytes to one or more network buffers sitting at the appropriate result subpartition queue. Although a RecordWriter can work on multiple subpartitions, each subpartition has only one RecordWriter writing data to it. The Netty server, on the other hand, is reading from multiple result subpartitions and multiplexing the appropriate ones into a single channel as described above. This is a classical producer-consumer pattern with the network buffers in the middle and as shown by the next picture. After (1) serialising and (2) writing data to the buffer, the RecordWriter updates the bufferŠ—Ès writer index accordingly. Once the buffer is completely filled, the record writer will (3) acquire a new buffer from its local buffer pool for any remaining bytes of the current record - or for the next one - and add the new one to the subpartition queue. This will (4) notify the Netty server of data being available if it is not aware yet4. Whenever Netty has capacity to handle this notification, it will (5) take the buffer and send it along the appropriate TCP channel. </p><p><a href=""https://i.stack.imgur.com/pruWU.png"" rel=""nofollow noreferrer"">Image 1</a></p><p>We can assume it already got the notification if there are more finished buffers in the queue. </p><h2><strong>Flush after Buffer Timeout</strong></h2><p>In order to support low-latency use cases, we cannot only rely on buffers being full in order to send data downstream. There may be cases where a certain communication channel does not have too many records flowing through and unnecessarily increase the latency of the few records you actually have. Therefore, a periodic process will flush whatever data is available down the stack: the output flusher. The periodic interval can be configured via StreamExecutionEnvironment#setBufferTimeout and acts as an upper bound on the latency5 (for low-throughput channels). The following picture shows how it interacts with the other components: the RecordWriter serialises and writes into network buffers as before but concurrently, the output flusher may (3,4) notify the Netty server of data being available if Netty is not already aware (similar to the Š—“buffer fullŠ— scenario above). When Netty handles this notification (5) it will consume the available data from the buffer and update the bufferŠ—Ès reader index. The buffer stays in the queue - any further operation on this buffer from the Netty server side will continue reading from the reader index next time. </p><p><a href=""https://i.stack.imgur.com/1nJwr.png"" rel=""nofollow noreferrer"">Image 2</a></p><p><strong>Reference:</strong></p><p>Below link may help you out.</p><p><a href=""https://flink.apache.org/2019/06/05/flink-network-stack.html"" rel=""nofollow noreferrer"">flink-network-stack-details</a></p>",11824552,11824552,2019-09-03T10:48:13.013,2019-09-03T10:48:13.013,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57434304
720,57457807,2,54566268,2019-08-12T08:25:07.270,0,"<p>It started happening for me, when my AWS password has expired and i had reset it. Locally git started giving me a 403 error because of outdated credentials. I was trying to connect to codecommit with my normal aws account login credentials , which again gave me the same 403 error. Finally i realized that i need to generate new git credentials through AWS account settings and use them . That solved my problem.</p>",4406866,,,2019-08-12T08:25:07.270,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57457807
721,57466036,2,57456318,2019-08-12T18:00:25.373,0,"<blockquote>  <p>I am calling the following REST API for fetching email id of the user  by passing Bearer token (consent token in permissions object received  in JSON input)</p></blockquote><p>According to this <a href=""https://developer.amazon.com/docs/custom-skills/device-address-api.html"" rel=""nofollow noreferrer"">doc</a>, <code>consentToken</code> has been deprecated, you should use <code>apiAccessToken</code> instead.</p><blockquote>  <p>Important: Requests from Alexa may also include a <code>consentToken</code> within  <code>session.user.permissions</code> and <code>context.System.user.permissions</code>. This  property is deprecated. Existing skills that use <code>consentToken</code> continue  to work, but the <code>context.System.apiAccessToken</code> property should be used  instead.</p></blockquote><p>Thus, <a href=""https://developer.amazon.com/docs/custom-skills/request-customer-contact-information-for-use-in-your-skill.html#get-the-api-access-token"" rel=""nofollow noreferrer""><code>accessToken = this.event.context.System.apiAccessToken</code></a>.</p><p>Also, double-check your header: <code>{""Authorization"": ""Bearer "" + apiAccessToken}</code>, make sure you have a space between <code>Bearer</code> and <code>apiAccessToken</code>.</p>",2072811,,,2019-08-12T18:00:25.373,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57466036
722,57523535,2,57517794,2019-08-16T11:06:18.997,0,"<p>This link </p><p><a href=""https://github.com/tinkerpop/gremlin/wiki/Traversal-Optimization"" rel=""nofollow noreferrer"">https://github.com/tinkerpop/gremlin/wiki/Traversal-Optimization</a></p><p>explicitly states that it refers to ""an outdated version of the TinkerPop framework and Gremlin language documentation"" - please ignore that...it is for TinkerPop 2.x.</p><p>That said, the <code>profile()</code> step is the best that Gremlin directly offers and it can tel you a lot about query execution as you can identify which steps are running slowest and if you are seeing the expected number of traversers at specific parts of your query.</p><p>If you need memory consumption information you will either need to use tools specific to the graph database that you are using to get that information (if they offer such things) or you will need to use standard profiling tools like Java Flight Recorder, VisualVM, etc.</p>",1831717,,,2019-08-16T11:06:18.997,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57523535
723,57545554,2,57541516,2019-08-18T14:37:28.740,1,"<p>In general, please heed three best practices:</p><ol><li><p>Avoid the <a href=""https://stackoverflow.com/a/36489724/1422451"">quadratic copy</a> of using <code>DataFrame.append</code> in a loop. Instead, build a list of data frames to be concatenated once outside the loop.</p></li><li><p>Use parameterization and not string concatenation which is supported with pandas <a href=""https://stackoverflow.com/a/24418294/1422451""><code>read_sql</code></a>. This avoids the need to string format and punctuate with quotes.</p></li><li><p>Discontinue using the modulo operator, <code>%</code>, for string concatenation as it is <a href=""https://stackoverflow.com/a/13452357/1422451"">de-emphasised</a> (not officially deprecated). Instead, use the superior <a href=""https://docs.python.org/3/library/stdtypes.html#str.format"" rel=""nofollow noreferrer""><code>str.format</code></a>.</p></li></ol><p>Specifically, for your needs iterate elementwise between two lists using <code>zip</code> without layering it in a dictionary:</p><pre><code>query= """"""SELECT count(distinct ids), first_date, last_date   FROM table1   WHERE first_date = %s and last_date = %s   GROUP BY 2, 3"""""" df_list = []for f, l in zip(fd, ld):   df = pd.read_sql(query, conn, params=[f, l])   df_list.append(df)final_df = pd.concat(df_list)</code></pre><hr><p>Alternatively, avoid the loop and parameters by aggregating on first and last of days of every month in table:</p><pre><code>query= """"""SELECT count(distinct ids), first_date, last_date   FROM table1   WHERE DATE_PART(d, first_date) = 1  AND last_date = LAST_DAY(first_date)  GROUP BY 2, 3  ORDER BY 2, 3"""""" final_df = pd.read_sql(query, conn) </code></pre>",1422451,,,2019-08-18T14:37:28.740,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57545554
724,57561951,2,57561465,2019-08-19T18:10:45.687,1,"<p>This is all documented at <a href=""https://docs.aws.amazon.com/lambda/latest/dg/runtime-support-policy.html"" rel=""nofollow noreferrer"">Runtime Support Policy</a>.</p><p>Specifically, <a href=""https://devguide.python.org/#status-of-python-branches"" rel=""nofollow noreferrer"">Python 2.7 will be EOL on 2020-01-01</a>. AWS will typically notify you 60 days in advance of a runtime deprecation. Upon deprecation, you can update an existing Lambda function for 30 days but you can't create new Lambda functions using that runtime. The Lambda function and its deprecated runtime environment will still be available for execution.</p><p>I would plan to migrate functions before the Python 2.7 EOL date.</p>",271415,,,2019-08-19T18:10:45.687,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57561951
725,57562266,2,50303093,2019-08-19T18:38:08.713,1,"<p>You can use a Stackdriver partner service, Blue Medora BindPlane, to monitor AWS Kubernetes or almost anything else in AWS for that matter or on-premise.  Here's an article from Google Docs about the partnership: <em><a href=""https://cloud.google.com/stackdriver/blue-medora"" rel=""nofollow noreferrer"">About Blue Medora</a></em>; you can signup for BindPlane through the <a href=""https://console.cloud.google.com/marketplace/details/bluemedora/bindplane"" rel=""nofollow noreferrer"">Google Cloud Platform Marketplace</a>.</p><p>It looks like BindPlane is handling deprecated Stackdriver monitoring agents. <em><a href=""https://cloud.google.com/monitoring/agent/plugins/bindplane-transition"" rel=""nofollow noreferrer"">Google Cloud: Transition guide for deprecated third-party integrations</a></em></p>",11886358,8049790,2019-08-19T19:58:03.407,2019-08-19T19:58:03.407,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57562266
726,57562991,2,57555082,2019-08-19T19:41:37.237,1,"<p>I don't see anything obviously wrong. </p><ol><li>Does it do hdfs-hdfs or s3a-s3a?</li><li>Upgrade your hadoop version; 2.7.x is woefully out of date, especially with the S3A code. It's unlikely to make whatever this problem go away, but it will avoid other issues. Once you've upgraded, switch to the <a href=""http://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/#Stabilizing:_S3A_Fast_Upload"" rel=""nofollow noreferrer"">fast upload</a> and it will do incremental updates of large files; currently your code will be saving each file to /tmp somewhere and then uploading it in the close() call.</li><li>turn on the logging for the org.apache.hadoop.fs.s3a module and see what it says</li></ol>",2261274,,,2019-08-19T19:41:37.237,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57562991
727,57571923,2,56261083,2019-08-20T10:47:16.660,0,"<p>We hit a very similar issue to that. In our case it turned out in the classpath of the Hive deployment there is an obsolete version of maxmind-db. This might even be by default on newer releases of Hive, but I haven't confirmed yet.</p><p>To fix, just shade any references to com.maxmind. Also, just to be safe, shade any any references to fasterxml.jackson as well - that's one of the core dependencies of GeoIP2 and conflicts of that in the classpath tend to cause weird runtime issues like this one. You can see an example of that in a <a href=""https://github.com/Spuul/hive-udfs/pull/4/files"" rel=""nofollow noreferrer"">PR addressing such issue</a> in one of the Hive UDF GeoIP libs.</p>",9362116,,,2019-08-20T10:47:16.660,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57571923
728,57615346,2,45691826,2019-08-22T18:38:56.493,4,"<p>There will be different answers to this question, but as Raf said a good place to start is your VPC, subnet, instance types, and AMI.</p><p>My issue was that AWS deprecated my instance type (m3) in my specific region. Easiest way to check is to go to the EC2 tab in AWS console and try to launch your specific setup manually.</p>",552036,552036,2019-08-22T20:19:50.410,2019-08-22T20:19:50.410,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57615346
729,57616000,2,57512577,2019-08-22T19:32:11.617,1,"<p>If you want to use Python3 with EMR notebooks, the recommended way is to use pyspark kernel and configure Spark to use Python3 within the notebook as,</p><pre><code>%%configure -f {""conf"":{ ""spark.pyspark.python"": ""python3"" }}</code></pre><p>Note that,</p><ul><li><p>Any on cluster configuration related to PYSPARK_PYTHON or PYSPARK_PYTHON_DRIVER is overridden by EMR notebook configuration. The only way to configure for Python3 is from within the notebook as mentioned above.</p></li><li><p>pyspark3 kernel is deprecated for Livy 4.0+, and henceforth pyspark kernel is recommended to be used for both Python2 and Python3 by configuring spark.pyspark.python accordingly.</p></li><li><p>If you want to install additional Python dependencies which are not already present on the cluster, you can use <a href=""https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks-scoped-libraries.html"" rel=""nofollow noreferrer"">notebook-scoped libraries</a>. It works for both Python2 as well as Python3.</p></li></ul>",5729017,,,2019-08-22T19:32:11.617,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57616000
730,57635038,2,57634353,2019-08-24T04:15:20.623,3,"<p>Please refer to the official documentation, the method you are using is deprecated and while it is maintained for backwards compatibility, it may not function as expected.  See <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.put_bucket_notification"" rel=""nofollow noreferrer"">here</a> for more information; you should be using <code>PutBucketNotificationConfiguration</code> which is <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.BucketNotification.put"" rel=""nofollow noreferrer"">bucket_notification.put()</a>.</p><p>Nonetheless, the error that you are seeing is:  </p><p><code>Not authorized to invoke function [arn:aws:lambda:...]</code></p><p>This is because when your Lambda executes, the service invoking it needs permission to invoke it; you simply need to give S3 permission to invoke your Lambda. You do this by creating a Resource Policy for your Lambda.  </p><p>In Python you're going to do this something like this:</p><pre><code> client = boto3.client('lambda') response = client.add_permission(  FunctionName='arn:aws:lambda:...',  StatementId='LambdaInvoke',  Action='lambda:InvokeFunction',  Principal='s3.amazonaws.com',  SourceArn='arn:aws:s3:::my_test_bucket',  SourceAccount='123456789012' )</code></pre><p>Don't forget to replace the <code>function-name</code> with your functions ARN, as well as the <code>source-arn</code> to be your S3 buckets ARN, and <code>source-account</code> to be the source account number.</p><p>An alternative way, if you wanted to do it from the CLI, you can do it like this:</p><pre><code>aws lambda add-permission --function-name arn:aws:lambda:... --principal s3.amazonaws.com --statement-id S3EventTrigger --action ""lambda:InvokeFunction"" --source-arn arn:aws:s3:::my_test_bucket --source-account 123456789012</code></pre><p>Both of these methods will attach a resource policy, that looks like the policy below, to your Lambda, allowing your S3 bucket to invoke it.</p><pre><code>{""Policy"" : ""{""Version"":""2012-10-17"",""Id"":""default"",""Statement"":[{""Sid"":""S3EventTrigger"",""Effect"":""Allow"",""Principal"":{""Service"":""s3.amazonaws.com""},""Action"":""lambda:InvokeFunction"",""Resource"":""arn:aws:lambda:..."",""Condition"":{""StringEquals"":{""AWS:SourceAccount"":""123456789012""},""ArnLike"":{""AWS:SourceArn"":""arn:aws:s3:::my_test_bucket""}}}]}"" ,""RevisionId"" : ""999a9999-99ab-9999-a9a9-9999999a999a""}</code></pre>",3167238,,,2019-08-24T04:15:20.623,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57635038
731,57644479,2,57642713,2019-08-25T08:36:58.550,0,"<p>In the new release of your application add User-Agent header to your requests.This header will contain type of OS (Android/iOS) and version of your application (for example 2.0). On backend check this header and if app version is 2.0 and above use v2 version of database for request else 1.0 version.</p><p>Also I advice to add ""You use out of date version of application. Please update"" screen so you can force all users to update after some period of time.</p>",5212417,,,2019-08-25T08:36:58.550,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57644479
732,57687605,2,57686698,2019-08-28T08:01:29.710,0,"<p>The main difference between Classic Load Balancers (v1 - old generation 2009) and Application Load Balancers (v2 - new generation 2016) is that ALBs have a port mapping feature to redirect to a dynamic port. In Comparison we would need to create one CLB per application.</p><p>Overall CLBs are deprecated and you use ALBs for HTTP/HTTPS and Websockets and Network Load Balancers for TCP.</p><p>Coming to your question. On ALBs you map certain paths (like an API endpoint) to a target group (e.g. EC2 instances). Within those instances you could trigger a lambda or whatever to execute your logic. This logic can stay the same as it is when you used it with a CLB.</p>",10413468,,,2019-08-28T08:01:29.710,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57687605
733,57715081,2,57708294,2019-08-29T17:18:04.190,0,"<p><strong>Disclaimer</strong>: My answer is based on the inspection of <code>DynamoDBMapperFieldModel</code> source code from the release 1.11.15 <a href=""https://github.com/aws/aws-sdk-java/blob/5b688f038b0ec5efd23df3fa2e48700af56361d3/aws-java-sdk-dynamodb/src/main/java/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBMapperFieldModel.java"" rel=""nofollow noreferrer"">found here</a>. I never have used this API, so take my answer more like an insight provided due the absence of any other answer.</p><p>In addition to the aforementioned <code>deprecated</code> constructor, the class <code>DynamoDBMapperFieldModel</code> provides the following one:</p><pre><code>/** * Creates a new field model instance. * @param builder The builder. */private DynamoDBMapperFieldModel(final Builder&lt;T,V&gt; builder) {  this.id = builder.id;  this.properties = builder.properties;  this.reflect = builder.reflect;  this.converter = builder.converter;  this.dynamoDBAttributeType = builder.dynamoDBAttributeType;}</code></pre><p>However, this constructor is <code>private</code>, what means that you cannot call it directly from outside the class scope. It seems that the solution is to use the <code>DynamoDBMapperFieldModel.Builder</code> static inner class to instantiate your DynamoDBMapperFieldModel objects. The <a href=""https://github.com/aws/aws-sdk-java/blob/7bf0fbec42de8a7d54875510f27fb363ca2d19f5/aws-java-sdk-dynamodb/src/main/java/com/amazonaws/services/dynamodbv2/datamodeling/ConversionSchemas.java"" rel=""nofollow noreferrer"">class com.amazonaws.services.dynamodbv2.datamodeling.ConversionSchemas</a> has some illustrative examples of this idiom, for instance:</p><pre><code>@Overridepublic DynamoDBMapperFieldModel getFieldModel(Method getter) {  //...  final StandardAnnotationMaps.FieldMap annotations = StandardAnnotationMaps.of(getter, null);  final DynamoDBMapperFieldModel.Builder builder = new DynamoDBMapperFieldModel.Builder(void.class, annotations);  builder.with(attributeType);  return builder.build();}</code></pre><p>This idiom is pretty common, very often called as <a href=""https://en.wikipedia.org/wiki/Builder_pattern"" rel=""nofollow noreferrer"">Builder Pattern</a>.</p><p>It is noteworthy that the DynamoDBMapperFieldModel.Builder inner class has ""default visibility"" (no explicit access modifier):</p><pre><code>static final class Builder&lt;T,V&gt; {  //...}</code></pre><p>You may check <a href=""https://docs.oracle.com/javase/tutorial/java/javaOO/accesscontrol.html"" rel=""nofollow noreferrer"">this tutorial</a> to be aware about the restrictions of each visibility modifier if necessary.</p><p>Hope this help you to find your final solution.</p>",3055724,3055724,2019-08-29T17:27:00.830,2019-08-29T17:27:00.830,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57715081
734,57733596,2,57729647,2019-08-30T21:54:29.750,2,"<p>From the Elasticsearch <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-body.html"" rel=""nofollow noreferrer"">documentation</a>:</p><blockquote>  <p>The results that are returned from a scroll request reflect the state of the index at the time that the initial search request was made, like a snapshot in time. Subsequent changes to documents (index, update or delete) will only affect later search requests.</p></blockquote><p>Therefore, you don't need to delete the scroll context.  In fact, you never <strong>need</strong> to delete the context as it will eventually delete itself.  However, it is best practice to delete the scroll context when you are finished to free up resources.</p><p>One use-case for the situation you described would be to see if the program is still using the outdated documents.  Depending on the code, you may not want it to be using deleted documents and instead want to retrieve a fresh scroll context.</p>",10271247,,,2019-08-30T21:54:29.750,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57733596
735,57766226,2,57762832,2019-09-03T06:39:01.583,1,"<p>About a year ago i implemented a fairly complex system for storing/accessing time-series using DDB as its underlying storage. To make things concrete let's say you want to store weather-related data. There are several metrics you are interested at (""temprature"", ""humidity"", etc.) and there are several physical locations from which you collect the data (let's assume it is cities: ""nyc"", ""san-francisco"", ""london"", etc.)</p><p>I used <code>&lt;location&gt;-&lt;metric&gt;</code> as the partition key (e.g., <code>""nyc.temprature""</code>, <code>""london.humidity""</code>) and the timestamp (seconds since epoch) as the sort key. This resulted in items such as:</p><pre><code>{name: 'nyc.temperature', timestamp: 1564617785, value: 35.1}{name: 'nyc.temperature', timestamp: 1564617786, value: 35.2}{name: 'nyc.temperature', timestamp: 1564617788, value: 35.1}{name: 'nyc.humidity', timestamp: 1564617786, value: 0.61}{name: 'nyc.humidity', timestamp: 1564617788, value: 0.61}{name: 'nyc.humidity', timestamp: 1564617791, value: 0.62}{name: 'london.temperature', timestamp: 1564617785, value: 33.8}{name: 'london.temperature', timestamp: 1564617786, value: 33.8}{name: 'london.temperature', timestamp: 1564617788, value: 33.9}{name: 'london.humidity', timestamp: 1564617786, value: 0.11}{name: 'london.humidity', timestamp: 1564617788, value: 0.12}{name: 'london.humidity', timestamp: 1564617791, value: 0.12}</code></pre><p>This allowed me to efficiently get all temperature values at any given location for any given time period (which a typical use case for reading time-series data): it was a simple DDB query with <code>KeyConditionExpression</code> set to <code>""#name = :v1 AND #timestamp BETWEEN :v2 and :v3""</code></p><h2>Further details</h2><p>To the best of my knowledge, the guideline of spreading the writes across partitions no longer needs to be followed. This is due to the introduction of <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html#bp-partition-key-partitions-adaptive"" rel=""nofollow noreferrer"">adaptive capacity</a>. As noted in the title of <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">this post</a>, ""what you know about DynamoDB might be outdated"", adaptive capacity dramatically changes the way one needs to think about spreading the keys across partitions. To double check, I also posted a <a href=""https://twitter.com/pembleton/status/1169153374568243200"" rel=""nofollow noreferrer"">question on Twitter</a> and got a similar reply.</p><p>My guess is that AWS did not update many of their documentation pages. That's why you still see pages such as this <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-sharding.html"" rel=""nofollow noreferrer"">Using Write Sharding to Distribute Workloads Evenly</a></p><p>However, even in the ""old"" (pre-adaptive capacity) mode you can get a quite far even if you don't shard your writes. Here's why:</p><p>As long as your table size &lt; 10GB you will have one partition so sharding will not spread your writes across partitions. Once your table size exceeds 10GB and a new partition is created for you, you will need to buy more capacity. If you don't buy more capacity you'll start seeing throttling errors so you will notice that. At that point (which, according to the data you provided, will be somewhere between 2.5 to 5 months from your starting point) you'll have better understanding of your usage pattern and will be able to make a more knowledgeable decision about the sharding scheme that best fits your needs (yes, you may need to but some temporary capacity to prevent those errors. Alternatively, you can monitor the size of your table and start the sharding preemptively). </p><p>For instance, if what you have is mostly writes and only occasional reads (this is typical in many time-series-based applications) then you can create a single table for your writes. This table will not grow beyond 10GB so you do not need to worry about splitting your keys in it. Once day you can move the data from this table to one of X several tables (say, sharded by the current date % X). During that process you can condense the data (collapse several items into one larger item) which may result in more compact footprint (thus requiring reduced capacity). It is possible that you can even move it to S3 instead of a different table. Anyhow, you'll probably have better knowledge about your read pattern then, thus allowing you to design an optimal read solution.</p>",27198,27198,2019-09-05T06:55:52.310,2019-09-05T06:55:52.310,6,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57766226
736,57770694,2,57676892,2019-09-03T11:23:37.243,3,"<p><code>ExternalID</code> is <a href=""https://github.com/kubernetes/kubernetes/pull/61877"" rel=""nofollow noreferrer"">deprecated</a> since 1.1. </p><p>Unfortunately you cannot get it in versions from 1.11, when it was <a href=""https://github.com/kubernetes-sigs/aws-alb-ingress-controller/issues/502"" rel=""nofollow noreferrer"">finally cleaned</a>.</p><p>The only way is to rollback/backport that changes and build your own version.</p>",5937420,,,2019-09-03T11:23:37.243,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57770694
737,57792013,2,57791307,2019-09-04T15:54:07.223,2,"<p>Lambda functions deployed with <code>serverless</code> do not default to your IAM user credentials, as far as I know. They use the IAM role/policy that you supply in serverless.yml, plus basic CloudWatch Logs permissions which are auto-generated by <code>serverless</code></p><p>The problem is that your Lambda function is using temporary credentials from STS (via an assumed IAM role) to generate the pre-signed URL. The URL will expire when the temporary session token expires (or earlier if you explicitly indicate an earlier timeout).</p><p>If you use IAM user credentials, rather than temporary credentials via an IAM role, you can extend the expiration to 7 days (with signature v4) or <a href=""https://stackoverflow.com/questions/35483723/what-is-the-longest-expiration-time-for-amazon-s3-generated-link"">end of epoch</a> (with the <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/"" rel=""nofollow noreferrer"">potentially deprecated</a> signature v2). So, you need to supply your Lambda function with IAM user credentials, possibly through environment variables or AWS Parameter Store or AWS Secrets Manager.</p><p>For more, see <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/presigned-url-s3-bucket-expiration/"" rel=""nofollow noreferrer"">Why is my presigned URL for an Amazon S3 bucket expiring before the expiration time that I specified?</a></p><p>Also, there are a couple of minor coding issues here:</p><ul><li>all AWS methods have a .promise() option to return a promise, so no need to use callbacks and no need to manually create Promise objects</li><li>while the <code>getSignedUrl</code> method offers an asynchronous option, the operation itself is synchronous so you should simply run <code>const url = s3.getSignedUrl(...)</code></li></ul>",271415,271415,2019-09-04T16:05:30.870,2019-09-04T16:05:30.870,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57792013
738,57792699,2,24014306,2019-09-04T16:44:50.127,12,"<p>It depends on how you generate the S3 pre-signed URL. Specifically, which signature version you use and what type of IAM credentials you use.</p><p>The credentials that you can use to create a pre-signed URL include:</p><ul><li>IAM instance profile (temporary, rotated credentials): valid up to 6 hours</li><li>STS (temporary credentials): valid up to 36 hours</li><li>IAM user (long-term credentials): valid up to 7 days when using signature v4</li><li>IAM user (long-term credentials): valid till the <a href=""https://en.wikipedia.org/wiki/Year_2038_problem"" rel=""noreferrer"">end of the epoch</a> when using signature v2</li></ul><p>Note specifically:</p><ul><li>signature v2 is <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/"" rel=""noreferrer"">potentially deprecated</a></li><li>the expiration limit for signature v2 is different to signature v4</li><li>signature v4 offers some <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/"" rel=""noreferrer"">security and efficiency benefits</a> over v2</li><li>if you use STS credentials to generate a pre-signed URL then the URL will expire when the STS credentials expire, if that is earlier than the explicit expiration that you requested for the pre-signed URL</li><li>creating pre-signed URLs that are valid till the end of the epoch is not the best security practice</li></ul><p>For more, see:</p><ul><li><a href=""https://aws.amazon.com/premiumsupport/knowledge-center/presigned-url-s3-bucket-expiration/"" rel=""noreferrer"">Why is my pre-signed URL for an Amazon S3 bucket expiring early?</a></li><li><a href=""https://stackoverflow.com/questions/35483723/what-is-the-longest-expiration-time-for-amazon-s3-generated-link"">What is the longest expiration time for amazon s3 generated link?</a></li></ul>",271415,271415,2019-10-17T22:42:48.077,2019-10-17T22:42:48.077,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57792699
739,57796867,2,57796827,2019-09-04T23:58:02.387,1,"<p>This isn't particularly surprising but it's going to be hard to give you a full answer simply because there are so many variables. Some things to consider:</p><ol><li>Reindexing documents is very expensive since it involves a delete first</li><li>As indexes get bigger the overhead of performing segment merges also goes up</li><li>As indexes get bigger, maintaining doc values and global ordinals gets more expensive</li><li>You are running on burstable t2 instances and so may be running out of CPU or IO credits</li><li>You don't mention doing optimization/force-merges which <em>may</em> help if you have lots of deleted/outdated documents</li></ol>",278836,,,2019-09-04T23:58:02.387,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57796867
740,57818720,2,57776244,2019-09-06T08:50:20.203,0,"<p>I was able to reach a better solution by using puppeteer library. Reasons for choosing a different library<li>html-pdf is deprecated</li><li>puppeteer has much better options</li><li>puppeteer has async/await feature</li></p><p>Although to make this work in AWS with serverless &amp; serverless-plugin-optimize , I did face many challenges. Note out the following points while implementing this kind of similar scenario</p><p><b>For API gateway to send any binary file(pdf / jpeg / jpg) as response</b><li>Binary Media Types should be set to <code>*/*</code> in API Gateway resource Settings options , if going through serverless under provider in serverless.yaml add <b>apiGateway:  binaryMediaTypes:  - <code>*/*</code></b></li><li>if by any chance you are using serverless-plugin-optimize to reduce lambda size , use ""external"" option for this package chrome-aws-lambda , ref link <a href=""https://www.npmjs.com/package/serverless-plugin-optimize"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/serverless-plugin-optimize</a></li></p>",9940701,9940701,2019-09-06T09:41:03.420,2019-09-06T09:41:03.420,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57818720
741,57834290,2,57831708,2019-09-07T13:49:02.507,1,<p>S3n is obsolete. S3A will work with it -look at the hadoop docs on fs.s3a.endpoint and signing algorithm. These are supported in Hadoop 2.8+</p>,2261274,,,2019-09-07T13:49:02.507,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57834290
742,57834323,2,57828760,2019-09-07T13:52:48.460,0,"<p>S3A is <em>not</em> deprecated in ASF hadoop; I will argue that it is now ahead of what EMR's own connector will do. If you are using EMR you may be able to use it, otherwise you get to work with what they implement.</p><p>FWIW in S3A we're looking at what it'd take to actually dynamically change the header for a specific query, so you go beyond specific users to specific hive/spark queries in shared clusters. Be fairly complex to do this though as you need to do it on a per request setting.</p>",2261274,,,2019-09-07T13:52:48.460,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57834323
743,57871746,2,57871002,2019-09-10T13:20:37.000,0,"<p>You added obsolete curly braces around your <code>Fn::ImportValue</code>.</p><p>Try the following:</p><pre><code>Environment:  - Name: DB_USERNAME  Value: !Join ['', ['{{resolve:secretsmanager:', Fn::ImportValue: !Sub NameOfRDSSecret, ':SecretString:username}}' ]]</code></pre>",3115607,,,2019-09-10T13:20:37.000,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57871746
744,57910112,2,57906632,2019-09-12T15:32:40.627,4,"<p>Since node.js v10 aws lambda does not support importing libraries from lambda itself.</p><p>from the docs : </p><blockquote>  <p>A deployment package is a ZIP archive that contains your function code  and dependencies. You need to create a deployment package if you use  the Lambda API to manage functions, or if you need to include  libraries and dependencies other than the AWS SDK.</p>   <p>If your function  depends on libraries other than the SDK for JavaScript, install them  to a local directory with NPM, and include them in your deployment  package. You can also include the SDK for JavaScript if you need a  newer version than the one included on the runtime, or to ensure that  the version doesn't change in the future.</p></blockquote><p>More about <a href=""https://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html"" rel=""nofollow noreferrer"">AWS Lambda Deployment Package in Node.js</a></p><p><strong>Update 02/05/2020:</strong></p><p>node.js 8.10 is now deprecated, you should use node.js 10 or 12.</p><p><a href=""https://docs.aws.amazon.com/lambda/latest/dg/runtime-support-policy.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/runtime-support-policy.html</a></p><p><a href=""https://i.stack.imgur.com/PTDLp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PTDLp.png"" alt=""enter image description here""></a></p>",9931092,9931092,2020-02-05T12:15:03.220,2020-02-05T12:15:03.220,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57910112
745,57935816,2,14077414,2019-09-14T13:10:42.377,2,"<p>As others have stated, you can increment with one operation, however ""action: ADD"" is now deprecated. The current way to achieve this is with UpdateExpression. I am also assuming the OP meant that there was a numeric [attribute] whose value needs incrementing. I am calling the attribute loginCount:</p><pre><code>dynamoDB.updateItem({  TableName: ""Users"",  Key: { ""UserId"": { S: ""c6af9ac6-7b61"" } },  ExpressionAttributeValues: { "":inc"": {N: ""1""} },  UpdateExpression: ""ADD loginCount :inc""})</code></pre>",2483011,,,2019-09-14T13:10:42.377,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57935816
746,57993950,2,57993884,2019-09-18T13:30:25.767,0,"<blockquote>  <p>storedBytes -> (long)</p>   <p>The number of bytes stored.</p>   <p>IMPORTANT: <strong>Starting on June 17, 2019, this parameter will be  deprecated for log streams, and will be reported as zero</strong>. This change  applies only to log streams. The storedBytes parameter for log groups  is not affected.</p></blockquote><p><a href=""https://docs.aws.amazon.com/cli/latest/reference/logs/describe-log-streams.html"" rel=""nofollow noreferrer"">Source</a></p>",3367818,,,2019-09-18T13:30:25.767,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/57993950
747,58124653,2,58046398,2019-09-26T20:54:38.527,0,"<p>Ultimately the solution here is to implement a <a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-create-api-as-simple-proxy"" rel=""nofollow noreferrer"">lambda proxy integration</a>. </p><p>A proxy integration in API Gateway tells API Gateway to simply forward all headers to the integration for processing, which means you will see all of those values in your lambda function.</p><p>NOTE: Lambda has to return a specific response format back to API Gateway if it's a proxy integration. Ultimately, the response should be something like:</p><pre>  {  'statusCode': 200,  'headers': {  'echo-proxy': True  },  'body': 'some payload'  }</pre><p>What you are trying to do right now is map everything manually, which is a deprecated approach and usually you don't want to do that unless you absolutely have to because it's kind of a pain.</p><p>If you have to map the headers manually start by mapping them in the the method request so it can carry on to the next step and so on. Basically like this:</p><blockquote>  <p>Method Request -> Maps variable to Integration Request -> Maps variable to Body Mapping Template -> Maps variable to actual request header</p></blockquote><p>What you have in your screenshot for the Integration Request -> HTTP Headers is:</p><blockquote>  <p>Name: cokers  Mapped from: 'blah'</p></blockquote><p>However, ""mapped from"" should look something like ""method.request.header.coker"" which is a standardized path (meaning to get the value from the Method Request Header field with name ""coker""). </p><p>Once you have added the coker header to the Method Request, and the Integration Request HTTP Headers are mapped correctly, you have to implement a mapping template. Set the content-type to application/json with passthrough set to ""When there are no templates defined(recommended)"" and a simple mapping template:</p><pre>  {  ""headers"": {  ""coker"" : ""$input.params('coker')""  }  }</pre><p>That is the way my API is setup and it returns the following to me because I had my lambda function return the event as a json object back to API GW:</p><pre>{""body"": ""{\""headers\"": {\""coker\"": \""mapped\""}}"", ""statusCode"": 200}</pre><p>NOTE: the value of my header ""coker"" in the request on the client side is ""mapped""</p><p><strong>UPDATED ANSWER</strong><br />To map the original ""coker"" header to ""coker2"" (or any other name you want to give it) you simply set the name of the header in your mapping template like so:</p><pre>  {  ""headers"": {  ""coker2"" : ""$input.params('coker')""  }  }</pre><p>Then edit your lambda function to return ""coker2"" header and you should get a response like this:</p><pre>{""body"": ""{\""headers\"": {\""coker2\"": \""mapped\""}}"", ""statusCode"": 200}</pre>",2526654,2526654,2019-11-04T23:14:51.127,2019-11-04T23:14:51.127,5,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58124653
748,58160119,2,48097007,2019-09-30T00:03:40.347,0,"<p>Even through there are couple of third party solutions available, when restoring a user pool users will be created using admin flow (users are not restored rather they will be created from an admin) and they will end up with ""Force Change Password"" status. So the users will be forced to change the password using the temporary password and that has to be facilitated from the front end of the application.</p><p>More info : <a href=""https://docs.amazonaws.cn/en_us/cognito/latest/developerguide/signing-up-users-in-your-app.html"" rel=""nofollow noreferrer"">https://docs.amazonaws.cn/en_us/cognito/latest/developerguide/signing-up-users-in-your-app.html</a></p><p>Tools available.</p><ol><li><a href=""https://www.npmjs.com/package/cognito-backup"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/cognito-backup</a></li><li><a href=""https://github.com/mifi/cognito-backup"" rel=""nofollow noreferrer"">https://github.com/mifi/cognito-backup</a> </li><li><a href=""https://github.com/rahulpsd18/cognito-backup-restore"" rel=""nofollow noreferrer"">https://github.com/rahulpsd18/cognito-backup-restore</a></li><li><a href=""https://github.com/serverless-projects/cognito-tool"" rel=""nofollow noreferrer"">https://github.com/serverless-projects/cognito-tool</a></li></ol><p>Pls bear in mind that some of these tools are outdated and can not be used. I have tested ""cognito-backup-restore"" and it is working as expected.</p><p>Also you have to think of how to secure the user information outputted by these tools. Usually they create a json file containing all the user information (except the passwords as passwords can not be backed up) and this file is not encrypted.</p><p>The best solution so far is  to prevent accidental deletion of user pools with AWS SCPs.</p>",2028135,2028135,2019-09-30T04:56:54.370,2019-09-30T04:56:54.370,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58160119
749,58169149,2,50231691,2019-09-30T13:40:48.753,5,"<p>Old question but the answer is outdated - you can now get this inside the step function - with <code>$$.Execution.id</code>. Example I'm using:</p><pre><code>""run_task"": {  ""Type"": ""Task"",  ""Parameters"": {  ""task.$"": ""$.task"",  ""executionId.$"": ""$$.Execution.Id""  },  ""Resource"": ""${runTaskLambdaArn}"",  ""End"": true}</code></pre>",1375972,,,2019-09-30T13:40:48.753,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58169149
750,58170219,2,52726914,2019-09-30T14:42:20.383,18,"<p>As of <code>September 2019</code> @bgdnip answer doesnt translate exactly for <code>typescript</code>. I got it working with the following:</p><pre><code>const api = new RestApi(this, 'RestAPI', {  restApiName: 'Rest-Name',  description: 'API for journey services.',});const putIntegration = new LambdaIntegration(handler);const auth = new CfnAuthorizer(this, 'APIGatewayAuthorizer', {  name: 'customer-authorizer',  identitySource: 'method.request.header.Authorization',  providerArns: [providerArn.valueAsString],  restApiId: api.restApiId,  type: AuthorizationType.COGNITO,});const post = api.root.addMethod('PUT', putIntegration, { authorizationType: AuthorizationType.COGNITO });const postMethod = post.node.defaultChild as CfnMethod;postMethod.addOverride('Properties.AuthorizerId', { Ref: auth.logicalId });</code></pre><p>This is from <a href=""https://docs.aws.amazon.com/cdk/latest/guide/cfn_layer.html#cfn_layer_resource_props"" rel=""noreferrer"">https://docs.aws.amazon.com/cdk/latest/guide/cfn_layer.html#cfn_layer_resource_props</a></p><p><strong>UPDATE October</strong></p><p>The above is already out of date and unnecessary and can be achieved with the following with <code>aws-cdk 1.12.0</code></p><pre><code>const api = new RestApi(this, 'RestAPI', {  restApiName: 'Rest-Name',  description: 'API for journey services.',});const putIntegration = new LambdaIntegration(handler);const auth = new CfnAuthorizer(this, 'APIGatewayAuthorizer', {  name: 'customer-authorizer',  identitySource: 'method.request.header.Authorization',  providerArns: [providerArn.valueAsString],  restApiId: api.restApiId,  type: AuthorizationType.COGNITO,});const post = api.root.addMethod('PUT', putIntegration, {  authorizationType: AuthorizationType.COGNITO,  authorizer: { authorizerId: auth.ref }});</code></pre>",5246655,5246655,2019-10-09T12:54:33.860,2019-10-09T12:54:33.860,6,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58170219
751,58193334,2,58188574,2019-10-01T23:02:08.307,0,"<p>It is strictly impossible to extend the lifetime of an S3 pre-signed URL beyond its original expiration timestamp.  </p><p>A review of the logic behind the two possible signing algorithms, both <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/auth-request-sig-v2.html"" rel=""nofollow noreferrer"">Signature Version 2</a> (deprecated, this can be identified by the presence of <code>AWSAccessKeyId</code> in the query string) and <a href=""https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html"" rel=""nofollow noreferrer"">Signature Version 4</a> (identified by <code>X-Amz-Credential</code> in the query string), makes it apparent why this is true -- the service doesn't retain a copy of the signed URL, and the expiration parameter is one of the inputs to the HMAC-SHA signing process, so it is effectively immune to tampering.</p><p>You need to generate or obtain a new pre-signed URL.</p><hr><p><em>Bonus trivia: when a pre-signed URL is expired, S3 doesn't even bother validating whether the signature is correct or the credentials used to generate it are valid -- the request is rejected immediately.</em></p>",1695906,,,2019-10-01T23:02:08.307,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58193334
752,58244718,2,58242247,2019-10-05T01:40:48.200,1,"<p>I am not sure why you are putting the log group name in the index name, but if you want to filter log-based you on log group, you can filter using log sources name that is log-group.</p><p><a href=""https://i.stack.imgur.com/NeZsb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NeZsb.png"" alt=""enter image description here""></a></p><p>There might be the case the ELK fail to create an index in case of append log-group name in the index due to naming convention, as AWS log-group normally include slash character.</p><p><strong>The rules for index names are encoded in MetaDataCreateIndexService 254. Essentially:</strong></p><pre><code>Lowercase onlyCannot include \, /, *, ?, "", &lt;, &gt;, |, space (the character, not the word), ,, #Indices prior to 7.0 could contain a colon (:), but that's been deprecated and won't be supported in 7.0+Cannot start with -, _, +Cannot be . or ..Cannot be longer than 255 characters</code></pre><p>Second thing, you are able to see with the hardcoded name because you are putting it on the right place as in first attempt you add the name in the day section.The purpose of datetime processor is to point documents to the right time based index based on a date or timestamp field in a document by using the date math index name support.</p><pre><code> var indexName = [  'cwl-'+source['@log_group'].toString()+ '-' + timestamp.getUTCFullYear(),  // year  ('0' + (timestamp.getUTCMonth() + 1)).slice(-2),  // month  ('0' + timestamp.getUTCDate()).slice(-2),   // day,  ].join('.');</code></pre><p>or you can replace all special character in log-group name.</p><pre><code>source['@log_group'].toString().replace(/[^A-Z0-9]+/ig, ""-"");</code></pre>",3288890,3288890,2019-10-05T02:56:54.153,2019-10-05T02:56:54.153,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58244718
753,58282926,2,58280969,2019-10-08T08:44:18.780,0,"<p>The notices that you get happen because these 2 variables <code>skip_requesting_account_id</code> and <code>skip_get_ec2_platforms</code> are deprecated, see <a href=""https://www.terraform.io/docs/backends/types/s3.html"" rel=""nofollow noreferrer"">docs</a>.</p><p>For the credentials error, move the credentials into ~/.aws/credentials and leave in the file just:</p><pre><code>provider ""aws"" {}</code></pre><p>This is a better practice in terms of security as well, as you do not store the credentials with the code. In your original code you also had a typo, is <code>access_key</code> not <code>acces_key</code>.</p>",95353,,,2019-10-08T08:44:18.780,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58282926
754,58307579,2,58307102,2019-10-09T15:32:24.837,1,"<p>You may be using an outdated version of the cli, please upgrade.  What version are you using?</p><pre><code>aws --version</code></pre><p>Support for intelligent tier was added in version 1.16.61 of the aws cli as can be seen in the <a href=""https://github.com/aws/aws-cli/commit/71f970fd26656190916423a6e3fd91ed3d33433c#diff-46607fbd76883995d41046735ccc8ac1"" rel=""nofollow noreferrer"">Github project</a>.</p>",1532435,1532435,2019-10-09T19:56:24.943,2019-10-09T19:56:24.943,10,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58307579
755,58310658,2,58303550,2019-10-09T19:04:17.183,0,"<p><a href=""https://github.com/aws-cloudformation/cfn-python-lint"" rel=""nofollow noreferrer"">cfn-lint</a> warns:</p><pre><code>E2531: Deprecated runtime (nodejs6.10) specified. Updating disabled since 2019-06-30, please consider to update to nodejs10.x</code></pre>",4122849,,,2019-10-09T19:04:17.183,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58310658
756,58323558,2,58306283,2019-10-10T13:02:56.840,0,"<p>I was simply using an out of date container agent. From the <a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html"" rel=""nofollow noreferrer"">docs</a>:</p><blockquote>  <p>For tasks that use the EC2 launch type, this feature requires that your container instance have version 1.22.0 or later of the container agent. </p></blockquote><p>After updating my <code>LaunchConfiguration</code>, everything worked:</p><pre><code>Parameters:  ECSAMI:  Description: AMI ID  Type: AWS::SSM::Parameter::Value&lt;AWS::EC2::Image::Id&gt;  Default: /aws/service/ecs/optimized-ami/amazon-linux/recommended/image_id  Description: The Amazon Machine Image ID used for the cluster, leave it as the default value to get the latest AMI  ContainerInstances:  Type: AWS::AutoScaling::LaunchConfiguration  Properties:  ImageId: !Ref ECSAMI  ...</code></pre>",6084948,,,2019-10-10T13:02:56.840,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58323558
757,58366247,2,42769975,2019-10-13T17:51:28.840,5,"<p>I got similar kind of error as fallows<a href=""https://i.stack.imgur.com/xzLAN.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xzLAN.png"" alt=""enter image description here""></a></p><p>When I looked at more details section I found this ,where it  was an error in the <code>bcrypt</code> library <a href=""https://i.stack.imgur.com/FjE59.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FjE59.png"" alt=""enter image description here""></a></p><p>So I removed that library and installed <a href=""https://www.npmjs.com/package/bcryptjs"" rel=""noreferrer"">bcryptjs</a>   library and deployed it again ,then it was successful deployed. So this kind of errors may appear when there are deprecated libraries and not supportive libraries .</p>",7865621,,,2019-10-13T17:51:28.840,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58366247
758,58422775,2,58419504,2019-10-16T23:06:48.637,3,"<p>In the old days, before DynamoDB Auto-Scaling, a common use pattern was:</p><ul><li>The application attempts to write to DynamoDB</li><li>If the request is throttled, the application stores the information in an Amazon SQS queue</li><li>A separate process regularly checks the SQS queue and attempts to write the data to DynamoDB. If successful, it removes the message from SQS</li></ul><p>This allowed DynamoDB to be provisioned for <em>average</em> workload rather than <em>peak</em> workload. However, it has more parts that need to be managed.</p><p>These days, DynamoDB can use <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">adaptive capacity</a> and <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html#bp-partition-key-throughput-bursting"" rel=""nofollow noreferrer"">burst capacity</a> to handle temporary changes. For larger changes, you can implement <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"" rel=""nofollow noreferrer"">DynamoDB Auto Scaling</a>, which is probably easier to implement than the SQS method.</p>",174777,,,2019-10-16T23:06:48.637,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58422775
759,58423308,2,58396928,2019-10-17T00:31:15.533,0,"<p>That sample code is for migrating a user from a legacy database, and the authenticateUser, lookupUser functions are just abstractions for your business logic (which AWS can't write for you). For instance if you have to migrate from a legacy database (not a user pool), then you would lookup their user in your table, grab their salt, hash the password passed in to the migration trigger using the same logic you did in your legacy authentication method, compare it against the stored hashed password in your legacy database, etc. (It gets a little simpler if you were storing passwords in plaintext, but let's not consider that.)</p><p>Here's a snippet that should do most of the migration for you. Someone asked a similar question on Github and referenced this StackOverflow issue.</p><pre><code>const AWS = require('aws-sdk');const cognitoIdentity = new AWS.CognitoIdentityServiceProvider({ region: '&lt;your-region-here&gt;' });const UserPoolId = process.env.deprecatedUserPoolId;exports.handler = async (event) =&gt; {  const { userName } = event;  const getUserParams = {  Username: userName,  UserPoolId  };  try {  const user = await cognitoIdentity.adminGetUser(getUserParams).promise();  //TODO: if you have custom attributes, grab them from the user variable and store them in the response below  event.response = { finalUserStatus: ""CONFIRMED"" }  return event;  } catch (e) {  throw e; //no user to migrate, give them an error in the client   }};</code></pre>",91713,,,2019-10-17T00:31:15.533,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58423308
760,58484134,2,50984454,2019-10-21T10:14:04.920,0,"<p>db2prereqcheck checks and understands only /etc/SuSE-release with the following content:</p><blockquote>  <p>SUSE   Linux Enterprise Server 15 (x86_64)</p>   <p>VERSION = 15</p>   <p>PATCHLEVEL= 1</p></blockquote><p>This works also with OpenSuse Leap 15.1. This file is deprecated since SLES 12. So you must create it your self. Then run db2prereqcheck and install all missing libraries and kernel sources mentioned. Having all requirements fulfilled you may finally see the segmentation fault message:</p><blockquote><pre><code>  Validating ""Intel TCO WatchDog Timer Driver modules"" ...  DBT3546E The db2prereqcheck utility failed to determine whether the  following file or package exists: """".  Segmentation fault (core dumped)</code></pre></blockquote><p>Don't worry!Simply retest with db2prereqcheck -i. The -i parameter checks for prerequisites that are not Db2 pureScale related. If we don't install pureScale and all requirement are fulfilled, we can ignore this ugly segmentation fault.Otherwise you must blacklist by adding:</p><blockquote><pre><code>  blacklist iTCO_wdt  blacklist iTCO_vendor_support  Into the file /etc/modprobe.d/blacklist.conf</code></pre></blockquote><p>A further issue is related to:</p><blockquote><pre><code> export DISPLAY=your.machine.ip:0.0</code></pre></blockquote><p>running ./db2setup as root doesn't work. ./db2_install is deprecated, but it works.First create the db2 users and groups as described by the IBM Knowledge Center.Then run ./db2_install as root, followed by creating an instance using db2icrt.Login as db2inst1 and test as described by the IBM Knowledge Center eventually creating the SAMPLE Database, etc. Normally ""first steps"" would do the job, but it crashes with javascript error. Hence you must do it manually!Additional manual configuration may be required as opening the firewall for port 50001 and setting this port within /etc/services and within dbm cfg with: </p><blockquote>  <p>db2 update dbm cfg using SVCENAME 50001</p></blockquote><p>or</p><blockquote>  <p>db2 update dbm cfg using SVCENAME db2c_db2inst1</p></blockquote><p>If you use the latter you must update /etc/services with the line: </p><blockquote>  <p>db2c_db2inst1   50001/tcp   #and a comment like db2 tcp/ip  connection port.</p></blockquote>",2496001,,,2019-10-21T10:14:04.920,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58484134
761,58509772,2,58499316,2019-10-22T17:47:02.820,3,"<p>Usually each container has 3 attempts before final fail (configurable, as @rbyndoor mentioned). If one attempt has failed, it is being restarted until the number of attempts reaches limit, and if it is failed, the whole vertex is failed, all other tasks being killed.</p><p>Rare failures of some task attempts is not so critical issue, especially when running on EMR cluster with spot nodes, which can be removed during execution, causing failures and partial restarts of some vertices. </p><p>In most cases the reason of failures you can find in tracker logs. </p><p>And of course this is not the reason to switch to the deprecated MR. Try to find what is the root cause and fix it.</p><p>In some marginal cases when even if the job with some failed attempts succeeded, the data produced may be partially corrupted. For example when using some non-deterministic function in the distribute by clause. Like rand(). In this case restarted container may try to copy data produced by previous step (mapper), and the spot node with mapper results is already removed. In such case some previous step containers are restarted, but the data produced may be different because of non-deterministic nature of rand function. </p><p>About killed tasks.</p><p>Mappers or reducers can be killed because of many reasons. First of all when one of the containers has failed completely, all other tasks running are being killed. If speculative execution is switched on, duplicated tasks are killed,  if the task is not responding for a long time, etc. This is quite normal and usually is not an indicator that something is wrong. If the whole job has failed or you have many attempts failures, you need to inspect failed tasks logs to find the reason, not killed ones.  </p>",2700344,2700344,2019-10-24T05:41:30.113,2019-10-24T05:41:30.113,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58509772
762,58513758,2,58509844,2019-10-23T00:04:06.670,4,"<p>I was having the same issue and it turned it out to be an outdated AWS session manager plugin for the aws cli. After updating the plugin it worked. </p><p>Instruction to install/update the plugin is <a href=""https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html#install-plugin-macos"" rel=""nofollow noreferrer"">here</a>.</p>",3513857,,,2019-10-23T00:04:06.670,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58513758
763,58542412,2,58540432,2019-10-24T13:25:16.247,1,"<p>For anyone with serious troubles with PhantomJS on Elastic Beanstalk, what finally solved my issue was installing AWS CLI, going through the steps of setting up EB:</p><pre><code>eb ssh [ENV] [REGION] - Don't worry, the CLI will guide you</code></pre><p>and after configuring IAM online with a user having ElasticBeanstalkFullAccess (and console password) and then the configuration files for aws cli at:</p><pre><code>~/.aws/config and credentials with the keys and secrets from IAM</code></pre><p>and finally connecting to the environment with eb ssh and then running:</p><pre><code>sudo npm install -g phantomjs-prebuilt --unsafe-permsudo npm install html-pdf -g (will give deprecated warning)sudo npm link html-pdfsudo npm link phantomjs-prebuilt</code></pre><p>Finally, restart the app server via the dashboard and check logs (last 100 lines) - Should just be:</p><pre><code>&gt; node server.jsServer is running.</code></pre><p>or similar. Good luck.</p><p>Thanks to <a href=""https://github.com/marcbachmann/node-html-pdf/issues/437#issuecomment-467463285"" rel=""nofollow noreferrer"">Paul-JO</a> for a solution that worked for me.</p>",533901,,,2019-10-24T13:25:16.247,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58542412
764,58558891,2,58425218,2019-10-25T12:52:23.503,5,"<p><strong>Update:</strong></p><p>If you are using aws-cli, take a look at my answer <a href=""https://stackoverflow.com/questions/58062415/stream-logs-to-elastic-using-aws-cli"">here</a>.</p><hr><p>Well, after a few hours of exploring and reading a lot of documentation I finally succeeded to create this template.</p><p>Designer Overview :</p><p><a href=""https://i.stack.imgur.com/ht7nR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ht7nR.png"" alt=""enter image description here""></a></p><p>In order to enable the stream logs to elasticsearch we need to create the following resources:</p><ol><li>The lambda function will forward the logs from cloudwatch log group to Elasticsearch.</li><li>Relevant IAM Role to get logs from cloudwatch and insert to Elasticsearch.</li><li><a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-permission.html"" rel=""nofollow noreferrer"">Lambda permission</a> - <code>The AWS::Lambda::Permission resource grants an AWS service or another account permission to use a function</code> to allow the cloudwatch log group to trigger the lambda.</li><li><a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-subscriptionfilter.html"" rel=""nofollow noreferrer"">Subscription Filter</a>  - <code>The AWS::Logs::SubscriptionFilter resource specifies a subscription filter and associates it with the specified log group. Subscription filters allow you to subscribe to a real-time stream of log events and have them delivered to a specific destination.</code></li></ol><p>Template usage:</p><ol><li>Download LogsToElasticsearch.zip from my <a href=""https://github.com/AmitBaranes/Stream-to-ElasticSearch/blob/master/LogsToElasticsearch.zip"" rel=""nofollow noreferrer"">Github page</a>.</li><li>Update <code>var endpoint = '${Elasticsearch_Endpoint}';</code> in index.js with your Elasticseatch url e.g - <code>'search-xxx-yyyy.eu-west-1.es.amazonaws.com';</code>.</li><li>Copy the zip file to s3 bucket which will be used in the template (LambdaArtifactBucketName).</li><li>Fill relevant Parameters - you can find descriptions to each resource.</li></ol><p>Template YAML:</p><pre><code>AWSTemplateFormatVersion: 2010-09-09Description: Enable logs to elasticsearchParameters:  ElasticsearchDomainName:  Description: Name of the Elasticsearch domain that you want to insert logs to  Type: String  Default: amitb-elastic-domain  CloudwatchLogGroup:  Description: Name of the log group you want to subscribe  Type: String  Default: /aws/eks/amitb-project/cluster  LambdaName:  Description: Name of the lambda function  Type: String  Default: amitb-cloudwatch-logs  LambdaRole:  Description: Name of the role used by the lambda function  Type: String  Default: amit-cloudwatch-logs-role  LambdaArtifactBucketName:  Description: The bucket where the lambda function located  Type: String  Default: amit-bucket  LambdaArtifactName:  Description: The name of the lambda zipped file  Type: String  Default: LogsToElasticsearch.zip  VPC:  Description: Choose which VPC the Lambda-functions should be deployed to  Type: 'AWS::EC2::VPC::Id'  Default: vpc-1111111  Subnets:  Description: Choose which subnets the Lambda-functions should be deployed to  Type: 'List&lt;AWS::EC2::Subnet::Id&gt;'  Default: 'subnet-123456789,subnet-123456456,subnet-123456741'  SecurityGroup:  Description: Select the Security Group to use for the Lambda-functions  Type: 'List&lt;AWS::EC2::SecurityGroup::Id&gt;'  Default: 'sg-2222222,sg-12345678'Resources:  ExampleInvokePermission:  Type: 'AWS::Lambda::Permission'  DependsOn: ExampleLambdaFunction  Properties:  FunctionName:  'Fn::GetAtt':  - ExampleLambdaFunction  - Arn  Action: 'lambda:InvokeFunction'  Principal: !Sub 'logs.${AWS::Region}.amazonaws.com'  SourceArn: !Sub &gt;-  arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:${CloudwatchLogGroup}:*  SourceAccount: !Ref 'AWS::AccountId'  LambdaExecutionRole:  Type: 'AWS::IAM::Role'  Properties:  RoleName: !Ref LambdaRole  ManagedPolicyArns:  - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'  AssumeRolePolicyDocument:  Version: 2012-10-17  Statement:  - Effect: Allow  Principal:  Service:  - lambda.amazonaws.com  Action:  - 'sts:AssumeRole'  Path: /  Policies:  - PolicyName: lambda-to-es-via-vpc-policy  PolicyDocument:  Version: 2012-10-17  Statement:  - Effect: Allow  Action:  - 'es:*'  Resource:  - !Sub &gt;-  arn:aws:es:${AWS::Region}:${AWS::AccountId}:domain/${ElasticsearchDomainName}  - PolicyName: logs-and-ec2-permissions  PolicyDocument:  Version: 2012-10-17  Statement:  - Effect: Allow  Action:  - 'ec2:CreateNetworkInterface'  - 'ec2:DescribeNetworkInterfaces'  - 'ec2:DeleteNetworkInterface'  - 'logs:CreateLogGroup'  - 'logs:CreateLogStream'  - 'logs:PutLogEvents'  Resource: '*'  ExampleLambdaFunction:  Type: 'AWS::Lambda::Function'  DependsOn: LambdaExecutionRole  Properties:  Code:  S3Bucket: !Ref LambdaArtifactBucketName  S3Key: !Ref LambdaArtifactName  FunctionName: !Ref LambdaName  Handler: !Sub '${LambdaName}.handler'  Role:  'Fn::GetAtt':  - LambdaExecutionRole  - Arn  Runtime: nodejs8.10  Timeout: '300'  VpcConfig:  SecurityGroupIds: !Ref SecurityGroup  SubnetIds: !Ref Subnets  MemorySize: 512  SubscriptionFilter:  Type: 'AWS::Logs::SubscriptionFilter'  DependsOn: ExampleInvokePermission  Properties:  LogGroupName: !Ref CloudwatchLogGroup  FilterPattern: '[host, ident, authuser, date, request, status, bytes]'  DestinationArn:  'Fn::GetAtt':  - ExampleLambdaFunction  - Arn</code></pre><p>Results:</p><p><a href=""https://i.stack.imgur.com/A6zbR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A6zbR.png"" alt=""enter image description here""></a></p><p><a href=""https://i.stack.imgur.com/3aQWn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3aQWn.png"" alt=""enter image description here""></a></p><p>Cloudwatch log:<a href=""https://i.stack.imgur.com/WneTp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WneTp.png"" alt=""enter image description here""></a></p><p>Hope you find it helpfull.</p><p><strong>Update 02/09/2020:</strong></p><p>node.js 8.10 is now deprecated, you should use node.js 10 or 12.</p><p><a href=""https://docs.aws.amazon.com/lambda/latest/dg/runtime-support-policy.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/runtime-support-policy.html</a></p><p><a href=""https://i.stack.imgur.com/snILM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/snILM.png"" alt=""enter image description here""></a></p>",9931092,9931092,2020-02-09T08:48:23.093,2020-02-09T08:48:23.093,9,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58558891
765,58560790,2,58550567,2019-10-25T14:47:06.467,1,"<p>A few things...</p><p>hot partitions, only come into play <em>if</em> you have multiple partitions...</p><p>Just because you've got multiple partition (hash) keys, doesn't automatically mean DDB will need multiple partitions.  You'll also need more than 10GB of data and/or more than 3000 RCU or 1000 WCU being used.</p><p>Next, DDB now supports ""Adaptive Capacity"", so hot partitions aren't as big a deal as they used to be.  <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">why what you know about DynamoDB might be outdated</a> </p><p>In connection with the even newer ""Instantaneous Adaptive Capacity"", you've got DDB on demand.  </p><p>One final note, you may be under the impression that a given partition (hash) key can only have a maximum of 10GB of data under it.  This is true if your table utilizes Local Secondary Indexes (LSI) but is not true otherwise.  Thus, consider using global secondary indexes (GSI).  There's extra cost associated with GSIs, so it's a trade off to consider.</p>",2933177,,,2019-10-25T14:47:06.467,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58560790
766,58572379,2,35429632,2019-10-26T15:56:58.450,0,"<p>I had this same problem and all the solutions I found on StackOverflow were outdated or way more complicated than necessary. After trying a couple propositions, I ended up writing a NodeJS script to move the data over. Maybe that could work for you? If you want to try, I documented my approach on my blog <a href=""https://tvernon.tech/blog/move-dynamodb-data-between-regions"" rel=""nofollow noreferrer"">here</a>.</p>",1385243,,,2019-10-26T15:56:58.450,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58572379
767,58601265,2,58601054,2019-10-29T03:58:51.107,-1,"<p>You haven't mentioned what the data type of the <code>time_stamp</code> column is, but assuming it's something that can be cast to <code>date</code> then I think the issue could be due to your use of the <code>NOW()</code> function which is deprecated on Redshift. Try replacing this with either <code>GETDATE()</code> or <code>SYSDATE</code> as per <a href=""https://docs.aws.amazon.com/redshift/latest/dg/Date_functions_header.html"" rel=""nofollow noreferrer"">the documentation</a>.</p>",46239,,,2019-10-29T03:58:51.107,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58601265
768,58800672,2,58800348,2019-11-11T11:41:26.710,1,"<p>Your <code>ORDER BY</code> clause has <code>o.date_added</code> in it but your actual result set does not have it. The <code>ORDER BY</code> is done after the query is done.</p><p>You can use:</p><pre><code>ORDER BY month asc, year asc LIMIT 25</code></pre><p>Also, you can remove the extra parentheses from the <code>GROUP BY</code>:</p><pre><code>GROUP BY EXTRACT(MONTH FROM o.date_added), EXTRACT(YEAR FROM o.date_added)</code></pre><p><a href=""https://www.db-fiddle.com/f/wNrnmASK89H83FAh3poiGi/3"" rel=""nofollow noreferrer"">DB-Fiddle</a></p><p>See Redshift <a href=""https://docs.aws.amazon.com/redshift/latest/dg/c_SQL_functions_leader_node_only.html"" rel=""nofollow noreferrer"">documentation</a> for use of <code>now()</code> function. Use <code>getdate()</code> instead, as the <code>now()</code> seems to be <a href=""https://docs.amazonaws.cn/en_us/redshift/latest/dg/Date_functions_header.html#date-functions-summary"" rel=""nofollow noreferrer"">deprecated</a>.</p>",1052130,1052130,2019-11-11T18:08:07.960,2019-11-11T18:08:07.960,6,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58800672
769,58924337,2,58924204,2019-11-18T23:17:03.967,0,"<p>Looks like <code>platform.linux_distribution</code> is deprecated starting Python 3.7 so it would not be available in Python 3.8. </p><p>Please see <a href=""https://stackoverflow.com/questions/49554443/platform-linux-distribution-deprecated-what-are-the-alternatives"">this post</a></p><p>The alternative is to use:</p><pre><code>import distrodistro.linux_distribution()</code></pre><p>Hope it helps!</p>",5287434,,,2019-11-18T23:17:03.967,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58924337
770,58924697,2,57291797,2019-11-18T23:59:28.173,1,"<p>I was facing a similar issue when trying to read from s3 files, ultimately solved by updating dask to most recent version (I think the one sagemaker instances start with by default is deprecated)</p><h2>Install/Upgrade packages and dependencies (from notebook)</h2><pre><code>! python -m pip install --upgrade dask! python -m pip install fsspec! python -m pip install --upgrade s3fs</code></pre><p>Hope this helps!</p>",6011629,6011629,2019-11-21T18:54:25.680,2019-11-21T18:54:25.680,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58924697
771,58926312,2,58908823,2019-11-19T03:44:36.240,0,"<p>I've figured it out, it should be </p><pre><code>use Aws\DynamoDb\SessionHandler;</code></pre><p>AWS guide is outdated...</p>",1530644,,,2019-11-19T03:44:36.240,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/58926312
772,59042329,2,28287464,2019-11-26T01:13:00.647,0,"<p>This question is old, but in case anyone else comes across this question, the halite project has been deprecated and is no longer developed.</p><p><a href=""https://github.com/saltstack/halite"" rel=""nofollow noreferrer"">https://github.com/saltstack/halite</a></p>",114866,,,2019-11-26T01:13:00.647,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59042329
773,59045972,2,59044712,2019-11-26T07:42:10.103,1,"<p>Since your function already is <code>async</code> you don't need to use the old, outdated <code>callback</code> approach.</p><p>The AWS SDK methods provide a <code>.promise()</code> method which you can append to every AWS asynchronous call and, with <code>async</code>, you can simply <code>await</code> on a Promise.</p><pre class=""lang-js prettyprint-override""><code>  var AWS = require('aws-sdk');  var iam = new AWS.IAM();  AWS.config.loadFromPath('./config.json');  let getUsers = async event =&gt; {  var params = {  UserName: ""5dc6f49d50498e2907f8ee69""  };  const user = await iam.getUser(params).promise()  console.log(user)  };</code></pre><p>Hopefully you have a handler that invokes this code, otherwise this is not going to work.If you want to export <code>getUsers</code> as your handler function, make sure export it through <code>module.exports</code> first. I.e: <code>module.exports.getUsers = async event...</code>. Double check that your Lambda handler is properly configured on the function itself, where <code>index.getUsers</code> would stand for <code>index</code> being the filename (<code>index.js</code>) and <code>getUsers</code> your exported function.</p>",10950867,,,2019-11-26T07:42:10.103,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59045972
774,59107642,2,58787789,2019-11-29T15:37:33.917,0,"<p>Solved it: switch processing to RMagick</p><p>Within the carriewave image_uploader.rb file</p><p>replace:</p><p>include CarrierWave::MiniMagick</p><p>include CarrierWave::Processing::MiniMagick</p><p>with:</p><p>include CarrierWave::RMagick</p><p>include CarrierWave::Processing::RMagick</p><p>MiniMagick is considered to have better memory management, but is rather outdated. Plus is corrupted the images. Fingers crossed RMagick has better memory management by now.</p><p>RMagick for the win!</p>",1368601,,,2019-11-29T15:37:33.917,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59107642
775,59112383,2,59112198,2019-11-30T01:10:06.503,0,"<p>I think <code>context.succeed</code> is deprecated. Also you can use <code>async/await</code>:</p><pre><code>exports.handler = async (event, context) =&gt; { var params = {  Destination: {  ToAddresses: [""recipientEmailAddress""]  },  Message: {  Body: {  Text: { Data: ""Test"" }  },  Subject: { Data: ""Test Email"" }  },  Source: ""sourceEmailAddress""};const data = ses.sendEmail(params).promise()return data};</code></pre>",1448679,1103953,2019-11-30T08:27:21.357,2019-11-30T08:27:21.357,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59112383
776,59142446,2,59141973,2019-12-02T15:52:52.393,1,"<p>This is a general question about storage technologies: ""how does a cache differ from a database?""</p><p>A cache is not (typically) a persistent data store. Its data is ephemeral. The purpose of the cache is to increase the perceived performance of an actual database, sitting behind the cache. The database stores the actual data persistently, and is the authoritative source of data. The cache sits in front of the database and tries to improve the performance of your application by detecting queries that it already knows the answer to and serving up cached results directly to your application, to save having to go to the database.</p><p>Of course, the cache will get out of date over time and so you need a process for expiring data from the cache when it becomes inaccurate, thus causing the next query for that piece of data to go to the actual database, and that new data can be cached until it expires.</p>",271415,271415,2019-12-02T16:47:55.693,2019-12-02T16:47:55.693,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59142446
777,59148868,2,55967456,2019-12-03T01:11:15.410,2,"<p>This is actually possible programmatically, without relying on workers to report their identity - I ran into the same problem, and found the following:</p><ol><li><p>Sagemaker Ground Truth does automatic logging of worker actions. Among the things it logs is the <code>workerId</code>, with which you're familiar, the <code>cognito_user_pool_id</code>, and the <code>cognito_sub_id</code> (take a look at the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/workteam-private-tracking.html"" rel=""nofollow noreferrer"">track worker performance docs</a>).</p></li><li><p>The <code>workerId</code> is Ground Truth specific and opaque, and there isn't a way to get Ground Truth to tell you which Cognito user a <code>workerId</code> maps to. However, a Cognito user <a href=""https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html"" rel=""nofollow noreferrer"">is uniquely mapped to by its <code>sub id</code></a>. </p></li><li><p>You can leverage the logs' pairing of <code>workerId</code> and <code>cognito_sub_id</code> to generate mappings, by <a href=""https://stackoverflow.com/questions/43488445/how-do-i-look-up-a-cognito-user-by-their-sub-uuid"">using the cognito sub id to query the cognito username</a> (make sure to read down the list of answers - the accepted one says it's not possible, but it's outdated).</p></li></ol><p>You can use the mappings above to maintain a database of <code>workerId - cognito sub id - username</code> triplets, and use that database whenever you need to figure out what user a <code>workerId</code> belongs to. Note, that this will mean the first time you see a <code>workerId</code> in a Ground Truth job, you won't have a way to find its mapping. If this is a problem, you can actually solve it by using a throwaway job as suggested previously. The logs of that job will include the mappings you need.</p>",7859515,7859515,2020-01-15T21:50:12.580,2020-01-15T21:50:12.580,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59148868
778,59154278,2,59153685,2019-12-03T09:51:57.710,1,"<p>Taken from here: <a href=""https://askubuntu.com/questions/1031921/php-mcrypt-package-missing-in-ubuntu-server-18-04-lts"">https://askubuntu.com/questions/1031921/php-mcrypt-package-missing-in-ubuntu-server-18-04-lts</a></p><p>Mcrypt has been deprecated in PHP 7.2, so it's not available by default.</p><p>You can still install the mcrypt extension using pecl. These instructions are for the apache web server.</p><pre><code># Install prerequisitessudo apt-get install php-dev libmcrypt-dev gcc make autoconf libc-dev pkg-config# Compile mcrypt extensionsudo pecl install mcrypt-1.0.1# Just press enter when it asks about libmcrypt prefix# Enable extension for apacheecho ""extension=mcrypt.so"" | sudo tee -a /etc/php/7.2/apache2/conf.d/mcrypt.ini# Restart apachesudo service apache2 restartThat should get you going.</code></pre><p>In the long term you might want to replace mcrypt, it's deprecated for a reason.</p>",7500779,,,2019-12-03T09:51:57.710,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59154278
779,59220336,2,57884885,2019-12-06T21:06:36.393,1,"<p>I am running the latest version of SAM on Windows (0.37.0) and nodejs10.x is still not supported.  Lambda has support for nodejs 10 and 12 now, plus 8.10 is deprecated.  So, the SAM team has still not updated their tooling yet.  A look at the Lambda api docs show support for the following:</p><blockquote>  <p>Runtime  The identifier of the function's runtime.  Required: Yes  Type: String  Allowed Values: dotnetcore1.0 | dotnetcore2.1 | go1.x | java11 | java8 | nodejs10.x | nodejs12.x | nodejs8.10 | provided | python2.7 | python3.6 | python3.7 | python3.8 | ruby2.5</p></blockquote>",6651502,6651502,2019-12-06T21:11:43.610,2019-12-06T21:11:43.610,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59220336
780,59276158,2,59268806,2019-12-10T22:15:50.560,0,"<p>""Bundle"" commands are outdated.</p><p>You should simply use <strong>Create Image</strong> to create an Amazon Machine Image (AMI) of the existing instance. Then, you can <strong>Launch an instance from the AMI</strong> and it will contain an exact copy of the AMI.</p><p>See: <a href=""https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/Creating_EBSbacked_WinAMI.html"" rel=""nofollow noreferrer"">Creating a Custom Windows AMI - Amazon Elastic Compute Cloud</a></p><p><strong>Recommendation:</strong> Run <code>sysprep</code> before creating the image. This will avoid conflicting machine IDs.</p><p>See: <a href=""https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ami-create-standard.html"" rel=""nofollow noreferrer"">Create a Standard Amazon Machine Image Using Sysprep - Amazon Elastic Compute Cloud</a></p>",174777,,,2019-12-10T22:15:50.560,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59276158
781,59316600,2,59316340,2019-12-13T05:26:56.113,3,"<p>You can set the <code>MaxPods</code> field in the <a href=""https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/"" rel=""nofollow noreferrer"">kubelet config file</a>:</p><pre><code>apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationMaxPods: 250</code></pre><p>You can then supply the config file to the kubelet binary with the <code>--config</code> flag, for example:</p><pre class=""lang-sh prettyprint-override""><code>kubelet --config my-kubelet-config.yaml</code></pre><p>Alternatively, the kubelet binary also has a <code>--max-pods</code> flag that allows you to set the value directly. However, this flag is deprecated and the use of a config file, as shown above, is recommended. See the <a href=""https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/"" rel=""nofollow noreferrer"">kubelet reference</a>.</p>",4747193,,,2019-12-13T05:26:56.113,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59316600
782,59331553,2,59316647,2019-12-14T01:16:17.493,0,"<p>I think the problem here is that your boto3 version might be outdated. Could you try updating?</p><pre><code>pip install boto3 --upgrade</code></pre>",1247948,,,2019-12-14T01:16:17.493,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59331553
783,59454463,2,59446807,2019-12-23T11:21:55.740,1,"<p>distributed training is model and framework specific. Not all models are easy to distribute, and from ML framework to ML framework things are not equally easy. <strong>It is rarely automatic, even less so with TensorFlow and Keras.</strong></p><p>Neural nets are conceptually easy to distribute under the data-parallel paradigm, whereby the gradient computation of a given mini-batch is split among workers, which could be multiple devices in the same host (multi-device) or multiple hosts with each multiple devices (multi-device multi-host). The D2L.ai course provides an in-depth view of how neural nets are distributed <a href=""http://d2l.ai/chapter_computational-performance/multiple-gpus.html"" rel=""nofollow noreferrer"">here</a> and <a href=""http://d2l.ai/chapter_computational-performance/parameterserver.html"" rel=""nofollow noreferrer"">here</a>.</p><p>Keras used to be trivial to distribute in multi-device, single host fashion with the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model"" rel=""nofollow noreferrer""><code>multi_gpu_model</code>, which will sadly get deprecated in 4 months</a>. In your case, you seem to refer to multi-host model (more than one machine), and that requires writing ad-hoc synchronization code such as the one seen in <a href=""https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras"" rel=""nofollow noreferrer"">this official tutorial</a>. </p><p>Now let's look at how does this relate to SageMaker.</p><p>SageMaker comes with 3 options for algorithm development. Using distributed training may require a varying amount of custom work depending on the option you choose:</p><ol><li><p>The <strong>built-in algorithms</strong> is a library of 18 pre-written algorithms. Many of them are written to be distributed in single-host multi-GPU or multi-GPU multi-host. With that first option, you don't have anything to do apart from setting <code>train_instance_count</code> > 1 to distribute over multiple instances</p></li><li><p>The <strong>Framework containers</strong> (the option you are using) are containers developed for popular frameworks (TensorFlow, PyTorch, Sklearn, MXNet) and provide pre-written docker environment in which you can write arbitrary code. In this options, some container will support one-click creation of ephemeral training clusters to do distributed training, <strong>however using <code>train_instance_count</code> greater than one is not enough to distribute the training of your model. It will just run your script on multiple machines. In order to distribute your training, you must write appropriate distribution and synchronization code in your <code>mnist_keras_tf.py</code> script.</strong> For some frameworks such code modification will be very simple, for example for TensorFlow and Keras, SageMaker comes with Horovod pre-installed. Horovod is a peer-to-peer ring-style communication mechanism that requires very little code modification and is highly scalable (<a href=""https://eng.uber.com/horovod/"" rel=""nofollow noreferrer"">initial annoucement from Uber</a>, <a href=""https://sagemaker.readthedocs.io/en/stable/using_tf.html#training-with-horovod"" rel=""nofollow noreferrer"">SageMaker doc</a>, <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_horovod/tensorflow_script_mode_horovod.ipynb"" rel=""nofollow noreferrer"">SageMaker example</a>, <a href=""https://aws.amazon.com/fr/blogs/machine-learning/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker/"" rel=""nofollow noreferrer"">SageMaker blog post</a>). My recommendation would be to try using Horovod to distribute your code. Similarly, in Apache MXNet you can easily create Parameter Stores to host model parameters in a distributed fashion and sync with them from multiple nodes. MXNet scalability and ease of distribution is one of the reason Amazon loves it.</p></li><li><p>The <strong>Bring-Your-Own Container</strong> requires you to write both docker container and algorithm code. In this situation, you can of course distribute your training over multiple machines but you also have to write machine-to-machine communication code</p></li></ol><p>For your specific situation my recommendation would be to scale horizontally first in a single node with multiple GPUs over bigger and bigger machine types, because latency and complexity increase drastically as you switch from single-host to multi-host context. If truly necessary, use multi-node context and things maybe easier if that's done with Horovod.In any case, things are still much easier to do with SageMaker since it manages creation of ephemeral, billed-per-second clusters with built-in, logging and metadata and artifacts persistence and also handles fast training data loading from s3, sharded over training nodes. </p><p><strong>Note on the relevancy of distributed training</strong>: Keep in mind that when you distribute over N devices a model that was running fine on one device, you usually grow the batch size by N so that the per-device batch size stays constant and each device keeps being busy. This will disturb your model convergence, because bigger batches means a less noisy SGD. A common heuristic is to grow the learning rate by N (more info in <a href=""https://arxiv.org/abs/1706.02677"" rel=""nofollow noreferrer"">this great paper from Priya Goyal et al</a>), but this on the other hand induces instability at the first couple epochs, so it is sometimes associated with a learning rate warmup. Scaling SGD to work well with very large batches is still an active research problem, with new ideas coming up frequently. Reaching good model performance with very large batches sometimes require ad-hoc research and a fair amount of parameter tuning, occasionally to the extent where the extra money spent on finding how to distribute well overcome the benefits of the faster training you eventually manage to run. A situation where distributed training makes sense is when an individual record represent too much compute to form a big enough physical batch on a device, a situation seen on big input sizes (eg vision over HD pictures) or big parameter counts (eg BERT). That being said, for those models requiring very big logical batch you don't necessarily have to distribute things physically: you can run sequentially N batches through your single GPU and wait N per-device batches before doing the gradient averaging and parameter update to simulate having an N times bigger GPU. (a clever hack sometimes called <em>gradient accumulation</em>)</p>",5331834,5331834,2019-12-23T11:44:58.307,2019-12-23T11:44:58.307,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59454463
784,59501562,2,59501208,2019-12-27T13:32:25.020,0,"<p>Ensure your dialect matches the appropriate version of Postgres if you are using Hibernate 5.</p><p>See here: <a href=""https://docs.jboss.org/hibernate/orm/5.2/javadocs/org/hibernate/dialect/package-summary.html"" rel=""nofollow noreferrer"">https://docs.jboss.org/hibernate/orm/5.2/javadocs/org/hibernate/dialect/package-summary.html</a></p><p>Since you are using Hibernate 5.1 you should try setting the dialect to:</p><p><code>org.hibernate.dialect.PostgreSQL94Dialect</code></p><p>Additional dialects can be found here:</p><p><a href=""https://docs.jboss.org/hibernate/orm/5.1/javadocs/"" rel=""nofollow noreferrer"">https://docs.jboss.org/hibernate/orm/5.1/javadocs/</a></p><p>In Hibernate 5 the PostgreSQLDialect is deprecated.</p>",714969,714969,2020-01-04T18:47:12.070,2020-01-04T18:47:12.070,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59501562
785,59508546,2,59508159,2019-12-28T06:04:17.777,1,"<p>That's a really poor question. I would recommend spending your time elsewhere rather than trying to diagnose this question.</p><p>I suspect the question is also pretty old because Auto Scaling groups can now also accept Launch Templates, and the link you provided doesn't work. (The updated link is: <a href=""https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-lc-with-instanceID.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-lc-with-instanceID.html</a>)</p><p>Questions that use ""should"" are not well-written. Exam questions are trying to test your knowledge of how to do something successfully, not how you ""should"" do something. For example, questions would normally ask for the lowest-cost solution or a solution with the highest availability, leaving no questions about the correct answer.</p><p>To answer your question:</p><ul><li>A Launch Configuration can be created in the management console, via the AWS CLI or via AWS SDK API calls (so C and D are valid and A is incorrect)</li><li>It is <em>not</em> possible to create a Launch Configuration from an existing Amazon EC2 instance (so B is incorrect)</li><li>It is now possible to create a <strong>Launch Template</strong> from an existing Amazon EC2 instance, and this can be used by an Auto Scaling group (but this isn't what they asked)</li></ul><p><strong>Bottom line:</strong> Let the author know that the question is outdated/incorrect.</p>",174777,,,2019-12-28T06:04:17.777,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59508546
786,59522054,2,54285471,2019-12-29T18:15:35.650,0,"<p>The error indicates that you have not specified the S3 bucket to upload the data to. You will have to pass that in either in the <code>uploadRequest1</code> variable you have created there or instantiate AWSS3TransferManager with the bucket information. AWSS3TransferManager is already deprecated and so if you are still looking for a client library to interact with your S3 resource, I would suggest starting off with AWS Amplify framework's Storage plugin <a href=""https://aws-amplify.github.io/docs/ios/storage"" rel=""nofollow noreferrer"">https://aws-amplify.github.io/docs/ios/storage</a></p>",2464632,,,2019-12-29T18:15:35.650,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59522054
787,59572123,2,59559826,2020-01-03T01:23:31.583,1,"<p>The problem is being caused by <strong>deprecated images</strong>.</p><p>For example, Windows images are deprecated when a newer version is made available. While the image object can be retrieved, it does not actually contain any information (name, tags, etc). Thus, it is not possible to display any information about it.</p><p>You will need to add a <code>try</code> statement to catch such situations and skip-over the deprecated image.</p><pre class=""lang-py prettyprint-override""><code>import boto3ec2_resource = boto3.resource('ec2')# Display AMI information for all instancesfor instance in ec2_resource.instances.all():  image = instance.image  # Handle situation where image has been deprecated  try:  tags = image.description  except:  continue  if image.tags is None:  description_tag = ''  else:  description_tag = [tag['Value'] for tag in image.tags if tag['Name'] == 'Description'][0]  print(image.description, image.image_type, image.name, image.platform, description_tag) </code></pre>",174777,,,2020-01-03T01:23:31.583,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59572123
788,59594292,2,59585572,2020-01-04T19:53:20.270,0,"<p>Signature Version 2 (which is what you are using, here) always expects the resource to be <code>/${bucket}/${file}</code> regardless of whether the  URL is path-based (bucket as the first element of the path) or virtual (bucket as part of the hostname).</p><p>Additionally, for legacy reasons, S3 expects <code>+</code> in the signature to be url-escaped as <code>%2B</code> and (iirc) will accept but does not require any <code>/</code> to be transformed to <code>%2F</code> and any <code>=</code> to be transformed to <code>%3D</code> so you need to verify (<code>curl -v ...</code>) whether curl is transforming it, and if not, use something like <code>sed</code> or maybe <code>tr</code> in the pipeline to do that encoding of the signature, or perhaps find a curl option that can enable the query string parameter encoding.</p><p>Be aware that <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/"" rel=""nofollow noreferrer"">Signature V2 is deprecated</a> and will be deactivated at some point after June, 2020, so even though it still works on existing buckets in regions where S3 originally launched prior to 2014, its usable lifespan is most definitely coming to an end... so your interests would be served by not creating new systems that use it.  The successor algorithm, Signature V4, is more complicated but there are solid security-related justifications for its increased complexity.</p>",1695906,,,2020-01-04T19:53:20.270,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59594292
789,59733935,2,54412088,2020-01-14T12:36:50.097,1,"<p>Yum can return a non-zero exit status for things that are not really errors, causing higher-level systems such as Elastic Beanstalk to think the script has failed.  </p><p>In particular, yum sometimes says ""Nothing to do"" with exit status of 1 - this can mean various things but includes the case where required packages are already installed. </p><p>The way I work around this for scripts using yum is:</p><pre><code>  yum -y install somepackage  if [ $? -ne 1 ]; then   # Exit on any any error except 'nothing to do'   exit 0  fi</code></pre><p>A simpler way is just to ignore all errors by appending a <code>true</code> or <code>exit 0</code> command - however, this is eventually going to bite you when the Yum repo is unreachable, or Yum has out of date metadata, etc.</p><h2>Advanced tip</h2><p>If you have several yum commands, or more error codes to handle, you might want to read up on the shell <code>trap</code> command, specifically on <code>EXIT</code> or <code>ERR</code> which lets you handle these cases in a single place, and potentially not exit on unwanted errors.  See <a href=""https://stackoverflow.com/questions/5312266/how-to-trap-exit-code-in-bash-script"">this stack</a> for more on this.</p><h2>Alternative for local installs</h2><p>See <a href=""https://serverfault.com/questions/880398/yum-install-local-rpm-throws-error-if-up-to-date"">this answer</a> for more, including a simple alternative when installing RPMs that you have downloaded.</p>",992887,992887,2020-01-16T08:23:57.197,2020-01-16T08:23:57.197,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59733935
790,59777560,2,59777111,2020-01-16T20:45:03.097,0,"<p>Now() has been deprecated.Getdate() works.</p>",11059442,,,2020-01-16T20:45:03.097,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59777560
791,59803102,2,59798049,2020-01-18T17:29:30.820,1,"<blockquote>  <p><em>Why is AWS S3 refusing to open a document because of an expiry time value, which it itself allowed us to use to generate the pre-signed URL?</em></p></blockquote><p>S3 didn't allow that.  Signed URLs are generated locally, and S3 doesn't see them or know about them (or validate their authenticity or authorization to fetch the specified object) until you actually try to use them.</p><p>This is probably best characterized as a bug in boto3... Signature Version 2 expirations are tied to the Unix epoch, which ends 2038-01-19T03:14:08Z (the ""Y2.038K bug"").  It's unlikely to be fixed at this point since <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/"" rel=""nofollow noreferrer"">Signature V2 is deprecated</a>.</p><p>Theoretically, you could V2-sign a URL that does't expire until mid-January, 2038 but this isn't viable, either, because signed URLs are (re)validated each time they are used.  Best practice is to periodically rotate your keys, so the AWS Access Key ID you are using today should not still be valid in 100 years, or even in the 18 years between now and 2038.  Once you deactivate those particular credentials, any URLs they signed will no longer be usable.</p>",1695906,,,2020-01-18T17:29:30.820,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59803102
792,59824753,2,59820194,2020-01-20T13:54:14.407,1,"<p>You're correct, there's no sign of <code>--log-opt</code> being implemented into Kubernetes, since dockerd is deprecated.</p><p>Instead of specifying the <code>awslogs-stream</code> have you tried to set a <code>tag</code>?</p><ul><li>Check this <strong><a href=""https://stackoverflow.com/questions/55609398/use-awslogs-with-kubernetes-natively"">Use Awslogs With Kubernetes 'Natively'</a></strong> as it may fit perfectly to your needs.</li></ul><p>From Docker documentation link you posted:</p><blockquote>  <p>Specify  <code>tag</code>  as an alternative to the  <code>awslogs-stream</code>  option.  <code>tag</code>  interprets Go template markup, such as  <code>{{.ID}}</code>,  <code>{{.FullID}}</code>  or  <code>{{.Name}}</code>  <code>docker.{{.ID}}</code>.  See the  <a href=""https://docs.docker.com/config/containers/logging/log_tags/"" rel=""nofollow noreferrer"">tag option documentation</a>  for details on all supported template substitutions.</p></blockquote><p><em>The other viable approach is using a sidecar container daemon to process the logs and then forward to awslogs but <code>tag</code> is a cleaner, simplier solution.</em></p><p>Here's the process with the fluentd: </p><blockquote>  <p><strong><a href=""https://stackoverflow.com/questions/46469076/how-to-send-kubernetes-logs-to-aws-cloudwatch"">How to Send Kubernetes Logs to AWS Cloudwatch</a></strong></p></blockquote>",12524159,12524159,2020-01-20T14:50:47.727,2020-01-20T14:50:47.727,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59824753
793,59899118,2,59874265,2020-01-24T15:10:28.777,0,"<p><code>AttachmenTargetType</code> has been wrongly deprecated.</p><p>Only some of its constants are deprecated: <code>secretsmanager.AttachmentTargetType.CLUSTER</code> is now <code>secretsmanager.AttachmentTargetType.RDS_DB_CLUSTER</code> and <code>secretsmanager.AttachmentTargetType.INSTANCE</code> is now <code>secretsmanager.AttachmentTargetType.RDS_DB_INSTANCE</code>.</p><p>Note that if in your example <code>this.database</code> is a <code>Cluster</code> instance, you can simply do:</p><pre class=""lang-ts prettyprint-override""><code>this.dbSecrets.attach(this.database);</code></pre>",11339287,,,2020-01-24T15:10:28.777,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59899118
794,59939258,2,59938378,2020-01-27T21:37:07.647,0,"<p>Meta: this isn't a programming issue and probably belongs on superuser or unix.SX, or possibly security.SX. But last time I tried to migrate something it was more hassle than it was worth.</p><p><a href=""https://man.openbsd.org/sshd.8#FILES"" rel=""nofollow noreferrer"">https://man.openbsd.org/sshd.8#FILES</a> (formatting simplified)</p><blockquote><p>~/.ssh/authorized_keys</p><p>If this file, the ~/.ssh directory, or the user's home directory are writable by other users, then the file could be modified or replaced by unauthorized users. In this case, sshd will not allow it to be used unless the StrictModes option has been set to Š—“noŠ—.</p></blockquote><p>You have both .ssh (drwxrwxr-x) and .ssh/authorized_keys (-rw-rw-r--) writable by group. On modern Linux systems userids are often put in their own groups so that 'group' access doesn't actually permit access by other users, but OpenSSH can't practically detect that (especially in a possible Kerberos, LDAP, or YP/NIS environment) and must assume that 'group' access does allow other users and is unsafe.</p><p>Also, recent versions of OpenSSH (since 7.0, including your 7.4) no longer support DSA keys by default. This can be fixed, see:<br /><a href=""https://superuser.com/questions/1016989/ssh-dsa-keys-no-longer-work-for-password-less-authentication"">https://superuser.com/questions/1016989/ssh-dsa-keys-no-longer-work-for-password-less-authentication</a><br /><a href=""https://unix.stackexchange.com/questions/247612/ssh-keeps-skipping-my-pubkey-and-asking-for-a-password"">https://unix.stackexchange.com/questions/247612/ssh-keeps-skipping-my-pubkey-and-asking-for-a-password</a><br /><a href=""https://security.stackexchange.com/questions/112802/why-openssh-deprecated-dsa-keys"">https://security.stackexchange.com/questions/112802/why-openssh-deprecated-dsa-keys</a></p><p>but unless you have some positive reason to use DSA specifically, I suggest changing to either rsa-2048 (compatible with everything and secure enough) or ecdsa(-any) or ed25519 (now widespread though not universal, more efficient, and slightly higher safety margin).</p>",2868801,-1,2020-06-20T09:12:55.060,2020-01-27T21:45:18.857,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59939258
795,59953367,2,59953053,2020-01-28T16:45:31.850,3,"<p>All AWS customers should understand the <a href=""https://aws.amazon.com/compliance/shared-responsibility-model/"" rel=""nofollow noreferrer"">Shared Responsibility Model</a>.</p><p>Specifically related to your question:</p><blockquote>  <p>Customers that deploy an Amazon EC2 instance are responsible for  management of the guest operating system (including updates and  security patches), any application software or utilities installed by  the customer on the instances, and the configuration of the  AWS-provided firewall (called a security group) on each instance.</p></blockquote><p>No, AWS will not prevent you from running deprecated versions of Ruby or old, buggy web servers. It might notify you, however, in certain situations such as your RDS database having an expiring SSL certificate.</p><p>If your application behaves in a way that threatens the platform itself or other computer systems, for example being part of a DDoS botnet, then it's conceivable that AWS will implement measures to prevent it.</p>",271415,,,2020-01-28T16:45:31.850,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59953367
796,59954650,2,59889088,2020-01-28T18:06:30.623,1,"<p>If anyone bumps in to this issue, the way I solved it was to create another project with exactly same settings and code (diff compare if you want) but just change the names of the stack, the app, the functions and the policies. </p><p>The initial version of this app was deployed using Node 8.10 which was recently deprecated. Maybe that was part of the problem.</p><p>If this helps you, like this post , I probably saved you a day of productivity :p</p>",11239755,,,2020-01-28T18:06:30.623,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/59954650
797,60015844,2,28245224,2020-02-01T10:03:53.543,2,"<p><code>/var/app/current</code> may be outdated. It doesn't exist on my instance.<br>As said there <a href=""https://stackoverflow.com/questions/27973619/i-cant-find-my-web-app-when-i-ssh-to-my-aws-elastic-beanstalk-instance"">I can&#39;t find my Web app when I SSH to my AWS Elastic Beanstalk instance</a>, for python the app is in <code>/opt/python/bundle/2/app/</code><br>Otherwise use <code>find</code> to search for the location (look at the link).</p>",7479997,,,2020-02-01T10:03:53.543,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60015844
798,60066826,2,30154461,2020-02-04T23:05:05.350,4,"<p>The main advantage of using CloudFront is to get your files from a source (S3 in your case) and store it on edge servers to respond to GET requests faster. CloudFront will not go back to S3 source for each http request. </p><p>To have CloudFront serve latest fiels/objects, you have multiple options:</p><h3>Use CloudFront to Invalidate modified Objects</h3><p>You can use CloudFront to invalidate one or more files or directories manually or using a trigger. This option have been described in other responses here. More information at <a href=""https://aws.amazon.com/about-aws/whats-new/2015/05/amazon-cloudfront-makes-it-easier-to-invalidate-multiple-objects/"" rel=""nofollow noreferrer"">Invalidate Multiple Objects in CloudFront</a>. This approach comes handy if you are updating your files infrequently and do not want to impact the performance benefits of cached objects.</p><h3>Setting object expiration dates on S3 objects</h3><p>This is now the recommended solution. It is straight forward:</p><ul><li>Log in to AWS Management Console</li><li>Go into S3 bucket</li><li>Select all files</li><li>Choose ""Actions"" drop down from the menu</li><li>Select ""Change metadata""</li><li>In the ""Key"" field, select ""Cache-Control"" from the drop down menu.</li><li>In the ""Value"" field, enter ""max-age=300"" (number of seconds)</li><li>Press ""Save"" buttonThe default cache value for CloudFront objects is 24 hours. By changing it to a lower value, CloudFront checks with the S3 source to see if a newer version of the object is available in S3.</li></ul><p>I use a combination of these two methods to make sure updates are propagated to an edge locations quickly and avoid serving outdated files managed by CloudFront. </p><p>AWS however recommends changing the object names by using a version identifier in each file name. If you are using a build command and compiling your files, that option is usually available (as in react npm build command).</p>",3984664,,,2020-02-04T23:05:05.350,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60066826
799,60114105,2,56825003,2020-02-07T13:03:41.703,0,"<p>Running the above code in a workflow gives the error:</p><pre><code>usage: workflow-test.py [-h] --JOB_NAME JOB_NAME --WORKFLOW_NAME WORKFLOW_NAME  --WORKFLOW_RUN_ID WORKFLOW_RUN_IDworkflow-test.py: error: the following arguments are required: --JOB_NAME</code></pre><p>followed by what you have pasted above.</p><p>Seems the AWS documentation is outdated and the <code>JOB_NAME</code> parameter is not used in jobs that are part of a workflow.</p><p>You'll get the workflow parameters with:</p><pre><code>args = getResolvedOptions(sys.argv, ['WORKFLOW_NAME', 'WORKFLOW_RUN_ID'])</code></pre>",7808647,,,2020-02-07T13:03:41.703,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60114105
800,60226327,2,60216343,2020-02-14T12:24:47.993,0,"<p>I have examined your Java code snippet to the best of my ability and there are two issues that I've noted:</p><p><strong>( i )</strong> The line below is incomplete :</p><pre><code>snsClient.setRegion(Region.getRegion(Regions.));</code></pre><p>and should be changed to :</p><pre><code>snsClient.setRegion(Region.getRegion(Regions.US_EAST_1)); //US_EAST_1 can be replaced with your desired region</code></pre><p><strong>( ii )</strong> The following declaration are deprecated:</p><pre><code>new AmazonSNSClient(credentials) </code></pre><p>and </p><pre><code>snsClient.setRegion(Region.getRegion(Regions.fromName(region)));</code></pre><p>Instead, AWS service clients should be created as follows :</p><p>If you are using a credential file then you can use the following :</p><pre><code>BasicAWSCredentials basicAwsCredentials = new BasicAWSCredentials(AccessKey,SecretAccessKey);AmazonSNS snsClient = AmazonSNSClient  .builder()  .withRegion(your_region)  .withCredentials(new AWSStaticCredentialsProvider(basicAwsCredentials))  .build();</code></pre><p>If you are going to use InstanceProfileCredentialProvider then you can use the following :</p><pre><code>AmazonSNS sns = AmazonSNSClientBuilder  .standard()  .withCredentials(new InstanceProfileCredentialsProvider(true))  .build();</code></pre><p>More information can be found on <a href=""https://github.com/awsdocs/aws-java-developer-guide/blob/master/doc_source/credentials.rst"" rel=""nofollow noreferrer"">AWS Java developer guide</a></p><p>Hope this helps!</p>",4723365,4723365,2020-03-08T06:06:27.160,2020-03-08T06:06:27.160,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60226327
801,60315500,2,60314981,2020-02-20T08:16:07.130,0,"<p>I'm not a DynamoDB insider, so I can only answer from what I understand from their documentation.</p><p>In on-demand pricing (see <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand</a>) you pay exactly by the number and size of your requests. If you make one million requests, you will pay the same whether these requests were to a million different partitions, or they all went to the same partition.</p><p>You might wonder, then, why there was such an issue of load imbalance pricing in <strong>provisioned</strong>-capacity pricing - or at least why is the Web full of stories of such an issue. There should never have been such an issue, but in the past there was. But since recently, this isn't an issue any more. Here is the story:</p><p>In the provisioned pricing page, Amazon claims that if you reserve 1000 WCU, you should be able to use this number of write units that you paid for, per second, and if you try to use more, you'll be throttled. There is no mention or warning of imbalanced loads or hot partitions... But people discovered that this wasn't quite true - Amazon had a <strong>bug</strong> in their throttling code: The usage counting wasn't done across the entire cluster. Instead, if your data was spread over 10 nodes, your reservation of 1000 was evenly split among them, so each of the 10 nodes would start to throttle you after 100 (1000/10) requests per second. This split worked well for well-balanced loads, but with hot partitions, it didn't work well. People were paying for a reservation of 1000 but when they measured how much they were getting, they saw throttling after just 800 (for example) requests per second. Amazon acknowledge this was a bug, and fixed it with their ""adaptive capacity"" technique where each of the nodes picks a different throttling limit, modified until the user's total usage approaches what he had paid for. This technique is explained in this excellent official talk<a href=""https://www.youtube.com/watch?v=yvBR71D0nAQ"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=yvBR71D0nAQ</a> - see time 19:38. Until very recently this ""adaptive capacity"" was a very blunt instrument, which only worked well if your workload doesn't change quickly, but since then, this issue was fixed too - as described in<br><a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/</a></p>",8891224,,,2020-02-20T08:16:07.130,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60315500
802,60327445,2,44612835,2020-02-20T19:44:10.567,-1,"<p>Updating this wonderful answer by John, for those that find this and see warning messages coming from the cryptography module in Python..  <code>signer</code> has been deprecated in favor of the <code>sign</code> function on the serialization object according to the documentation.</p><pre><code>import datetimefrom cryptography.hazmat.backends import default_backendfrom cryptography.hazmat.primitives import hashesfrom cryptography.hazmat.primitives import serializationfrom cryptography.hazmat.primitives.asymmetric import paddingfrom botocore.signers import CloudFrontSignerdef rsa_signer(message):  with open('path/to/key.pem', 'rb') as key_file:  private_key = serialization.load_pem_private_key(  key_file.read(),  password=None,  backend=default_backend()  )  return private_key.sign(message, padding.PKCS1v15(), hashes.SHA1())key_id = 'AKIAIOSFODNN7EXAMPLE'url = 'http://d2949o5mkkp72v.cloudfront.net/hello.txt'expire_date = datetime.datetime(2017, 1, 1)cloudfront_signer = CloudFrontSigner(key_id, rsa_signer)# Create a signed url that will be valid until the specfic expiry date# provided using a canned policy.signed_url = cloudfront_signer.generate_presigned_url(  url, date_less_than=expire_date)print(signed_url)</code></pre><p>This just updates the answer so it doesn't throw the deprecation warning, and doesn't look at security aspects of pieces in use.</p>",5910171,,,2020-02-20T19:44:10.567,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60327445
803,60359380,2,60225185,2020-02-23T05:11:12.083,3,"<p>DurandA - I believe you are absolutely correct: the simplified Lambda Proxy Integration approach relies on you catching your exceptions and returning the standardized format:</p><pre class=""lang-py prettyprint-override""><code>def lambda_handler(event, context):  return {  'statusCode': 400,  'body': json.dumps('This is a bad request!')  }</code></pre><p>The simplified Lambda Proxy Integration feature was announced in a <a href=""https://aws.amazon.com/blogs/aws/api-gateway-update-new-features-simplify-api-development/"" rel=""nofollow noreferrer"">September 2016 blog post</a>, but one of the examples you cited was posted earlier, in a <a href=""https://aws.amazon.com/blogs/compute/error-handling-patterns-in-amazon-api-gateway-and-aws-lambda/"" rel=""nofollow noreferrer"">June 2016 blog post</a>, back when the more complicated Integration Response method was the only way.   Maybe you have stumbled on an out of date example.</p><p>You also posted a link to the <a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/handle-errors-in-lambda-integration.html"" rel=""nofollow noreferrer"">product documentation for error handling</a>, at the top in the section covering the Lambda proxy integration feature, it says:</p><blockquote>  <p>With the Lambda proxy integration, Lambda is required to return an  output of the following format:</p>   <pre><code>{  ""isBase64Encoded"" : ""boolean"",  ""statusCode"": ""number"",  ""headers"": { ... },  ""body"": ""JSON string""}</code></pre></blockquote><p>Here is a working example that returns a HTTP 400 with message ""This is an exception!"" using Lambda Proxy Integration.</p><pre class=""lang-py prettyprint-override""><code>import jsondef exception_handler(e):  # exception to status code mapping goes here...  status_code = 400  return {  'statusCode': status_code,  'body': json.dumps(str(e))  }def lambda_handler(event, context):  try:  raise Exception('This is an exception!')  return {  'statusCode': 200,  'body': json.dumps('This is a good request!')  }  except Exception as e:  return exception_handler(e)</code></pre><p>Output from the above:</p><pre class=""lang-sh prettyprint-override""><code>$ http https://**********.execute-api.us-east-2.amazonaws.com/testHTTP/1.1 400 Bad RequestConnection: keep-aliveContent-Length: 23Content-Type: application/jsonDate: Sun, 23 Feb 2020 05:06:59 GMTX-Amzn-Trace-Id: Root=1-********-************************;Sampled=0x-amz-apigw-id: ****************x-amzn-RequestId: ********-****-****-****-************""This is an exception!""</code></pre><p>I understand your frustration that you do not want to build a custom exception handler.   Fortunately, you only have to build a single handler wrapping your lambda_handler function.  Wishing you all the best!</p>",3933995,,,2020-02-23T05:11:12.083,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60359380
804,60494793,2,60493487,2020-03-02T18:41:13.003,0,"<p>Solution:Import this<code>var unmarshalItem = require('dynamodb-marshaler').unmarshalItem;</code>I have the response here in ""results.Items"" and now you can do this to get the normal json format perfectly.</p><pre><code>var items = results.Items.map(unmarshalItem);  console.log(items);  callback(null, items);</code></pre><p>But i see this npm package was deprecated, So i will try doing this with latest <code>DynamoDB/Converter</code> and will update here.</p>",1930155,,,2020-03-02T18:41:13.003,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60494793
805,60614655,2,60610487,2020-03-10T09:27:05.567,6,"<p>You need to install <code>uuid</code> package. <code>uuid</code> recently did a <a href=""https://www.npmjs.com/package/uuid#deep-requires-now-deprecated"" rel=""noreferrer"">breaking change</a>, the way you use it. New way is</p><pre><code>const {""v4"": uuidv4} = require('uuid');</code></pre><p>Hope this helps.</p>",1021796,39709,2020-03-31T00:52:57.203,2020-03-31T00:52:57.203,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60614655
806,60668860,2,60666957,2020-03-13T10:34:03.570,-1,"<p>Compose files only dictate communication between containers running on the same node. Service discovery is usually considered a desirable feature of distributed systems, so I'm going to assume that you're running your stack on several nodes and load-balance between them in some way to give you resilience and high-availability (HA). Otherwise, you'd just use EC2 and install docker.</p><p>You need to run your containers/services on a node, so you can continue to use Compose files for this. You shouldn't be using this to define the communication between services though, as you want this to pass through your service-mesh (services + discovery + proxy == service-mesh). Alternatively, it would be relatively easy to replace Compose with a bunch of <code>docker container run</code> statements in a bash script.</p><p>Your choice of service-mesh is (arguably) outdated. I'd take a look at consul and consul-connect these days rather than eureka, with the Envoy proxy -- this tech is a couple of generations on from Eureka and a lot simpler. </p>",2051454,,,2020-03-13T10:34:03.570,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60668860
807,60764721,2,60761162,2020-03-19T20:39:56.200,1,"<p>t1.micro is an obsolete instance class, available only in some older AWS regions.  The us-east-2 region was never equipped with any t1.micro hardware. </p><p>If for some reason you still need to try to launch a t1.micro,  you may find capacity in some availability zones of us-east-1 or us-west-2, but the AMI would be different since those are regional.</p><p>Alternately, you might be able to launch this as t2.micro or t3.micro if the AMI is compatible with those classes.</p>",1695906,,,2020-03-19T20:39:56.200,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60764721
808,60770478,2,8931175,2020-03-20T08:03:24.887,0,"<p>It seems that this <strong>XSD files are outdated</strong>.</p><p>Just checked the official sellercentral help page for the XSD files <a href=""https://sellercentral-europe.amazon.com/gp/help/G1611"" rel=""nofollow noreferrer"">https://sellercentral-europe.amazon.com/gp/help/G1611</a>For the OrderReport there is still <strong>release_4_1</strong> referenced.</p><p>Some time ago amazon has added a new field to <a href=""https://images-na.ssl-images-amazon.com/images/G/01/rainier/help/xsd/release_4_1/OrderReport.xsd"" rel=""nofollow noreferrer"">OrderReport</a> for EU markets. The new field is <code>IsSoldByAB</code>.I am using the xsd files since many years for automatic code generation. And this fails from time to time because of new fields like this. This field is not descriped in one of this:</p><ul><li><strong>release_1_9</strong> ($Revision: #7 $, $Date: 2006/05/23 $)</li><li><strong>release_4_1</strong> ($Revision: #10 $, $Date: 2007/09/06 $)</li></ul><p>XSD files and I am not able to find a version that include this field.</p><p>Since some years I extend the XSD file on my own to generate my code. <code>IsSoldByAB</code> is just a boolean field as <code>IsPrime</code> or <code>IsBusinessOrder</code>. So this was an easy task but not ""official""...</p>",2306226,,,2020-03-20T08:03:24.887,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60770478
809,60797039,2,42757905,2020-03-22T07:51:54.447,0,"<p>3 years later, python 2.7 is deprecated on AWS Lambda.</p><p>With Python 3.x you may want some steps to add this dependency.</p><p>You can check [1] for more information about performing a non-standard build of psycopg2.</p><p>These are the bash commands that I used to build a layer zip on an EC2 instance using the Amazon Linux 1 AMI from [2]:</p><pre><code>$ sudo yum update$ sudo yum install python36$ curl -O https://bootstrap.pypa.io/get-pip.py$ python3 get-pip.py --user$ sudo yum groupinstall ""Development Tools""$ sudo yum install python36-devel$ sudo yum install postgresql-devel$ mkdir -p package/python/lib/python3.6/site-packages/$ pip3 install psycopg2 -t package/python/lib/python3.6/site-packages/$ mkdir package/lib$ sudo find / -name libpq.so*$ cp /usr/lib64/libpq.so package/lib$ cp /usr/lib64/libpq.so.5 package/lib$ cp /usr/lib64/libpq.so.5.5 package/lib$ cd package$ zip -r psycopg2.zip *</code></pre><p>You can then use the zip file created when creating a layer on the Lambda console. </p><p>[1] <a href=""http://initd.org/psycopg/docs/install.html"" rel=""nofollow noreferrer"">http://initd.org/psycopg/docs/install.html</a></p><p>[2] <a href=""https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html</a></p>",3478533,,,2020-03-22T07:51:54.447,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60797039
810,60823374,2,60821313,2020-03-24T00:23:20.553,1,"<p>The <a href=""https://www.npmjs.com/package/azure"" rel=""nofollow noreferrer"">azure</a> NPM package is a) deprecated and b) a rollup of all features so it's much larger than you strictly need. If you really want to continue to use this then consider using a subset of the package, specifically <code>azure-arm-*</code> or <code>azure-*</code>.</p><p>You should consider moving to the <a href=""https://github.com/azure/azure-sdk-for-node#announcing-the-new-azure-sdk-for-javascript"" rel=""nofollow noreferrer"">newer SDK</a>, specifically the <a href=""https://www.npmjs.com/package/azure-arm-sb"" rel=""nofollow noreferrer"">azure-arm-sb</a> ServiceBus package.  Note that it too will be deprecated next year, when MS migrates it fully to TypeScript.</p>",271415,271415,2020-03-24T00:28:27.267,2020-03-24T00:28:27.267,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60823374
811,60970700,2,60966945,2020-04-01T12:17:08.933,3,"<p>Ozone s3-gateway uses by default the <a href=""https://hadoop.apache.org/ozone/docs/0.4.0-alpha/s3.html"" rel=""nofollow noreferrer"">path-style addressing</a> while updated sdk libraries use the virtual-hosted addressing. The quickest solution would be to switch to path-style:</p><pre><code>// AmazonS3Config config = new AmazonS3Config();config.ForcePathStyle = true;</code></pre><p>Alternatively, as mentioned on the docs, you could enable the virtual-hosted schema in ozone.</p><p>Please notice that the path-style <a href=""https://aws.amazon.com/es/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/"" rel=""nofollow noreferrer"">is going to be deprecated</a> in aws s3. </p>",2706709,,,2020-04-01T12:17:08.933,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60970700
812,60981713,2,60938911,2020-04-01T23:08:57.273,3,"<p>You'll need to look at the nested stacks themselves. The <code>AbcLambdaFunction</code> and <code>DefLambdaFunction</code> resources should have more detailed failure stack events in the nested stacks than in the parent stacks. You'll likely need to fix <code>AbcLambdaFunction</code> and <code>DefLambdaFunction</code> in the nested stack templates, as the inconsistency is likely just due to whichever resource happened to fail first and started the rollback</p><p>If it's been a while since those templates have been run, it's likely <a href=""https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html"" rel=""nofollow noreferrer"">Lambda Runtimes</a> have been deprecated. The <a href=""https://github.com/aws-cloudformation/cfn-python-lint/"" rel=""nofollow noreferrer"">CloudFormation Linter</a> should be able to check your templates for this and more possibilities</p><p><a href=""https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html"" rel=""nofollow noreferrer"">AWS Lambda limits</a> are likely as well, I'd recommend trying <a href=""https://stackoverflow.com/a/58884131/4122849"">things like this</a></p><p>Check to see if there any CloudWatch logs as well</p>",4122849,4122849,2020-04-05T02:31:15.573,2020-04-05T02:31:15.573,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60981713
813,60986488,2,34689445,2020-04-02T07:40:44.073,19,"<h2><strong>Update</strong></h2><p>Since AWS CLI version 2 - <code>aws ecr get-login</code> is deprecated and the correct method is <a href=""https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html"" rel=""noreferrer""><code>aws ecr get-login-password</code></a>.</p><p>Therefore the correct and updated answer is the following:<code>docker login -u AWS -p $(aws ecr get-login-password --region us-east-1) xxxxxxxx.dkr.ecr.us-east-1.amazonaws.com</code></p>",1326702,,,2020-04-02T07:40:44.073,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/60986488
814,61003238,2,50354503,2020-04-03T00:14:59.610,0,"<p>Stumbled upon similar issue in my project.If the intention is to guarantee cross-file consistency (between files A,B,C) the only possible solution (purely within s3) is:</p><p>1) to put them as NEW objects </p><p>2) do not explicitly check for existence using HEAD or GET request prior to the put.</p><p>These two constraints above are required for fully consistent read-after-write behavior (<a href=""https://aws.amazon.com/about-aws/whats-new/2015/08/amazon-s3-introduces-new-usability-enhancements/"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aws/whats-new/2015/08/amazon-s3-introduces-new-usability-enhancements/</a>)</p><p>Each time you update the files, you need to generate a unique prefix (folder) name and put this name into your marker file (the manifest) which you are going to UPDATE. </p><p>The manifest will have a stable name but will be eventually consistent. Some clients may get the old version and some may get the new one.The old manifest will point to the old Š—“folderŠ— and the new one will point the new Š—“folderŠ—. Thus each client will read only old files or only new files but never mixed, so cross file consistency will be achieved. Still different clients may end up having different versions. If the clients keep pulling the manifest and getting updated on change, they will eventually become consistent too.</p><p>Possible solution for client inconsistency is to move manifest meta data out of s3 into a consistent database (such as dynamo db)</p><p>A few obvious caveats with pure  s3 approach:</p><p>1) requires full set of files to be uploaded each time (incremental updates are not possible)</p><p>2) needs eventual cleanup of old obsolete folders</p><p>3) clients need to keep pulling manifest to get updated</p><p>4) clients may be inconsistent between each other</p>",4233586,4233586,2020-04-03T01:14:12.077,2020-04-03T01:14:12.077,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61003238
815,61052742,2,56436632,2020-04-06T03:57:57.643,0,"<p>Add <code>use_deprecated_int96_timestamps=True</code> to <code>df.to_parquet()</code> when you first write the file, and it will save as a nanosecond timestamp. <a href=""https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetWriter.html"" rel=""nofollow noreferrer"">https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetWriter.html</a></p>",5148439,,,2020-04-06T03:57:57.643,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61052742
816,61055622,2,50202582,2020-04-06T08:20:49.457,2,"<p>Amazon have <a href=""https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html#registry_auth"" rel=""nofollow noreferrer"">well documented</a> how to use the AWS CLI to allow docker to authenticate to an Amazon ECR registry.</p><p>However, <code>get-login</code> has now been <a href=""https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html"" rel=""nofollow noreferrer"">deprecated</a>. You will need to use <code>get-login-password</code> instead.</p><p>They note in their documentation that you can pass the authentication token to the docker login command. You will need to specify the Amazon ECR registry URI you want to authenticate to as well.</p><p>For example</p><pre><code>$ aws ecr get-login-password --region us-east-1 \  | docker login --username AWS --password-stdin \  123456789012.dkr.ecr.us-east-1.amazonaws.com</code></pre><p>I can then pull the image according to <code>registry/repository[:tag]</code>.</p><p><a href=""https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html"" rel=""nofollow noreferrer"">Registry</a> is a combination of your Account ID and the regional ECR endpoint, e.g:</p><pre><code>123456789012.dkr.ecr.us-east-1.amazonaws.com</code></pre><p><a href=""https://docs.aws.amazon.com/AmazonECR/latest/userguide/Repositories.html"" rel=""nofollow noreferrer"">Repository</a> is the name of the place to store images e.g:</p><pre><code>myrepo</code></pre><p><a href=""https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr-using-tags.html"" rel=""nofollow noreferrer"">Tag</a> is the usual image metadata, e.g.: <code>latest</code></p><p>Below is a complete example of authenticating and pulling an image:</p><pre><code>$ aws ecr get-login-password --region ap-southeast-2 \  | docker login --username AWS --password-stdin \  123456789012.dkr.ecr.ap-southeast-2.amazonaws.com$ docker pull 123456789012.dkr.ecr.ap-southeast-2.amazonaws.com/myrepo:latestlatest: Pulling from myrepo5bed26d33875: Pull completeDigest: sha256:aabbccddStatus: Downloaded newer image for 123456789012.dkr.ecr.ap-southeast-2.amazonaws.com/myrepo:latest123456789012.dkr.ecr.ap-southeast-2.amazonaws.com/myrepo:latest</code></pre>",244008,,,2020-04-06T08:20:49.457,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61055622
817,61058484,2,36154484,2020-04-06T11:13:19.293,0,"<p>as of </p><p>aws-java-sdk-core-1.11.163</p><blockquote>  <p><strong>SDKGlobalConfiguration.ENABLE_S3_SIGV4_SYSTEM_PROPERTY is deprecated.</strong></p></blockquote><p>I used the below code to call using signature_version='s3'</p><pre><code>private static ClientConfiguration config = new ClientConfiguration();static {  config.setProtocol(Protocol.HTTP);  config.setSignerOverride(""S3SignerType"");}private static AmazonS3 s3client = AmazonS3ClientBuilder  .standard()  .withClientConfiguration(config)  .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(""endpoint"", null))  .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(""access_key_id"", ""secret_Key"")))  .build();</code></pre>",3892213,,,2020-04-06T11:13:19.293,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61058484
818,61059883,2,11031608,2020-04-06T12:29:05.903,0,"<h2>java.time</h2><p>I am providing the modern answer: use java.time, the modern Java date and time API, for your date and time work. First of all because it is so much nicer to work with than the old date and time classes like <code>Date</code> and (oh, horrors) <code>SimpleDateFormat</code>, which are poorly designed. WeŠ—Ère fortunate that they are long outdated. An added advantage is: Your date-time string is in ISO 8601 format, and the classes of java.time parse this format as their default, that is, without any explicit formatter.</p><pre><code>  String stringFromCloud = ""2014-06-14T08:55:56.789Z"";  Instant timestamp = Instant.parse(stringFromCloud);  System.out.println(""Parsed timestamp: "" + timestamp);</code></pre><p>Output:</p><blockquote>  <p>Parsed timestamp: 2014-06-14T08:55:56.789Z</p></blockquote><p>Now itŠ—Ès clear to see that the string has been parsed with full millisecond precision (<code>Instant</code> can parse with nanosecond precision, up to 9 decimals on the seconds). <code>Instant</code> objects will work fine as keys for your <code>SortedMap</code>.</p><p>Corner case: if the fraction of seconds i 0, it is not printed.</p><pre><code>  String stringFromCloud = ""2014-06-14T08:56:59.000Z"";</code></pre><blockquote>  <p>Parsed timestamp: 2014-06-14T08:56:59Z</p></blockquote><p>You will need to trust that when no fraction is printed, it is because it is 0. The <code>Instant</code> will still work nicely for your purpose, being sorted before instants with fraction .001, .002, etc.</p><h2>What went wrong in your parsing?</h2><p>First, youŠ—Ève got a problem that is much worse than missing milliseconds: You are parsing into the wrong time zone. The trailing <code>Z</code> in your incoming string is a UTC offset of 0 and needs to be parsed as such. What happened in your code was that <code>SimpleDateFormat</code> used the time zone setting of your JVM instead of UTC, giving rise to an error of up to 14 hours. In most cases your sorting would still be correct. Around transition from summer time (DST) in your local time zone the time would be ambiguous and parsing may therefore be incorrect leading to wrong sort order.</p><p>As the Mattias Isegran Bergander says in his answer, parsing of milliseconds should work in your code. The reason why you didnŠ—Èt think so is probably because just a minor one of the many design problems with the old <code>Date</code> class: even though internally it has millisecond precision, its <code>toString</code> method only prints seconds, it leaves out the milliseconds.</p><h2>Links</h2><ul><li><a href=""https://docs.oracle.com/javase/tutorial/datetime/"" rel=""nofollow noreferrer"">Oracle tutorial: Date Time</a> explaining how to use java.time.</li><li><a href=""https://en.wikipedia.org/wiki/ISO_8601"" rel=""nofollow noreferrer"">Wikipedia article: ISO 8601</a></li></ul>",5772882,,,2020-04-06T12:29:05.903,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61059883
819,61068801,2,61068708,2020-04-06T20:54:39.617,0,"<p>Seems like your node version is out of date.</p><p>You can:</p><ul><li>update node version (for example using <a href=""https://www.npmjs.com/package/n"" rel=""nofollow noreferrer"">n package</a>)</li><li>transpile your code to es standard that can be handled with your version of node</li></ul>",11911285,,,2020-04-06T20:54:39.617,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61068801
820,61099600,2,61085087,2020-04-08T11:28:32.490,0,<p>It turned out that I was using a pre-made lambda deployment package that was a little bit outdated. I created a venv then used a build script to create the zip file. It worked perfectly fine. </p>,3719201,,,2020-04-08T11:28:32.490,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61099600
821,61183724,2,61182599,2020-04-13T08:02:12.893,2,"<p>I would recommend that you <em>not</em> use the Classic Load Balancer. These days, you should use the <strong>Application Load Balancer</strong> or <strong>Network Load Balancer</strong>. (Anything with the name 'classic' basically means it is outdated, but still available for legacy use.)</p><p>There are <strong>many ways to create scaling triggers</strong>. The easiest method is to use <a href=""https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html"" rel=""nofollow noreferrer"">Target Tracking Scaling Policies for Amazon EC2 Auto Scaling</a>. This allows you to provide a <strong>target</strong> (eg <em>""CPU Utilization of 75%""</em>) and Auto Scaling will handle the details.</p><p>However, I note that you tagged this question as using Elastic Beanstalk. I don't think it supports Target Tracking, so instead you can specify a ""Scale-out"" and ""Scale-In"" threshold.</p><p>As to what number you should put in... this <strong>depends totally on your application and its typical usage patterns</strong>. You can only determine the 'correct' setting by observing your normal traffic, or by creating a test system and simulating typical usage.</p><p><strong>CPU Utilization</strong> <em>might</em> be a good metric to use for scaling, but this depends on what the application is doing. For example, if it is doing heavy calculations (eg video encoding), it is a good metric. However, there might be other indications of heavy usage, such as the amount of free memory or the number of users. You can only figure out which is the 'right' metric by observing what your system does when it is under load.</p>",174777,,,2020-04-13T08:02:12.893,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61183724
822,61193214,2,34009873,2020-04-13T17:25:20.550,0,"<blockquote>  <p><strong>It is possible to use Spring Data Elasticsearch with Amazon Elasticsearch</strong></p></blockquote><p>As Excerpt from Spring-data elastic search <a href=""https://docs.spring.io/spring-data/elasticsearch/docs/current/reference/html/#elasticsearch.clients.transport"" rel=""nofollow noreferrer"">doc</a></p><blockquote>  <p>TransportClient is deprecated as of Elasticsearch 7 and will be  removed in Elasticsearch 8. This is in favor of Java High Level REST  Client. Spring Data Elasticsearch will support the TransportClient as  long as it is available in the Elasticsearch.</p>   <p>The Java High Level REST Client now is the default client of  Elasticsearch, it provides a straight forward replacement for the  TransportClient as it accepts and returns the very same  request/response objects and therefore depends on the Elasticsearch  core project</p></blockquote><p><strong>Spring Data ElasticSearch</strong> has streamlined with the latest standards of <strong>ElasticSearch</strong>, hence from <code>spring-data-elasticsearch:3.2.X</code> it provides a flexible way to achieve a custom <code>RestHighLevelClient</code>.(<a href=""https://docs.spring.io/spring-data/elasticsearch/docs/3.2.x/api/org/springframework/data/elasticsearch/config/AbstractElasticsearchConfiguration.html#elasticsearchClient"" rel=""nofollow noreferrer"">link</a>)Even though it is possible to use HTTP based elastic search API calls with or without authentication, it will not solve the problem associated with the AWS elastic-search API calls. </p><p>Because any <strong>HTTP requests</strong> to <strong>AWS services or APIGW backed services</strong> have to follow <strong><code>""Signature Version 4 Signing Process(SigV4)""</code></strong> which eventually adds authentication information to AWS requests sent by HTTP. For security, most requests to AWS must be signed with an access key, which consists of an <strong><code>accesskey ID</code></strong> and <strong><code>secret access key</code></strong>. Hence, we have to follow the standards when calling the AWS ElasticSearch service.</p><p><strong>Let's get our hands dirty with code and delve deep into the implementation</strong></p><p>Please follow the steps:</p><p><strong>Step 1: Adding required dependencies</strong></p><pre><code>   &lt;dependency&gt;  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;  &lt;version&gt;2.2.2.RELEASE&lt;/version&gt;  &lt;/dependency&gt;  &lt;dependency&gt;  &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;  &lt;artifactId&gt;aws-java-sdk-elasticsearch&lt;/artifactId&gt;  &lt;version&gt;1.11.346&lt;/version&gt;  &lt;/dependency&gt;</code></pre><p><strong>Step 2: Adding AWS CredentialsProvider</strong></p><pre><code>import com.amazonaws.auth.AWSStaticCredentialsProvider;import com.amazonaws.auth.BasicAWSCredentials;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class AWSCredentialsConfiguration {  @Value(""${aws.es.accessKey}"")  private String esAccessKey = null;  @Value(""${aws.es.secretKey}"")  private String esSecretKey = null;  @Bean  public AWSStaticCredentialsProvider awsDynamoCredentialsProviderDevelopment() {  return new AWSStaticCredentialsProvider(new BasicAWSCredentials(  esAccessKey, esSecretKey));  }}</code></pre><p>Or if your Application running on AWS instance and you don't want to use the property-driven/hardcoded AccessKey and SecretKey then you have to assign the <strong>IAM role</strong> to your <strong>Amazon ECS task</strong> <a href=""https://docs.aws.amazon.com/AmazonECS/latest/userguide/task-iam-roles.html"" rel=""nofollow noreferrer"">for more</a>.</p><pre><code>import com.amazonaws.auth.AWSCredentialsProvider;import com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class AWSCredentialsConfiguration {  @Bean  public AWSCredentialsProvider amazonAWSCredentialsProvider() {  return new EC2ContainerCredentialsProviderWrapper();  }}</code></pre><p><strong>Step 3: Adding ElasticSearchRestClientConfiguration</strong></p><ul><li>If you observe the below code we are providing a custom implementation of <code>**RestHighLevelClient**</code> to abstract method <code>**AbstractElasticsearchConfiguration#elasticsearchClient()**</code>. In this way, we are injecting the incorporated customRestHighLevelClient of <strong><code>""elasticsearchOperations""</code></strong>, <strong><code>""elasticsearchTemplate""</code></strong> beans into the spring container.</li><li><strong><code>HttpRequestInterceptor</code></strong> is another important thing to notice. A Special thanks to <a href=""https://github.com/awslabs/aws-request-signing-apache-interceptor/blob/master/src/main/java/com/amazonaws/http/AWSRequestSigningApacheInterceptor.java"" rel=""nofollow noreferrer"">AWSRequestSigningApacheInterceptor.java</a>, a sample implementation provided by <strong><code>AWSLabs</code></strong> helps us to add the interceptor to the RestClient with the <strong><code>AWS4Signer</code></strong> mechanism.</li><li><strong><code>@EnableElasticsearchRepositories</code></strong> annotation help to enable the elasticsearch data repositories.</li></ul><pre><code>import com.amazonaws.auth.AWS4Signer;import com.amazonaws.auth.AWSCredentialsProvider;import org.apache.http.HttpHost;import org.apache.http.HttpRequestInterceptor;import org.elasticsearch.client.RestClient;import org.elasticsearch.client.RestHighLevelClient;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.elasticsearch.config.AbstractElasticsearchConfiguration;import org.springframework.data.elasticsearch.repository.config.EnableElasticsearchRepositories;@Configuration@EnableElasticsearchRepositories(basePackages = ""com.demo.aws.elasticsearch.data.repository"")public class ElasticSearchRestClientConfiguration extends AbstractElasticsearchConfiguration {  @Value(""${aws.es.endpoint}"")  private String endpoint = null;  @Value(""${aws.es.region}"")  private String region = null;  @Autowired  private AWSCredentialsProvider credentialsProvider = null;  @Override  @Bean  public RestHighLevelClient elasticsearchClient() {  AWS4Signer signer = new AWS4Signer();  String serviceName = ""es"";  signer.setServiceName(serviceName);  signer.setRegionName(region);  HttpRequestInterceptor interceptor = new AWSRequestSigningApacheInterceptor(serviceName, signer, credentialsProvider);  return new RestHighLevelClient(RestClient.builder(HttpHost.create(endpoint)).setHttpClientConfigCallback(e -&gt; e.addInterceptorLast(interceptor)));  }}</code></pre><p>Bravo Zulu! that's it. We have completed the configuration part. Now with this solution, you can leverage the spring-data elastic benefits along with Amazon elastic search service. The complete solution has been documented in <a href=""https://medium.com/@prasanth_rajendran/how-to-integrate-spring-boot-elasticsearch-data-with-aws-445e6fc72998"" rel=""nofollow noreferrer"">Medium Post</a></p>",3303074,,,2020-04-13T17:25:20.550,8,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61193214
823,61199645,2,61199563,2020-04-14T02:08:29.793,2,"<p>You'd use the built-in <a href=""https://nodejs.org/api/crypto.html"" rel=""nofollow noreferrer"">crypto API</a> to compute hashes in node.js.</p><pre class=""lang-js prettyprint-override""><code>const crypto = require('crypto');//...const etag = crypto.createHash('md5');// .update means to add to the buffer, you can call .update multiple timesetag.update(Buffer.from(fileBase64, 'base64'));// .digest(encoding) gives you the computed value of bufferconst localHash = etag.digest('hex');console.log(`localHash: ${localHash}`);</code></pre><p>And as a tip, using <code>new</code> with <code>Buffer</code> is deprecated, see <a href=""https://nodejs.org/api/buffer.html#buffer_new_buffer_array"" rel=""nofollow noreferrer"">the documentation</a>.</p>",12989672,,,2020-04-14T02:08:29.793,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61199645
824,61226374,2,61211601,2020-04-15T10:19:38.643,1,"<p>For anyone that's interested. Herewith the solution to the problem (actually multiple problems)</p><p>Step 1 - Startup File Code</p><pre><code>  public Startup(IConfiguration configuration)  {  AWSXRayRecorder.InitializeInstance(configuration: Configuration); // Inititalizing Configuration object with X-Ray recorder  AWSSDKHandler.RegisterXRayForAllServices(); // All AWS SDK requests will be traced  }  public void Configure(IApplicationBuilder app, IWebHostEnvironment env)  {   if (env.IsDevelopment())  {  app.UseDeveloperExceptionPage();  }   //Make sure this is after env.IsDevelopment()  app.UseXRay(&quot;WeatherForecast&quot;);  .....  }</code></pre><p>Make sure appsettings.json and sampling-rules.json mimic's the <a href=""https://github.com/aws-samples/aws-xray-dotnet-webapp"" rel=""nofollow noreferrer"">Sample App</a></p><p>Once the code runs, the log file of the app would look something like this.</p><p>I felt that the AWS.SDK package generates a lot of noise even when using the Sample App, which I omitted here. That said, DEBUG logs tend to be that way.</p><pre><code>2020-04-15 11:34:04,262 [5] INFO Amazon.XRay.Recorder.Core.Internal.Utils.DaemonConfig - The given daemonAddress () is invalid, using default daemon UDP and TCP address 127.0.0.1:2000.2020-04-15 11:34:04,368 [5] INFO Amazon.Runtime.Internal.RuntimePipelineCustomizerRegistry - Applying runtime pipeline customization X-Ray Registration Customization2020-04-15 11:34:04,389 [5] INFO Amazon.XRay.Recorder.Core.Sampling.DefaultSamplingStrategy - No effective centralized sampling rule match. Fallback to local rules.2020-04-15 11:34:04,390 [5] DEBUG Amazon.XRay.Recorder.Core.Sampling.Local.LocalizedSamplingStrategy - Can't match a rule for host = localhost, path = /index.html, method = GET2020-04-15 11:34:04,573 [5] DEBUG **Amazon.XRay.Recorder.Core.Internal.Emitters.UdpSegmentEmitter - UDP Segment emitter endpoint: 127.0.0.1:2000.**</code></pre><p>Ultimately, you are looking for the last line <strong>Amazon.XRay.Recorder.Core.Internal.Emitters.UdpSegmentEmitter - UDP Segment emitter endpoint: 127.0.0.1:2000.</strong></p><p>Step 2 - Configure the Daemon</p><p>If you install the Daemon as a <a href=""https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-local.html"" rel=""nofollow noreferrer"">Windows Service</a> locally.  I ran into a couple of additional problems.</p><blockquote><p>A - It doesn't put everything in one place and it doesn't look at the configuration file that it extracted. Unless you put the cfg.yaml file in System32.</p><p>B - The service probably won't have access to the .aws folder where the credentials are stored.</p></blockquote><p>I fixed problems A, by doing the following (i'm sure you could achieve the same goal in multiple ways)</p><p>Since i'm not a powershell expert, I just moved the extracted content to a folder of my choosing and modified the service path in the registry to point to that folder as well as added the appropriate flags so that it logs to the location you expect as well as use the cfg.yaml file you expect.</p><blockquote><p>regedit -&gt; Computer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\AWSXRayDaemon</p></blockquote><p>Set image path with flags -f for log file and -c for config file</p><blockquote><p>C:\YOUR USER\.aws\aws-xray-daemon\xray.exe -f C:\YOUR USER\.aws\aws-xray-daemon\xray-daemon.log -c C:\YOUR USER\.aws\aws-xray-daemon\cfg.yaml</p></blockquote><p>The last problem was the Daemon not having the appropriate permissions to access the credentials file inside the .aws folder.</p><p>Log file will look something like this</p><pre><code>2020-04-15T09:35:54+02:00 [Debug] processor: sending partial batch2020-04-15T09:35:54+02:00 [Debug] processor: segment batch size: 1. capacity: 502020-04-15T09:35:54+02:00 [Error] Unable to sign request: NoCredentialProviders: no valid providers in chain. Deprecated.  For verbose messaging see aws.Config.CredentialsChainVerboseErrors2020-04-15T09:35:54+02:00 [Error] Sending segment batch failed with: NoCredentialProviders: no valid providers in chain. Deprecated.  For verbose messaging see aws.Config.CredentialsChainVerboseErrors</code></pre><p>The NoCredentialProviders line indicates a permission issue.</p><p>I then modified the service to run as an administrator, which solved problem B.</p><p>daemon.log</p><pre><code>2020-04-15T09:41:31+02:00 [Debug] Received request on HTTP Proxy server : /GetSamplingRules2020-04-15T09:41:32+02:00 [Debug] processor: sending partial batch2020-04-15T09:41:32+02:00 [Debug] processor: segment batch size: 1. capacity: 502020-04-15T09:41:33+02:00 [Debug] Received request on HTTP Proxy server : /GetSamplingRules2020-04-15T09:41:33+02:00 [Info] Successfully sent batch of 1 segments (0.871 seconds)2020-04-15T09:41:34+02:00 [Debug] processor: sending partial batch2020-04-15T09:41:34+02:00 [Debug] processor: segment batch size: 1. capacity: 502020-04-15T09:41:34+02:00 [Info] Successfully sent batch of 1 segments (0.197 seconds)</code></pre><p>You are looking for the line <em>successfully sent batch</em> as confirmation that the Daemon sent the trace to the X-Ray service.</p><p>Hope this helps someone.</p><p>Cheers</p>",11758702,-1,2020-06-20T09:12:55.060,2020-04-15T10:19:38.643,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61226374
825,61236224,2,54094750,2020-04-15T18:42:17.703,1,"<p>From the Secret Manager <a href=""https://docs.aws.amazon.com/secretsmanager/latest/userguide/terms-concepts.html"" rel=""nofollow noreferrer"">documentation</a>:</p><blockquote>  <p>Secrets Manager can automatically rotate your secret for you on a specified schedule. You can rotate credentials without interrupting the service if you choose to store a complete set of credentials for a user or account, instead of only the password. If you change or rotate only the password, then the old password immediately becomes obsolete, and clients must immediately start using the new password or fail. If you can instead create a new user with a new password, or at least alternate between two users, then the old user and password can continue to operate side by side with the new one, until you choose to deprecate the old one. This gives you a window of time when all of your clients can continue to work while you test and validate the new credentials. After your new credentials pass testing, you commit all of your clients to using the new credentials and remove the old credentials.</p></blockquote>",2387977,,,2020-04-15T18:42:17.703,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61236224
826,61275194,2,52087307,2020-04-17T15:27:01.637,1,"<p>The <a href=""https://github.com/alliefitter/boto3_type_annotations"" rel=""nofollow noreferrer"">boto3_type_annotations</a> mentioned by Allie Fitter is deprecated, but she links to an alternative: <a href=""https://pypi.org/project/boto3-stubs/"" rel=""nofollow noreferrer"">https://pypi.org/project/boto3-stubs/</a></p>",3927228,,,2020-04-17T15:27:01.637,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61275194
827,61339229,2,41951978,2020-04-21T08:34:29.433,1,"<p>Deprecated with only creentials in constructor, you can use something like this:</p><pre><code> val awsConfiguration = AWSConfiguration(context) val awsCreds = CognitoCachingCredentialsProvider(context, awsConfiguration) val s3Client = AmazonS3Client(awsCreds, Region.getRegion(Regions.EU_CENTRAL_1))</code></pre>",5599807,,,2020-04-21T08:34:29.433,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61339229
828,61351243,2,61349955,2020-04-21T19:06:25.493,0,"<p>This works fine for me with both the awscli and boto3. For example:</p><pre><code>import boto3client = boto3.client('ec2')subnets = client.describe_subnets()for subnet in subnets['Subnets']:  print(subnet['AvailabilityZone'], subnet['AvailabilityZoneId'])</code></pre><p>Output is:</p><pre><code>us-east-1b use1-az2us-east-1e use1-az3us-east-1d use1-az6...</code></pre><p>I think your installation of awscli and boto3 may be out of date.</p>",271415,,,2020-04-21T19:06:25.493,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61351243
829,61360061,2,61335975,2020-04-22T08:01:39.453,0,"<p>Got the same error, and this is how I solved it.</p><p><strong>1. Cognito policy as</strong></p><pre><code>{  ""Version"": ""2012-10-17"",  ""Statement"": [  {  ""Sid"": ""VisualEditor0"",  ""Effect"": ""Allow"",  ""Action"": [  ""iot:Receive"",  ""cognito-identity:*"",  ""iot:Subscribe"",  ""iot:AttachPolicy"",  ""iot:AttachPrincipalPolicy"",  ""iot:Connect"",  ""mobileanalytics:PutEvents"",  ""iot:GetThingShadow"",  ""iot:DeleteThingShadow"",  ""iot:UpdateThingShadow"",  ""iot:Publish"",  ""cognito-sync:*""  ],  ""Resource"": ""*""  }  ]}</code></pre><p>Also note that AttachPrincipalPolicy is deprecated, but for safer side I included it</p><p><strong>2. IoT Policy as</strong></p><pre><code>{  ""Version"": ""2012-10-17"",  ""Statement"": [  {  ""Effect"": ""Allow"",  ""Action"": ""iot:*"",  ""Resource"": ""*""  }  ]}</code></pre><p><strong>3. Attach IoT policy to individual cognito Identity through lambda or AWS CLI.</strong> Using CLI this command looks like</p><pre><code>aws iot attach-policy --policy-name ""iot-policy"" --target ""ap-south-1:XXXX-USER-COGNITO-IDENTITYŠ—</code></pre><p>Again note AttachPrincipalPolicy is deprecated, use AttachPolicy</p><p>Using lambda:</p><pre><code>export const main = async (event, context, callback) =&gt; {  const principal = event.requestContext.identity.cognitoIdentityId;  const policyName = 'iot-policy';  const iot = new AWS.Iot();  await iot.attachPrincipalPolicy({ principal, policyName }).promise();  callback(null, ""success"");};</code></pre><p><strong>4. Test</strong> If your frontend is configured properly, you should be able to solve <em>errorCode: 8, errorMessage: AMQJS0008I Socket closed</em> error.</p><p><strong>5. Fine Tune</strong>Now Fine tune iot-policy according to your requirements and check immediately if the changes are working</p>",3739896,,,2020-04-22T08:01:39.453,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61360061
830,61363291,2,61348310,2020-04-22T10:50:20.027,1,"<p>When you install the dashboard, the service is set as <code>ClusterIP</code>. To let users from the same VPC access it you need to change the service to <code>NodePort</code>.</p><pre><code>$ kubectl get service kubernetes-dashboard -n kube-systemNAME   TYPE   CLUSTER-IP   EXTERNAL-IP   PORT(S)  AGEkubernetes-dashboard   ClusterIP   10.0.184.227   &lt;none&gt;  80/TCP  15m</code></pre><p>To change it you have to edit the service:</p><pre><code>kubectl edit service kubernetes-dashboard -n kube-system</code></pre><p>And change the <code>.spec.type</code> from <code>ClusterIP</code> to <code>NodePort</code>.</p><p>Another option is to patch the service with the following command: </p><pre><code>$ kubectl patch service -n kube-system kubernetes-dashboard --patch '{""spec"": {""type"": ""NodePort""}}'</code></pre><p>After you edit or patch it your service is ready to be acceded as you need.</p><pre><code>$ kubectl get service kubernetes-dashboard -n kube-systemNAME   TYPE   CLUSTER-IP   EXTERNAL-IP   PORT(S)  AGEkubernetes-dashboard   NodePort   10.0.184.227   &lt;none&gt;  80:30334/TCP   18m...</code></pre><p>Now to connect to the dashboard you have to point your browser to <a href=""http://master-node-ip:nodePort"" rel=""nofollow noreferrer"">http://master-node-ip:nodePort</a></p><pre><code>$ kubectl describe service kubernetes-dashboard -n kube-system...NodePort:   &lt;unset&gt;  30334/TCP...</code></pre><pre><code>$ kubectl get node -o wideNAME  STATUS   ROLES   AGE   VERSION  INTERNAL-IP   EXTERNAL-IP   OS-IMAGE   KERNEL-VERSION  CONTAINER-RUNTIMEaks-agentpool-20139558-vmss000000   Ready  agent   16m   v1.15.10   10.240.0.5  &lt;none&gt;  Ubuntu 16.04.6 LTS   4.15.0-1071-azure   docker://3.0.10+azure...</code></pre><p>So based on this example it looks like: <code>http://10.240.0.5:30334</code></p><p>And it can be accessed from anyone in the same network as your master node.</p><pre><code>$ curl http://10.240.0.5:30334 &lt;!doctype html&gt; &lt;html ng-app=""kubernetesDashboard""&gt; &lt;head&gt; &lt;meta charset=""utf-8""&gt; &lt;title ng-controller=""kdTitle as $ctrl"" ng-bind=""$ctrl.title()""&gt;&lt;/title&gt; &lt;link rel=""icon"" type=""image/png"" href=""assets/images/kubernetes-logo.png""&gt; &lt;meta name=""viewport"" content=""width=device-width""&gt; &lt;link rel=""stylesheet"" href=""static/vendor.93db0a0d.css""&gt; &lt;link rel=""stylesheet"" href=""static/app.ddd3b5ec.css""&gt; &lt;/head&gt; &lt;body ng-controller=""kdMain as $ctrl""&gt; &lt;!--[if lt IE 10]&gt;  &lt;p class=""browsehappy""&gt;You are using an &lt;strong&gt;outdated&lt;/strong&gt; browser.  Please &lt;a href=""http://browsehappy.com/""&gt;upgrade your browser&lt;/a&gt; to improve your  experience.&lt;/p&gt;  &lt;![endif]--&gt; &lt;kd-login layout=""column"" layout-fill ng-if=""$ctrl.isLoginState()""&gt; &lt;/kd-login&gt; &lt;kd-chrome layout=""column"" layout-fill ng-if=""!$ctrl.isLoginState()""&gt; &lt;/kd-chrome&gt; &lt;script src=""static/vendor.bd425c26.js""&gt;&lt;/script&gt; &lt;script src=""api/appConfig.json""&gt;&lt;/script&gt; &lt;script src=""static/app.91a96542.js""&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt;</code></pre><p>To know more about the different between all Kubernetes services type, check the following links: </p><p><a href=""https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types"" rel=""nofollow noreferrer"">Publishing Services (ServiceTypes)</a>Kubernetes Š—– Service Publishing</p>",12153576,12153576,2020-04-22T15:46:38.267,2020-04-22T15:46:38.267,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61363291
831,61376872,2,57906632,2020-04-22T23:33:48.390,1,"<p>As per accepted answer, it's now required that lambda won't have third party support directly. (because Node 8 version is deprecated now which might've still accepted). </p><pre><code>const aws = require('aws-sdk');const s3 = new aws.S3();// const uuid = require('uuid'); comment this outexports.handler = async (event, context) =&gt; {  console.log(""Get the event to our S3POC class - "" + JSON.stringify(event));  // const newUUID = context.awsRequestId();  const newUUID = context.awsRequestId;  console.log(""The file name is:"" + newUUID);  //put our sentence into the s3 bucket  return s3.putObject({  Bucket: ""helloworld-s3.arkhadbot.com"",  Key: ""test"" + "".json""  });};</code></pre><p>AWS Request IDs might look like so: requestId: 'daf9dc5e-1628-4437-9e2d-2998efaa73b4'</p>",7622204,5501996,2020-06-18T10:46:50.463,2020-06-18T10:46:50.463,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61376872
832,61383401,2,59282601,2020-04-23T09:08:41.630,0,"<p>So, as it turned out - these annotations will be supported only since Kubernetes <code>1.16</code>, which is ""coming soon"" on AWS.Currently supported version is <code>1.15</code>, which just ignores those annotations...</p><p>Considering that you are using AWS-specific annotations here (<code>service.beta.kubernetes.io/aws-load-balancer-eip-allocations</code>) - I assume that this is exactly the reason why it does not work on your case.</p><p>As a workaround, I would advice:</p><ol><li>Create custom post-deployment script that re-configures newly-created LoadBalancer, after each Kubernetes Service Update.</li><li>Switch to use something more conventional, like ELB with your Container, and AutoScaling groups (that's what we did.)</li><li>Setup your own Kubernetes Controller (super-hard thingie, which will become completely obsolete and will just be basically a lost of time, as soon as 1.16 is officially out). See <a href=""https://www.linuxschoolonline.com/how-to-set-up-kubernetes-1-16-on-aws-from-scratch/"" rel=""nofollow noreferrer"">this how-to</a></li><li>Wait...</li></ol><p>Official statement:<a href=""https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html#1-16-prequisites"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html#1-16-prequisites</a></p><p>Full list of annotations (when they will be ""supported"" ofc):<a href=""https://github.com/kubernetes/kubernetes/blob/v1.16.0/staging/src/k8s.io/legacy-cloud-providers/aws/aws.go#L208-L211"" rel=""nofollow noreferrer"">https://github.com/kubernetes/kubernetes/blob/v1.16.0/staging/src/k8s.io/legacy-cloud-providers/aws/aws.go#L208-L211</a></p><p>Stay tuned! :(</p>",1266071,,,2020-04-23T09:08:41.630,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61383401
833,61396304,2,61371945,2020-04-23T20:21:53.577,0,"<p>problem is apparently the source code of this EMR connector is somewhat outdated and lacks Hive 3.x support recently introduced by AWS for EMR 6.0.</p><p>However, you can find a working 3.1 implementation here, forked from the official EMR connector: <a href=""https://github.com/ramsesrm/emr-dynamodb-connector"" rel=""nofollow noreferrer"">https://github.com/ramsesrm/emr-dynamodb-connector</a></p><p>Installation steps as follow:1- compile the mentioned code (mvn clean package)2- install the 3 JARs in your hive.aux.jars.path, along with aws-java-sdk-core and aws-java-sdk-dynamodb JARs from AWS (shim JARs are not required), 5 in total.</p><p>Tha's it. Don't forget to specify the region as a TBLPROPERTIES if you're not using the default US one.</p>",13394093,13394093,2020-04-28T15:08:49.427,2020-04-28T15:08:49.427,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61396304
834,61475242,2,61383190,2020-04-28T07:53:19.803,3,"<h2><code>EC2</code> Security groups</h2><p>There is a <a href=""https://us-west-1.console.aws.amazon.com/ec2/v2/home?region=us-west-1#SecurityGroups:"" rel=""nofollow noreferrer"">security group</a> on your screen.</p><p>See more about security groups:</p><ul><li><p><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html#security-group-rules"" rel=""nofollow noreferrer"">EC2 Security Groups</a></p></li><li><p><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html#creating-security-group"" rel=""nofollow noreferrer"">Creating a Security Group</a></p></li></ul><h2><code>CLI</code> for <code>AWS Security groups</code></h2><p>As for <code>CLI</code> for working with <code>AWS Security groups</code>, see this article: <a href=""https://docs.aws.amazon.com/cli/latest/userguide/cli-services-ec2-sg.html"" rel=""nofollow noreferrer"">Creating, Configuring, and Deleting Security Groups for Amazon EC2 - AWS Command Line Interface</a></p><pre><code>$ aws ec2 create-security-group --group-name my-sg --description ""My security group"" --vpc-id vpc-1a2b3c4d{  ""GroupId"": ""sg-903004f8""}$ aws ec2 authorize-security-group-ingress --group-id sg-903004f8 --protocol tcp --port 3389 --cidr 203.0.113.0/24</code></pre><blockquote>  <p>The following command adds another rule to enable SSH to instances in the same security group.</p></blockquote><pre><code>$ aws ec2 authorize-security-group-ingress --group-id sg-903004f8 --protocol tcp --port 22 --cidr 203.0.113.0/24</code></pre><blockquote>  <p>To view the changes to the security group, run the <a href=""https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-security-groups.html"" rel=""nofollow noreferrer"">describe-security-groups</a> command.</p></blockquote><pre><code>$ aws ec2 describe-security-groups --group-ids `sg-903004f8`</code></pre><p>O/P is:</p><pre><code>{  ""SecurityGroups"": [  {  ""IpPermissionsEgress"": [  {  ""IpProtocol"": ""-1"",  ""IpRanges"": [  {  ""CidrIp"": ""0.0.0.0/0""  }  ],  ""UserIdGroupPairs"": []  }  ],  ""Description"": ""My security group""  ""IpPermissions"": [  {  ""ToPort"": 22,  ""IpProtocol"": ""tcp"",  ""IpRanges"": [  {  ""CidrIp"": ""203.0.113.0/24""  }  ]  ""UserIdGroupPairs"": [],  ""FromPort"": 22  }  ],  ""GroupName"": ""my-sg"",  ""OwnerId"": ""123456789012"",  ""GroupId"": ""sg-903004f8""  }  ]}</code></pre><h2>P.S. <code>awless.io</code> - A Mighty CLI for AWS</h2><p>There is also a bit outdated but still convenient <code>CLI</code> tool: <a href=""https://github.com/wallix/awless"" rel=""nofollow noreferrer"">wallix/awless: A Mighty CLI for AWS</a></p><blockquote>  <p>A Mighty CLI for AWS <a href=""http://awless.io/"" rel=""nofollow noreferrer"">http://awless.io/</a></p></blockquote><p>Here the <a href=""https://medium.com/@hbbio/awless-io-a-mighty-cli-for-aws-a0d48bdb59a4"" rel=""nofollow noreferrer"">Medium post about it</a></p>",5720818,5720818,2020-04-28T09:35:03.107,2020-04-28T09:35:03.107,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61475242
835,61498340,2,61464170,2020-04-29T09:20:06.753,1,"<p>No, i think not.</p><p>Currently you have to aggregate the data anyways because it is only possible to request data for one day per request. Performance data is only returned for datasets that have at least one impression. So on campaign level it is very hard to reach row limitations if there are any (i did not yet reached any limit with the reporting service).</p><p>For the keyword report i get definitly a lot more than just 100 rows (just tested).</p><p>And btw.: API endpoints beginning with /v2/hsa are deprecated. Use /v2/sb instead.<a href=""https://advertising.amazon.com/API/docs/en-us/info/release-notes"" rel=""nofollow noreferrer"">https://advertising.amazon.com/API/docs/en-us/info/release-notes</a></p>",850547,,,2020-04-29T09:20:06.753,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61498340
836,61520167,2,53233499,2020-04-30T09:27:21.150,0,"<p>As follow up on @jarmod's answer:</p><p>If you want to call <em>update_item</em> with a String Set, then you'll insert a set via <em>ExpressionAttributeValues</em> property like shown below:</p><pre><code>entry = table.put_item(  ExpressionAttributeNames={  ""#begintime"": ""begintime"",  ""#description"": ""description"",  ""#endtime"": ""endtime"",  ""#name"": ""name"",  ""#type"": ""type"",  ""#weekdays"": ""weekdays""  },  ExpressionAttributeValues={  "":begintime"": '09:00',  "":description"": 'Hello there',  "":endtime"": '14:00',  "":name"": 'james',  "":type"": ""period"",  "":weekdays"": set(['mon', 'wed', 'fri'])  },  UpdateExpression=""""""  SET #begintime= :begintime,  #description = :description,  #endtime = :endtime,  #name = :name,  #type = :type,  #weekdays = :weekdays  """""")</code></pre><p>(Hint: Usage of <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LegacyConditionalParameters.html"" rel=""nofollow noreferrer"">AttributeUpdates</a> (related <em>Item</em>s equivalent for <em>put_item</em> calls) is deprecated, therefore I recommend using <em>ExpressionAttributeNames</em>, <em>ExpressionAttributeValues</em> and <em>UpdateExpression</em>).</p>",3680249,,,2020-04-30T09:27:21.150,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61520167
837,61576684,2,21575428,2020-05-03T15:05:11.457,1,"<p>I think the <strong>aws-cloudfront-sign</strong> package is deprecated now. You can use this package<a href=""https://www.npmjs.com/package/aws-sdk"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/aws-sdk</a></p><p>Here is the link to know how you can use this:-</p><p><a href=""https://medium.com/roam-and-wander/using-cloudfront-signed-urls-to-serve-private-s3-content-e7c63ee271db"" rel=""nofollow noreferrer"">https://medium.com/roam-and-wander/using-cloudfront-signed-urls-to-serve-private-s3-content-e7c63ee271db</a></p>",9358652,,,2020-05-03T15:05:11.457,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61576684
838,61632539,2,61619575,2020-05-06T10:10:09.033,0,"<p>I finally managed to find a solution to the issue, albeit it is not explaining the strange behavior with the charts that I explained in the question.</p><p>My problem was similar to what Abhinaya suggested in her response. The Lambda function was not sending the signal properly because of a programming error. Essentially, I took the code from <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html"" rel=""nofollow noreferrer"">the documentation</a> (the one for Python 3, second fragment starting by the end) and apparently I mistakenly removed the line for retrieving the <code>ResponseURL</code>.  Of course, that was failing.</p><p>A side-comment about this: be careful when using Python's <code>cfnresponse</code> library or even the code snippet I linked in the documentation. It relies on <code>botocore.vendored</code> which was deprecated and no longer exist in latest <code>botocore</code> releases. Therefore, it will fail if your code relies on new versions of this library (as in my case). A simple solution is to replace <code>botocore.vendored.requests</code> with the <code>requests</code> library.</p><p>Still, there is some strange behavior that I cannot understand. On creation, the Lambda function is not recording anything to CloudWatch and there is this strange behavior in the charts that I explained in my question. However, this only happens on creation. If the function is manually invoked, or is invoked as part of the delete process (when removing the CFN stack), then it does write to CloudWatch. Therefore, the problem only occurs in the first invokation, apparently.</p><p>Best.</p>",2377885,,,2020-05-06T10:10:09.033,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61632539
839,61679858,2,61679775,2020-05-08T13:05:32.290,1,"<p>Try updating your version of the CLI, it is likely out of date if the verb does not display yet.</p>",13460933,,,2020-05-08T13:05:32.290,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61679858
840,61705390,2,61667335,2020-05-09T23:55:47.710,1,"<p>Just adding to the existing comment. SMTPClient is actually obsolete/deprecated but is not being marked as so. See comments here (<a href=""https://github.com/dotnet/dotnet-api-docs/issues/2986#issuecomment-430805681"" rel=""nofollow noreferrer"">https://github.com/dotnet/dotnet-api-docs/issues/2986#issuecomment-430805681</a>)</p><p>Essentially it boils down to SmtpClient hasn't been updated in years and is missing many features. The .NET Core team wanted to mark it as obsolete, but some developers have existing projects with ""Warnings as Errors"" turned on. It would instantly make any project that is using SmtpClient with Warnings as Errors turned on suddenly stop building. So... It's kinda deprecated, but not being marked so in some official docs. </p><p>MailKit is actually being pushed by Microsoft full stop for people to use when it comes to Email. Much to some developers chargrin who don't want to use a third party library for such a ""simple"" and common feature. Just in my personal experience, I think Mailkit is great and super easy to use. A quick guide to getting up and running is here : <a href=""https://dotnetcoretutorials.com/2017/11/02/using-mailkit-send-receive-email-asp-net-core/"" rel=""nofollow noreferrer"">https://dotnetcoretutorials.com/2017/11/02/using-mailkit-send-receive-email-asp-net-core/</a></p>",177516,,,2020-05-09T23:55:47.710,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61705390
841,61718592,2,61718277,2020-05-10T20:53:59.283,2,"<p>You question is very well described. Thanks for the little graph you drew to help clarify the overall architecture. After reading your question, here are the things that I want to point out.</p><ol><li><p>The link to the CloudFront data transfer price is very outdated. That blog post was written by Jeff Barr in 2010. The latest CloudFront pricing page is linked <a href=""https://aws.amazon.com/cloudfront/pricing/"" rel=""nofollow noreferrer"">here</a>. </p></li><li><p>The data transfer from CloudFront out to the origin S3 is not free. This is listed in ""Regional Data Transfer Out to Origin (per GB)"" section. In your region, it's $0.02 per GB. Same thing applies to the data from CloudFront to ALB.</p></li><li><p>You said ""within the same region, there should be no charge between an EC2 and an RDS DB Instance"". This is not correct. Only the data transfer between RDS and EC2 Instances <strong>in the same Availability Zone</strong> is free. [<a href=""https://aws.amazon.com/rds/mysql/pricing/?pg=pr&amp;loc=2"" rel=""nofollow noreferrer"">ref</a>]</p></li></ol><p>Also be aware that S3 has request and object retrieval fees. It will still apply in your architecture.</p><p>In addition, here is a nice graph made by the folks in <a href=""https://www.lastweekinaws.com/"" rel=""nofollow noreferrer"">lastweekinaws</a> which visually listed all the AWS data transfer costs.</p><p><img src=""https://19x50e48lpyz2s9tzz3qjjsn-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/dbg010-datatxfr-infographic-20191219.jpg"" alt=""""></p><p>Source: <a href=""https://www.lastweekinaws.com/blog/understanding-data-transfer-in-aws/"" rel=""nofollow noreferrer"">https://www.lastweekinaws.com/blog/understanding-data-transfer-in-aws/</a></p>",10692493,10692493,2020-05-10T21:55:31.063,2020-05-10T21:55:31.063,4,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61718592
842,61740781,2,61719005,2020-05-11T22:52:18.423,2,"<p>There are several SDK's that can be used with Cognito.</p><p>You can use the AWS js SDK to make low level API calls but you will lose the benefits that the Cognito Client Side SDKS provide like session management, caching of tokens and SRP calculations.<a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityServiceProvider.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityServiceProvider.html</a></p><p>You can also use the Cognito Identity SDK which Amplify uses under the hood:<a href=""https://www.npmjs.com/package/amazon-cognito-identity-js"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/amazon-cognito-identity-js</a></p><p>The Auth SDK if you are going to be integrating with the OAuth endpoints, which is now deprecated but can still be used or referenced, Amplify also uses or has similar functionality.<a href=""https://github.com/amazon-archives/amazon-cognito-auth-js"" rel=""nofollow noreferrer"">https://github.com/amazon-archives/amazon-cognito-auth-js</a></p><p>And then finally Amplify which is the go to, feature rich client side SDK which for the majority of usecases should be the SDK of choice in my opinion.</p>",12750871,,,2020-05-11T22:52:18.423,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61740781
843,61807402,2,61807279,2020-05-14T21:07:54.893,0,"<p>You might have an outdated version of setuptools. Try the following script:</p><pre><code>#!/bin/bashsudo pip3 install --upgrade setuptoolssudo pip3 install --user matplotlib pandas pyarrow pyspark</code></pre>",158668,158668,2020-05-14T21:12:59.773,2020-05-14T21:12:59.773,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61807402
844,61848227,2,61845389,2020-05-17T07:13:48.327,0,<p>I'd say it really does not matter as both share 95% of the same content. There are many good resources out there for it but do be aware the 001 exam is soon due to be deprecated so will not be available to sit (you'll still keep the cert).</p>,13460933,,,2020-05-17T07:13:48.327,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61848227
845,61854149,2,61842861,2020-05-17T15:18:52.527,1,"<p>The usual way to get the DDL from a Glue table is to run <a href=""https://docs.aws.amazon.com/athena/latest/ug/show-create-table.html"" rel=""nofollow noreferrer""><code>SHOW CREATE TABLE foo</code></a>, but since Glue has created a table that does not work in Athena, I assume this fails.</p><p>In your specific case you should just us the schema suggested by the <a href=""https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html"" rel=""nofollow noreferrer"">Athena documentation on querying CloudTrail</a>. Apart from the partitioning it's as good as it gets. As you know, there are free-form properties that are service-dependent in CloudTrail events, and there is no schema that will capture everything (even if there was, as soon as a new service was launched it would be outdated). Stick with <code>string</code> for the columns corresponding to the free-form properties and use <a href=""https://prestodb.github.io/docs/0.172/functions/json.html"" rel=""nofollow noreferrer"">Athena/Presto's JSON functions</a> to query these columns.</p><p>I would make one small modification to the schema and have a different set of partition keys. The docs use region, year, month, day. You may want to add account ID and organization ID (if your trail is an organization trail) Š—– but more importantly you should not have year, month, and date as separate partition keys, it makes querying date ranges unnecessarily complicated. There is no reason not to simply use ""date"" (or ""dt"" if you want to avoid having to quote it), typed either as a <code>string</code> or a <code>date</code> as the partition key, and add partitions like this:</p><pre><code>ALTER TABLE cloud_trail ADD PARTITION (account_id = '1234567890', region = 'us-east-1', dt = '2020-05-17')LOCATION 's3://trails/AWSLogs/Account_ID/CloudTrail/us-east-1/2020/05/17/'</code></pre><p>Just because something is separated by slashes in the S3 key doesn't mean it has to be separate partition keys. Using a single key makes it easy to do range queries like <code>WHERE ""date"" BETWEEN '2019-12-01' AND '2020-06-01'</code>.</p><p>Glue Crawlers are pretty terrible when you don't hit the use case they were intended for spot on, and I'm not surprised at all that it creates an unusable schema. In a way it's pretty amazing that there are so many cases of AWS services that predate Glue and where Glue just creates unusable results when used on their outputs.</p><p>For CloudTrail the schema-discovery aspect of Glue Crawlers is not necessary, and will probably mostly cause problems due to the free-form properties. The other aspect, adding new partitions, can instead be solved with a Lambda function running once per day adding the next day's partitions (since tomorrow's partitions are deterministic you don't have to wait until there is data to add partitions).</p>",1109,,,2020-05-17T15:18:52.527,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61854149
846,61861373,2,61857002,2020-05-18T02:12:00.830,0,"<p>The cognito sdk is specific to aws and is not a general purpose oauth sdk. Also note that this sdk (<a href=""https://github.com/amazon-archives/amazon-cognito-identity-js"" rel=""nofollow noreferrer"">https://github.com/amazon-archives/amazon-cognito-identity-js</a>) is now deprecated in favour of aws amplify js (<a href=""https://github.com/aws-amplify/amplify-js"" rel=""nofollow noreferrer"">https://github.com/aws-amplify/amplify-js</a>).</p><p>If you want to use OAuth and openId connect approach, I would recommend to use oidc-client-js (<a href=""https://github.com/IdentityModel/oidc-client-js"" rel=""nofollow noreferrer"">https://github.com/IdentityModel/oidc-client-js</a>).  </p>",139799,,,2020-05-18T02:12:00.830,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61861373
847,61868761,2,61863957,2020-05-18T11:41:15.863,1,"<p>Sorted elements are sorted within a partition. You would need to have all results on the same partition.</p><p>But obviously you don't want just one partition, you are back to an SQL database. The uncomfortable and uneasy way this is done in DynamoDB is using Streams. When you have new elements or updates, you can check if those elements are in the top <code>N</code> positions. If they are then replace that value, for example, say you have people with money:</p><pre><code>PK  Attributes:  #Entity#21  name=Fred.  money=5,000 #Entity#22  name=Bob.   money=10,000 #Entity#23  name=Smith.   money=1,000 ...</code></pre><p>Then we can keep track of the top 10 richest people:</p><pre><code>PK  SORT   Attributes:  #Money#Highest  1   id=#Entity#22  value=10,000#Money#Highest  2   id=#Entity#102   value=9,000...</code></pre><p>Then when you want the richest people, you do a query with <code>PK=#Money#Highest</code>. You might also copy more attributes in depending on your query. This is pretty much the go, if you want to calculate the top <code>something</code> across partitions you setup streams and do it yourself. Note that though these totals will be out of date by some seconds depending on your stream settings. You stream Lambda would be something like:</p><pre><code>const handler = (event, context, callback) =&gt; {  event.Records.forEach((ev, i) =&gt; {  if (ev.eventName === ""INSERT"" || ev.eventName === ""UPDATE"" || ) {  // TODO  }  }  }</code></pre><p>Pretty annoying I know! But this is the weird way this stuff is implemented. But it is extremely quick as you are only ever retrieving pre-calculated values. And that is the whole way which you work with Dynamo, Storage is cheap, compute expensive, optimize compute, and duplicate data as you need because hey it's cheap anyway.</p>",4033292,,,2020-05-18T11:41:15.863,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61868761
848,61892065,2,61883615,2020-05-19T13:18:58.450,4,"<p>You can get almost the result you need by simply projecting the results. I will add an example at the end of how to flatten it even more. You should be able to tweak this query any way you need adding more <code>valueMap</code> steps etc. This does not yield a single list but groups each edge with its properties, label and ID. </p><p>Note that I used <code>valueMap(true)</code> which is deprecated and the new form is <code>valueMap().with(WithOptions.tokens)</code>. Either will still work currently. The advantage of this approach is that no tracking of the <code>path</code> is needed which in general is more efficient in terms of memory usage etc. by the query engine.</p><pre><code>gremlin&gt; graph = TinkerFactory.createModern()==&gt;tinkergraph[vertices:6 edges:6]gremlin&gt; g = graph.traversal()==&gt;graphtraversalsource[tinkergraph[vertices:6 edges:6], standard]gremlin&gt; g.V().has('name','marko').  outE().  project('from','edge','to').  by(outV()).  by(valueMap(true)).  by(inV())==&gt;[from:v[1],edge:[id:9,label:created,weight:0.4],to:v[3]]==&gt;[from:v[1],edge:[id:7,label:knows,weight:0.5],to:v[2]]==&gt;[from:v[1],edge:[id:8,label:knows,weight:1.0],to:v[4]]  </code></pre><p>If you want to flatten this result into a single list you can just add a little more to the query:</p><pre><code>gremlin&gt; g.V().has('name','marko').  outE().  project('from','edge','to').  by(outV()).  by(valueMap(true)).  by(inV()).  union(select('edge').unfold(),  project('from').by(select('from')).unfold(),  project('to').by(select('to')).unfold()).fold() [id=9,label=created,weight=0.4,from=v[1],to=v[3],id=7,label=knows,weight=0.5,from=v[1],to=v[2],id=8,label=knows,weight=1.0,from=v[1],to=v[4]]  </code></pre><p>Lastly if you wanted a series of such lists back rather than one big one you can wrap the <code>union</code> step in <code>local</code> scope.</p><pre><code>gremlin&gt; g.V().has('name','marko').  outE().  project('from','edge','to').  by(outV()).  by(valueMap(true)).  by(inV()).local(  union(select('edge').unfold(),  project('from').by(select('from')).unfold(),  project('to').by(select('to')).unfold()).fold())  ==&gt;[id=9,label=created,weight=0.4,from=v[1],to=v[3]]==&gt;[id=7,label=knows,weight=0.5,from=v[1],to=v[2]]==&gt;[id=8,label=knows,weight=1.0,from=v[1],to=v[4]]  </code></pre>",5442034,5442034,2020-05-19T14:43:34.003,2020-05-19T14:43:34.003,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61892065
849,61895560,2,61893719,2020-05-19T16:04:33.693,14,"<p>It looks like your existing pickle save file (<code>model_d2v_version_002</code>) encodes a reference module in a non-standard location Š—– a <code>joblib</code> that's in <code>sklearn.externals.joblib</code> rather than at top-level. </p><p>The current <code>scikit-learn</code> documentation only talks about a top-level <code>joblib</code> Š—– eg in <a href=""https://scikit-learn.org/stable/modules/model_persistence.html"" rel=""noreferrer"">3.4.1 Persistence example</a> Š—– but I do see a <a href=""https://github.com/EpistasisLab/tpot/issues/869"" rel=""noreferrer"">reference in someone else's old issue to a DeprecationWarning</a> in <code>scikit-learn</code> version 0.21 about an older <code>scikit.external.joblib</code> variant going away:</p><blockquote>  <p>Python37\lib\site-packages\sklearn\externals\joblib_init_.py:15:  DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and  will be removed in 0.23. Please import this functionality directly  from joblib, which can be installed with: pip install joblib. If this  warning is raised when loading pickled models, you may need to  re-serialize those models with scikit-learn 0.21+.</p></blockquote><p>'Deprecation' means marking something as inadvisable to rely-upon, as it is likely to be discontinued in a future release (often, but not always, with a recommended newer way to do the same thing). </p><p>I suspect your <code>model_d2v_version_002</code> file was saved from an older version of <code>scikit-learn</code>, and you're now using <code>scikit-learn</code> (aka <code>sklearn</code>) version 0.23+ which has totally removed the <code>sklearn.external.joblib</code> variation. Thus your file can't be directly or easily loaded to your current environment.</p><p>But, per the <code>DeprecationWarning</code>, you can probably temporarily use an older <code>scikit-learn</code> version to load the file the old way once, then re-save it with the now-preferred way. Given the warning info, this would probably require <code>scikit-learn</code> version 0.21.x or 0.22.x, but if you know exactly which version your <code>model_d2v_version_002</code> file was saved from, I'd try to use that. The steps would roughly be:</p><ul><li><p>create a temporary working environment (or roll back your current working environment) with the older <code>sklearn</code></p></li><li><p>do imports something like:</p></li></ul><pre><code>import sklearn.external.joblib as extjoblibimport joblib</code></pre><ul><li><p><code>extjoblib.load()</code> your old file as you'd planned, but then immediately re-<code>joblib.dump()</code> the file using the top-level <code>joblib</code>. (You likely want to use a distinct name, to keep the older file around, just in case.)</p></li><li><p>move/update to your real, modern environment, and only <code>import joblib</code> (top level) to use <code>joblib.load()</code> - no longer having any references to `sklearn.external.joblib' in either your code, or your stored pickle files. </p></li></ul>",130288,130288,2020-05-19T17:41:36.083,2020-05-19T17:41:36.083,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61895560
850,61910173,2,61886239,2020-05-20T09:56:00.733,1,"<ul><li>Access to S3 data in EMR should be with with their connector and <code>s3://</code> URLs; any other schema references code they don't support.</li><li>You get the access of the IAM role the VM/container was deployed with. Want to access a specific bucket, choose the right role</li></ul><p>It's moot, but the s3n connector (obsolete, unsupported) doesn't support JCEKs files</p>",2261274,,,2020-05-20T09:56:00.733,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61910173
851,61957641,2,61938052,2020-05-22T14:47:09.187,0,"<p>I got help in the lightbend forum <a href=""https://discuss.lightbend.com/t/alpakka-s3-connection-issue/6551"" rel=""nofollow noreferrer"">here</a>. </p><p>The issue was solved by setting the following parameter: </p><pre><code>alpakka.s3.path-style-access = true</code></pre><p>Since the documentation says, this value is going to be deprecated, I did not consider to specify it.</p><p>In my original post, I outlined two approaches setting the params, one via <code>application.conf</code> and one programmatically via <code>S3Ext</code>. First does work by setting the value as shown above, the second approach looks like this:</p><pre><code>val settings: S3Settings = S3Ext(system).settings  .withEndpointUrl(s3Host)  .withBufferType(MemoryBufferType)  .withCredentialsProvider(credentialsProvider)  .withListBucketApiVersion(ApiVersion.ListBucketVersion2)  .withS3RegionProvider(regionProvider)  .withPathStyleAccess(true)</code></pre><p>Here the last line is crucial, even though I'm getting a deprecation warning.</p><p>But in the end, this was solving the issue.</p>",979457,,,2020-05-22T14:47:09.187,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/61957641
852,62003291,2,62001500,2020-05-25T13:21:25.183,0,"<p>I have in the meantime found the answer to my own question, I had a fatal mistake in my cloudformation attribute mapping:</p><pre><code>AttributeMapping:  apple_sub: ""sub""  email: ""email""</code></pre><ul><li>the apple_sub attribute is nonexistent in my schema. At the same time, the AWS console wrongly shows that the sub attribute is mapped to User pool Username attribute, even though it isn't</li></ul><p>thus, the correct format of my UserPoolAppleIdentityProvider attribute mapping is:</p><pre><code>UserPoolAppleIdentityProvider:  Type: AWS::Cognito::UserPoolIdentityProvider  Properties:  AttributeMapping:  username: ""sub""  email: ""email</code></pre><p>or ideally, delete the username: ""sub"" from the mapping alltogether, since it's obsolete.</p>",5536488,5536488,2020-05-25T14:29:23.883,2020-05-25T14:29:23.883,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62003291
853,62019491,2,62013028,2020-05-26T09:59:58.160,1,"<blockquote>  <p>Does anyone happen to know which it is? Why doesn't the AWS walkthrough need me to install Kube2iam?</p></blockquote><p>Yes, I can authoritatively answer this. In 09/2019 <a href=""https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/"" rel=""nofollow noreferrer"">we launched</a> a feature in EKS called <a href=""https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html"" rel=""nofollow noreferrer"">IAM Roles for Service Accounts</a>. This makes <code>kube2iam</code> and other solutions obsolete since we support least-privileges access control on the pod level now natively.</p><p>Also, yes, the ALB IC walkthrough should be updated.</p>",396567,,,2020-05-26T09:59:58.160,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62019491
854,62049410,2,62049230,2020-05-27T17:37:12.170,1,"<p>If you are using CloudFront : </p><ol><li>Upload your code </li><li>Invalidate cache in cloudfront</li></ol><p>Step to invalidate cache forcefully: </p><ol><li><p>Select the distribution for which you want to invalidate files.</p><p>Choose Distribution Settings.</p><p>Choose the Invalidations tab.</p><p>Choose Create Invalidation.</p></li></ol><p><a href=""https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html</a>For the files that you want to invalidate, enter one invalidation path per line. For information about specifying invalidation paths, see Specifying the Files to Invalidate.</p><p><a href=""https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serving-outdated-content-s3/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serving-outdated-content-s3/</a></p><p>Another way to user versioning in content like js/css/images etc.</p>",6285054,,,2020-05-27T17:37:12.170,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62049410
855,62066189,2,62061528,2020-05-28T13:41:31.777,0,"<p>The C# example provided by AWS is out of date, specifically this serialization package:</p><pre><code>Amazon.Lambda.Serialization.SystemTextJson</code></pre><p>To solve this issue, simply replace the package with this one:</p><pre><code>Amazon.Lambda.Serialization.Json</code></pre><p>and update the assembly attribute like so:</p><pre><code>[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.Json.JsonSerializer))]namespace LambdaFunctions{  ...</code></pre><p>This serialization package is mentioned in the AWS Documentation <a href=""https://docs.aws.amazon.com/lambda/latest/dg/csharp-handler.html"" rel=""nofollow noreferrer"">here</a>, under ""Serializing Lambda functions"". </p><p>However, Amazon have not yet updated the SDK examples to reflect this change (or at least, this example specifically), causing the function to fail when deployed.</p>",5630449,5630449,2020-05-28T13:49:03.713,2020-05-28T13:49:03.713,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62066189
856,62087601,2,62086741,2020-05-29T13:55:04.757,-1,"<p>Could you provide the handler code using this function callServerEndpoint?</p><p>The issue can also be a timeout on the lambda function. It occurs when the post is longer than the timeout set on the lambda function. Cloud watch should trace it by the way.</p><p>PS: The request package is deprecated see <a href=""https://github.com/request/request/issues/3142"" rel=""nofollow noreferrer"">link</a>. It might be interesting to look at axios, superagent or other http client packages</p>",1910637,,,2020-05-29T13:55:04.757,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62087601
857,62098397,2,40383470,2020-05-30T05:50:13.120,0,"<p>botocore.vendored is deprecated and will be removed from Lambda after 2021/01/30.</p><p>Here is an update version</p><pre><code>Type: AWS::Lambda::FunctionProperties:  Code:   ZipFile:   !Sub |  import json, boto3, logging  import cfnresponse  logger = logging.getLogger()  logger.setLevel(logging.INFO)  def lambda_handler(event, context):  logger.info(""event: {}"".format(event))  try:  bucket = event['ResourceProperties']['BucketName']  logger.info(""bucket: {}, event['RequestType']: {}"".format(bucket,event['RequestType']))  if event['RequestType'] == 'Delete':  s3 = boto3.resource('s3')  bucket = s3.Bucket(bucket)  for obj in bucket.objects.filter():  logger.info(""delete obj: {}"".format(obj))  s3.Object(bucket.name, obj.key).delete()  sendResponseCfn(event, context, cfnresponse.SUCCESS)  except Exception as e:  logger.info(""Exception: {}"".format(e))  sendResponseCfn(event, context, cfnresponse.FAILED)  def sendResponseCfn(event, context, responseStatus):  responseData = {}  responseData['Data'] = {}  cfnresponse.send(event, context, responseStatus, responseData, ""CustomResourcePhysicalID"")    Handler: ""index.lambda_handler""  Runtime: python3.7  MemorySize: 128  Timeout: 60  Role: !GetAtt TSIemptyBucketOnDeleteFunctionRole.Arn  </code></pre>",3445514,,,2020-05-30T05:50:13.120,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62098397
858,62099715,2,62054022,2020-05-30T08:18:53.860,1,"<p>The issue has nothing to do with ""old dlls, outdated dependencies"", etc. If you look at the example that you are following, they are not trying to call the default ctor, but instead they are initialising it through a <em>bucket region</em>.</p><p>If you take a look at <a href=""https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/quick-start-s3-1-cross.html"" rel=""nofollow noreferrer"">Simple cross-platform application using the AWS SDK for .NET</a> (or even in the documentation inside the IDE that you are using), you can see that when the default ctor is being used, you need to:</p><blockquote>  <p>Before running this app, credentials must be specified either in the [default] profile or in another profile and then by setting the AWS_PROFILE environment variable. A region must be specified either in the [default] profile or by setting the AWS_REGION environment variable.</p></blockquote><p>The docs also say that App.config can be used. </p><p>But since we will be using it for Xamarin, it will be a lot easier to use some of the others overloads. Here are some of them:</p><p><a href=""https://i.stack.imgur.com/wQPAJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wQPAJ.png"" alt=""enter image description here""></a></p><p>Let's say that you want to use <code>AWSCredentials</code> in order to generate your client. Again, you have many options here:<a href=""https://i.stack.imgur.com/wCJT6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wCJT6.png"" alt=""enter image description here""></a></p><p>NB: When you initialise your client, it is a good practice to specify the <code>RegionEndpoint</code>. In most of the cases, if you forget, you will get a <code>AmazonClientException: No RegionEndpoint or ServiceURL configured</code> so it will remind you that it is required. </p><p>Edit: Due to an update in your question, here is an update from me:The same rules apply for there ctors, that you have tried with. They are looking for credentials in config files, that are not present for a Xamarin.Forms app. In order to use the client, it needs to know about its credentials. If you need to use the client, then provide it with some credentials, during initialisation - use either some of the other AWSCredentials - <code>Basic</code>, <code>Federated</code>, etc, or use the simples one - with <code>accessKeyId</code> + <code>accessKey</code>. </p><p>If you are curious why the ctors, that you have tried, are not working, or what they are doing behind-the-scenes, their SDK is open-sourced <a href=""https://github.com/aws/aws-sdk-net"" rel=""nofollow noreferrer"">here</a>. The empty ctor's code is <a href=""https://github.com/aws/aws-sdk-net/blob/master/sdk/src/Services/S3/Generated/_mobile/AmazonS3Client.cs#L66"" rel=""nofollow noreferrer"">here</a> and the more interesting <code>FallbackCredentialsFactory</code> <a href=""https://github.com/aws/aws-sdk-net/blob/master/sdk/src/Core/Amazon.Runtime/Credentials/FallbackCredentialsFactory.cs"" rel=""nofollow noreferrer"">here</a>.</p>",4090219,4090219,2020-06-01T20:19:59.510,2020-06-01T20:19:59.510,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62099715
859,62124324,2,56531567,2020-06-01T01:10:01.477,1,"<p>Windows <em>Network Discovery</em> actually opens a bunch of different inbound/outbound ports on the Windows Server instance. A (somewhat) outdated list is available in some scientific publications and books, e.g. [1]:</p><p><strong>Inbound</strong></p><blockquote>  <p>Š_¾ Network Discovery (LLMNR-UDP-In) Creates an inbound rule to allow Link Local Multicast Name Resolution on UDP port 5355.</p>   <p>Š_¾  Network Discovery (NB-Datagram-ln) Creates an inbound rule to allow NetBIOS Datagram transmission and reception on UDP port 138.</p>   <p>Š_¾  Network Discovery (NB-Name-In) Creates an inbound rule to allow NetBIOS Name Resolution on UDP port 137.</p>   <p>Š_¾  Network Discovery (Pub-WSD-In) Creates an inbound rule to discover devices via Function Discovery on UDP port 3702.</p>   <p>Š_¾  Network Discovery (SSDP-In) Creates an inbound rule to allow use of the Simple Service Discovery Protocol on UDP port 1900.</p>   <p>Š_¾  Network Discovery (UPnP-In) Creates an inbound rule to allow use of Universal Plug and Play on TCP port 2869.</p>   <p>Š_¾  Network Discovery (WSD Events-In) Creates an inbound rule to allow WSDAPI Events via Function Discovery on TCP port 5357.</p>   <p>Š_¾  Network Discovery (WSD EventsSecure-In) Creates an inbound rule to allow Secure WSDAPI Events via Function Discovery on TCP port 5358.</p>   <p>Š_¾  Network Discovery (WSD-In) Creates an inbound rule to discover devices via Function Discovery on UDP port 3702.</p></blockquote><p><strong>Outbound</strong></p><blockquote>  <p>Š_¾ Network Discovery (LLMNR-TCP-Out) Creates an outbound rule to allow LLMNIL on TCP port 5355.</p>   <p>Š_¾  Network Discovery (LLMNR-UDP-Out) Creates an outbound rule to allow LLMNR on UDP port 5355.</p>   <p>Š_¾  Network Discovery (NB-Datagram-Out) Creates an outbound rule to allow NetBIOS Datagram transmission and reception on UDP port 138.</p>   <p>Š_¾  Network Discovery (NB-Name-Out) Creates an outbound rule to allow NetBIOS Name Resolution on UDP port 137.</p>   <p>Š_¾  Network Discovery (Pub WSD-Out) Creates an outbound rule to discover devices via Function Discovery on UDP port 3702.</p>   <p>Š_¾  Network Discovery (SSDP-Out) Creates an outbound rule to allow use of the Simple Service Discovery Protocol on UDP port 1900.</p>   <p>Š_¾  Network Discovery (UPnPHost-Out) Creates an outbound rule to allow the use of Universal Plug and Play over TCP (all ports).</p>   <p>Š_¾  Network Discovery (UPnP-Out) Creates a second outbound rule to allow the use of Universal Plug and Play over TCP (all ports).</p>   <p>Š_¾  Network Discovery (WSD Events-Out) Creates an outbound rule to allow WSDAPI Events via Function Discovery on TCP port 5357.</p>   <p>Š_¾  Network Discovery (WSD EventsSecure-Out) Creates an outbound rule to allow for Secure WSDAPI Events via Function Discovery on TCP port 5358.</p>   <p>Š_¾  Network Discovery (WSD-Out) Creates an outbound rule to discover devices via Function Discovery on UDP port 3702.</p></blockquote><p>If your security groups for the EC2 instance are set up correctly, there should probably be no real security implications. Inbound ports used by several network discovery features should be blocked by AWS Security Groups by default. </p><p>What I honestly do not know: Whether the Windows services create outbound traffic which makes the EC2 instance visible to other instances inside the same VPC subnet. That is definitely possible... Maybe there is someone reading this thread and knows this for sure??</p><p>Btw.: I found two discussions on the offical AWS forums. Maybe they are useful to someone reading this thead: [2][3].</p><h3>References</h3><p>[1] <a href=""https://books.google.de/books?id=O3XsuCoYj6kC&amp;pg=PA200&amp;lpg=PA200&amp;dq=%22Enabling+Network+Discovery+opens+the+following+inbound+ports+in+the+Windows+Firewall:%22&amp;source=bl&amp;ots=jEHw02AMfR&amp;sig=ACfU3U20QrJMbcOgOcRp38uDnh7ZSqwtdg&amp;hl=de&amp;sa=X&amp;ved=2ahUKEwiu2Pm0sd_pAhUiw8QBHS2IBQYQ6AEwAXoECAoQAQ#v=onepage&amp;q=%22Enabling%20Network%20Discovery%20opens%20the%20following%20inbound%20ports%20in%20the%20Windows%20Firewall%3A%22&amp;f=false"" rel=""nofollow noreferrer"">Google Books: How to Cheat at Microsoft Vista Administration</a><br>[2] <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=62760"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=62760</a><br>[3] <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=110844"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=110844</a></p>",10473469,,,2020-06-01T01:10:01.477,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62124324
860,62125290,2,59430339,2020-06-01T03:32:34.017,0,"<p>Im my opinion I would not use an email address as a key.</p><p>How would this affect the system if the user decided to change the email address in the future or for some reason the email address simply becomes obsolete.</p><p>The sub I think would be a decent choice to use as a key. However, whenever I am thinking about primary keys I'm always thinking a million years in the future.</p><p>Ultimately I would choose a key that will forever be unique throughout the org eliminating the possibility of any future conflicts or in your case, confidentiality issues. </p>",5795395,5104596,2020-06-01T05:35:51.267,2020-06-01T05:35:51.267,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62125290
861,62340675,2,62339253,2020-06-12T08:41:50.330,0,"<p>Based on the comments, the issue was not the command used. <strong>The command was correct</strong>. The problem was with the outdated AWS CLI used. </p><p>The solution was to <strong>updated the AWS CLI</strong>.</p>",248823,,,2020-06-12T08:41:50.330,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62340675
862,62346744,2,57099238,2020-06-12T14:49:37.073,1,"<p>Did you resolve it? I struggled for weeks but with a 502 after following exactly the instructions. </p><p>Then I found this <a href=""https://stackoverflow.com/questions/39460892/gunicorn-no-module-named-myproject"">post</a> and deduced it was a typo or outdated instructions (<code>*/wsgi.py</code> should be <code>*.wsgi:application</code>). </p><p>Created a <a href=""https://github.com/awsdocs/aws-elastic-beanstalk-developer-guide/pull/86"" rel=""nofollow noreferrer"">pull request</a> to AWS docs and they've updated since.</p>",4517898,1324,2020-06-12T17:04:39.437,2020-06-12T17:04:39.437,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62346744
863,62407506,2,62269305,2020-06-16T11:34:53.313,0,"<p>if you want to monitor Application Load Balancer, not a deprecated Classic one, use this configuration</p><pre><code>---region: us-east-1metrics:- aws_namespace: AWS/ApplicationELB  aws_metric_name: HTTPCode_Target_5XX_Count  aws_dimensions: [AvailabilityZone, LoadBalancer, TargetGroup]  aws_statistics: [Sum]</code></pre>",13751405,,,2020-06-16T11:34:53.313,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62407506
864,62416042,2,62330876,2020-06-16T19:18:49.480,1,"<p>Currently, in Amazon Connect, there is no way to delete or, for that matter, disable a contact flow.</p><p>From the <a href=""https://docs.aws.amazon.com/connect/latest/adminguide/create-contact-flow.html"" rel=""nofollow noreferrer"">Administrator Guide for Amazon Connect</a>:</p><blockquote>  <p><strong>You can't delete a contact flow.</strong> To get obsolete contact flows out of your way, we recommend appending <strong>zzTrash_</strong> to their name. This will also make them easy to find should you want to reuse them in the future.</p></blockquote>",13758310,3025856,2020-06-16T21:14:21.543,2020-06-16T21:14:21.543,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62416042
865,62425495,2,62425263,2020-06-17T09:13:20.243,0,"<p>I have run your template with <a href=""https://github.com/aws-cloudformation/cfn-python-lint"" rel=""nofollow noreferrer"">CloudFormation Linter</a> and got the following warnings:</p><pre><code>E3012 Property Resources/SAVPC/Properties/EnableDnsSupport should be of type Booleanlogesh.template:33:9E3012 Property Resources/SAVPC/Properties/EnableDnsHostnames should be of type Booleanlogesh.template:34:9W3010 Don't hardcode eu-west-1a for AvailabilityZoneslogesh.template:45:9E3012 Property Resources/PublicSubnetA/Properties/MapPublicIpOnLaunch should be of type Booleanlogesh.template:46:9W3010 Don't hardcode eu-west-1b for AvailabilityZoneslogesh.template:58:9E3012 Property Resources/PublicSubnetB/Properties/MapPublicIpOnLaunch should be of type Booleanlogesh.template:59:9W3010 Don't hardcode eu-west-1a for AvailabilityZoneslogesh.template:71:9W3010 Don't hardcode eu-west-1b for AvailabilityZoneslogesh.template:83:9E3012 Property Resources/SAInstance1/Properties/DisableApiTermination should be of type Booleanlogesh.template:133:9E3012 Property Resources/SAInstance1/Properties/Monitoring should be of type Booleanlogesh.template:137:9E3012 Property Resources/SAInstance1/Properties/NetworkInterfaces/0/AssociatePublicIpAddress should be of type Booleanlogesh.template:145:11E3012 Property Resources/SAInstance1/Properties/NetworkInterfaces/0/DeleteOnTermination should be of type Booleanlogesh.template:146:11E3012 Property Resources/SAInstance1/Properties/NetworkInterfaces/0/DeviceIndex should be of type Stringlogesh.template:148:11E3012 Property Resources/SANACLEntry1/Properties/Egress should be of type Booleanlogesh.template:200:9E3012 Property Resources/SANACLEntry1/Properties/Protocol should be of type Integerlogesh.template:201:9E3012 Property Resources/SANACLEntry1/Properties/RuleNumber should be of type Integerlogesh.template:203:9E3012 Property Resources/SANACLEntry2/Properties/Protocol should be of type Integerlogesh.template:210:9E3012 Property Resources/SANACLEntry2/Properties/RuleNumber should be of type Integerlogesh.template:212:9W3005 Obsolete DependsOn on resource (SAIGW), dependency already enforced by a ""Ref"" at Resources/publicroute/Properties/GatewayId/Reflogesh.template:275:7</code></pre><p>You might want to correct these problems before proceeding.</p>",303810,,,2020-06-17T09:13:20.243,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62425495
866,62497316,2,60836878,2020-06-21T10:31:34.100,0,"<p>I had the same error myself, but there is a fast solution to it which is not obvious to the AWS novices as myself.</p><p>Apparently there can be a console image running an outdated version of the CLI.To fix, do as follows:</p><ol><li>Go to build settings</li><li>Scroll to bottom, to Build Image Settings</li><li>Click <code>[Edit]</code>-button</li><li>Click  <code>[Add package version override]</code></li><li>Choose <code>Amplify CLI</code>, version latest, and <code>[Save]</code></li></ol><p>The end result should look like this:<a href=""https://i.stack.imgur.com/qcJee.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qcJee.png"" alt=""Build image settings"" /></a></p><p>Then just trigger another build, which takes a couple of minutes. The build takes longer the first time after adding to the backend, such as after adding api or auth, but will be a bit faster in your future builds</p><p><em>Credit goes to @jimtheplant on <code>AWS Amplify</code> Discord community who helped me with this issue</em></p>",10011801,10011801,2020-06-21T10:36:36.703,2020-06-21T10:36:36.703,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62497316
867,62509428,2,53420385,2020-06-22T07:30:25.077,3,"<p>I know this is kind of late, but hopefully this will help others like me who came to this question even in 2020 :)</p><p>Using your code's example, you should try out this</p><pre><code>AWSSimpleSystemsManagement ssm =   AWSSimpleSystemsManagementClientBuilder  .standard()  .withRegion(region)  .withCredentials(new EC2ContainerCredentialsProviderWrapper())  .build();</code></pre><p>The <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/EC2ContainerCredentialsProviderWrapper.html"" rel=""nofollow noreferrer"">EC2ContainerCredentialsProviderWrapper</a> implements AWSCredentialsProvider, and loads credentials from Amazon Container (e.g. EC2) Credentials, solving them in the following order:</p><blockquote><ol><li>If environment variable &quot;AWS_CONTAINER_CREDENTIALS_RELATIVE_URI&quot; isset (typically on EC2) it is used to hit the metadata service at thefollowing endpoint: <a href=""http://169.254.170.2"" rel=""nofollow noreferrer"">http://169.254.170.2</a></li><li>If environment variable &quot;AWS_CONTAINER_CREDENTIALS_FULL_URI&quot; is set it is used to hit a metadata service at that URI.Optionally an authorization token can be included in the &quot;Authorization&quot; header of the request by setting the &quot;AWS_CONTAINER_AUTHORIZATION_TOKEN&quot; environment variable.</li><li>If neither of the above environment variables are specified credentials are attempted to be loaded from Amazon EC2 Instance Metadata Service using the InstanceProfileCredentialsProvider.</li></ol></blockquote><p>This is similar to the deprecated<a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/ContainerCredentialsProvider.html"" rel=""nofollow noreferrer"">ContainerCredentialsProvider()</a> :</p><blockquote><p>By default, the URI path is retrieved from the environment variable &quot;AWS_CONTAINER_CREDENTIALS_RELATIVE_URI&quot; in the container's environment.</p></blockquote><hr /><p>Update: If you are not sure which mechanism will be used or want to be compatible with environment variables, system properties, profile credentials and container credentials, you could use the <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html"" rel=""nofollow noreferrer""><code>DefaultAWSCredentialsProviderChain</code></a> which will make sure to try out all options (as @Imran pointed out in a comment):</p><pre><code>AWSSimpleSystemsManagement ssm =   AWSSimpleSystemsManagementClientBuilder  .standard()  .withRegion(region)  .withCredentials(new DefaultAWSCredentialsProviderChain())  .build();</code></pre><p>For example, the implementation for the 1.11 SDK looks like this (it basically tries all the options until it finds one that works):</p><pre><code>public DefaultAWSCredentialsProviderChain() {  super(new EnvironmentVariableCredentialsProvider(),  new SystemPropertiesCredentialsProvider(),  new ProfileCredentialsProvider(),  new EC2ContainerCredentialsProviderWrapper());}</code></pre><p>This way you are compatible with new versions that may introduce another type of authentication or if one option gets deprecated.</p>",13151296,906264,2020-08-13T02:22:01.540,2020-08-13T02:22:01.540,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62509428
868,62547048,2,62512786,2020-06-24T03:16:52.573,1,"<p>Even if you don't have items, you will still consume RCU's. The amount will depend on the type of read request you use (ie: eventually consistent, strongly consistent or transactional), and the size of the item you're returning. See aws docs <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html"" rel=""nofollow noreferrer"">here</a> for more info, with the main point being:</p><blockquote><p>If you perform a read operation on an item that does not exist, DynamoDB still consumes provisioned read throughput</p></blockquote><p>To get a bit more detailed about <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html"" rel=""nofollow noreferrer"">how different read operations affect the consumed rcus for missing items</a>:</p><blockquote><p>One read request unit represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size ... For example, if your item size is 8 KB, you require 2 read request units to sustain one strongly consistent read, 1 read request unit if you choose eventually consistent reads, or 4 read request units for a transactional read request</p></blockquote><p>the key phrase in the above is <em>up to 4KB</em>. So you don't necessarily have to have data to consume RCU's by that logic.</p><p>As far as your question about hot partitioning, you will run into that problem if you have an unequally distributed access pattern (ie: you have non-uniform bursts of read/write requests for the same hash key). However, dynamodb has <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">adaptive capacity</a> in place to scale individual partitions to avoid throttling. They mention:</p><blockquote><p>In this post, we explain why capacity and provisioning in DynamoDB are no longer concerns. To do this, we first cover the basics of how DynamoDB shards your data across partitions and servers. Then, we highlight a feature called adaptive capacity that corrects the nonuniform workload issues that you might have experienced in the past</p></blockquote>",7359500,,,2020-06-24T03:16:52.573,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62547048
869,62553122,2,62552039,2020-06-24T10:38:31.507,3,"<p>It's not currently possible.</p><p>The <a href=""https://www.terraform.io/docs/providers/aws/r/vpc.html"" rel=""nofollow noreferrer""><code>aws_vpc</code> resource</a> has support for <a href=""https://www.terraform.io/docs/providers/aws/r/vpc.html#enable_classiclink"" rel=""nofollow noreferrer"">enabling ClassicLink</a> but there's no way to attach an EC2 instance to a VPC via ClassicLink. There's an <a href=""https://github.com/terraform-providers/terraform-provider-aws/issues/293"" rel=""nofollow noreferrer"">existing, stale feature request for it</a> but not much interest in it.</p><p>Unfortunately EC2 Classic things are deprecated in favour of VPC based EC2 resources and it's not even possible for a person without an EC2 Classic enabled account to be able to test the functionality either so feature requests like this must be driven entirely by the very few AWS customers still using EC2 Classic but are also using Terraform for automation. If you feel up to the task you could raise a pull request to expose the functionality requested in that issue via the <a href=""https://docs.aws.amazon.com/sdk-for-go/api/service/ec2/#EC2.AttachClassicLinkVpc"" rel=""nofollow noreferrer""><code>AttachClassicLinkVPC API call</code></a>.</p><p>I'd recommend migrating your remaining EC2 Classic instances to VPC based ones to avoid issues around the use of EC2 Classic.</p>",2291321,,,2020-06-24T10:38:31.507,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62553122
870,62663804,2,62462790,2020-06-30T18:19:11.550,1,"<p>For older containers using the deprecated <code>sagemaker_containers</code>, the approach you described is correct.</p><p>For newer containers that use <a href=""https://github.com/aws/sagemaker-training-toolkit"" rel=""nofollow noreferrer""><code>sagemaker-training-toolkit</code></a>, this is how you retrieve information about the environment: <a href=""https://github.com/aws/sagemaker-training-toolkit#get-information-about-the-container-environment"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-training-toolkit#get-information-about-the-container-environment</a></p><pre class=""lang-py prettyprint-override""><code>from sagemaker_training import environmentenv = environment.Environment()job_name = env[&quot;job_name&quot;]</code></pre><p>You can check the <a href=""https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/dlc-release-notes.html"" rel=""nofollow noreferrer"">DLC Release Notes</a> to see what's installed in each version.</p>",1344518,,,2020-06-30T18:19:11.550,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62663804
871,62692054,2,25900224,2020-07-02T08:22:22.887,1,"<p>Today (2020) The answer is well outdated. Bitbucket pipelines exists today and there are fairly good documentations on this:</p><p><a href=""https://support.atlassian.com/bitbucket-cloud/docs/deploy-to-aws-with-elastic-beanstalk/"" rel=""nofollow noreferrer"">https://support.atlassian.com/bitbucket-cloud/docs/deploy-to-aws-with-elastic-beanstalk/</a></p>",3101643,,,2020-07-02T08:22:22.887,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62692054
872,62714846,2,60757289,2020-07-03T11:51:13.293,0,"<p>Following answer is in chronological order corresponding to 3 questions.</p><ol><li>You can attach only identity_id (user) to IoT policy. Also, I can see you have used &quot;attach-principal-policy&quot; API which is deprecated now, so instead of that please use <a href=""https://docs.aws.amazon.com/iot/latest/apireference/API_AttachPolicy.html"" rel=""nofollow noreferrer"">AttachPolicy</a> API</li><li>I'm unsure here, still I'd recommend to evaluate and verify it on Cognito's <a href=""https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-confirmation.html"" rel=""nofollow noreferrer"">post confirmation trigger</a></li><li>Absolutely right, you can attach a IoT policy to myriad of certificates; technically it is known as Simplified Permission Management</li></ol><p>For #3, Relevant Snippet from AWS (Ref - <a href=""https://aws.amazon.com/iot-core/faqs/"" rel=""nofollow noreferrer"">https://aws.amazon.com/iot-core/faqs/</a> where find Q. What is Simplified Permission Management?)</p><p>&quot;You can share a single generic policy for multiple devices. A generic policy can be shared among the same category of devices instead of creating a unique policy per device. For example, a policy that references the Š—“serial-numberŠ— as a variable, can be attached to all the devices of the same model. When devices of the same serial number connect, policy variables will be automatically substituted by their serial-number.&quot;</p>",7657860,7657860,2020-07-03T13:14:04.377,2020-07-03T13:14:04.377,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62714846
873,62719148,2,62716606,2020-07-03T16:12:04.853,3,"<p>That is the Redshift error for referencing leader node information in a compute node based query - yes, it is a little cryptic.  This often happens when you references leader tables and try to join them with compute node tables.  But this isn't what you are doing.  In your case I believe that &quot;now()&quot; is a leader function and deprecated - you need to use &quot;getdate()&quot; instead.</p>",13350652,,,2020-07-03T16:12:04.853,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62719148
874,62733334,2,62713932,2020-07-04T19:07:01.873,1,"<p>Follow this steps:</p><ol><li>Go to cloudfront :</li><li>Do invalidation of objects</li><li>Create entry <code>/*</code></li></ol><p><a href=""https://i.stack.imgur.com/R1hoi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R1hoi.png"" alt=""enter image description here"" /></a></p><p>Reference : <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serving-outdated-content-s3/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serving-outdated-content-s3/</a></p>",6285054,6285054,2020-07-04T19:12:25.687,2020-07-04T19:12:25.687,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62733334
875,62762520,2,62757921,2020-07-06T18:44:57.540,2,"<p>My recommendation is that you migrate from boto, which is essentially deprecated, to boto3 because boto3 supports signature v4 by default (with the exception of S3 pre-signed URLs which has to be explicitly configured).</p>",271415,271415,2020-07-06T20:32:57.447,2020-07-06T20:32:57.447,5,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62762520
876,62772000,2,62163801,2020-07-07T09:21:35.007,-1,"<p>As in <a href=""https://docs.aws.amazon.com/cdk/latest/guide/tools.html"" rel=""nofollow noreferrer"">cdk deploy</a>, there is no such option -r. CDK thinks it is a CDK stack name.</p><p>I believe you need to use a different IAM role with different IAM permissions. Which has nothing to do with CDK itself. You need to assume role first and get an STS token. Please research assume role AWS CLI of STS.</p><pre><code>cdk deploy [STACKS..]Deploys the stack(s) named STACKS into your AWS accountOptions:  --build-exclude, -E  Do not rebuild asset with the given ID. Can be  specified multiple times.   [array] [default: []]  --exclusively, -e  Only deploy requested stacks, don't include  dependencies  [boolean]  --require-approval   What security-sensitive changes need manual approval  [string] [choices: &quot;never&quot;, &quot;any-change&quot;, &quot;broadening&quot;]  --ci   Force CI detection (deprecated)  [boolean] [default: false]  --notification-arns  ARNs of SNS topics that CloudFormation will notify with  stack related events  [array]  --tags, -t   Tags to add to the stack (KEY=VALUE)  [array]  --execute  Whether to execute ChangeSet (--no-execute will NOT  execute the ChangeSet)  [boolean] [default: true]  --force, -f  Always deploy stack even if templates are identical  [boolean] [default: false]  --parameters   Additional parameters passed to CloudFormation at  deploy time (STACK:KEY=VALUE)   [array] [default: {}]  --outputs-file, -O   Path to file where stack outputs will be written as  JSON   [string]  --previous-parameters  Use previous values for existing parameters (you must  specify all parameters on every deployment if this is  disabled)   [boolean] [default: true]</code></pre>",4281353,,,2020-07-07T09:21:35.007,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62772000
877,62798266,2,62796511,2020-07-08T15:25:39.763,0,"<p>While there's some great info in the other answer, I don't think it quite answers your question of</p><blockquote><p>How can DynamoDB be HA and eventually consistent at the same time?</p></blockquote><p>The answer is simple, DDB partitions are 3 way replicated...<a href=""https://i.stack.imgur.com/PnD7B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PnD7B.png"" alt=""from AWS reInvent DDB deep dive"" /></a></p><p>If you have watch the <a href=""https://www.youtube.com/watch?v=HaEPXoXVf2k"" rel=""nofollow noreferrer"">AWS Dynamo Deep Dive video</a>, I highly recommend it.</p><p>So when you write to DDB, it passes the write along to all three replica's and returns success to you after 2 of the three write succeeds.</p><p>An eventually consistent read to DDB also passes the read to all three replicas and returns to the caller the first response it gets.</p><p>So it's possible that a write could succeed to two replicas, and still be pending on another; and that an eventually consistent read returns the data from the out of date replica.</p><p>Obviously, having three replicas gives you the high availability.</p>",2933177,2933177,2020-07-08T17:56:41.873,2020-07-08T17:56:41.873,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62798266
878,62827015,2,61893719,2020-07-10T03:44:47.160,2,"<p>Maybe your code is outdated. For anyone who aims to use <code>fetch_mldata</code> in digit handwritten project, you should <code>fetch_openml</code> instead. (<a href=""https://stackoverflow.com/questions/47324921/cant-load-mnist-original-dataset-using-sklearn/52297457"">link</a>)</p><p>In old version of sklearn:</p><pre><code>from sklearn.externals import joblibmnist = fetch_mldata('MNIST original')</code></pre><p>In <strong>sklearn 0.23</strong> (stable release):</p><pre><code>import sklearn.externalsimport joblib dataset = datasets.fetch_openml(&quot;mnist_784&quot;)features = np.array(dataset.data, 'int16')labels = np.array(dataset.target, 'int')</code></pre><p>For more info about deprecating <code>fetch_mldata</code> see scikit-learn <a href=""https://scikit-learn.org/0.20/modules/generated/sklearn.datasets.fetch_mldata.html"" rel=""nofollow noreferrer"">doc</a></p>",7165473,,,2020-07-10T03:44:47.160,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62827015
879,62899207,2,62887695,2020-07-14T15:59:23.927,0,"<p>Assuming that <code>data_test</code> is a pandas matrix, then you should be able to directly use <code>to_numpy()</code> instead.</p><p>See <a href=""https://pandas.pydata.org/pandas-docs/version/0.25.1/reference/api/pandas.DataFrame.as_matrix.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/version/0.25.1/reference/api/pandas.DataFrame.as_matrix.html</a>:</p><pre><code>Deprecated since version 0.23.0: Use DataFrame.values() instead.</code></pre><p><code>DataFrame.values()</code>: <a href=""https://pandas.pydata.org/pandas-docs/version/0.25.1/reference/api/pandas.DataFrame.values.html#pandas.DataFrame.values"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/version/0.25.1/reference/api/pandas.DataFrame.values.html#pandas.DataFrame.values</a></p><pre><code>We recommend using DataFrame.to_numpy() instead.</code></pre>",2596715,,,2020-07-14T15:59:23.927,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62899207
880,62943701,2,62942913,2020-07-16T21:25:03.837,0,"<p>You are running this on arm64/aarch64. It's a relatively new architecture. It's also incompatible with the Intel 64-bit architecture (<code>x86_64</code> or <code>x64</code>). So you need to watch out for that.</p><h1>Installing via RPM</h1><p>Edit: So, this is just not going to work if you want to use RPM packages.</p><p>Quoting <a href=""https://docs.microsoft.com/en-us/dotnet/core/install/linux-centos"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/core/install/linux-centos</a>:</p><blockquote><p>Package manager installs are only supported on the x64 architecture. Other architectures, such as ARM, must manually install the .NET Core SDK or .NET Core Runtime. For more information, see the manually install section below.</p></blockquote><p>You are using <code>aarch64</code>/<code>arm64</code>. You are not using <code>x64</code>, so this is not going to work.</p><p>You need to use the tarball installation method.</p><p><s>Out of date suggestions:</p><blockquote><p>I am trying to install dotnet-sdk-3.0 on linux AMI 2 ec2 instance (c6g).</p></blockquote><blockquote><pre><code>sudo rpm -Uvh https://packages.microsoft.com/config/centos/7/packages-microsoft-prod.rpm</code></pre></blockquote><p>You are running Amazon Linux 2, right? As the URL here says, this is for CentOS 7. It may (or it may not) work on your Linux distribution. Anyway, try it out.</p><blockquote><pre><code>$ sudo yum install dotnet-sdk-3.0No package dotnet-sdk-3.0 available.Error: Nothing to do</code></pre></blockquote><p>The error says that it can't find this package. Maybe a package with this name doesn't exist? Maybe you are using the wrong name? Try using <code>yum list</code> to find the correct name:</p><pre><code>sudo yum list 'dotnet-sdk*'</code></pre><p>It should show you a list of packages, including names like <code>dotnet-sdk-3.0.103</code>. You can install that package by name, then:</p><pre><code>sudo yum install dotnet-sdk-3.0.103</code></pre><p>If that doesn't work, try another package name from <code>yum list</code> and try installing that.</s></p><h1>Installing manually</h1><blockquote><p>Then i tried</p><pre><code>mkdir -p &quot;$HOME/dotnet&quot; &amp;&amp; tar zxf dotnet-sdk-3.0.100-linux-x64.tar.gz -C &quot;$HOME/dotnet&quot;export DOTNET_ROOT=$HOME/dotnetexport PATH=$PATH:$HOME/dotnet</code></pre><p>After this tried the <code>dotnet</code> command but got the error. dotnet: command not found</p></blockquote><p>You are running an <code>aarch64</code> machine. You need to use the <code>arm64</code> tarball, not the <code>x64</code> tarball. The <code>x64</code> tarball is for an Intel processor. It will not work on an ARM processor.</p><p><s>That's surprising. Let me break down what this set of steps is doing:</p><ol><li><code>mkdir -p &quot;$HOME/dotnet&quot;</code> creates a directory named <code>dotnet</code> in your home directory</li><li><code>tar xf ...</code> extracts the dotnet SDK tarball in the <code>dotnet</code> directory you created in step 1</li><li><code>export DOTNET_ROOT=$HOME/dotnet</code> defines an environment variable <code>DOTNET_ROOT</code>. .NET Runtime needs it; I am a bit fuzzy myself on why</li><li><code>export PATH=$PATH:$HOME/dotnet</code> adds the directory you installed the .NET SDK into to the environment variable <code>PATH</code>. <code>PATH</code> is a list of locations that the OS uses to search for a command that you enter. For example, when you type <code>dotnet</code> in the command line it searches for <code>dotnet</code> executable (think <code>dotnet.exe</code> on Windows) in this list of directories.</li></ol><p>So let's try and debug it one by one:</p><ul><li>Does the directory <code>dotnet</code> exist in your main home directory (aka <code>$HOME</code>)? Can you <code>cd ~/dotnet</code>? Does that work?</li><li>After you extract the tarball, do you see a file named <code>dotnet</code> in the <code>dotnet</code> directory in your <code>$HOME</code>? Does <code>ls $HOME/dotnet/dotnet</code> work? What does it show you?</li><li>What does <code>echo $PATH</code> show you? Does it include that dotnet directory in the value?</li><li>If you run <code>which dotnet</code>, does it find the <code>dotnet</code> executable in your main <code>$HOME</code> directory? </s></li></ul><h1>Running the SDK</h1><blockquote><p>when i run this, i got below error</p></blockquote><blockquote><pre><code>[ec2-user@ip-0-0-0-0 home]$ dotnet --list-sdkProcess terminated. Couldn't find a valid ICU package installed on the system. Set the configuration flag System.Globalization.Invariant to true if you want to run with no globalization support</code></pre></blockquote><p>The error includes this phrase: Couldn't find a valid ICU package installed on the system.</p><p>It really means that. You need to install the ICU package for your Linux distribution:</p><pre><code>sudo yum install libicu</code></pre><p>And then try running <code>dotnet --list-sdk</code> again.</p><h1>Error Running <code>dll</code></h1><blockquote><pre><code>Failed to load Š_êrŠ_ê), error: /home/ec2-user/dotnet/shared/Microsoft.NETCore.App/3.0.0/libhostpolicy.so: cannot open shared object file: No such file or directoryAn error occurred while loading required library libhostpolicy.so from [/home/ec2-user/dotnet/shared/Microsoft.NETCore.App/3.0.0]</code></pre></blockquote><p>This is strange. It says it can't find a file that should be part of the .NET Core installation.</p><ul><li><p>What does <code>dotnet --list-runtimes</code> say? Does it show the 3.0.0 runtime installed? If not, that means your installation is messed up. You should probably install .NET Core 3.0 again. (Or better yet, install 3.1 because 3.0 has been end-of-life'd).</p></li><li><p>Does the file <code>/home/ec2-user/dotnet/shared/Microsoft.NETCore.App/3.0.0/libhostpolicy.so</code> exist? If it doesn't it's the same problem as above: your installation is messed up.</p></li><li><p>What does <code>file /home/ec2-user/dotnet/shared/Microsoft.NETCore.App/3.0.0/libhostpolicy.so</code> say? Is it an <code>ELF 64-bit LSB shared object</code>?</p><ul><li><p>The output is: <code>ELF 64-bit LSB shared object, x86-64</code></p></li><li><p>This is a <code>x86-64</code> file! In other words, you have (somehow) installed an <code>linux-x64</code> (Intel 64-bit architecture) runtime. Not too surprisingly, it doens't work on the ARM 64 bit architecture. You need to delete this and re-install the SDK. I suggest just blowing away your current installation (<code>rm -rf $HOME/dotnet</code>) and install the <code>linux-arm64</code> SDK again.</p></li></ul></li></ul>",3561275,3561275,2020-07-19T04:09:06.760,2020-07-19T04:09:06.760,7,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62943701
881,62944334,2,62943396,2020-07-16T22:17:59.073,3,"<p>Sadly <code>CloudFormer</code> is deprecated and <strong>no longer maintained</strong> by AWS. Thus Its not recommended for use as its not reliable.</p><p>You can have a look at a third party tool called <a href=""https://former2.com/"" rel=""nofollow noreferrer"">Former2</a> which seems to be much more useful and reliable than the <code>CloudFormer</code>:</p><blockquote><p>Former2 allows you to <strong>generate Infrastructure-as-Code outputs</strong> from your existing resources within your AWS account. By making the relevant calls using the AWS JavaScript SDK, Former2 will scan across your infrastructure and present you with the list of resources for you to choose which to generate outputs for.</p></blockquote>",248823,,,2020-07-16T22:17:59.073,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62944334
882,62944571,2,62943437,2020-07-16T22:44:17.210,2,"<p>Yeah from <a href=""https://docs.aws.amazon.com/lambda/latest/dg/runtime-support-policy.html"" rel=""nofollow noreferrer"">this</a> page you can see that Node.js 8.10 was deprecated March 6, 2020.</p><p>Deprecation occurs in two phases. During the first phase, you can no longer create functions that use the deprecated runtime. For at least 30 days, you can continue to update existing functions that use the deprecated runtime. After this period, both function creation and updates are disabled permanently. However, the function continues to be available to process invocation events.</p><p>You will however be able to change your runtime to a newer version of node and then you'll be able to update your function with new code.</p>",9115027,,,2020-07-16T22:44:17.210,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62944571
883,62979598,2,62978643,2020-07-19T11:22:58.010,0,"<p><code>create_default_context</code> will use ssl certificates stored on your instance. Seems one for google/gmail is outdated or can be found.</p><p>You can try to manually add them from <a href=""https://support.google.com/a/answer/6180220?hl=en"" rel=""nofollow noreferrer"">google page</a>. Also try updating your instance. Its not clear which OS or its version you use. Maybe it needs update or upgrade.</p><p>A quick and rather not-recommended workaround would be to disable ssl cert verrification:</p><pre><code>cont = ssl.create_default_context()cont.check_hostname = Falsecont.verify_mode = ssl.CERT_NONE</code></pre>",248823,,,2020-07-19T11:22:58.010,2,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/62979598
884,63010234,2,56982947,2020-07-21T08:13:54.443,0,"<p><code>fluent.**</code> has been deprecated, we could use this match annotation in order to exclude fluentd logs:</p><pre class=""lang-xml prettyprint-override""><code>  &lt;match @FLUENT_LOG&gt;  @type null  &lt;/match&gt;</code></pre>",7690356,,,2020-07-21T08:13:54.443,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63010234
885,63011905,2,30902851,2020-07-21T09:54:16.047,0,"<p>As <code>Request</code> has been deprecated, here's a solution utilizing Axios</p><pre><code>const AWS = require('aws-sdk');const axios = require('axios');const downloadAndUpload = async function(url, fileName) {  const res = await axios({ url, method: 'GET', responseType: 'stream' });  const s3 = new AWS.S3(); //Assumes AWS credentials in env vars or AWS config file  const params = {  Bucket: IMAGE_BUCKET,   Key: fileName,  Body: res.data,  ContentType: res.headers['content-type'],  };  return s3.upload(params).promise();}</code></pre><p>Note, that the current version of the AWS SDK doesn't throw an exception if the AWS credentials are wrong or missing - the promise simply never resolves.</p>",1156825,1156825,2020-07-29T13:38:30.230,2020-07-29T13:38:30.230,3,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63011905
886,63074854,2,62632590,2020-07-24T13:45:03.340,1,"<p>Amazon Linux 2 has a fundamentally different setup than AL1, and the current documentation as of Jul 24, 2020 is out of date. <code>django-admin</code> of the installed environment by beanstalk does not appear to be on the path, so you can source the environment to activate and make sure it is.</p><p>I left my answer <a href=""https://stackoverflow.com/a/63074781/5180047"">here</a> as well which goes into much more detail in how I arrived at this answer, but the solution (which I don't love) is:</p><pre><code>container_commands:  01_migrate:  command: &quot;source /var/app/venv/*/bin/activate &amp;&amp; python3 manage.py migrate&quot;  leader_only: true</code></pre><p>Even though I don't love it, I have verified with AWS Support that this is in fact the recommended way to do this. You <strong>must</strong> source the python environment, as with AL2 they use virtual environments in an effort to stay more consistent.</p>",5180047,5180047,2020-07-28T14:06:33.953,2020-07-28T14:06:33.953,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63074854
887,63130562,2,62809583,2020-07-28T09:01:27.643,0,"<p>Check you account restrictions. The point is Amazon could restrict access to affiliates since they don't produce enough sales at the time. Amazon granted unlimited access till January 2019, so now you account may be considered as &quot;deprecated&quot; as it don't have sales for last 30 days or somthing like that.Anyway, it is better to address this question to Amazon Associates Support.</p>",5705048,,,2020-07-28T09:01:27.643,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63130562
888,63209965,2,10222843,2020-08-01T20:53:22.577,0,"<p>There has been some development since this question was asked. The question was discussed in more detail in jira <a href=""https://issues.apache.org/jira/browse/MAPREDUCE-2389"" rel=""nofollow noreferrer"">MAPREDUCE-2389</a>.</p><p>A key part in this jira, and any other jira that was linked here or there, is that the issue was linked to the jetty version. At the time it was not possible to go to a newer jetty version.</p><p>By now the referred version is long deprecated, and as such the situation should be fully resolved for everyone.</p>",983722,,,2020-08-01T20:53:22.577,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63209965
889,63216719,2,51457441,2020-08-02T13:43:57.970,0,"<p>Before, there were &quot;hooks&quot; but now they're almost deprecated <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/custom-platform-hooks.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/custom-platform-hooks.html</a></p><p>Now you can use Buildfile and Procfile as described here <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/platforms-linux-extend.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/platforms-linux-extend.html</a></p>",1768553,,,2020-08-02T13:43:57.970,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63216719
890,63252409,2,63236398,2020-08-04T17:46:21.153,0,<p>I was using and outdated browser. Updating the browser solved the issues for me.</p>,13299635,,,2020-08-04T17:46:21.153,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63252409
891,63293266,2,13455168,2020-08-06T23:47:47.047,0,"<p>Here is another whay to upload a null (or o Byte) file to S3. I verified this works You can also use the S3 API to upload a file with no body, like so:</p><pre><code>aws s3api put-object --bucket &quot;myBucketName&quot; --key &quot;dir-1/my_null_file&quot;</code></pre><p>Normally you would specify a <code>--body</code> blob, but its option and will just add the key as expected. See more on <a href=""https://docs.aws.amazon.com/cli/latest/reference/s3api/put-object.html#examples"" rel=""nofollow noreferrer"">S3 API put-object</a></p><p>The version of AWS CLI tested with is: <code>aws-cli/2.0.4 Python/3.7.5 Windows/10 botocore/2.0.0dev8</code></p><p>Here's how I did it in PHP (even works in outdated 5.4, had to go way back):</p><pre class=""lang-php prettyprint-override""><code>// Init an S3Client$awsConfig = $app-&gt;config('aws');$aws   = Aws::factory($awsConfig);$s3Bucket  = $app-&gt;config('S3_Bucket');$s3Client  = $aws-&gt;get('s3');// Set null/empty file.$result = $s3Client-&gt;putObject([  'Bucket' =&gt; $s3Bucket,  'Key' =&gt; &quot;dir-1/my_null_file&quot;,  'Body' =&gt; '',  'ServerSideEncryption' =&gt; 'AES256',]);</code></pre>",419097,,,2020-08-06T23:47:47.047,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63293266
892,63360269,2,63357287,2020-08-11T14:26:22.527,0,"<p>Your method for establishing a connection is correct, albeit a now deprecated way of doing that.  You should now use:</p><pre><code>g = Traversal().withRemote(remoteConnection);</code></pre><p>However, that is not what is causing your issue.  Neptune is expecting a request that contains an alias of 'g'.  It might be good to enable audit logging on your cluster to see exactly what your code is sending as the alias instead of 'g'.</p>",10130372,1839439,2020-08-11T16:47:38.577,2020-08-11T16:47:38.577,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63360269
893,63452269,2,63438293,2020-08-17T13:51:47.257,1,"<p>It is a compatibility issue. Current version of <code>aws-appsync</code> doesn't support <code>apollo-client</code> v3, see this thread for progress:<a href=""https://github.com/awslabs/aws-mobile-appsync-sdk-js/issues/448"" rel=""nofollow noreferrer"">https://github.com/awslabs/aws-mobile-appsync-sdk-js/issues/448</a></p><p>Best workaround is this: <a href=""https://stackoverflow.com/questions/60804372/proper-way-to-setup-awsappsyncclient-apollo-react"">Proper way to setup AWSAppSyncClient, Apollo &amp; React</a></p><p>Note the workaround does use two deprecated libraries but can be slightly improved as:</p><pre><code>import { ApolloClient, ApolloLink, InMemoryCache } from &quot;@apollo/client&quot;;import { createAuthLink } from &quot;aws-appsync-auth-link&quot;;import { createHttpLink } from &quot;apollo-link-http&quot;;import AppSyncConfig from &quot;./aws-exports&quot;;const url = AppSyncConfig.aws_appsync_graphqlEndpoint;const region = AppSyncConfig.aws_project_region;const auth = {  type: AppSyncConfig.aws_appsync_authenticationType,  apiKey: AppSyncConfig.aws_appsync_apiKey,};const link = ApolloLink.from([  // @ts-ignore  createAuthLink({ url, region, auth }),  // @ts-ignore  createHttpLink({ uri: url }),]);const client = new ApolloClient({  link,  cache: new InMemoryCache(),});export default client;</code></pre>",5080370,,,2020-08-17T13:51:47.257,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63452269
894,63455463,2,63419776,2020-08-17T17:06:07.217,0,"<p>It's difficult to say without knowing how much volume you expect...</p><p>Generally, you want a partition key with high cardinality.  The sort key cardinality generally doesn't matter.</p><p>However, if you anticipate a small number of the itemId getting the most traffic, then you might run into issues with &quot;hot partitions&quot;; although the risk is <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">greatly reduced now-a-days</a>.</p><p>Furthermore, hot partitions are only a problem if you DDB table ends up partitioned in the first place.  DDB partitions your data when storage gets above 10GB* or when RCU/WCU required is more than 3000/1000 respectively.</p><p>*10GB isn't even guaranteed, a DDB table w/o local secondary indexes can have a partition larger than 10GB.</p>",2933177,,,2020-08-17T17:06:07.217,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63455463
895,63459678,2,63455876,2020-08-17T22:58:56.913,1,"<p>The reason why <code>WebServerSecurityGroup</code> (SG) is not found is because you are creating the SG in a  <strong>different VPC</strong> then your EB environment. Specifically, you are lunching EB in a default VPC, while you seem to be creating your  SG in different VPC as specified in the following line:</p><pre><code>  VpcId: !Ref Vpc # &lt;--- your EB will be in different VPC than your SG</code></pre><p>Since its not clear what you are doing with the VPC (are you launching EB in custom VPC, creating new VPC or using default VPC?), the easiest fix to your template is simply removing the <code>VpcId: !Ref Vpc</code>.</p><p>Also your <strong>platform version is outdated</strong> and needs to be changed. The list of available PHP platform versions is <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/platforms/platforms-supported.html#platforms-supported.PHP"" rel=""nofollow noreferrer"">here</a>.</p><p>I <strong>fixed</strong> the template and I can <strong>verify that it works</strong> in <code>us-east-1</code>. It launches EB and its SG in a <code>default VPC</code>. For custom VPC many more changes are required to your template, such as definitions of subnets, route tables and VPC specific changes to EB environment itself.</p><pre><code>AWSTemplateFormatVersion: '2010-09-09'Description: &quot;Pathein Directory web application deployment template.&quot;Parameters:  KeyName:  Default: 'PatheinDirectory'  Type: String  InstanceType:  Default: 't2.micro'  Type: String  SSHLocation:  Description: The IP address range that can be used to SSH to the EC2 instances  Type: String  MinLength: '9'  MaxLength: '18'  Default: 0.0.0.0/0  AllowedPattern: &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;  ConstraintDescription: Must be a valid IP CIDR range of the form x.x.x.x/x  # Vpc:  #   Default: &quot;vpc-dd53ada4&quot;  #   Type: String  # VpcCidr:  #   Default: &quot;172.31.0.0/16&quot;  #   Type: StringMappings:  Region2Principal:  us-east-1:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  us-west-2:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  us-west-1:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  eu-west-1:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  eu-west-2:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  eu-west-3:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  ap-southeast-1:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  ap-northeast-1:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  ap-northeast-2:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  ap-northeast-3:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  ap-southeast-2:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  ap-south-1:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  us-east-2:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  ca-central-1:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  sa-east-1:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  cn-north-1:  EC2Principal: ec2.amazonaws.com.cn  OpsWorksPrincipal: opsworks.amazonaws.com.cn  cn-northwest-1:  EC2Principal: ec2.amazonaws.com.cn  OpsWorksPrincipal: opsworks.amazonaws.com.cn  eu-central-1:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.com  eu-north-1:  EC2Principal: ec2.amazonaws.com  OpsWorksPrincipal: opsworks.amazonaws.comResources:  WebServerSecurityGroup:  Type: AWS::EC2::SecurityGroup  Properties:  GroupDescription: Security Group for EC2 instances  SecurityGroupIngress:  - IpProtocol: tcp  FromPort: '80'  ToPort: '80'  CidrIp: 0.0.0.0/0  - IpProtocol: tcp  FromPort: '22'  ToPort: '22'  CidrIp:  Ref: SSHLocation  #VpcId: !Ref Vpc  WebServerRole:  Type: AWS::IAM::Role  Properties:  AssumeRolePolicyDocument:  Statement:  - Effect: Allow  Principal:  Service:  - Fn::FindInMap:  - Region2Principal  - Ref: AWS::Region  - EC2Principal  Action:  - sts:AssumeRole  Path: /  WebServerRolePolicy:  Type: AWS::IAM::Policy  Properties:  PolicyName: WebServerRole  PolicyDocument:  Statement:  - Effect: Allow  NotAction: iam:*  Resource: '*'  Roles:  - Ref: WebServerRole  WebServerInstanceProfile:  Type: AWS::IAM::InstanceProfile  Properties:  Path: /  Roles:  - Ref: WebServerRole  Application:  Type: AWS::ElasticBeanstalk::Application  Properties:  Description: AWS Elastic Beanstalk Pathein Directory Laravel application  ApplicationVersion:  Type: AWS::ElasticBeanstalk::ApplicationVersion  Properties:  Description: Version 1.0  ApplicationName:  Ref: Application  SourceBundle:  S3Bucket:  Fn::Join:  - '-'  - - elasticbeanstalk-samples  - Ref: AWS::Region  S3Key: php-sample.zip  ApplicationConfigurationTemplate:  Type: AWS::ElasticBeanstalk::ConfigurationTemplate  Properties:  ApplicationName:  Ref: Application  Description: SSH access to Pathein Directory Laravel application  SolutionStackName: 64bit Amazon Linux 2018.03 v2.9.9 running PHP 7.2   OptionSettings:  - Namespace: aws:autoscaling:launchconfiguration  OptionName: EC2KeyName  Value:  Ref: KeyName  - Namespace: aws:autoscaling:launchconfiguration  OptionName: IamInstanceProfile  Value:  Ref: WebServerInstanceProfile  - Namespace: aws:autoscaling:launchconfiguration  OptionName: SecurityGroups  Value:  Ref: WebServerSecurityGroup  Environment:  Type: AWS::ElasticBeanstalk::Environment  Properties:  Description: AWS Elastic Beanstalk Environment running Pathein Directory Laravel application  ApplicationName:  Ref: Application  EnvironmentName: PatheinDirectoryTesting  TemplateName:  Ref: ApplicationConfigurationTemplate  VersionLabel:  Ref: ApplicationVersion  OptionSettings:  - Namespace: aws:elasticbeanstalk:environment  OptionName: EnvironmentType  Value: SingleInstance</code></pre>",248823,,,2020-08-17T22:58:56.913,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63459678
896,63470916,2,63417331,2020-08-18T14:36:13.930,1,"<p>The <code>url_for</code> function is deprecated and therefore had to be change to <code>public_url</code>.</p><p>This is a working version of my original code:</p><pre><code>class Upload  attr_reader :filename  require 'aws-sdk-s3'  Aws.config.update({  region: 'us-east-1',  credentials: Aws::Credentials.new(Rails.application.credentials.dig(:aws, :access_key_id), Rails.application.credentials.dig(:aws, :secret_access_key)),  })  def initialize(filename)  @filename = filename  end  def url  @url ||= bucket_file.public_url().to_s  end  def content_type  @content_type ||= MIME::Types.type_for(filename).first.content_type  end  def to_json(*args)  { url: url, content_type: content_type }.to_json  end  private   def bucket_file  @bucket_file ||= bucket.object(&quot;uploads/#{SecureRandom.uuid}/${filename}&quot;)  end  def bucket   @bucket ||= Aws::S3::Resource.new(region: 'us-east-1').bucket(Rails.application.credentials.dig(:aws, :bucket))    endend</code></pre>",13679971,13679971,2020-08-18T17:17:41.630,2020-08-18T17:17:41.630,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63470916
897,63503061,2,63493401,2020-08-20T10:24:33.180,1,"<p>I think you may have made the same mistake I did before. In your development.rb file, do not edit the text to add your specific S3 keys. Just copy-paste the text directly as is listed in the tutorial.</p><pre class=""lang-rb prettyprint-override""><code>#development.rbconfig.paperclip_defaults = {  :storage =&gt; :s3,  :s3_credentials =&gt; {  :bucket =&gt; ENV['AWS_BUCKET'],  :access_key_id =&gt; ENV['AWS_ACCESS_KEY_ID'],  :secret_access_key =&gt; ENV['AWS_SECRET_ACCESS_KEY']  }}</code></pre><p>Then, set the environmental variables AWS_BUCKET, AWS_ACCESS_KEY_ID, and AWS_SECRET_ACCESS_KEY as described by the author of the dev center article.</p><p>Btw, I'm not sure why you are using PaperClip(deprecated gem) on Rails 6. Rails 6 has ActiveStorage library to do the same thing perfectly.</p>",8540965,12364558,2020-08-20T10:46:34.853,2020-08-20T10:46:34.853,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63503061
898,63532405,2,63526333,2020-08-22T03:52:15.740,1,"<p><code>external.metrics.k8s.io</code> is not part of upstream kube API as mentioned in <a href=""https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis"" rel=""nofollow noreferrer"">docs</a></p><blockquote><p>For external metrics, this is the external.metrics.k8s.io API. It may be provided by the custom metrics adapters provided above.</p></blockquote><p>So you need to install custom metric provider such as <a href=""https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter"" rel=""nofollow noreferrer"">GCP stackdriver</a>. I assume you are using EKS with prometheus, here is one good option that I used before <a href=""https://github.com/DirectXMan12/k8s-prometheus-adapter"" rel=""nofollow noreferrer"">prometheus-adapter</a>. Kindly note that the API is <code>v1beta1.custom.metrics.k8s.io</code> though.</p><p>PS: I thought EKS 1.13 is already deprecated, you might need to update it first.</p>",8763847,,,2020-08-22T03:52:15.740,1,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63532405
899,63572667,2,61893719,2020-08-25T05:41:56.313,-1,"<p>When getting error:</p><p><strong>from sklearn.externals import joblib</strong> it deprecated older version.</p><p>For new version follow:</p><ol><li>conda install -c anaconda scikit-learn  (install using &quot;Anaconda Promt&quot;)</li><li>import joblib (Jupyter Notebook)</li></ol>",13030784,,,2020-08-25T05:41:56.313,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63572667
900,63579364,2,63578368,2020-08-25T12:57:27.673,2,"<p>Depending on what you want, you can use eventual consistency or strong consistency:<a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a></p><p>By default, the AWS SDK uses eventual consistency. You have to set the ConsistentRead parameter to use strong consistency.</p><p>Caveats:</p><ul><li>strong consistency is a little bit slower (it has to wait for any conflicting operations to make sure that the result that is returned is absolutely, 100%, the accurate value at the time of the request)</li><li>strongly consistent reads are also more expensive as they consume more throughput</li></ul><p>In most cases, you should design your application to accept reads that are not 100% accurate after write, as this should only take a few microseconds/milliseconds for your data to be eventually consistent. For instance, you can just optimistically return the new Item when you write it (&quot;I'm confident that this is what the database will look like in a few milliseconds&quot;) and account for this maybe wrong/inaccurate/outdated data in your application, so that you don't have to read it from the database again) (also, it's expensive and slow to do that, for no good reason, so just don't). But when you really, really, really need to make sure that you are reading the accurate, updated data, use strong consistency. There is a good reason it's not the default though.</p><blockquote><p>One user is updating the status, and another is trying to read it simultaneously, will one or both tasks fail?</p></blockquote><p>No, the read operation simply <em>may or may not</em> return outdated data.</p><blockquote><p>And what if one is updating the status and the other is requesting theinfo?</p></blockquote><p>Read might have the new or the old value, depending on how precisely simultaneous it was. But basically, just the time it takes for the networking latency between the user's machine and the database is usually enough for eventual/strong consistency to not really matter. If your application is sensitive to race conditions or must 100% ensure that the read data is the same as the write data 1 microsecond after it was written, use strong consistency.</p>",3227486,3227486,2020-08-25T13:06:45.730,2020-08-25T13:06:45.730,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63579364
901,63618135,2,63617992,2020-08-27T14:22:53.153,2,"<p>The SDK returns at most 1000 results. If results are paginated, then you need to re-issue the list call with a continuation token. See <code>IsTruncated</code> and <code>NextContinuationToken</code> in the response, and <code>ContinuationToken</code> in the request.</p><p>Also, you should use <code>listObjectsV2</code> rather than the deprecated <code>listObjects</code>.</p>",271415,,,2020-08-27T14:22:53.153,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63618135
902,63626009,2,63602642,2020-08-28T00:56:33.453,0,"<p>If someone has the same problem, I managed to do it with CDK with <a href=""https://github.com/aws/aws-cdk/issues/49#issuecomment-402885800"" rel=""nofollow noreferrer"">this example</a>. It's outdated but I applied the same logic. Probably some of the steps mentioned in this answer are not necesary because of recent changes. I found a more recent example <a href=""https://github.com/aws/aws-cdk/pull/1924#issue-257632787"" rel=""nofollow noreferrer"">here</a>, but I haven't tried it yet.</p><h3>Important: Make sure the resources in both accounts are in the same region.</h3><p>Let's call ID_ACC_WITH_REPO to the AWS account ID with the CodeCommit repo and ID_ACC_WITH_PIPELINE to the account ID with the pipeline and where we want to deploy the architecture.</p><h2>CDK code for the pipeline</h2><h3>ACC_WITH_REPO</h3><ol><li>Create a stack in ID_ACC_WITH_REPO. The region <strong>must</strong> be especified because it's required for cross-account pipelines.</li></ol><pre class=""lang-js prettyprint-override""><code>const repoAccStack = new cdk.Stack(app, 'RepoAccStack', {  env: {  account: ID_ACC_WITH_REPO,  region: REPO_REGION  }});</code></pre><ol start=""2""><li>Create a cross-account role in ACC_WITH_REPO and attach full access policies for S3 (to store the artifacts), CodeCommit and KMS (encryption). The role will be used by the pipeline in ACC_WITH_PIPELINE and the CodeCommit source action in the source stage. I guess you can restrict them more to be extra secure.</li></ol><pre class=""lang-js prettyprint-override""><code>// Create roleconst crossAccRole = new iam.Role(repoAccStack, 'OtherAccRole', {  roleName: 'CrossAccountRole',  assumedBy: new iam.AccountPrincipal(pipelineAcc),});// Attach policiesconst policy = new iam.PolicyStatement();policy.addAllResources();policy.addActions('s3:*', 'codecommit:*', 'kms:*');crossAccRole.addToPolicy(policy);</code></pre><ol start=""3""><li>Import the repo.</li></ol><pre><code>const repo = codecommit.Repository.fromRepositoryArn(  repoAccStack,  'AppRepository',  `arn:aws:codecommit:${REPO_REGION}:${ID_ACC_WITH_REPO}:${REPO_NAME}`);</code></pre><h3>ACC_WITH_PIPELINE</h3><ol start=""4""><li>Create a stack for the pipeline in ID_ACC_WITH_PIPELINE.</li></ol><pre class=""lang-js prettyprint-override""><code>const pipelineAccStack = new cdk.Stack(app, 'PipelineAccStack', {  env: {  account: ID_ACC_WITH_PIPELINE,  region: REGION_WITH_PIPELINE  }});</code></pre><ol start=""5""><li>Create the KMS key. The method <code>EncryptionKey</code> used in the example is deprecated, use <code>Key</code> instead.</li></ol><pre class=""lang-js prettyprint-override""><code>const key = new kms.Key(pipelineAccStack, 'CrossAccountKmsKey');</code></pre><p>Actually I got a <code>kms.model.MalformedPolicyDocumentException</code> error when trying to create the key, so I did it manually from the AWS Console and then imported it with <code>kms.Key.fromKeyArn</code>. I probably did something wrong with my account (I got a lot of errors before getting to this solution), but if you get the same error it's a workaround. Just make sure of assigning usage permissions to the pipeline role.</p><ol start=""6""><li>Create the S3 bucket in ACC_WITH_PIPELINE with the KMS created before. A bucket name is required. The <code>HackyIdentity</code> used in the example is not necessary, several methods used in the class implementation are now deprecated.</li></ol><pre class=""lang-js prettyprint-override""><code>const artifactsBucket = new s3.Bucket(pipelineAccStack, &quot;ArtifactsBucket&quot;, {  bucketName: BUCKET_NAME,  encryptionKey: key,  encryption: s3.BucketEncryption.KMS});artifactsBucket.grantReadWrite(new iam.ArnPrincipal(crossAccRole.roleArn));</code></pre><ol start=""7""><li>Create the pipeline and add the cross-account role created in step 5.</li></ol><pre class=""lang-js prettyprint-override""><code>// Create pipelineconst pipeline = new codepipeline.Pipeline(pipelineAccStack, 'Pipeline', {  pipelineName: 'CrossAccountPipeline',  artifactBucket: artifactsBucket});// Add cross-account roleconst policy = new iam.PolicyStatement();policy.addResources(crossAccRole.roleArn)policy.addActions('s3:*', 'codecommit:*', 'kms:*');pipeline.addToRolePolicy(policy);</code></pre><ol start=""8""><li>Add the source stage to the pipeline with the CodeCommit repo imported in step 3.</li></ol><pre class=""lang-js prettyprint-override""><code>// Create artifact for source codeconst sourceArtifact = new codepipeline.Artifact();// Create source stage with rolepipeline.addStage({  stageName: 'Source',  actions: [  new codepipeline_actions.CodeCommitSourceAction({  actionName: 'CodeCommit_Source',  repository: repo,  output: sourceArtifact,  branch: 'dev',  role: crossAccRole  })  ]});</code></pre><ol start=""9""><li>And finally add the build stage</li></ol><pre class=""lang-js prettyprint-override""><code>// Create CodeBuild projectconst buildProject = new codebuild.PipelineProject(this, 'Build', {  environment: { buildImage: codebuild.LinuxBuildImage.AMAZON_LINUX_2_2 }});// Create artifact for buildconst buildArtifact = new codepipeline.Artifact();// Add build stagepipeline.addStage({  stageName: 'Build',  actions: [  new codepipeline_actions.CodeBuildAction({  actionName: 'Build',  project: buildProject,  input: sourceArtifact,  outputs: [buildArtifact],  }),  ],});</code></pre><h2>Deploy</h2><p>When the CDK app contains more than one stack you can't just <code>cdk deploy</code>. This is explained in <a href=""https://github.com/aws/aws-cdk/issues/2750"" rel=""nofollow noreferrer"">here</a>. However if you try to <code>cdk deploy '*'</code> there's another error: <code>Need to perform AWS calls for account ACCOUNT_ID, but the current credentials are for ACCOUNT_ID</code>.</p><p>I managed to deploy the stacks with <code>cdk deploy -e</code> and switching accounts with <code>aws configure</code>.There are three stacks, not two. An EventBusPolicy stack is automatically generated by CDK to create an event bus. The other two events are added (also automatically) by CDK in PipelineAccStack and RepoAccStack. Marcin's answer explain how cross-account events are configured. The EventBusPolicy stack should be created in ACC_WITH_PIPELINE. To get the exact name of the stack, use <code>cdk list</code>.</p><p>Taking all of this into account, in this example I would deploy with:</p><pre class=""lang-sh prettyprint-override""><code># with aws configure in ACC_WITH_PIPELINEcdk deploy -e &quot;PipelineAccStack&quot;cdk deploy -e &quot;EventBusPolicy-$ID_ACC_WITH_REPO-$REGION-$ID_ACC_WITH_PIPELINE&quot;# switch aws configure to ACC_WITH_REPOcdk deploy -e &quot;RepoAccStack&quot;</code></pre>",11472024,11472024,2020-08-28T07:42:44.643,2020-08-28T07:42:44.643,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63626009
903,63631475,2,63626179,2020-08-28T09:52:58.280,0,"<p>There are some options you should consider:</p><ul><li><p>don't update anything and just stick to Kubernetes 1.15 (not recommended as it is 4 main versions behind the latest one)</p></li><li><p><code>git clone</code> your repo and change <code>apiVersion</code> to <code>apps/v1</code> in all your resources</p></li><li><p>use <a href=""https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#convert"" rel=""nofollow noreferrer"">kubectl convert</a> in order to change the <code>apiVersion</code>, for example: <code>kubectl convert -f deployment.yaml --output-version apps/v1</code></p></li></ul><p>It is worth to mention that stuff gets deprecated for a reason and it is strongly not recommended to stick to old ways if they are not supported anymore.</p>",11560878,,,2020-08-28T09:52:58.280,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63631475
904,63654504,2,63654443,2020-08-30T06:09:13.340,0,"<p>The <code>field</code> and <code>values</code> were <strong>deprecated</strong> in aws provider version 2.x as shown <a href=""https://registry.terraform.io/providers/hashicorp/aws/2.64.0/docs/resources/lb_listener_rule#condition-blocks"" rel=""nofollow noreferrer"">here</a>. In the provider version 3.x they are  removed and <strong>no longer valid</strong> as shown in the new docs for <a href=""https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lb_listener_rule#condition-blocks"" rel=""nofollow noreferrer"">condition</a> block.</p><p>This is further explained in the official &quot;Terraform AWS Provider Version 3 Upgrade Guide&quot; in:</p><ul><li><a href=""https://registry.terraform.io/providers/hashicorp/aws/latest/docs/guides/version-3-upgrade#resource-aws_lb_listener_rule"" rel=""nofollow noreferrer"">condition.field and condition.values Arguments Removal</a></li></ul><p>So you either have to migrate your code to work with aws provider 3.x or go back to using older version.</p>",248823,,,2020-08-30T06:09:13.340,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63654504
905,63712880,2,63698692,2020-09-02T20:08:17.563,0,"<p>this worked</p><pre><code>  let data = await ec2.runInstances(params).promise();    let result = await codeDeploy.createDeployment({  applicationName: '&lt;name&gt;', /* required */  autoRollbackConfiguration: {  enabled: true,  events: [  'DEPLOYMENT_FAILURE'  ]  },  deploymentConfigName: 'CodeDeployDefault.AllAtOnce',  deploymentGroupName: `&lt;your-group-name&gt;`,  fileExistsBehavior: 'OVERWRITE',  updateOutdatedInstancesOnly: true   }).promise();</code></pre>",8822238,,,2020-09-02T20:08:17.563,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63712880
906,63733871,2,60547409,2020-09-04T02:09:02.063,0,"<p>Adding to <a href=""https://stackoverflow.com/a/60573982/137601"">Simon's great answer</a>, you may want to also pass the session in the <code>GetTokenOptions</code> struct, like so:</p><pre class=""lang-golang prettyprint-override""><code>opts := &amp;token.GetTokenOptions{  ClusterID: aws.StringValue(cluster.Name),  Session: sess,  }  tok, err := gen.GetWithOptions(opts)</code></pre><p>Otherwise the <code>gen.GetWithOptions(opts)</code> call will try to read your AWS credentials from local sources (e.g. <code>~/.aws/credentials</code>) and may file with this error:</p><pre><code>NoCredentialProviders: no valid providers in chain. Deprecated.  For verbose messaging see aws.Config.CredentialsChainVerboseErrors</code></pre>",137601,,,2020-09-04T02:09:02.063,0,CC BY-SA 4.0,,,,https://stackoverflow.com/questions/63733871