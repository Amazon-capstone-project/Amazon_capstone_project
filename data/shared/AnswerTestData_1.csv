,Id,PostId,link,outdated (manually checked),description,link outdated,answer outdated,service in question outdated,other reasons,partly outdated,totally outdated,is aws-related,related service,keywords,phrases,strucutures,point out deprecated version,provide alternatives,share a link,provide code,others,Text
0,2012755,2012754,https://stackoverflow.com/questions/2012755,TRUE,link in question outdated,1,0,0,0,,,yes,aws,deprecated,has been deprecated,"provide alternatives, share a link",0,1,1,0,0,"<p>The word 'service' is only in the xml 3 times, so I'm assuming that what you're looking for is:</p><pre><code>&lt;service name=""AmazonSearchService""&gt;   &lt;!-- Endpoint for Amazon Web APIs --&gt;   &lt;port name=""AmazonSearchPort"" binding=""typens:AmazonSearchBinding""&gt;   &lt;soap:address location=""http://soap.amazon.com/onca/soap2""/&gt;   &lt;/port&gt; &lt;/service&gt;</code></pre><p>Edit: Visiting <a href=""http://soap.amazon.com/onca/soap2"" rel=""nofollow noreferrer"">that url</a> in a browser shows a page with this message:</p><blockquote>  <p>Amazon Ecommerce Web Service 3.0 has  been deprecated after many years of  useful service on March 31st 2008.  Please upgrade to the Amazon  Associates Web Service 4.0 as detailed  in the <a href=""http://developer.amazonwebservices.com/connect/entry.jspa?categoryID=12&amp;externalID=627"" rel=""nofollow noreferrer"">migration guide</a>. Please visit  <a href=""http://developer.amazonwebservices.com/connect/forum.jspa?forumID=9&amp;start=0"" rel=""nofollow noreferrer"">Amazon Associates Web Service  Developer Forum</a> for more information.  If you came to this page from an RSS  feed, visit <a href=""http://www.amazon.com/b/?ie=UTF8&amp;node=390052011"" rel=""nofollow noreferrer"">Amazon's Product RSS Feeds</a>  page for an upgrade.</p></blockquote><p>The migration guide has many WSDL locations, depending on which national site you want; the US site's wsdl is at <a href=""http://webservices.amazon.com/AWSECommerceService/AWSECommerceService.wsdl"" rel=""nofollow noreferrer"">http://webservices.amazon.com/AWSECommerceService/AWSECommerceService.wsdl</a> and the schema is at <a href=""http://webservices.amazon.com/AWSECommerceService/AWSECommerceService.xsd"" rel=""nofollow noreferrer"">http://webservices.amazon.com/AWSECommerceService/AWSECommerceService.xsd</a></p>"
1,3096855,3096631,https://stackoverflow.com/questions/3096855,TRUE,answer outdated and pointed out by answerer himself,0,1,0,0,,,yes,amazon-emr,deprecated,,point out deprecated version,0,1,0,0,0,"<p>Dirk's comments are spot on w.r.t multicore/foreach/doMC. </p><p>If you are doing thousands of simulations you may want to consider Amazon's Elastic Map Reduce (EMR) service. When I wanted to scale my simulations in R I started with huge EC2 instances and the multicore package (just like you!). It went well but I ran up a hell of an EC2 bill. I didn't really need all that RAM yet I was paying for it. And my jobs would finish at 3 AM then I would not get into the office until 8 AM so I paid for 5 hours I didn't need. </p><p>Then I discovered that I could use the EMR service to fire up 50 cheap small Hadoop instances, run my simulations, and then have them automatically shut down! I've totally abandoned running my sims on EC2 and now use EMR almost exclusively. This worked so well that my firm is beginning to test ways to migrate more of our periodic simulation activity to EMR. </p><p>Here's a <a href=""http://www.cerebralmastication.com/2010/02/using-the-r-multicore-package-in-linux-with-wild-and-passionate-abandon/"" rel=""nofollow noreferrer"">blog post</a> I wrote when I first started using multicore on EC2. Then when I discovered I could do this with Amazon EMR I wrote a <a href=""http://www.cerebralmastication.com/2010/02/you-can-hadoop-it-its-elastic-boogie-woogie-woog-ie/"" rel=""nofollow noreferrer"">follow up post</a>. </p><p><strong>EDIT:</strong>  since this post I've been working on a package for making it easier to use EMR with R for parallel apply functions. I've named the project <a href=""http://code.google.com/p/segue/"" rel=""nofollow noreferrer"">Segue and it's on Google Code</a>. </p><p><strong>Further Update:</strong> I've since deprecated Segue because there are much better and more mature offerings for accessing Amazon's services from R. </p>"
2,3895343,3893211,https://stackoverflow.com/questions/3895343,FALSE,code format in question outdated,0,0,0,1,,,no,,deprecated,,provide alternatives,1,0,0,0,0,"<p>This is probably not the best time to criticize your code but:</p><p>All your script should be included externally, not inline.Do not use , this is a deprecated, ancient way of including inline script.  If you must, use //I see in your script you are using document.write().  DON'T!</p><p>You cannot cheat and include a script in such a way and expect it to be executed.</p><p>Use <a href=""http://api.jquery.com/jQuery.getScript/"" rel=""nofollow"">jQuery.getScript()</a></p>"
3,3991880,3931881,https://stackoverflow.com/questions/3991880,TRUE,service in question outdated,0,0,1,0,,,yes,aws-api,deprecated,stopped working,point out deprecated version,1,0,1,0,0,"<p>It stopped working on my site. I assumed deprecated meant it won't work in the future, so don't build around it. I didn't think they'd remove it completely.</p>"
4,4150436,4149973,https://stackoverflow.com/questions/4150436,TRUE,service in question outdated,0,0,1,0,,,yes,amazon-ecs,outdated,,"point out deprecated version, share a link",1,0,1,0,0,"<p>A while back I had wrote a client library for communicating with Amazon's ECS (A2S) services.  Seems that Amazon hates their developers and keeps changing their service name and framework.  Well, my client is now completely broken because of <a href=""http://developer.amazonwebservices.com/connect/ann.jspa?annID=483"" rel=""nofollow"">Recent API Changes</a>.</p><p>Your best bet would be to look at their samples <a href=""http://aws.amazon.com/code/Product-Advertising-API/3941"" rel=""nofollow"">here</a>.</p><p>Good luck with this. Their API is huge and outdated from the experience I had, but that was two years ago. I get the impression that they really cater to developers for their enterprise services, not the free A2S.</p>"
5,4337337,4336842,https://stackoverflow.com/questions/4337337,TRUE,service in question outdated,0,0,1,0,,,yes,aws-sdk-ruby,out of date,version way out of date,"provide alternatives, provide code",0,1,0,1,0,"<p>The answer is yes. You can test your bootstrap script like this:</p><pre><code>elastic_mapreduce --create --alive --ssh</code></pre><p>This will create a node and give you a ssh connection to it, from which you can test your bootstrap script.</p><p>UPDATE: For reference here is what I'm running:</p><pre><code>#!/bin/bashsudo apt-get -y -V install irb1.8 libreadline-ruby1.8 libruby libruby1.8 rdoc1.8 ruby ruby1.8 ruby1.8-devwget http://production.cf.rubygems.org/rubygems/rubygems-1.8.11.zipunzip rubygems-1.8.11.zipcd rubygems-1.8.11sudo ruby setup.rbsudo gem1.8 install bson bson_ext json tzinfo i18n activesupport --no-rdoc --no-ri</code></pre><p>UPDATE2: to install aws-sdk</p><pre><code>#!/bin/bash# ruby developer packagessudo apt-get -y -V  install ruby1.8-dev ruby1.8 ri1.8 rdoc1.8 irb1.8sudo apt-get -y -V  install libreadline-ruby1.8 libruby1.8 libopenssl-ruby# nokogiri requirementssudo apt-get -y -V  install libxslt-dev libxml2-devwget http://production.cf.rubygems.org/rubygems/rubygems-1.8.11.zipunzip rubygems-1.8.11.zipcd rubygems-1.8.11sudo ruby setup.rbsudo gem1.8 install aws-sdk --no-rdoc --no-ri</code></pre><p>-y on apt-get makes it not prompt you</p><p>I wget rubygems because the version you get with apt-get is way out of date, and some gems won't build using an old version.</p>"
6,4979599,4976451,https://stackoverflow.com/questions/4979599,TRUE,service in question outdated,0,0,1,0,,,yes,"amazon-ec2, amazon-ami",outdated,kinda outdated now,"provide alternatives, share a link",0,1,1,0,0,"<p>The AMI's are kinda outdated now, but if you follow <a href=""http://groups.drupal.org/node/70268"" rel=""nofollow"" title=""these instructions"">these instructions from the pantheon group on g.d.o</a> using one of the Alestic Ubuntu 10.4 ami's as a base you should be fine to roll your own.</p>"
7,5029447,5027097,https://stackoverflow.com/questions/5029447,TRUE,service in question outdated,0,0,1,0,,,yes,amazon-emr,deprecated,,provide alternatives,0,1,0,0,0,"<p>The difference is that the mapred packages are deprecated. You should use since 0.20.x the new stuff inside mapreduce package. <br>For example the new way to implement a mapper, with mapred-package you have to implement the mapper interface. With the mapreduce-package you simply extend from a basic mapper class and override just the method you need.</p>"
8,5265964,4498192,https://stackoverflow.com/questions/5265964,TRUE,service in question outdated,0,0,1,0,,,yes,amazon-simpledb,deprecated,,"provide alternatives, share a link",0,1,1,0,0,"<p>The ""query"" interface was the original search interface for SimpleDB.  It was set-based, non-standard and quite lovely, I thought.  However, over time AWS introduced a SQL-like query language (accessed via the Select request) and then deprecated and eventually removed the original query interface.</p><p>So, the reason it doesn't work in boto is because it is no longer supported by SimpleDB.  For more up-to-date boto documentation, look <a href=""http://boto.cloudhackers.com/"" rel=""noreferrer"">here</a>.</p>"
9,5654982,5639803,https://stackoverflow.com/questions/5654982,FALSE,outdated software-possible cause raised by answerer,0,0,0,1,,,yes,amazon-ses,outdated,,"provide alternatives, share a link",0,1,1,0,0,"<p>For some reason, your OpenSSL does not get the intermediate certificate. (Outdated software?)</p><p>Install the intermediate certificate with the subject hash 0xeb99629b, available at <a href=""https://knowledge.verisign.com/support/ssl-certificates-support/index?page=content&amp;actp=CROSSLINK&amp;id=AR1513"" rel=""nofollow"">https://knowledge.verisign.com/support/ssl-certificates-support/index?page=content&amp;actp=CROSSLINK&amp;id=AR1513</a>.</p><p>You can use it with the <code>-CAfile</code> parameter in OpenSSL tools and with the environment variable <code>HTTPS_CA_FILE</code> for the Perl HTTPS stack. To use it system-wide, place it in the appropriate ca-certificates directory, e.g. <code>/etc/ssl/certs</code>, and <a href=""http://www.manpagez.com/man/1/c_rehash/"" rel=""nofollow""><code>c_rehash</code></a> the directory.</p>"
10,5820746,5820120,https://stackoverflow.com/questions/5820746,FALSE,file extension in question outdated,0,0,0,1,,,no,ogg,deprecated,,"provide alternatives, share a link",0,1,1,0,0,"<p>Man, that sounds incredibly frustrating. Looks like you're doing all of the things I would recommend, and trying lots of variations/tests. I've got one idea, and two maybe not so good ideas:</p><p>1) I ran into encoding issues when I used Miro, and ended up having files that would play fine locally... but ended up behaving differently when I viewed them online. To ensure that it's not an encoding issue, try subbing in an existing .ogv file that you know works - possibly from some other site/tutorial.</p><p>Standby reference: <a href=""http://www.bigbuckbunny.org/index.php/download/"" rel=""nofollow"">Big Buck Bunny</a></p><p>2) This is a longshot, and it's me showing how little I know about these formats... but maybe try using .ogg as an extension, instead of .ogv? I looked and see that .ogv is meant for video, with .ogg being deprecated... but changing the file extension might reveal something about your encoding process.</p><p>3) Another longshot: kill the codecs attribute for your mp4 source.</p>"
11,5935495,4811761,https://stackoverflow.com/questions/5935495,TRUE,service in question outdated,0,0,1,0,,,yes,amazon-simpledb,outdated,all others are outdated,"provide alternatives, share a link",0,1,1,0,0,"<p>Just for completeness, I ended up using the SDK provided by Amazon for Java. It can be found here:</p><p><a href=""http://aws.amazon.com/sdkforjava/"" rel=""nofollow"">http://aws.amazon.com/sdkforjava/</a></p><p>All other client libraries were outdated or had missing functionality.</p>"
12,7129403,4521846,https://stackoverflow.com/questions/7129403,TRUE,service in question outdated,0,0,1,0,,,yes,mturk,deprecated,,"provide alternatives, share a link",0,1,1,0,0,"<p>As msha points out, the sending of a workerID parameter to an ExternalQuestion page seems to be deprecated, or at least taken out of the latest version of the documentation.</p><p>However, a fellow researcher who's been using MTurk a lot says: ""People seem to be using it in the forums. I would go ahead with it...if it ever actually disappears, I'm sure that the developer community will yell very loudly. :) ""</p><p>I tried it empirically today (2011-08-19), and indeed a workerID is being sent to the ExternalQuestion page I set up on my own server, after the HIT has been accepted. This was in the sandbox. My ExternalQuestion page contained a Java Web Start button (as described here: <a href=""http://download.oracle.com/javase/tutorial/deployment/deploymentInDepth/createWebStartLaunchButtonFunction.html"" rel=""nofollow"">http://download.oracle.com/javase/tutorial/deployment/deploymentInDepth/createWebStartLaunchButtonFunction.html</a> ); I don't know if that made any difference.</p>"
13,7322249,7259601,https://stackoverflow.com/questions/7322249,FALSE,outdated kernels,0,0,0,1,,,yes,amazon-ec2,outdated,,user found issues himself,0,0,0,0,1,"<p>We found the issue. We noticed that some instances in our cluster always produced the problem and some never did. Apparently the issue is unique to instances that run more recent CPU versions combined with slightly outdated kernels.</p><p>The issue is explained in full here : <a href=""https://bugs.launchpad.net/ubuntu/+source/linux/+bug/727459"" rel=""nofollow noreferrer"">https://bugs.launchpad.net/ubuntu/+source/linux/+bug/727459</a></p><p>As the running time increases the spikes shown in the graph below become longer and longer, possible due to time shift and at some point it'll spin the one core for a long time.</p><p>Cpu usage graphs. Affected instance versus unaffected:</p><p><img src=""https://i.stack.imgur.com/sM8r6.png"" alt=""Cpu usage graphs. Affected instance versus unaffected""></p><p>The issue has been fixed recently so a kernel update fixed this issue for us.</p>"
14,7543698,6645026,https://stackoverflow.com/questions/7543698,TRUE,service in question outdated,0,0,1,0,,,no,ruby-rails,deprecated,has been deprecated,provide alternatives,0,1,0,0,0,<p>i think you are facing this problem because you are using RAILS_ROOT which has been deprecated in Rails 3. try using Rails.root instead and see if you face the same problem. Please check the correct usage for Rails.root before making the correction. </p>
15,7642625,7638646,https://stackoverflow.com/questions/7642625,FALSE,outdated solution provided due to costs (but still practical),0,0,0,1,,,yes,amazon-ec2,outdated,,"provide alternatives, share a link",0,1,1,0,0,"<p>As described in the question both Azure and EC2 will do the job very well. This is the kind of task both systems are designed for.</p><p>So the question becomes really: which is <em>best</em>? That depends on two things: what the application needs to do and your own experience and preference.</p><p>As it's a Windows application there should probably be a leaning towards Azure. While EC2 supports Windows, the tooling and support resources for Azure are probably deeper at this point. </p><p>If cost is a factor then a (somewhat outdated) resource is here: <a href=""http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost-comparison/"" rel=""nofollow"">http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost-comparison/</a> -- the conclusion is that, by and large, Azure and Amazon are roughly similar for compute charges.</p>"
16,7651730,6675018,https://stackoverflow.com/questions/7651730,TRUE,service in question outdated,0,0,1,0,,,yes,"amazon-mws, amazon-product-api",deprecated,,"point out deprecated version, provide alternatives, share a link",1,1,1,0,0,"<p>What you really want is the equivalent of the <code>OpenListings</code> report from the <code>Product Advertising API</code> in MWS and that is the <code>RequestReport</code> call with a report type of <code>_GET_MERCHANT_LISTINGS_DATA_</code>. This returns you all the inventory a seller has listed on Amazon and from here to getting your ASIN from that list it's close.</p><p>You can find out more details in their <a href=""https://developer.amazonservices.com/gp/mws/docs.html"" rel=""nofollow"">documentation</a></p><p>Also, I advise you to not use the Product Advertising API anymore as Amazon deprecated it and it will be out of use this time next year.</p>"
17,7718816,7704946,https://stackoverflow.com/questions/7718816,FALSE,outdated' mentioned to answer questions,0,0,0,1,,,yes,amazon-s3,outdated,,provide alternatives,0,1,0,0,0,<p>Changing URLs is a safe way to invalidate outdated assets.</p>
18,7865273,7864988,https://stackoverflow.com/questions/7865273,TRUE,code format in question outdated,0,0,0,1,,,no,UIDevice-uniqueIdentifier,deprecated,,"point out deprecated version, provide alternatives, share a link",1,1,1,0,0,"<p>Judging from your post, the error lies in </p><pre><code>[[UIDevice currentDevice] uniqueIdentifier]</code></pre><p>The method UIDevice <code>uniqueIdentifier</code> is deprecated in iOS 5 and should not be called. From your code I cant really see what exactly you are trying to do but this post<a href=""https://stackoverflow.com/questions/6993325/uidevice-uniqueidentifier-deprecated-what-to-do-now"">UIDevice uniqueIdentifier Deprecated - What To Do Now?</a></p><p>should be helpful in overcoming the deprecated method. You should change the deprecated calls and also use the ones listed in the post above. That should do the trick.</p>"
19,8106724,8106239,https://stackoverflow.com/questions/8106724,FALSE,outdated' mentioned to answer questions,0,0,0,1,,,no,amazon-ec2,out of date,,share a link,0,0,1,0,0,"<p>There are lots of guides out there, some out of date, and some recent ones, below are some links. You might also be interested in <a href=""http://aws.amazon.com/elasticbeanstalk/"" rel=""nofollow"">Amazon Elastic Beanstalk</a>. Here are some links</p><ul><li><a href=""http://blog.peterdelahunty.com/2009/06/deploying-grails-app-on-ec2-in-from.html"" rel=""nofollow"">Deploying Grails App on Amazon EC2 form scratch</a></li><li><a href=""http://grails.1312388.n4.nabble.com/Amazon-EC2-td3055649.html"" rel=""nofollow"">Question about EC2 and Grails on the mailing list</a></li></ul><p>To answer the question about the credit card, I think you can't get away without one, since if you go over the free usage tier you need to have some sort of payment method to pay for the extra usage.</p>"
20,8109806,8109792,https://stackoverflow.com/questions/8109806,TRUE,code format in question outdated,0,0,0,1,,,yes,amazon-s3,deprecated,,"point out deprecated version, provide alternatives",1,1,0,0,0,"<p>The constant <code>RAILS_ROOT</code> is deprecated and is not available in 3.1 (IMHO)</p><p>Try using :<code>:s3_credentials =&gt; ""#{Rails.root}/config/s3.yml""</code></p><p>Instead of <code>:s3_credentials =&gt; ""#{RAILS_ROOT}/config/s3.yml""</code></p>"
21,8125958,8056962,https://stackoverflow.com/questions/8125958,TRUE,service in question outdated,0,0,1,0,,,yes,amazon-product-api,deprecated,has been deprecated,"point out deprecated version, provide alternatives",1,1,0,0,0,"<p>It used to be be possible by doing a brute force search with the Seller* API calls. However these calls have been deprecated since Nov 1 2011, so you're out of luck. If you happen to be working for this particular seller (instead of being simply a customer or a competitor), you'll want to use the APIs available to the seller (MWS) to download inventory reports.</p>"
22,8127487,7800800,https://stackoverflow.com/questions/8127487,FALSE,outdated solution provided due to costs (but still practical),0,0,0,1,,,yes,amazon-web-services,deprecated,,"provide alternatives, provide code",0,1,0,0,0,"<p>I'm working on the same issue and I know how to execute the GA code you want to execute, especially since you are working with the older version of the code. I am working on the same issue with an Amazon webstore -- if you are able to upload a js file, and it looks like you are able to do this in files, I would suggest using the init() function which is deprecated but it still works - here is an example where I am doing something similar. I am setting the contents of a custom variable and creating the tracking object -- but I'm not firing the page tracker until later in the code.</p><p>Using the code below, replace the Custom Variable setting code with the linker and set Domain code you need for cross domain tracking. Since the GA code added by amazon doesn't try to set the domain the linker functions should prevent a second cookie from being created, and use the cookie data passed in the URL.</p><pre><code>&lt;script type=""text/javascript""&gt; var gaJsHost = ((""https:"" ==   document.location.protocol) ? ""https://ssl."" : ""http://www."");document.write(unescape(""%3Cscript src='"" + gaJsHost + ""google-analytics.com/ga.js'   type='text/javascript'%3E%3C/script%3E""));&lt;/script&gt;&lt;script type=""text/javascript""&gt;try {var pageTracker = _gat._getTracker(""UA-xxxxxxx-x""); pageTracker._initData();pageTracker._setCustomVar(1, ""GWO"", utmx('combination').toString(), 1);} catch(err) {}&lt;/script&gt;</code></pre>"
23,8173888,8173561,https://stackoverflow.com/questions/8173888,FALSE,outdated' mentioned to answer questions,0,0,0,1,,,yes,"amazon-ec2, amazon-web-services",obsolete,,provide alternatives,0,1,0,0,0,"<p>Zach, the simple answer is that there's not a simple path to there from here :)</p><p>When I wrote Segue I hoped that someone would soon come out with something that would make Segue obsolete. Cloudnumbers may be it one day, but probably not yet. I have toyed with making Segue a foreach backend, but since I don't use it that way, my motivation has been pretty low to take the time to learn how to build the backend. </p><p>One of the things that is very promising, in my opinion, is using the <code>doRedis()</code> package with workers on Amazon EC2. doRedis uses a Redis server as the job controller and then lets workers connect to the Redis server and get/return jobs and results. I've been thinking for a while that it would be nice to have a dead simple way to deploy a doRedis cluster on EC2. But nobody has written one yet that I know of. </p>"
24,8176018,8151783,https://stackoverflow.com/questions/8176018,TRUE,service in question outdated,0,0,1,0,,,yes,"amazon-product-api, amazon-web-services",deprecated,,"point out deprecated version, provide alternatives, share a link",1,1,1,0,0,"<p>On November 8, 2010 the Reviews response group of the Product Advertising API will no longer return customer reviews content and instead will return a link to customer reviews content hosted on Amazon.com.So you won't be able to fetch back the AverageRating and Totalreviews.</p><p>Amazon API latest documentation:</p><p><a href=""http://docs.amazonwebservices.com/AWSECommerceService/latest/DG/"" rel=""nofollow"">http://docs.amazonwebservices.com/AWSECommerceService/latest/DG/</a></p><p>Looks like you can't get sellerfeedback from the new API:</p><p>Product Advertising API Change DetailsThe following changes will take effect on 11/1/2011:</p><p>Seller Operations: The SellerLookup, SellerListingLookup and SellerListingSearch operations will be deprecated. All requests for these operations will be rejected with a corresponding error message. </p>"
25,8677786,8677696,https://stackoverflow.com/questions/8677786,FALSE,outdated solution corrected by answerer,0,0,0,1,,,yes,"amazon-s3, amazon-web-services",deprecated,,"point out deprecated version, provide alternatives, share a link",1,1,1,0,0,"<p>Yes this is very much possible. You will only have to deal with how would like to send the URL back after creating the pre signed URL. Maybe as XML or as plain text.. </p><p>I would recommend using the jets3t  library to create the URL. You could call this API:</p><p>org.jets3t.service.S3Service.createSignedPutUrl(String, String, Map, Date, boolean)</p><p>You could also an IAM policy instead. But your way is easier.</p><p>Edit: Added more information..</p><p>That's the beauty of it, when you are using the jets3t library you dont need to any of that. Just call the method : </p><pre><code> S3Service s3Service = new RestS3Service(awsCredentials);// Create a signed HTTP PUT URL valid for 5 minutes.  String putUrl = s3Service.createSignedPutUrl(bucket.getName(), object.getKey(),  object.getMetadataMap(), expiryDate, false);</code></pre><p>And use the URL to upload your image like below:</p><pre><code>SignedUrlHandler signedUrlHandler = new RestS3Service(null);S3Object object = new S3Object(bucket,&lt;your file object goes here&gt;); S3Object putObject = signedUrlHandler.putObjectWithSignedUrl(putUrl, object);</code></pre><p><strong>Edit for signedGetURL():</strong></p><p>Only the ones with ProviderCredential being passed in are deprecated.. This one is not : </p><p><a href=""http://jets3t.s3.amazonaws.com/api/org/jets3t/service/S3Service.html#createSignedGetUrl(java.lang.String"" rel=""nofollow"">http://jets3t.s3.amazonaws.com/api/org/jets3t/service/S3Service.html#createSignedGetUrl(java.lang.String</a>, java.lang.String, java.util.Date)</p><p>You wouldnt need to pass the credentials into this method anyway. Its only the s3service object that needs the credentials.. like this...</p><pre><code>S3Service s3service=new RestS3Service(credentials); // pass your credentials hereString createSignedGetUrl = s3service.createSignedGetUrl(storageObject.getBucketName(),  storageObject.getKey(),new DateTime().plusMinutes(5).toDate());</code></pre>"
26,8718053,8691975,https://stackoverflow.com/questions/8718053,FALSE,data in service outdated or wrong,0,0,1,0,,,yes,"amazon-mws, amazon-product-api",out of date,,"provide alternatives, share a link",0,1,1,0,0,"<p>I'm sorry to say that there is no way to get this info using the MWS API. The MWS API doesn't have any relative information, that is, it doesn't know anything about <em>other sellers' items</em> so there is no way to see what your items are priced at relative to others (which is the info you would need to determine if you own the Buy Box).</p><p>As you already know, you can get this info through the Product Advertising API but with the new limitations in place this may not be practical for the size of your inventory (would take <strike>three days</strike> two and a half hours at 20,000 items per <strike>day</strike> hour). The ""re-pricing"" service that you've used in the past was most likely affected by the new limitations. However, any existing accounts were given a grace period to change their software. The new limitations will go into effect for these accounts on February 12, 2012.</p><p>The only other option open to you is to get the info from the site (screen scrape). This isn't a very attractive alternative due to the latency issues but if you've got the infrastructure to do massive amounts of parallel calls then go for it. In certain situations I prefer to get this type of info from the site since this is what the buyers are seeing (most current info). In the past I've seen data coming from the Product Advertising API that was out of date or just plain wrong.</p><hr><p>The limits are defined in the documentation under the obscure subtitle of ""Efficiency Guidelines"" and is located <a href=""http://docs.amazonwebservices.com/AWSECommerceService/latest/DG/TroubleshootingApplications.html?r=2536"" rel=""nofollow"">here</a> (at the bottom of the page). </p><p>I must admit that it's been a while since I had worked with the PA-API and had forgotten the limits. I thought it was something like 2,000 a day but it's actually 2,000 per hour at one call per second. If you're making calls too quickly they'll return a 503 response which is documented <a href=""http://docs.amazonwebservices.com/AWSECommerceService/latest/DG/ErrorNumbers.html"" rel=""nofollow"">here</a>.</p><hr><p><strong>Update:</strong> Amazon has added a <a href=""https://developer.amazonservices.com/gp/mws/api.html?ie=UTF8&amp;section=products&amp;group=products&amp;version=latest"" rel=""nofollow"">Products API</a> to the MWS APIs. The <code>GetCompetitivePricingForSKU</code> gives you pricing information similar to the Product Advertising API's ItemLookup function. With this information you should be able to determine the price of the Buy Box owner. </p>"
27,8927333,8407750,https://stackoverflow.com/questions/8927333,TRUE,service in question outdated,0,0,1,0,,,yes,"amazon-s3, amazon-web-services",out of date,,"provide alternatives, provide code",0,1,1,1,0,<p>Apparently I was looking at some badly out of date documentation.</p>
28,8995926,8995304,https://stackoverflow.com/questions/8995926,TRUE,service in question outdated,0,0,1,0,,,yes,"amazon-ec2, amazon-iam, amazon-web-services",deprecated,,"point out deprecated version, provide alternatives, share a link",1,1,1,0,0,"<h2>Preface</h2><p>First and foremost, you might want to reconsider whether you actually need these X.509 certificates - the tutorial is correct in principle:</p><blockquote>  <p>There are three types: access keys, X.509 certificates and key pairs.  The first and second type allow you to connect to the Amazon APIs.  Which type of credential depends on which API and tool you are using.  Some APIs and tools support both options, whereas others support just  one.</p></blockquote><p>However, nowadays most modern APIs and tools are interacting with AWS by means of access keys only rather than X.509 certificates.</p><p><strike>Unfortunately this is not the case for the EC2 API Tools the tutorial is based on though, which indeed require the use X.509 certificates due to being (mostly) based on the older EC2 SOAP API still.</strike></p><p><strong>Update</strong>: The EC2 API Tools meanwhile support AWS access keys as well and <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/setting_up_ec2_command_linux.html#set_aws_credentials_linux"">deprecated using X.509 certificates</a> accordingly:</p><blockquote>  <p>Although we don_Ã‘Ã©t encourage it, for a limited time you can still use  EC2_PRIVATE_KEY and EC2_CERT instead of AWS_ACCESS_KEY and  AWS_SECRET_KEY. For more information, see Deprecated Options in <a href=""http://docs.aws.amazon.com/AWSEC2/latest/CommandLineReference/CLTRG-common-args-api.html"">Common Options</a>   for API Tools in the Amazon Elastic Compute Cloud CLI  Reference. If you specify both sets of credentials, the command line  tools use the access key ID and secret access key.</p></blockquote><h3>Alternative</h3><p>You might want to check out an alternative first though: If you are comfortable in Python, I'd highly recommend the excellent <a href=""http://code.google.com/p/boto/"">boto</a> (<em>An integrated interface to current and future infrastructural services offered by Amazon Web Services</em>), which works just fine with access keys, offers almost the same feature set as the <em>EC2 API tools</em> (plus most other AWS APIs) and performs significantly faster due to targeting the newer AWS REST APIs only.</p><h2>Solution</h2><p><a href=""http://aws.amazon.com/iam/"">AWS Identity and Access Management (IAM)</a> does not support accessing the actual AWS account, it only covers the <a href=""http://aws.amazon.com/console/"">AWS Management Console</a>, and most AWS APIs of course. You'll need to sign in with the AWS account's login and password (i.e. those of the account owner) to access the <a href=""http://aws-portal.amazon.com/gp/aws/developer/account/index.html?action=access-key"">Security Credentials</a> page.</p><p>This is not recommended anymore though (see section <em>Security Credentials</em> within <a href=""http://docs.amazonwebservices.com/IAM/latest/UserGuide/IAM_Concepts.html#IAM_SecurityCredentials"">IAM Concepts</a>):</p><blockquote>  <p>[...] when you create an AWS account, AWS gives the  AWS account its own Secret Access Key and Access Key ID by default.  The AWS account can make API calls to AWS with them. <strong>We expect that  you won't use those credentials on a regular basis, but will use them  only to initially set up an administrators group for your  organization</strong>. We recommend that all further API interaction between  your AWS account and your AWS resources be at the user level (for  example, using users' security credentials). <em>[emphasis mine]</em></p></blockquote><p>However, <strong>you can still achieve your goal</strong> by using your own certificate as outlined further down in section <em>X.509 Certificates</em>:</p><blockquote>  <p>Although you can use IAM to create an access key, you can't use IAM to  create a signing certificate. However, you can use free third-party  tools such as OpenSSL to create the certificate. [...] After you have the  signing certificate, you must upload it to IAM; [...]</p></blockquote><p>How to actually do the latter is illustrated in <a href=""http://docs.amazonwebservices.com/IAM/latest/UserGuide/Using_UploadCertificate.html"">Uploading a Signing Certificate</a>.</p>"
29,9064647,5302394,https://stackoverflow.com/questions/9064647,TRUE,service in question outdated,0,0,1,0,,,yes,amazon-ec2,outdated,outdated version,"point out deprecated version, provide alternatives, share a link",1,1,1,0,0,"<p>Well, as stated by Geoff Appleford, Elasticfox is an outdated extension. I dont know why it's development stopped as it is a great tool.</p><p>If you wish to do things that are achieved through Elasticfox, I would suggest you to take a look at Hybridfox. Hybrdifox is a fork of Elasticfox which supports other private clouds as well. It is also a great tool too.</p><p>It is also open source and actively updated. Check it's project home(http://code.google.com/p/hybridfox/)</p><p>Sources:<a href=""http://code.google.com/p/hybridfox/"" rel=""nofollow"">Hybridfox home</a></p>"
30,59148868,55967456,https://stackoverflow.com/questions/59148868,TRUE,,0,1,0,0,0,1,yes,"amazon-sagemaker, amazon-cognito",outdated,,,1,1,1,0,0,"<p>This is actually possible programmatically, without relying on workers to report their identity - I ran into the same problem, and found the following:</p><ol><li><p>Sagemaker Ground Truth does automatic logging of worker actions. Among the things it logs is the <code>workerId</code>, with which you're familiar, the <code>cognito_user_pool_id</code>, and the <code>cognito_sub_id</code> (take a look at the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/workteam-private-tracking.html"" rel=""nofollow noreferrer"">track worker performance docs</a>).</p></li><li><p>The <code>workerId</code> is Ground Truth specific and opaque, and there isn't a way to get Ground Truth to tell you which Cognito user a <code>workerId</code> maps to. However, a Cognito user <a href=""https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html"" rel=""nofollow noreferrer"">is uniquely mapped to by its <code>sub id</code></a>. </p></li><li><p>You can leverage the logs' pairing of <code>workerId</code> and <code>cognito_sub_id</code> to generate mappings, by <a href=""https://stackoverflow.com/questions/43488445/how-do-i-look-up-a-cognito-user-by-their-sub-uuid"">using the cognito sub id to query the cognito username</a> (make sure to read down the list of answers - the accepted one says it's not possible, but it's outdated).</p></li></ol><p>You can use the mappings above to maintain a database of <code>workerId - cognito sub id - username</code> triplets, and use that database whenever you need to figure out what user a <code>workerId</code> belongs to. Note, that this will mean the first time you see a <code>workerId</code> in a Ground Truth job, you won't have a way to find its mapping. If this is a problem, you can actually solve it by using a throwaway job as suggested previously. The logs of that job will include the mappings you need.</p>"
31,53772908,48340877,https://stackoverflow.com/questions/53772908,FALSE,outdated' mentioned to answer questions,0,0,0,1,0,0,yes,aws-fargate,deprecated,,,0,1,0,1,0,"<p>After reading your final comment on the boot sequence and answering that question instead, I solved this (even in non-AWS) using the docker-compose <code>depends</code>.</p><p>Simple e.g.</p><pre><code>services:  web:  depends_on:  - ""web_db""  web_db:  image: mongo:3.6  container_name: my_mongodb</code></pre><p>You should be able to remove the deprecated <code>links</code> and just use the hostnames  that docker creates from the service container names.  e.g. above the website would connect to the hostname: ""my_mongodb"".</p>"
32,37363867,37347003,https://stackoverflow.com/questions/37363867,TRUE,,0,0,1,0,0,1,yes,"amazon-dynamodb, aws-lambda",deprecated,deprecated a year ago,,1,1,1,1,0,"<p>Your demo repo does not appear to be including the AWS SDK &amp; setting the region as noted in the <a href=""https://github.com/awslabs/dynamodb-document-js-sdk"" rel=""nofollow"">Getting Started guide</a>.  I.e.:</p><pre><code>var AWS = require(""aws-sdk"");var DOC = require(""dynamodb-doc"");AWS.config.update({region: ""us-west-1""});var docClient = new DOC.DynamoDB();... </code></pre><p>Note that <code>dynamo-doc</code> was deprecated almost a year ago.  You may want to try the <a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html"" rel=""nofollow"">DynamoDB DocumentClient</a> instead.  This updated API has much more clear error-handling semantics that will probably help point out where the problem is.</p>"
33,49291577,48964473,https://stackoverflow.com/questions/49291577,TRUE,,0,0,1,0,0,1,yes,"aws-sdk, aws-cli",out of date,,,1,1,1,0,1,"<p>I think the documentation may be out of date. At AWS re:Invent 2017, there was an excellent session called ""AWS CLI: 2017 and Beyond"" which is <a href=""https://www.youtube.com/watch?v=W8IyScUGuGI"" rel=""nofollow noreferrer"">currently available on YouTube</a>. The presentation goes into detail about some of the new features, including dynamic credentials for the AWS CLI, and there is a corresponding GitHub repository <a href=""https://github.com/awslabs/awsprocesscreds"" rel=""nofollow noreferrer"">awslabs/awsprocesscreds</a> which may have some useful examples.</p>"
34,58124653,58046398,https://stackoverflow.com/questions/58124653,FALSE,approaches in question outdated,0,0,0,1,0,0,yes,aws-api-gateaway,deprecated,,,1,1,1,1,0,"<p>Ultimately the solution here is to implement a <a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-create-api-as-simple-proxy"" rel=""nofollow noreferrer"">lambda proxy integration</a>. </p><p>A proxy integration in API Gateway tells API Gateway to simply forward all headers to the integration for processing, which means you will see all of those values in your lambda function.</p><p>NOTE: Lambda has to return a specific response format back to API Gateway if it's a proxy integration. Ultimately, the response should be something like:</p><pre>  {  'statusCode': 200,  'headers': {  'echo-proxy': True  },  'body': 'some payload'  }</pre><p>What you are trying to do right now is map everything manually, which is a deprecated approach and usually you don't want to do that unless you absolutely have to because it's kind of a pain.</p><p>If you have to map the headers manually start by mapping them in the the method request so it can carry on to the next step and so on. Basically like this:</p><blockquote>  <p>Method Request -> Maps variable to Integration Request -> Maps variable to Body Mapping Template -> Maps variable to actual request header</p></blockquote><p>What you have in your screenshot for the Integration Request -> HTTP Headers is:</p><blockquote>  <p>Name: cokers  Mapped from: 'blah'</p></blockquote><p>However, ""mapped from"" should look something like ""method.request.header.coker"" which is a standardized path (meaning to get the value from the Method Request Header field with name ""coker""). </p><p>Once you have added the coker header to the Method Request, and the Integration Request HTTP Headers are mapped correctly, you have to implement a mapping template. Set the content-type to application/json with passthrough set to ""When there are no templates defined(recommended)"" and a simple mapping template:</p><pre>  {  ""headers"": {  ""coker"" : ""$input.params('coker')""  }  }</pre><p>That is the way my API is setup and it returns the following to me because I had my lambda function return the event as a json object back to API GW:</p><pre>{""body"": ""{\""headers\"": {\""coker\"": \""mapped\""}}"", ""statusCode"": 200}</pre><p>NOTE: the value of my header ""coker"" in the request on the client side is ""mapped""</p><p><strong>UPDATED ANSWER</strong><br />To map the original ""coker"" header to ""coker2"" (or any other name you want to give it) you simply set the name of the header in your mapping template like so:</p><pre>  {  ""headers"": {  ""coker2"" : ""$input.params('coker')""  }  }</pre><p>Then edit your lambda function to return ""coker2"" header and you should get a response like this:</p><pre>{""body"": ""{\""headers\"": {\""coker2\"": \""mapped\""}}"", ""statusCode"": 200}</pre>"
35,16531304,16531252,https://stackoverflow.com/questions/16531304,FALSE,user asks for new version,0,0,0,1,0,0,no,,deprecated,,,1,1,1,1,0,"<p><a href=""http://php.net/manual/en/function.mysql-connect.php"">The documentation for PHP says</a> </p><blockquote>  <p>This extension is deprecated as of PHP 5.5.0, and will be removed in the future. Instead, the <a href=""http://www.php.net/manual/en/book.mysqli.php"">MySQLi</a> or <a href=""http://www.php.net/manual/en/ref.pdo-mysql.php"">PDO_MySQL</a> extension should be used.</p></blockquote><pre><code>$mysqli = new mysqli('localhost', 'my_user', 'my_password', 'my_db');/* * This is the ""official"" OO way to do it, * BUT $connect_error was broken until PHP 5.2.9 and 5.3.0. */if ($mysqli-&gt;connect_error) {  die('Connect Error (' . $mysqli-&gt;connect_errno . ') '  . $mysqli-&gt;connect_error);}</code></pre>"
36,43889585,43883077,https://stackoverflow.com/questions/43889585,FALSE,code format in question outdated,0,0,0,1,0,0,yes,"amazon-s3, amazon-emr",deprecated,,,1,1,0,1,0,"<p>Not sure, how did you manage to upload .part files to s3 without authentication even if you have tweaked s3 policies. I guess you might have added aws keys in the system environment as properties or in conf files.In order to access aws resource, atleast its required to supply access key and secret key. Also, s3 scheme is deprecated now.Following code works with hadoop-aws-2.8.0.jar and spark 2.1.(note: I should have used s3a scheme as its the preferred over s3n (native scheme).</p><pre><code>val spark = SparkSession  .builder  .appName(""SparkS3Integration"")  .master(""local[*]"")  .getOrCreate()  spark.sparkContext.hadoopConfiguration.set(""fs.s3n.awsAccessKeyId"", awsAccessKey)  spark.sparkContext.hadoopConfiguration.set(""fs.s3n.awsSecretAccessKey"", awsSecretKey) val rdd = spark.sparkContext.parallelize(Seq(1,2,3,4)) rdd.saveAsTextFile(""s3n://&lt;bucket_name&gt;/&lt;path&gt;"")</code></pre>"
37,58307579,58307102,https://stackoverflow.com/questions/58307579,FALSE,outdated' mentioned to answer questions,0,0,0,1,0,0,yes,amazon-web-services,outdated,outdated version,,1,1,1,1,0,"<p>You may be using an outdated version of the cli, please upgrade.  What version are you using?</p><pre><code>aws --version</code></pre><p>Support for intelligent tier was added in version 1.16.61 of the aws cli as can be seen in the <a href=""https://github.com/aws/aws-cli/commit/71f970fd26656190916423a6e3fd91ed3d33433c#diff-46607fbd76883995d41046735ccc8ac1"" rel=""nofollow noreferrer"">Github project</a>.</p>"
38,15586927,5285085,https://stackoverflow.com/questions/15586927,TRUE,,0,0,1,0,1,0,yes,amazon-s3,outdated,mostly outdated,,1,1,1,0,0,"<p>Some time ago I did my fork especially for S3 in mind. My fork work with official AWS-SDK, instead of old aws-s3 which is mostly outdated.</p><p>If anybody will search for S3 solution for paperclip this is one that work (today)</p><p><a href=""https://github.com/krzak/dm-paperclip-s3"" rel=""nofollow"">https://github.com/krzak/dm-paperclip-s3</a></p><p>take a look at readme to get how to configure paperclip for S3</p>"
39,62762520,62757921,https://stackoverflow.com/questions/62762520,TRUE,,0,0,1,0,1,0,no,,deprecated,essentially deprecated,,1,1,0,0,0,"<p>My recommendation is that you migrate from boto, which is essentially deprecated, to boto3 because boto3 supports signature v4 by default (with the exception of S3 pre-signed URLs which has to be explicitly configured).</p>"
40,10772392,10235959,https://stackoverflow.com/questions/10772392,TRUE,,0,0,1,0,1,0,yes,amazon-web-services,out of date,,,1,1,1,0,0,"<p>The Amazon Documentation is incorrect/out of date - use <a href=""http://ecs.amazonaws.com"" rel=""nofollow"">ecs.amazonaws.com</a> instead of <a href=""http://webservices.amazon.com"" rel=""nofollow"">webservices.amazon.com</a></p><p>So sign this:</p><blockquote>  <p>GET\necs.amazonaws.com\n/onca/xml\nAWSAccessKeyId=AKIAIOSFODNN7EXAMPLE&amp;ItemId=0679722769&amp;Operation=ItemLookup&amp;ResponseGroup=ItemAttributes%2COffers%2CImages%2CReviews&amp;Service=AWSECommerceService&amp;Timestamp=2009-01-01T12%3A00%3A00Z&amp;Version=2009-01-06</p></blockquote><p>Sorry this is a really quick reply!</p>"
41,33181296,31709257,https://stackoverflow.com/questions/33181296,TRUE,,0,0,1,0,1,0,yes,"aws-sdk, amazon-dynamodb, aws-lambda",deprecated,,,1,1,1,0,0,"<p>Previous <a href=""https://github.com/awslabs/dynamodb-document-js-sdk"" rel=""nofollow"">DynamoDB Document SDK</a> was deprecated, new client from standard Javascript SDK should be used from now on:</p><p><a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html</a></p>"
42,61099600,61085087,https://stackoverflow.com/questions/61099600,TRUE,user found outdated version by himslef,0,0,1,0,1,0,yes,"aws-lambda, amazon-dynamodb",outdated,a little bit outdated,,1,1,0,0,0,<p>It turned out that I was using a pre-made lambda deployment package that was a little bit outdated. I created a venv then used a build script to create the zip file. It worked perfectly fine. </p>
43,54194074,54191023,https://stackoverflow.com/questions/54194074,FALSE,code format in question outdated,0,0,0,1,1,0,yes,"aws-lambda, aws-codepipeline, aws-sdk-nodejs",deprecated,has been deprecated,,1,1,0,1,0,"<p>With Lambda's Node 6/8 runtimes, it's easier if you use Promises (or even the <code>async/await</code> syntax).</p><p>The old <code>context.succeed()</code>/<code>context.fail()</code> syntax was for the old Node versions and those have been deprecated.</p><pre><code>// Leave this outside your handler so you don't instantiate in every invocation.const codepipeline = new aws.CodePipeline();exports.handler = event =&gt; {  if (!('CodePipeline.job' in event)) {  return Promise.resolve();  }  const jobId = event['CodePipeline.job'].id;  const environment = event['CodePipeline.job'].data.actionConfiguration.configuration.UserParameters;  return doStuff  .then(() =&gt; putJobSuccess(message, jobId));}function putJobSuccess(message, jobId) {  console.log('Post Job Success For JobID: '.concat(jobId));  const params = {  jobId: jobId  };  return codepipeline.putJobSuccessResult(params).promise()  .then(() =&gt; {  console.log(`Post Job Success Succeeded Message: ${message}`);  return message;  })  .catch((err) =&gt; {  console.log(`Post Job Failure Message: ${message}`);  throw err;  });}</code></pre>"
44,24741171,24728634,https://stackoverflow.com/questions/24741171,FALSE,outdated' mentioned to answer questions,,,,1,,,no,,outdated,,,,,,,,"<p>In terms of load, they have the same goal, but they differ in other areas:</p><p>Up-to-dateness of data:</p><ul><li>A read replica will continuously sync from the master. So your results will probably lag 0 - 3s (depending on the load) behind the master.</li><li>A cache takes the query result at a specific point in time and stores it for a certain amount of time. The longer your queries are being cached, the more lag you'll have; but your master database will experience less load. It's a trade-off you'll need to choose wisely depending on your application.</li></ul><p>Performance / query features:</p><ul><li>A cache can only return results for queries it has already seen. So if you run the same queries over and over again, it's a good match. Note that queries must not contain changing parts like <code>NOW()</code>, but must be equal in terms of the actual data to be fetched.</li><li>If you have many different, frequently changing, or dynamic (<code>NOW()</code>,...) queries, a read replica will be a better match.</li><li>ElastiCache should be much faster, since it's returning values directly from RAM. However, this also limits the number of results you can store.</li></ul><p>So you'll first need to evaluate how outdated your data can be and how cacheable your queries are. If you're using ElastiCache, you might be able to cache more than queries _Ã‘Ã“ like caching whole sections of a website instead of the underlying queries only, which should improve the overall load of your application.</p><p>PS: Have you tuned your indexes? If your main problems are writes that won't help. But if you are fighting reads, indexes are the #1 thing to check and they do make a huge difference.</p>"
45,48882087,48881195,https://stackoverflow.com/questions/48882087,TRUE,service in question outdated,,,1,,1,,yes,"aws-lambda, amazon-sns",deprecated,,is currently marked as deprecated.,1,1,,,,"<p>A few things you might want to look into:</p><p>You'll need to make sure you're using either Node.js runtime v6.10 or v4.3. (Node v0.10.42 is currently marked as deprecated. AWS recommends migrating existing functions to the newer Node.js runtime versions as soon as possible)</p><p>The IAM role for your lambda function needs to have an <em>Allow</em> rule for the <code>sns:Publish</code> action.</p><p>AWS recommends that specify the phone number using the E.164 format. For example: +44xxxxxxxxxx. (<a href=""https://docs.aws.amazon.com/sns/latest/dg/sms_publish-to-phone.html"" rel=""nofollow noreferrer"">more info</a>)</p><p>Also, AWS <strong>strongly</strong> recommends updating any use of the <code>context</code> method and replacing it with the <code>callback</code> approach (<a href=""https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html"" rel=""nofollow noreferrer"">more info</a>). For example:</p><pre><code>const AWS = require(""aws-sdk"");const sns = new AWS.SNS({apiVersion: ""2010-03-31""});exports.handler = (event, context, callback) =&gt; {  const params = {  PhoneNumber: ""+44xxxxxxxxxx"", // E.164 format.  Message: ""STRING_VALUE"",  MessageStructure: ""STRING_VALUE""  }  sns.publish(params, (err, data) =&gt; {  if (err) {  console.error(`Error ${err.message}`);  callback(err);  } else {  console.log(""Success"");  callback(null, data); // callback instead of context.  }  }};</code></pre>"
46,60494793,60493487,https://stackoverflow.com/questions/60494793,TRUE,package in answer outdated,,,1,,1,,yes,amazon-dynamodb,deprecated,,,1,1,,,,"<p>Solution:Import this<code>var unmarshalItem = require('dynamodb-marshaler').unmarshalItem;</code>I have the response here in ""results.Items"" and now you can do this to get the normal json format perfectly.</p><pre><code>var items = results.Items.map(unmarshalItem);  console.log(items);  callback(null, items);</code></pre><p>But i see this npm package was deprecated, So i will try doing this with latest <code>DynamoDB/Converter</code> and will update here.</p>"
47,54819944,54818630,https://stackoverflow.com/questions/54819944,FALSE,,,,,,,,,,,,,,,,,,"<p>Burst capacity does not cause your table to create a new partition. <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html"" rel=""nofollow noreferrer"">The docs</a> are pretty clear that new partitions are caused by the size of your data and the amount of <em>provisioned</em> capacity.</p><blockquote><p>DynamoDB allocates additional partitions to a table in the following situations:</p><p>_Ã‘Â¢   If you increase the table's provisioned throughput settings beyond what the existing partitions can support.</p><p>_Ã‘Â¢   If an existing partition fills to capacity and more storage space is required.</p></blockquote><p>There_Ã‘Ã©s no way for you to view to number of partitions in your table. You <em>might</em> be able to find out by contacting AWS support.</p><p>That being said recent improvements to DynamoDB mean that you shouldn_Ã‘Ã©t really have to worry about the number of partitions you have. Hot partitions are not a concern like they used to be because <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">DynamoDB adaptive capacity accommodates uneven data access patterns</a>.</p>"
48,49181531,49180428,https://stackoverflow.com/questions/49181531,TRUE,service in question outdated,,,1,,1,,yes,"amazon-s3, aws-sdk",deprecated,,"SOAP support over HTTP is deprecated, but it is still available over HTTPS.",1,,,,,"<p>You cannot use non-ASCII characters for S3 user-defined metadata when using either REST API or the AWS SDK (since AWS SDK is basically wrapper libraries that wrap the underlying Amazon S3 REST API):</p><blockquote>  <p>User-defined metadata is a set of key-value pairs. Amazon S3 stores  user-defined metadata keys in lowercase. Each key-value pair must  conform to <strong>US-ASCII when you are using REST</strong> and to UTF-8 when you are  using SOAP or browser-based uploads via POST.</p></blockquote><p>While UTF-8 is supported for both; SOAP and browser-based uploads using POST, AWS <a href=""https://docs.aws.amazon.com/AmazonS3/latest/API/APISoap.html"" rel=""nofollow noreferrer"">recommends</a> that you use either the REST API or the AWS SDKs saying that new features will not be supported for SOAP:</p><blockquote>  <p>SOAP support over HTTP is deprecated, but it is still available over  HTTPS. New Amazon S3 features will not be supported for SOAP. We  recommend that you use either the REST API or the AWS SDKs.</p></blockquote>"
49,46720353,46719462,https://stackoverflow.com/questions/46720353,TRUE,service in question outdated,,,1,,1,,yes,amazon-rds,DeprecatedÂ ,,override executeQuery(String) and make it @Deprecated with a nice javadoc.,1,1,,,,"<p>A typical mistake.</p><pre><code>ResultSet rs = ps.executeQuery(count);</code></pre><p>should be</p><pre><code>ResultSet rs = ps.executeQuery();</code></pre><p>as <code>ps</code> is a <code>PreparedStatement</code> extending <code>Statement</code>, and <code>executeQuery(String)</code> is an unusable Statement method.</p><p><em>Should one ever create a similar API, override <code>executeQuery(String)</code> and make it <code>@Deprecated</code> with a nice javadoc.</em></p>"
50,55485222,55477419,https://stackoverflow.com/questions/55485222,TRUE,service in question outdated,,,1,,1,,yes,aws-api-gateway,deprecatedÂ ,,The deprecated api aws.apigateway.x.API has been removed.,1,1,,,,"<p>Figured I'd take a look at the actual code (go figure, right?) and found that <code>apigateway.x.API</code> is now under <code>awsx.apigateway.API</code>. Was wondering if <code>awsx</code> was related. Feel better now.</p><p><a href=""https://github.com/pulumi/pulumi-aws/pull/508"" rel=""nofollow noreferrer"">https://github.com/pulumi/pulumi-aws/pull/508</a></p><blockquote>  <p>The deprecated api aws.apigateway.x.API has been removed. Its replacement is in the @pulumi/awsx package with the name awsx.apigateway.API.</p></blockquote><p><code>pulumi-awsx</code> is described as ""AWS infrastructure best practices in component form.</p><p><a href=""https://github.com/pulumi/pulumi-awsx"" rel=""nofollow noreferrer"">https://github.com/pulumi/pulumi-awsx</a></p>"
51,53062912,33455244,https://stackoverflow.com/questions/53062912,FALSE,mentionÂ ,,,,1,,,yes,amazon-rds,deprecated,,"Ensure that running an updated version of MySQL Engine(not deprecated). Trying to upgrade a deprecated MySQL Version via the AWS Console(UI) results in an error message.""",,,,,,"<p>Upgrading a MySQL DB instance can be tricky but easy to achieve through the following steps.</p><p><strong><em>1. Ensure that running an updated version of MySQL Engine(not deprecated).</em></strong>Trying to upgrade a deprecated MySQL Version via the AWS Console(UI) results in an error message.""</p><blockquote>  <p>""Cannot find version 5.... for mysql (Service: AmazonRDS; Status Code:  400; Error Code: InvalidParameterCombination; Request ID:........""</p></blockquote><p><strong>Even Snapshot Restore is most likely to run for several mins/hours without any success.</strong> </p><p>2.<strong>Use AWS CLI</strong> </p><p>Use 'modify-db-instance' command to scale the storage size[1] and applied the version upgrade on your DB instance. Here's the example command:</p><pre><code>aws rds modify-db-instance \   --db-instance-identifier &lt;RDS_identifier&gt; \   --allocated-storage &lt;storage_size&gt; \   --apply-immediately </code></pre><p>You may also refer to this guide on to install AWS CLI toll: Installing the AWS Command Line Interface - [<a href=""https://docs.aws.amazon.com/cli/latest/userguide/installing.html][1]"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cli/latest/userguide/installing.html][1]</a></p><p>3.After the successful upgrade, optionally modify/upgrade your MySQL version to a non depreciated Version via the AWS Console(UI).</p>"
52,33306070,33298186,https://stackoverflow.com/questions/33306070,TRUE,service in question outdated,,,1,,1,,yes,amazon-ses,removedÂ ,,,1,1,,,,"<p><strong>Update</strong></p><p>At some point after this answer was written, most of the information about Signature Version 3 has been removed from the AWS docs.  The documentation at the link mentioned below is still present, but the page no longer describes how to actually sign requests using Signature Version 3.  It does still mention that SES supports both Signature V3 and V4, but it also states that Signature V4 is recommended.</p><p>For the historically curious, the instructions for signing SES requests using Signature Version 3 can still be found at an <a href=""https://web.archive.org/web/20150905231547/https://docs.aws.amazon.com/ses/latest/DeveloperGuide/query-interface-authentication.html"" rel=""nofollow noreferrer"">archived version of the page</a> but using (or switching to) Signature Version 4 would be the future-proof course of action, as it is highly unlikely for Signature V3 to be supported if SES launches any new regions in the future (Signature V2 has not been supported in any region launched since 2014, but SES does not have endpoints in any of those regions).</p><p>Signature V3 had significant limitations compared to Signature V4, so it may at some point (or may already) be deprecated and may eventually be discontinued. </p><p>tl;dr: Always use Signature Version 4.</p><p><strike><a href=""https://docs.aws.amazon.com/ses/latest/DeveloperGuide/query-interface-authentication.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/ses/latest/DeveloperGuide/query-interface-authentication.html</a> explains how to use Signature Version 3 with SES to sign requests you will submit over HTTPS.</strike></p><p><strike>This is an algorithm that is different from most other AWS services, but is very simple to implement.  The standard Signature V4 is also supported by SES.</strike></p>"
53,52462657,52462585,https://stackoverflow.com/questions/52462657,TRUE,Toturial mentioned might outdated,,,,1,1,,yes,amazon-dynamodb,outdated,,,1,,1,,,"<p>DynamoDB Streams enables a read-only view of the actions that have been performed against DynamoDB tables. Think of streams as analogous to a Kinesis stream that provides an audit of all writes to the tables. Streams does not provide the ability to enforce atomicity on writes to tables, but could be used for monitoring after the fact that the sources are out of sync. The best I can see for transactional storage within DynamoDB is in the <a href=""https://aws.amazon.com/blogs/developer/performing-conditional-writes-using-the-amazon-dynamodb-transaction-library/"" rel=""nofollow noreferrer"">AWS Developer Blog</a>, but it is from 2014 so it may be outdated.</p><p><a href=""https://aws.amazon.com/about-aws/whats-new/2014/11/10/introducing-dynamodb-streams/"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aws/whats-new/2014/11/10/introducing-dynamodb-streams/</a><a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>"
54,19691660,4488793,https://stackoverflow.com/questions/19691660,TRUE,Toturial mentioned might outdated,,,,1,,1,yes,"amazon-ec2, amazon-ebs",outdated,,"i tried quite a few blogs but as you said, they are outdated.",1,,,,,"<p>You can follow the official documentation of setting up Amazon ec2 instance: <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-instance_linux.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-instance_linux.html</a></p><p>You should start with an AMI that you are familiar with. For example, if you use Ubuntu, you can just use one of the Ubuntu AMI in the recommended page. I didn't use the BitNami server and my Django site is deployed smoothly. </p><p>If you are using Apache server, just follow the instructions on the official Django doc:<a href=""https://docs.djangoproject.com/en/1.5/howto/deployment/wsgi/modwsgi/"" rel=""nofollow"">https://docs.djangoproject.com/en/1.5/howto/deployment/wsgi/modwsgi/</a></p><p>I tried quite a few blogs but as you said, they are outdated. Just use the official docs and it will save you a lot of time. </p>"
55,9489329,9489036,https://stackoverflow.com/questions/9489329,TRUE,Toturial mentioned might outdated,,,,1,1,,no,,outdated,,,1,,,,,"<p>Although it may be a bit outdated, <a href=""http://freemarker.sourceforge.net/"" rel=""nofollow"">FreeMarker</a> is primarily a framework for rendering template files given some set of inputs.</p>"
56,36339430,36239224,https://stackoverflow.com/questions/36339430,TRUE,service in question outdated,,,1,,1,,yes,amazon-echo,deprecated,,,1,,,,,"<p><strong>Update: <a href=""https://developer.amazon.com/docs/custom-skills/literal-slot-type-reference.html"" rel=""nofollow noreferrer"">This is no longer possible</a> as of October 2018.</strong></p><blockquote>  <p>AMAZON.LITERAL is deprecated as of October 22, 2018. Older skills  built with AMAZON.LITERAL do continue to work, but you must migrate  away from AMAZON.LITERAL when you update those older skills, and for  all new skills.</p></blockquote><p><s>You can use the <a href=""https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-interaction-model-reference#LITERAL%20Slot%20Type%20Reference"" rel=""nofollow noreferrer""><code>AMAZON.LITERAL</code></a> slot type to capture freeform text. Amazon recommends providing sample phrases, <a href=""https://forums.developer.amazon.com/forums/message.jspa?messageID=17841"" rel=""nofollow noreferrer"">but according to this thread</a>, you may be able to get away with not providing them.</s></p>"
57,54204722,54204287,https://stackoverflow.com/questions/54204722,TRUE,service in question outdated,,,1,,1,,no,,outdated,,Maybe you should update node.js because version 4.x is absolutely outdated,1,1,,,,"<p>If you use express.js for the Webapplication, you first could use <code>pm2</code> (<a href=""https://www.npmjs.com/package/pm2"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/pm2</a>) to create a ""container"" for your application. If you want to Host your application, I recommend using Nginx with a reverse proxy.Here are some Links: </p><ol><li><a href=""https://serverfault.com/questions/601332/how-to-configure-nginx-so-it-works-with-express"">https://serverfault.com/questions/601332/how-to-configure-nginx-so-it-works-with-express</a></li><li><a href=""https://stackoverflow.com/questions/53807393/nginx-reverse-proxy-expressjs-angular-ssl-configuration-issues"">Nginx Reverse Proxy + ExpressJS + Angular + SSL configuration issues</a></li></ol><p>I hope that helps you. And if you want to install your application, just run <code>npm install</code> in the directory where the <code>package.json</code> is. Maybe you should update node.js because version 4.x is absolutely outdated.</p>"
58,47945307,43994883,https://stackoverflow.com/questions/47945307,TRUE,,,,1,,,,yes,lambda,,API functions are deprecated and the AWS docu is,,1,1,0,1,,"<p>A lot of the used API functions are deprecated and the AWS docu is ... I share a new implemented example. The Lambda function ""updateS3Chart"" calls another Lambda function ""AsyncUpdate"" asynchronous: </p><pre><code>public class LambdaInvoker {static final Logger logger = LogManager.getLogger(LambdaInvoker.class);static final String LambdaFunctionName = ""AsyncUpdate""; private class AsyncLambdaHandler implements AsyncHandler&lt;InvokeRequest, InvokeResult&gt;{  public void onSuccess(InvokeRequest req, InvokeResult res) {  logger.debug(""\nLambda function returned:"");  ByteBuffer response_payload = res.getPayload();  logger.debug(new String(response_payload.array()));  }  public void onError(Exception e) {  logger.debug(e.getMessage());  }}public void updateS3Chart(UpdateS3ChartRequest updateS3ChartRequest) {  Gson gson = new Gson();  try {  //issue: aws region is not set to debug-time. solution for eclipse:  //environment variable is set by lambda container or eclipse ide environment variables  //use instead for eclipse debugging: project -&gt; Run as -&gt; Run Configurations -&gt; Environment -&gt; Add variable: ""AWS_REGION"": ""eu-central-1""  AWSLambdaAsync lambda = AWSLambdaAsyncClientBuilder.defaultClient(); //Default async client using the DefaultAWSCredentialsProviderChain and DefaultAwsRegionProviderChain chain  InvokeRequest req = new InvokeRequest()  .withFunctionName(LambdaFunctionName)  .withPayload(gson.toJson(updateS3ChartRequest));  Future&lt;InvokeResult&gt; future_res = lambda.invokeAsync(req, new AsyncLambdaHandler());  logger.debug(""Waiting for async callback"");  while (!future_res.isDone() &amp;&amp; !future_res.isCancelled()) {  // perform some other tasks...  try {  Thread.sleep(1000);  }  catch (InterruptedException e) {  logger.debug(""Thread.sleep() was interrupted!"");  }  System.out.print(""."");  }  } catch (Exception e) {  logger.fatal(""Execute async lambda function: "" + LambdaFunctionName + "" failed: "" + e.getMessage());  }}}</code></pre><p>You have to set your AWS region as system property in your IDE for debugging (see comment in source code for Eclipse). UpdateS3ChartRequest is simple POJO with property set/get.</p>"
59,56234110,37385411,https://stackoverflow.com/questions/56234110,FALSE,,,,,,,,yes,ec2,,your service is obsolete,,,,,,,"<p>As the error message rather clearly tells you, you somehow ended up passing in <code>169.254.169.254http</code> as the host name, and <code>169.254.169.254http:80</code> as the host.  Just to spell this out completely, you probably wanted the host to be <code>169.254.169.254</code>. You need to figure out why your request was botched like this, and correct the code or your configuration files so you send what you wanted to send.</p><p><code>ENOTFOUND</code> in response to <code>getaddrinfo</code> simply means that you wanted to obtain the address of something which doesn't exist or is unknown. Very often this means that you have a typo, or that the information you used to configure your service is obsolete or otherwise out of whack (attempting to reach a private corporate server when you are outside the  corporate firewall, for example).</p>"
60,47772403,47765493,https://stackoverflow.com/questions/47772403,TRUE,,,,1,,,,yes,s3,,Nobody should be using,,1,0,0,0,,"<ul><li>Nobody should be using S3n as the connector. It is obsolete and removed from Hadoop 3. If you have the Hadoop 2.7.x JARs on the classpath, use s3a</li><li>The issue with rename() is not just the consistency, but the bigger the file, the longer it takes.</li></ul><p>Really checkpointing to object stores needs to be done differently. If you look closely, there is no <code>rename()</code>, yet so much existing code expects it to be an O(1) atomic operation.</p>"
61,31038374,28706078,https://stackoverflow.com/questions/31038374,TRUE,,,,1,,,,yes,Elastic Beanstalk,deprecated,,,1,1,1,1,,"<p>I had the same issue. I was using aws.push to update my application. Then I moved to a new computer and I had to set everything up again.</p><p>You can use</p><pre><code>eb deploy</code></pre><p>However, depending upon how you have your project setup, you might need to map your deployment to a branch. Use:</p><pre><code>eb branch</code></pre><p>I was in a bind and wanted to make sure that I did not screw up a deployment by introducing any new issues into the production environment and I wanted to use:</p><pre><code>git aws.push</code></pre><p>This can still be done.</p><p>Download the deprecated version of the AWS Elastic Beanstalk Command Line Tool <a href=""http://aws.amazon.com/code/6752709412171743"" rel=""nofollow"">here</a> </p><p>Then from within your repo run <strong>AWSDevTools-RepositorySetup.sh</strong>. You can find this file in the zip file you just downloaded, AWS-ElasticBeanstalk-CLI-2.6.4 / AWSDevTools / Linux</p><p>Now run</p><pre><code>git aws.config</code></pre><p>Once configured you should be able to run git aws.push without any problems. </p><p><strong>I am now using eb deploy, but I was in a bind and had never used it and did not have time to test it. So this worked for me.</strong></p>"
62,56420710,49918991,https://stackoverflow.com/questions/56420710,TRUE,,,,1,,,,no,,,functional calls (which are deprecated),,1,0,0,1,,"<p>Note: so far as I can tell, adding 'replica' servers does nothing unless you are using the db_* functional calls (which are deprecated), or if you manually instantiate the database.replica connection in any of your custom queriese.g. <code>/** @var \Drupal\Core\Database\Connection $database_replica */$database_replica = \Drupal::service('database.replica');$query = $database_replica-&gt;select('node', 'n');....</code></p>"
63,21087369,21083772,https://stackoverflow.com/questions/21087369,TRUE,,,,1,,,,yes,boto,,tutorial is simply out of date and needs to be corrected,,1,1,1,0,,"<p>The documentation quote you provide comes from the SQS tutorial.  The <a href=""http://docs.pythonboto.org/en/latest/ref/sqs.html#boto.sqs.queue.Queue.write_batch"" rel=""noreferrer"">SQS API docs</a> correctly describe the current return value.  The SQS tutorial is simply out of date and needs to be corrected.  I have created an <a href=""https://github.com/boto/boto/issues/1985"" rel=""noreferrer"">issue</a> to track this.</p><p>If the write fails for any reason, the service will return an HTTP error code which, in turn, will cause boto to raise an SQSError exception.  If no exception is raised, the write was successful.</p>"
64,22690887,22690644,https://stackoverflow.com/questions/22690887,FALSE,,,,,,,,no,,,archlinux,,,,,,,"<p>Environment variables are not passed to <code>sudo</code>: <a href=""https://stackoverflow.com/questions/8633461/how-to-keep-environment-variables-when-using-sudo"">How to keep Environment Variables when Using SUDO</a></p><blockquote>  <p>the trick is to add enviroment variables to sudoers config:</p><pre><code>sudo visudo</code></pre>   <p>add these lines</p><pre><code>Defaults env_keep +=""http_proxy""Defaults env_keep +=""https_proxy""</code></pre>   <p>form ArchLinux wiki  <a href=""https://wiki.archlinux.org/index.php/Sudo#Environment_variables_.28Outdated.3F.29"" rel=""nofollow noreferrer"">https://wiki.archlinux.org/index.php/Sudo#Environment_variables_.28Outdated.3F.29</a></p></blockquote>"
65,43285441,21917661,https://stackoverflow.com/questions/43285441,TRUE,,,1,,,,,yes,ElastiCache,,These answers are out of date.,,0,1,1,0,,"<p><strong>These answers are out of date.</strong></p><p>You can access elastic-cache outside of AWS by following these steps:</p><ol><li>Create a NAT instance in the same VPC as your cache cluster but in apublic subnet. </li><li>Create security group rules for the cache cluster andNAT instance. </li><li>Validate the rules. </li><li>Add an iptables rule to the NATinstance. </li><li>Confirm that the trusted client is able to connect to thecluster. </li><li>Save the iptables configuration.</li></ol><p>For a more detailed description see the aws guide:</p><p><a href=""https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/accessing-elasticache.html#access-from-outside-aws"" rel=""noreferrer"">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/accessing-elasticache.html#access-from-outside-aws</a></p>"
66,62049410,62049230,https://stackoverflow.com/questions/62049410,FALSE,,,,,,,,no,s3,,,,,,,,,"<p>If you are using CloudFront : </p><ol><li>Upload your code </li><li>Invalidate cache in cloudfront</li></ol><p>Step to invalidate cache forcefully: </p><ol><li><p>Select the distribution for which you want to invalidate files.</p><p>Choose Distribution Settings.</p><p>Choose the Invalidations tab.</p><p>Choose Create Invalidation.</p></li></ol><p><a href=""https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html</a>For the files that you want to invalidate, enter one invalidation path per line. For information about specifying invalidation paths, see Specifying the Files to Invalidate.</p><p><a href=""https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serving-outdated-content-s3/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serving-outdated-content-s3/</a></p><p>Another way to user versioning in content like js/css/images etc.</p>"
67,63733871,60547409,https://stackoverflow.com/questions/63733871,FALSE,,,,,,,,yes,eks,,,error message: `... Deprecated.`,,,,,,"<p>Adding to <a href=""https://stackoverflow.com/a/60573982/137601"">Simon's great answer</a>, you may want to also pass the session in the <code>GetTokenOptions</code> struct, like so:</p><pre class=""lang-golang prettyprint-override""><code>opts := &amp;token.GetTokenOptions{  ClusterID: aws.StringValue(cluster.Name),  Session: sess,  }  tok, err := gen.GetWithOptions(opts)</code></pre><p>Otherwise the <code>gen.GetWithOptions(opts)</code> call will try to read your AWS credentials from local sources (e.g. <code>~/.aws/credentials</code>) and may file with this error:</p><pre><code>NoCredentialProviders: no valid providers in chain. Deprecated.  For verbose messaging see aws.Config.CredentialsChainVerboseErrors</code></pre>"
68,28455874,28449363,https://stackoverflow.com/questions/28455874,FALSE,,,,,,,,yes,lambda,,,,,,,,,"<p>Of course, I was misunderstanding the problem. <a href=""https://aws.amazon.com/blogs/compute/container-reuse-in-lambda/"" rel=""noreferrer"">As AWS themselves put it</a>:</p><blockquote>  <p>For those encountering nodejs for the first time in Lambda, a common   error is forgetting that callbacks execute asynchronously and calling   <code>context.done()</code> in the original handler when you really meant to wait   for another callback (such as an S3.PUT operation) to complete, forcing  the function to terminate with its work incomplete.</p></blockquote><p>I was calling <code>context.done</code> way before any callbacks for the request fired, causing the termination of my function ahead of time.</p><p>The working code is this:</p><pre><code>var http = require('http');exports.handler = function(event, context) {  console.log('start request to ' + event.url)  http.get(event.url, function(res) {  console.log(""Got response: "" + res.statusCode);  context.succeed();  }).on('error', function(e) {  console.log(""Got error: "" + e.message);  context.done(null, 'FAILURE');  });  console.log('end request to ' + event.url);}</code></pre><p><strong>Update:</strong> starting 2017 AWS has deprecated the old Nodejs 0.10 and only the newer 4.3 run-time is now available (old functions should be updated). This runtime introduced some changes to the handler function. The new handler has now 3 parameters.</p><pre><code>function(event, context, callback)</code></pre><p>Although you will still find the <code>succeed</code>, <code>done</code> and <code>fail</code> on the context parameter, AWS suggest to use the <code>callback</code> function instead or <code>null</code> is returned by default.</p><pre><code>callback(new Error('failure')) // to return errorcallback(null, 'success msg') // to return ok</code></pre><p>Complete documentation can be found at <a href=""http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-handler.html"" rel=""noreferrer"">http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-handler.html</a></p>"
69,41784099,27780356,https://stackoverflow.com/questions/41784099,FALSE,,,,,,,,yes,dynamo-db,,if you have outdated clients,,,,,,,"<p>I'm not sure that the accepted answer is complete because it does not acknowledge use cases, and it does not address the question asked of ""what if I want to change the data structure."" Well, if you have outdated clients, and change the data structure of the documents in your nosql database, then those clients will not be able to access it. I don't believe DynamoDB offers a middleware platform to support this kind of old-to-new model adaptation. You'll have to force an update to your clients.</p><p>In fact, there are many operations beyond user-based permissions (which Cognito does do well) like this that you might need middleware for. Perhaps you want sorting logic to occur at request-time, and not maintain a copy of that sorting logic in every client application.</p><p>The question of ""is it worth it"" probably depends on the complexity of your application and your users' relationship with the data (ie. if the presentation layer basically just wrapped data -- then directly access DynamoDB. If you your presentation layer is not just wrapped data, then you should probably use custom middleware). In truth, I stumbled upon this question while running my own cost-benefit analysis, and am not sure which approach I will take. Another major factor of mine is that I might choose to switch database solutions in the future.. this will be more challenging to update on every client, if my clients are directly accessing the DB.</p><p>The one certain conclusion I've reached is that you should use middleware <em>somewhere</em> in your system, such that you can decouple your database vendor from either the client logic or the server logic as much as possible, eg. in a mobile app:  writeToDatabase(Data data){writeToDynamo(data);}</p><p>To achieve this, AWS suggests using Amazon Api Gateway as a proxy for AWS services, and even has premade configurations for Amazon API Gateway to behave as AWS service proxy.</p>"
70,39292876,10528540,https://stackoverflow.com/questions/39292876,TRUE,,,,1,,,,yes,EMR,,they've changed things there,"Apache s3:// should be considered deprecated -you don't need it any more, and shouldn't be using it.",1,1,1,0,,"<p>Updating this for Apache Hadoop 2.7+, and ignoring Amazon EMR as they've changed things there.</p><ol><li>If you are using Hadoop 2.7 or later, use s3a over s3n. This also applies to recent versions of HDP and, AFAIK, CDH.</li><li>This supports 5+GB files, has other nice features, etc. It is tangibly better when reading files _Ã‘Ã“and will only get better over time.</li><li>Apache s3:// should be considered deprecated -you don't need it any more, and shouldn't be using it.</li><li>Amazon EMR use ""s3://"" to refer to their own, custom, binding to S3. That's what you should be using if you are running on EMR.</li></ol><p>Improving distcp reliability and performance working with object stores is still and ongoing piece of work...contributions are, as always, welcome.</p>"
71,51272109,36604024,https://stackoverflow.com/questions/51272109,TRUE,,,,1,,,,yes,credential,,,The AWSSecurityTokenServiceClient is deprecated,1,0,0,1,,"<p>The AWSSecurityTokenServiceClient is deprecated.  The following code also works.</p><pre><code>BasicAWSCredentials theAWSCredentials= new BasicAWSCredentials("""","""");AWSCredentialsProvider theAWSCredentialsProvider = new AWSStaticCredentialsProvider(theAWSCredentials);AWSSecurityTokenService theSecurityTokenService = AWSSecurityTokenServiceClientBuilder.standard().withCredentials(theAWSCredentialsProvider).build();</code></pre>"
72,30995231,30994401,https://stackoverflow.com/questions/30995231,FALSE,code should be removed,0,0,0,1,0,0,1,amazon-s3,deprecated,ones above this may be deprecated?,,1,1,0,1,0,"<p>In order to stream an S3 bucket. you need to provide the path to S3 bucket. And it will stream all data from all the files in this bucket. Then whenever w new file is created in this bucket, it will be streamed. If you are appending data to existing file which are read before, these new updates will not be read.</p><p>here is small piece of code that works</p><pre><code>import org.apache.spark.streaming._val conf = new SparkConf().setAppName(""Simple Application"").setMaster(""local[*]"")  val sc = new SparkContext(conf)val hadoopConf=sc.hadoopConfiguration;hadoopConf.set(""fs.s3.impl"", ""org.apache.hadoop.fs.s3native.NativeS3FileSystem"")hadoopConf.set(""fs.s3.awsAccessKeyId"",myAccessKey)hadoopConf.set(""fs.s3.awsSecretAccessKey"",mySecretKey)//ones above this may be deprecated?hadoopConf.set(""fs.s3n.awsAccessKeyId"",myAccessKey)hadoopConf.set(""fs.s3n.awsSecretAccessKey"",mySecretKey)val ssc = new org.apache.spark.streaming.StreamingContext(  sc,Seconds(60))val lines = ssc.textFileStream(""s3n://path to bucket"")lines.print()ssc.start()   // Start the computationssc.awaitTermination()  // Wait for the computation to terminate</code></pre><p>hope it will help.</p>"
73,27586721,27584823,https://stackoverflow.com/questions/27586721,TRUE,SERVICE in question outdated,0,0,1,0,0,1,1,"amazon-web-services, amazon-elastic-beanstalk",deprecated,is deprecated by,,1,1,1,1,0,"<blockquote>  <p>unable to load Private Key  6312:error:0906D06C:PEM routines:PEM_read_bio:no start line:pem_lib.c:647:Expecting: ANY PRIVATE KEY</p></blockquote><p>I ran your commands on OS X, and I could not reproduce the results.</p><p>I <em>did</em> use the <code>-config</code> option because I have an ""OpenSSL server config template"" that makes it easy to generate CSRs and self signed certificates:</p><pre><code>$ mkdir test$ cd test$ openssl req -new -key privatekey.pem -out csr.pem -config example-com.conf</code></pre><p>The configuration file is named <code>example-com.conf</code>, and you can find it at <a href=""https://stackoverflow.com/questions/26019957/how-do-i-edit-a-self-signed-certificate-created-using-openssl-xampp/26029695#26029695"">How do I edit a self signed certificate created using openssl xampp?</a>. Edit it to suit your taste (in particular, the DNS names).</p><p>If interested, here's the OpenSSL man pages on the <a href=""https://www.openssl.org/docs/apps/req.html"" rel=""nofollow noreferrer""><code>req</code> sub-command</a>.</p><hr><blockquote>  <p>I checked the generated key and it looks like<br></p>   <p>-----BEGIN RSA PRIVATE KEY-----<br>  {lots of characters}<br>  -----END RSA PRIVATE KEY-----<br></p></blockquote><p>You can validate the key you just created with:</p><pre><code>$ openssl rsa -in privatekey.pem -inform PEM -text -nooutPrivate-Key: (2048 bit)modulus:  00:b0:91:ce:57:28:0f:5c:3a:c3:29:d7:23:6a:71:  ca:64:49:fc:24:ea:69:a3:09:d6:49:94:17:b9:09:  65:fa:5a:10:47:a4:9b:b8:cd:6d:32:74:19:8d:5c:  79:92:f0:a6:43:9c:75:a3:7b:ef:c4:c3:d9:c2:db:  b9:bd:ec:14:a8:b1:52:73:8f:56:c8:5c:16:08:56:  ff:c2:2b:35:3c:0a:0f:34:d0:91:c1:54:7e:72:e8:  97:bf:ea:46:69:5f:e4:21:8d:7a:f5:a5:6b:6a:e8:  00:56:bc:02:f6:b4:ae:6e:89:a6:50:aa:5b:2f:d8:  7d:99:04:61:51:76:b3:5e:9e:30:52:99:54:26:e2:  3a:54:ec:78:34:e6:9a:b7:c2:58:5c:51:3d:39:52:  d4:6e:0c:6e:a1:a0:a5:f1:4d:5a:f5:0b:1a:6e:dc:  f3:bb:0d:d0:53:51:b0:1a:04:ee:86:35:d5:f3:8b:  0d:bc:19:61:6c:0c:b2:7b:a9:7c:47:97:01:bb:a2:  6a:74:d9:19:e9:df:60:07:d4:95:4c:83:f8:3b:84:  c2:b8:3d:b9:a7:34:0a:9b:a3:c6:70:cc:ef:de:f4:  64:88:f1:56:d3:2a:fd:5a:82:88:96:66:93:6c:a0:  b8:ec:e4:4c:e8:76:5c:9c:fc:c4:60:72:b6:9a:3f:  98:a3publicExponent: 65537 (0x10001)privateExponent:  00:87:ab:f1:65:ac:e5:68:93:ca:64:3a:e7:fe:a1:  62:c7:7e:c5:dc:c3:b5:d9:cd:f4:36:e3:30:fb:40:  0a:78:bc:7d:67:df:46:bc:50:34:88:a1:07:05:44:  ba:31:ba:f1:b6:5f:e1:50:76:29:bd:02:54:2f:d2:  cf:bc:ec:4a:cf:78:39:07:8c:6b:3d:56:ec:a3:09:  de:49:9f:13:af:87:77:39:b8:cd:56:45:0b:48:56:  0a:4c:2f:c2:5c:b3:8e:c2:6d:48:be:b9:95:79:36:  bd:13:e8:31:4a:c9:78:82:7d:08:2b:51:4a:f1:cf:  a2:6a:52:20:49:0d:31:34:10:88:02:d7:a7:07:70:  32:b5:f5:8c:cc:d4:b2:8d:b9:aa:bb:33:82:1a:74:  bd:4d:4f:e9:e0:cc:f2:27:fb:98:34:2c:77:56:6f:  88:3a:66:32:5d:7d:57:c6:5b:63:39:fa:32:04:9d:  e3:cc:a5:b6:44:91:fd:7d:d1:b6:2d:16:47:59:81:  3d:cf:d9:a7:58:2a:d6:61:5d:c6:69:3b:7a:70:50:  4f:80:f4:d9:fb:c8:7d:5e:44:9e:ac:c8:e6:aa:49:  c3:d6:df:6b:03:68:25:a3:2b:89:8f:9a:35:3a:58:  7d:71:b4:08:d9:04:7b:b9:96:17:f3:a5:19:c5:07:  4e:c1prime1:  00:d7:d0:d8:8c:b5:86:ed:0e:06:70:c9:54:00:25:  d7:8c:e4:65:51:1b:c5:ba:33:c2:02:1a:dc:80:a6:  ae:8e:1e:e8:c0:b7:04:11:5a:e3:98:52:8f:4a:7a:  43:b8:e8:1b:c8:d6:d3:b2:dc:70:59:a5:ca:83:bb:  35:f1:6c:f5:cb:d0:f4:04:5e:aa:7c:d0:ec:d7:4a:  d5:1c:7c:e2:67:e4:e8:17:95:9b:4e:2b:a0:26:74:  61:d0:a0:15:27:18:e5:84:b5:54:ef:be:82:35:7e:  78:e0:49:6b:4e:ae:93:53:a0:81:a3:8e:de:d3:e5:  dc:c5:ba:03:36:14:47:97:03prime2:  00:d1:72:3b:f5:34:b1:11:78:b2:79:f4:3e:d7:be:  bf:cc:b3:09:ea:24:a4:cc:7f:64:73:96:d2:48:9e:  55:bc:79:23:c2:d9:80:81:7d:a4:a5:4b:43:33:8e:  62:04:ec:8d:22:d7:43:5e:41:b6:4d:e9:b0:cc:70:  63:17:70:93:88:81:f5:84:a6:3f:2b:98:33:a3:69:  53:11:c7:95:8c:30:ea:e8:58:c7:77:10:b4:a8:f5:  bf:5e:cf:e1:99:bb:b3:4e:57:d2:4c:f7:73:de:8a:  98:8e:7c:26:37:6c:e4:77:c6:d2:ed:5d:53:a7:15:  c3:9c:67:61:d3:24:9a:f5:e1exponent1:  00:83:34:59:e2:b9:9d:8c:d2:e1:01:82:b4:89:de:  77:bc:15:42:af:5b:c6:0a:dc:da:8e:f3:0b:a9:3f:  2c:92:04:a2:96:3e:ed:bf:2b:55:80:ce:78:84:db:  ed:fe:25:46:77:04:7b:f1:9a:68:c7:67:ae:c6:05:  73:d7:11:da:21:0e:28:bb:db:5d:a4:c2:53:aa:d3:  b8:da:37:e6:61:29:5e:1c:b0:7c:99:ba:96:03:aa:  ef:a8:a9:1a:13:09:e4:c7:98:82:49:ba:b5:68:96:  3a:20:89:22:2e:d4:9d:86:d2:e6:dd:ab:c7:36:65:  e1:a1:67:e3:f9:e5:bc:5c:47exponent2:  00:81:6d:b9:55:8f:09:39:05:c0:2d:12:dd:5e:cf:  56:91:35:b6:93:c5:af:3d:5c:20:04:3a:18:9a:9d:  95:d7:d1:78:62:e9:ab:ba:d9:9c:cc:34:95:43:9f:  e2:3c:ae:bd:8c:e1:3f:95:58:c0:42:a7:7e:04:e8:  12:a4:22:82:59:22:0e:49:b9:be:61:bf:3d:71:e7:  1d:59:68:5f:a6:f1:77:c8:bb:4c:0f:ec:f7:e7:4d:  6d:c4:36:6c:70:67:08:a8:0a:27:40:3e:ce:90:a0:  4f:24:05:de:4b:f3:f3:bf:7c:d3:4d:b1:95:87:34:  30:dc:4f:1a:a9:b2:fe:3b:a1coefficient:  6d:51:b3:6e:87:8d:aa:f0:55:c4:22:21:62:a9:ea:  24:b3:b7:91:40:f5:78:5d:f1:40:45:7e:0d:a2:a3:  54:46:ba:42:33:b6:cd:57:a1:85:bc:3d:ba:1c:eb:  87:33:a9:e9:63:1e:7c:2c:89:98:b9:0f:4b:e8:c4:  79:bd:00:6a:f5:3e:ea:63:f1:9e:aa:47:35:5a:22:  fc:4e:e3:61:7e:eb:dc:a6:c0:2c:d5:fd:22:9f:01:  59:32:15:db:41:99:b7:a8:c1:eb:1e:42:c7:1b:c7:  c8:56:86:a8:34:fe:1c:48:b6:6e:f1:c1:5c:96:bf:  9d:fa:e5:4c:d0:2a:d9:09</code></pre><hr><blockquote>  <p>unable to write 'random state'</p></blockquote><p>This is a well known problem. OpenSSL uses a default configuration file. You can locate the configuration file with <a href=""https://stackoverflow.com/q/21477210/608639"">correct location of openssl.cnf file</a>.</p><p>The default configuration file includes these lines:</p><pre><code>$ cat /usr/local/ssl/macosx-x64/openssl.cnf ...HOME  = .RANDFILE  = $ENV::HOME/.rnd...</code></pre><p>To save the random file, you should point <code>HOME</code> and <code>RANDFILE</code> to a valid location. On Windows, you type <code>set HOME=...</code> and <code>set RANDFILE=...</code> in the command prompt. Or better, change it in the OpenSSL configuration file you use.</p><p>Also see <a href=""https://stackoverflow.com/q/12507277/608639"">How to fix _Ã‘Ã’unable to write 'random state' _Ã‘Âù in openssl</a> and <a href=""https://stackoverflow.com/q/2229723/608639"">How do I make OpenSSL write the RANDFILE on Windows Vista?</a>.</p><hr><blockquote>  <p>I'm trying to configure HTTPS for my ElasticBeanstalk environment following these instructions.</p></blockquote><p>The instructions are wrong in the image below. <strong><em>Do not place a DNS name in the Common Name (CN)</em></strong>.</p><p><img src=""https://i.stack.imgur.com/3PYVL.png"" alt=""enter image description here""></p><p>Placing a DNS name in the Common Name is deprecated by both the IETF (the folks who publish RFCs) and the CA/B Forums (the cartel where browsers and CAs collude). You should pay articular attention to what the CA/B recommends because Browsers and CAs come up with those rules, and the browsers follow them (and they don't follow the RFCs). For reference, see <a href=""http://www.ietf.org/rfc/rfc5280.txt"" rel=""nofollow noreferrer"">RFC 5280</a>, <a href=""http://www.ietf.org/rfc/rfc6125.txt"" rel=""nofollow noreferrer"">RFC 6125</a> and the <a href=""https://cabforum.org/baseline-requirements-documents/"" rel=""nofollow noreferrer"">CA/B Baseline Requirements</a>.</p><p>Instead, <strong><em>place DNS names in the Subject Alternate Name (SAN)</em></strong>. Both the IETF and CA/B specifies it.</p><p>The custom OpenSSL configuration file handles this for you. You just have to change the DNS names listed under the section <code>[ alternate_names ]</code>. For example, here's a set of names set up for the domain <code>example.com</code>. Notice there is no DNS name in the CN:</p><pre><code>[ subject ]...commonName  = Common Name (e.g. server FQDN or YOUR name)commonName_default  = Example Company[ alternate_names ]DNS.1   = example.comDNS.2   = www.example.comDNS.3   = mail.example.comDNS.4   = ftp.example.com</code></pre>"
74,53749018,53748832,https://stackoverflow.com/questions/53749018,TRUE,outdated answer,1,1,0,0,0,1,1,amazon-iam,outdated,is outdated,,1,0,1,0,1,"<p>There is no way of doing that yet, officially supported, at least.</p><p><a href=""https://github.com/Azure/AKS/issues/371"" rel=""nofollow noreferrer"">https://github.com/Azure/AKS/issues/371</a></p><p>EDIT: this answer is outdated, this is now possible</p>"
75,57818720,57776244,https://stackoverflow.com/questions/57818720,TRUE,SERVICE in question outdated,0,0,1,0,0,1,1,amazon-web-services,deprecated,is deprecated,,1,1,1,0,0,"<p>I was able to reach a better solution by using puppeteer library. Reasons for choosing a different library<li>html-pdf is deprecated</li><li>puppeteer has much better options</li><li>puppeteer has async/await feature</li></p><p>Although to make this work in AWS with serverless &amp; serverless-plugin-optimize , I did face many challenges. Note out the following points while implementing this kind of similar scenario</p><p><b>For API gateway to send any binary file(pdf / jpeg / jpg) as response</b><li>Binary Media Types should be set to <code>*/*</code> in API Gateway resource Settings options , if going through serverless under provider in serverless.yaml add <b>apiGateway:  binaryMediaTypes:  - <code>*/*</code></b></li><li>if by any chance you are using serverless-plugin-optimize to reduce lambda size , use ""external"" option for this package chrome-aws-lambda , ref link <a href=""https://www.npmjs.com/package/serverless-plugin-optimize"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/serverless-plugin-optimize</a></li></p>"
76,43867972,43451370,https://stackoverflow.com/questions/43867972,TRUE,SERVICE in question outdated,0,0,1,0,0,1,1,"amazon-s3, amazon-sdk",deprecated,is deprecated,,1,1,0,1,0,"<p>Apparently the <code>AmazonS3Client</code> class is deprecated in the latest SDKs. Creating client in the following manner solves the problem as it also takes region into account and hence attaches proper signature versions :</p><pre><code>BasicAWSCredentials awsCreds = new BasicAWSCredentials(accessKeyId, secretAccessKey);  AmazonS3ClientBuilder builder = AmazonS3ClientBuilder.standard().withCredentials(new AWSStaticCredentialsProvider(awsCreds));  builder.setRegion(regionName);  AmazonS3 s3client = builder.build();</code></pre>"
77,12820595,5402013,https://stackoverflow.com/questions/12820595,TRUE,approaches in question outdated,0,0,1,0,0,1,1,"amazon-s3, amazon-ec2, amazon-ami",outdated,now outdated by,,1,1,1,1,0,"<p>I think that is now outdated by ec2-bundle-vol and ec2-migrate-image, BTW you can also take a look at this Perl script by Lincoln D. Stein:<a href=""http://search.cpan.org/~lds/VM-EC2/bin/migrate-ebs-image.pl"" rel=""nofollow"">http://search.cpan.org/~lds/VM-EC2/bin/migrate-ebs-image.pl</a></p><p>Usage:</p><blockquote>  <p>$ migrate-ebs-image.pl --from us-east-1 --to ap-southeast-1 ami-123456</p></blockquote>"
78,59220336,57884885,https://stackoverflow.com/questions/59220336,TRUE,SERVICE in question outdated,0,0,1,0,0,1,1,"amazon-serverless, aws-sam-cli",deprecated,is deprecated,,1,0,0,0,1,"<p>I am running the latest version of SAM on Windows (0.37.0) and nodejs10.x is still not supported.  Lambda has support for nodejs 10 and 12 now, plus 8.10 is deprecated.  So, the SAM team has still not updated their tooling yet.  A look at the Lambda api docs show support for the following:</p><blockquote>  <p>Runtime  The identifier of the function's runtime.  Required: Yes  Type: String  Allowed Values: dotnetcore1.0 | dotnetcore2.1 | go1.x | java11 | java8 | nodejs10.x | nodejs12.x | nodejs8.10 | provided | python2.7 | python3.6 | python3.7 | python3.8 | ruby2.5</p></blockquote>"
79,54045954,52758171,https://stackoverflow.com/questions/54045954,TRUE,,0,0,1,0,0,1,1,aws-lambda,"deprecated, removed",is deprecated... and removed from,,1,1,1,0,0,"<p>Make sure Eclipse is running with Java 8.  AWS Toolkit requires JAXB for the upload to AWS S3, but JAXB was deprecated in Java 9 &amp; 10, and removed from Java 11.  If you're running Eclipse under Java 9, 10, or 11, Eclipse/AWS Toolkit won't find JAXB and you'll get this error.</p><p>You can resolve the problem by specifying the JVM Eclipse should use on startup.  Instructions for various platforms found here: <a href=""https://wiki.eclipse.org/Eclipse.ini"" rel=""noreferrer"">https://wiki.eclipse.org/Eclipse.ini</a></p>"
80,27772632,27764533,https://stackoverflow.com/questions/27772632,TRUE,,0,0,1,0,0,1,1,amazon-s3,deprecated,has been deprecated,,1,1,1,1,0,"<p><code>@filename</code> has been deprecated in PHP >= 5.5.0 as stated <a href=""http://www.php.net/manual/en/function.curl-setopt.php"" rel=""nofollow noreferrer"">here</a> under the CURLOPT_POSTFIELDS description , So thats the reason why you got the error .</p><p>you have your answer here at this <a href=""https://stackoverflow.com/questions/17032990/can-anyone-give-me-an-example-for-phps-curlfile-class"">stack overflow thread</a>, where different solutions are discussed . Also here is a snippet from <a href=""https://wiki.php.net/rfc/curl-file-upload"" rel=""nofollow noreferrer"">RFC</a> for the code. </p><blockquote>  <p><strong>Currently, cURL file uploading is done as:</strong></p><pre><code> curl_setopt($curl_handle, CURLOPT_POST, 1);  $args['file'] = '@/path/to/file'; curl_setopt($curl_handle, CURLOPT_POSTFIELDS, $args);</code></pre>   <p>This API is both invonvenient and insecure, it is impossible to send  data starting with '@' to the POST, and any user data that is being  re-sent via cURL need to be sanitized so that the data value does not  start with @. In general, in-bound signalling usually vulnerable to  all sorts of injections and better not done in this way.</p>   <p>Instead of using the above method, <strong>the following should be used to  upload files with CURLOPT_POSTFIELDS</strong>:</p><pre><code> curl_setopt($curl_handle, CURLOPT_POST, 1);  $args['file'] = new CurlFile('filename.png', 'image/png'); curl_setopt($curl_handle, CURLOPT_POSTFIELDS, $args);</code></pre></blockquote>"
81,30416677,30399068,https://stackoverflow.com/questions/30416677,FALSE,obsolete files mentioned in solution,0,0,0,0,0,0,1,amazon-ec2,obsolete,obsolete files,,0,1,0,0,0,"<p>A little more complicated, and assuming that you have just a few easily-identified chunks of ""too-big"" directory trees (such as <code>/opt</code> and <code>/var/log</code>) would be to create filesystem(s) on the unallocated 28GB, use rsync to copy your working files to the new filesystems, and wipe the obsolete files (to free up diskspace), mounting the new filesystems in their place (by editing <code>/etc/fstab</code>, of course).</p><p>If you do this for <code>/var/log</code>, you have to do a reboot immediately after migrating the files, since the system still references the original files.  Of course, you might want to practice the procedure on a test-machine.</p><p>It would be a lot simpler if Amazon's root volumes used LVM.</p>"
82,27494547,27477730,https://stackoverflow.com/questions/27494547,TRUE,,0,0,1,0,1,0,1,amazon-s3,deprecated,has been deprecated on... should only use if ...,,1,1,1,1,0,"<p>The underlying Hadoop API that Spark uses to access S3 allows you specify input files using a <a href=""http://en.wikipedia.org/wiki/Glob_%28programming%29"" rel=""noreferrer"">glob expression</a>.</p><p>From <a href=""http://spark.apache.org/docs/1.1.1/programming-guide.html#external-datasets"" rel=""noreferrer"">the Spark docs</a>:</p><blockquote>  <p>All of Spark_Ã‘Ã©s file-based input methods, including textFile, support running on directories, compressed files, and wildcards as well. For example, you can use <code>textFile(""/my/directory"")</code>, <code>textFile(""/my/directory/*.txt"")</code>, and <code>textFile(""/my/directory/*.gz"")</code>.</p></blockquote><p>So in your case you should be able to open all those files as a single RDD using something like this:</p><pre><code>rdd = sc.textFile(""s3://bucket/project1/20141201/logtype1/logtype1.*.gz"")</code></pre><p>Just for the record, you can also specify files using a comma-delimited list, and you can even mix that with the <code>*</code> and <code>?</code> wildcards.</p><p>For example:</p><pre><code>rdd = sc.textFile(""s3://bucket/201412??/*/*.gz,s3://bucket/random-file.txt"")</code></pre><p>Briefly, what this does is:</p><ul><li>The <code>*</code> matches all strings, so in this case all <code>gz</code> files in all folders under <code>201412??</code> will be loaded.</li><li>The <code>?</code> matches a single character, so <code>201412??</code> will cover all days in December 2014 like <code>20141201</code>, <code>20141202</code>, and so forth.</li><li>The <code>,</code> lets you just load separate files at once into the same RDD, like the <code>random-file.txt</code> in this case.</li></ul><p>Some notes about the appropriate URL scheme for S3 paths:</p><ul><li>If you're running Spark on EMR, <a href=""https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html"" rel=""noreferrer"">the correct URL scheme is <code>s3://</code></a>.</li><li>If you're running open-source Spark (i.e. no proprietary Amazon libraries) built on Hadoop 2.7 or newer, <code>s3a://</code> is the way to go.</li><li><a href=""https://wiki.apache.org/hadoop/AmazonS3"" rel=""noreferrer""><code>s3n://</code> has been deprecated</a> on the open source side in favor of <code>s3a://</code>. You should only use <code>s3n://</code> if you're running Spark on Hadoop 2.6 or older.</li></ul>"
83,28781862,28736850,https://stackoverflow.com/questions/28781862,TRUE,,0,0,0,1,0,1,1,amazon-emr,deprecated,is deprecated,,1,1,1,1,0,"<p>Finally i was able to set the bootstrap option:</p><pre><code>Set Replication Factor  s3://elasticmapreduce/bootstrap-actions/configure-hadoop  -h, dfs.replication=2Set Block Size  s3://elasticmapreduce/bootstrap-actions/configure-hadoop  -h, dfs.block.size=67108864</code></pre><p>-s option is deprecated. This URL tell about the option to use while creating bootstrap option:<a href=""http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-bootstrap.html#PredefinedbootstrapActions_ConfigureHadoop"" rel=""nofollow"">http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-bootstrap.html#PredefinedbootstrapActions_ConfigureHadoop</a></p>"
84,54264937,52108481,https://stackoverflow.com/questions/54264937,TRUE,,0,0,0,1,0,1,1,amazon-cognito,outdated,outdated screenshot above,,0,1,0,0,1,"<p>When I first asked this question, AWS had the ability to <strong>record</strong> metrics with the Swift SDK, but <strong>not</strong> view them in the Pinpoint API, which is absurd, because then you can only record metrics. What's the point? I asked in the AWS forums, and a couple months later, they responded something along the lines of ""Please wait - coming soon.""</p><p>This feature is now available, whereas before it simply wasn't.</p><p>Go to Pinpoint, your project, then click the Analytics drop-down menu, then click events. You can see that you can sort by metric. If you look at my outdated screenshot above, you'll see that this was <strong>not</strong> an option.</p><p><a href=""https://i.stack.imgur.com/DBTLt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DBTLt.png"" alt=""Image of Pinpoint console""></a></p>"
85,39499329,39362847,https://stackoverflow.com/questions/39499329,FALSE,obsolete version as an example in solution,0,0,1,0,0,0,1,"amazon-s3, aws-code-deploy",obsolete,obsolete version,,0,1,1,0,1,"<p>CodeDeploy is purely a deployment tool, it cannot handle the revisions in S3 bucket.</p><p>I would recommend you look into the ""lifecycle management"" for S3. Since you are using version controlled bucket (I assume), there is always one latest version and 0 to many obsolete version. You can set a lifecycle configuration of type ""NoncurrentVersionExpiration"" so that the obsolete version will be deleted after some days.</p><p>This method is still not possible to maintain a fixed number of deployments as AWS only allows specifying lifecycle in number of days. But it's probably the best alternative to your use-case. </p><p>[1] <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-set-lifecycle-configuration-intro.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-set-lifecycle-configuration-intro.html</a>[2] <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/intro-lifecycle-rules.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonS3/latest/dev/intro-lifecycle-rules.html</a></p>"
86,29436189,5187496,https://stackoverflow.com/questions/29436189,TRUE,,0,1,0,0,0,1,1,amazon-s3,out of date,,,1,1,1,0,,"<p>Tools recommend by other answers are out of date.</p><p>This one is up to date: <a href=""https://github.com/schickling/git-s3"" rel=""nofollow"">https://github.com/schickling/git-s3</a></p>"
87,52899085,52824961,https://stackoverflow.com/questions/52899085,TRUE,,0,0,1,0,0,1,1,ElastiCache,out of date,,,1,1,1,0,,"<p>ElastiCache docs are way out of date; new announcements change what's available even when the three-year old docs remain unchanged. Redis on ElastiCache introduced support for online resizing in 2017. From <a href=""https://aws.amazon.com/blogs/aws/amazon-elasticache-update-online-resizing-for-redis-clusters/"" rel=""nofollow noreferrer"">the announcement</a>:</p><blockquote>  <p>You can now adjust the number of shards in a running ElastiCache for  Redis cluster while the cluster remains online and responding to  requests. This gives you the power to respond to changes in traffic  and data volume without having to take the cluster offline or to start  with an empty cache. You can also rebalance a running cluster to  uniformly redistribute slot space without changing the number of  shards.</p></blockquote><p>I wish they'd update their 2015 (!) docs, but at any rate, this is the latest we have on the subject. As of October 19, 2018, on a cluster with cluster mode enabled:</p><p>You can:</p><ul><li>Scale out (add shards)</li><li>Scale in (remove shards)</li><li>Rebalance (move keys among shards)</li><li>Online resharding and shard balancing</li></ul><p>You cannot:</p><ul><li>Scale up/down (change node type)</li><li>Upgrade your engine</li><li>Configure shards independently</li></ul><p>Source: <a href=""https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/redis-cluster-resharding-online.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/redis-cluster-resharding-online.html</a></p>"
88,43794424,43793382,https://stackoverflow.com/questions/43794424,FALSE,,0,0,0,0,0,0,1,"amazon-web-services,Â aws-cli",out of date,,,0,0,0,0,,"<p>Are you sure you found the root cause because of <code>out of date</code>?</p><p>The problem you reported is about <a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/bucket-policy-s3-sigv4-conditions.html"" rel=""nofollow noreferrer"">Amazon S3 Signature Version 4 Authentication Specific Policy Keys</a> </p><p>You should be fine to fix with the command</p><pre><code>aws configure set profile.jacoblambert.s3.signature_version s3v4</code></pre><p>or add below lines to that profile <code>[jacoblambert]</code> in ~/.aws/config</p><pre><code>s3 =  signature_version = s3v4</code></pre><p>Refer:</p><p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version"" rel=""nofollow noreferrer"">Specifying Signature Version in Request Authentication</a></p>"
89,56830225,56829651,https://stackoverflow.com/questions/56830225,FALSE,,0,0,0,0,0,0,1,amazon-dynamodb,,,,0,0,0,0,,"<p>You don_Ã‘Ã©t need UUIDs or any pseudo-random ID. </p><p>It was once possible that you could have a hot partition if one user is particularly active, but hot partitions are <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">basically a non-issue</a> now because of DynamoDB_Ã‘Ã©s adaptive capacity. Furthermore, you should probably be limiting how fast users can create comments/posts, which would prevent hot partitions even if adaptive capacity didn_Ã‘Ã©t exist.</p><p>(Why should you limit the rate a user can post? You don_Ã‘Ã©t want a malicious actor to be able to create a new post every few milliseconds_Ã‘Ã“you should have some sort of rate limit as a protection against denial of service attacks.)</p>"
90,11996761,10463360,https://stackoverflow.com/questions/11996761,TRUE,,0,1,0,0,0,1,1,amazon-s3,outdated,,,1,1,1,1,,"<p>Was researching my own (now resolved problem), and found this question via <a href=""http://www.uploadify.com/forum/#/discussion/1416/uploading-to-amazon-s3/p1"" rel=""nofollow"">http://www.uploadify.com/forum/#/discussion/1416/uploading-to-amazon-s3/p1</a></p><p>That very thread got me running a little over a year ago, but I have recently started experiencing problems with Chrome 21/win.</p><p>Your code run through <a href=""http://jsbeautifier.org/"" rel=""nofollow"">http://jsbeautifier.org/</a></p><pre><code>$(document).ready(function () {  $('#file').uploadify({  ""formData"": {  ""AWSAccessKeyId"": ""AKIAJOVJ2J3JX5Z6AIKA"",  ""bucket"": ""my-stupid-bucket"",  ""acl"": ""private"",  ""key"": ""uploads/${filename}"",  ""signature"": ""2I0HPQ2SoZOmhAUhYEb4nANFSCQ%253D"",  ""policy"": ""[... snip ...]""  },  ""fileObjName"": ""file"",  ""uploader"": ""http://my-stupid-bucket.amazonaws.com"",  ""swf"": ""/static/uploadify/uploadify.swf""  });});</code></pre><p>I just upgraded to uploadify 3.1 and ran into similar invalid signature problems.  The answer seems to be that the code in that thread is simply outdated.  </p><p>IIRC, that thread states you should encodeURIComponent the signature/etc.  This is no longer the case, it seems.  My 2.x code is working with 3.1 without encoding the signature and policy.  Guessing this was some workaround to an uploadify/flash problem that has since been resolved.</p><p>Additionally, it looks like your signature is double URL encoded.</p><p>TL;DR;<br>Don't encodeURIComponent your form data with uploadify 3.1 (and possibly earlier versions).</p>"
91,40861054,40749213,https://stackoverflow.com/questions/40861054,TRUE,,0,0,1,0,0,1,1,amazon-dynamodb,outdated,outdated and not maintained,,1,1,1,0,,"<p>AWS does not offer a library for geospatial indexing for node.js. They only have the java version that you referred to. You could use the java library with a lambda function (I tried that), but the library is outdated and not maintained (last commit was 3 years ago)My <a href=""https://github.com/awslabs/dynamodb-geo/pull/15"" rel=""nofollow noreferrer"">pull request</a> with a fix was ignored. Even after fixing things myself I ran into issues like <a href=""https://stackoverflow.com/questions/36153366/update-user-location-with-aws-dynamodb-geo-and-lambda"">this one</a>.</p><p>To answer your questions 1 and 2: AWS wrote <a href=""https://aws.amazon.com/blogs/mobile/geo-library-for-amazon-dynamodb-part-1-table-structure/"" rel=""nofollow noreferrer"">a document series</a> about the way their library calculates geohashes and how it executes queries on DynamoDB. It's too much to summarize here.</p><p>The lack of maintenance and support by AWS and the issues I encountered made me conclude that AWS gave up on the idea of geo querying DynamoDB. Besides, I could not find any other developers that have applied this library successfully.</p><p>My current solution is based on CloudSearch. I write the items (with their location) into a DynamoDB table. I use a DynamoDB stream that uploads all changes in the table to CloudSearch. CloudSearch has geoquery features out-of-the-box.</p>"
92,52563919,52495435,https://stackoverflow.com/questions/52563919,FALSE,,0,0,0,0,0,0,,,,,,1,0,0,0,,"<p>The error message refers to <code>part_number</code> which is the name of a CSV column (called ""flat files"" in MWS). In XML you need to specify a <code>StandardProductID</code> instead, like this:</p><pre><code>  &lt;StandardProductID&gt;  &lt;Type&gt;EAN&lt;/Type&gt;  &lt;Value&gt;1234567890123&lt;/Value&gt;  &lt;/StandardProductID&gt;  </code></pre><p>Please also note, that my <code>Beauty.xsd</code> does not have <code>HairCareProduct</code>. I had to change this to <code>BeautyMisc</code> to validate the feed - but that may be due to an outdated set of XSDs.</p>"
93,17487502,13859906,https://stackoverflow.com/questions/17487502,TRUE,,0,0,1,0,0,1,1,amazon ec2,deprecated,,,1,1,0,1,,"<p>As of Riak 1.3, riak-admin reip is deprecated and the use of riak-admin cluster replace is the recomended way of replacing a cluster's name.</p><p>These are the commands I had to issue:</p><pre><code>riak stop # stop the noderiak-admin down riak@127.0.0.1 # take it downsudo rm -rf /var/lib/riak/ring/* # delete the riak ringsudo sed -i ""s/127.0.0.1/`hostname -i`/g"" /etc/riak/vm.args # Change the name in configriak-admin cluster force-replace riak@127.0.0.1 riak@""`hostname -i`"" # replace the nameriak start # start the node</code></pre><p>That should set the node's name to riak@[your EC2 internal IP address].</p>"
94,33685242,33684609,https://stackoverflow.com/questions/33685242,TRUE,,0,0,1,0,0,1,1,amazon-web-services,deprecated,,,1,1,1,0,,"<p>I would remove the <code>provider</code> key. The <code>carrierwave-aws</code> gem <a href=""https://github.com/sorentwo/carrierwave-aws#usage"" rel=""nofollow"">readme</a> (I'm guessing you are using that or something similar) does not even mention the <code>provider</code> key. That might have been an old requirement that has been deprecated. </p>"
95,36988951,36964256,https://stackoverflow.com/questions/36988951,TRUE,,0,0,1,0,0,1,1,amazon-sns,deprecated,,,1,1,0,1,,"<p>This line has a bug:</p><pre><code>AWSSNS *snsManager = [[AWSSNS new] initWithConfiguration:configuration];</code></pre><p><code>- new</code> is an equivalent of <code>- alloc</code> plus <code>- init</code>. You are calling two <code>init</code> methods. It should be changed to:</p><pre><code>AWSSNS *snsManager = [[AWSSNS alloc] initWithConfiguration:configuration];</code></pre><p>Also, <code>- initWithConfiguration:</code> had been deprecated for a while, and now it's removed from the SDK. You need to use <a href=""http://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSSNS.html#//api/name/defaultSNS"" rel=""nofollow""><code>+ defaultSNS</code></a> or <a href=""http://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSSNS.html#//api/name/SNSForKey:"" rel=""nofollow""><code>+ SNSForKey:</code></a> instead.</p>"
96,28485802,28480220,https://stackoverflow.com/questions/28485802,TRUE,,0,0,1,0,0,1,1,amazon-web-services,deprecated,,,1,1,1,0,,"<p><code>AmazonS3Client</code> is from the version 1 of the AWS Mobile SDK for iOS, which has been deprecated. You should use <code>AWSS3TrasnferManager</code> or <code>AWSS3</code> in the version 2 of the SDK instead. You can take a look at <a href=""http://docs.aws.amazon.com/mobile/sdkforios/developerguide/"" rel=""nofollow"">AWS Mobile SDK Guide</a> and the S3TransferManager Sample at our <a href=""https://github.com/awslabs/aws-sdk-ios-samples"" rel=""nofollow"">GitHub repo</a> for further details.</p>"
97,47897459,47897088,https://stackoverflow.com/questions/47897459,TRUE,,0,0,1,0,0,1,1,aws-powershell,deprecated,,,1,1,1,0,,"<p>Try creating the ApiKey without StageKeys at all. It isn't a required parameter, and per the <a href=""http://docs.aws.amazon.com/powershell/latest/reference/items/New-AGApiKey.html"" rel=""nofollow noreferrer"">New-AGApiKey documentation</a> this parameter has been deprecated in favor of <a href=""https://aws.amazon.com/blogs/aws/new-usage-plans-for-amazon-api-gateway/"" rel=""nofollow noreferrer"">usage plans</a>:</p><blockquote>  <p>-StageKey StageKey[]</p>   <p>DEPRECATED FOR USAGE PLANS - Specifies stages associated with the API key.<br>  Required? False<br>  Position? Named<br>  Accept pipeline input?  False</p></blockquote><p>This option has likewise been deprecated in the CLI and other SDKs.</p><p>If you still need to use StageKeys, the <a href=""http://docs.aws.amazon.com/sdkfornet/v3/apidocs/index.html?page=EC2/TEC2Filter.html&amp;tocid=Amazon_EC2_Model_Filter"" rel=""nofollow noreferrer"">Amazon.APIGateway.Model.StageKey type is defined here</a>, in the AWS SDK for .NET documentation. You can create a new instance of this type in powershell as described below, where a powershell with matching property names and values is used as input for the new object:</p><pre><code>$obj = New-Object Amazon.APIGateway.Model.StageKey -Property @{ RestApiId = ""myId""; StageName = ""myName"" }</code></pre><p>To verify the type is correct:</p><pre><code>$obj | get-member  TypeName: Amazon.APIGateway.Model.StageKeyName  MemberType Definition----  ---------- ----------Equals  Method   bool Equals(System.Object obj)GetHashCode Method   int GetHashCode()GetType   Method   type GetType()ToString  Method   string ToString()RestApiId   Property   string RestApiId {get;set;}StageName   Property   string StageName {get;set;}</code></pre>"
98,59594292,59585572,https://stackoverflow.com/questions/59594292,TRUE,,0,0,1,0,0,1,1,amazon-s3,deprecated,,,1,1,1,0,,"<p>Signature Version 2 (which is what you are using, here) always expects the resource to be <code>/${bucket}/${file}</code> regardless of whether the  URL is path-based (bucket as the first element of the path) or virtual (bucket as part of the hostname).</p><p>Additionally, for legacy reasons, S3 expects <code>+</code> in the signature to be url-escaped as <code>%2B</code> and (iirc) will accept but does not require any <code>/</code> to be transformed to <code>%2F</code> and any <code>=</code> to be transformed to <code>%3D</code> so you need to verify (<code>curl -v ...</code>) whether curl is transforming it, and if not, use something like <code>sed</code> or maybe <code>tr</code> in the pipeline to do that encoding of the signature, or perhaps find a curl option that can enable the query string parameter encoding.</p><p>Be aware that <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/"" rel=""nofollow noreferrer"">Signature V2 is deprecated</a> and will be deactivated at some point after June, 2020, so even though it still works on existing buckets in regions where S3 originally launched prior to 2014, its usable lifespan is most definitely coming to an end... so your interests would be served by not creating new systems that use it.  The successor algorithm, Signature V4, is more complicated but there are solid security-related justifications for its increased complexity.</p>"
99,27893162,27747455,https://stackoverflow.com/questions/27893162,FALSE,,0,0,0,0,0,0,0,,,,,0,0,0,0,,"<p>Homebrew relies on volunteers to keep the formulas updated.  If you notice an outdated formula, please submit a bug or pull request.</p>"