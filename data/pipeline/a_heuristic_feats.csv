,Id,PostTypeId,ParentId,CreationDate,Score,Body,OwnerUserId,OwnerDisplayName,LastActivityDate,CommentCount,ContentLicense,cnt_keywords,subject,punctuation,negative_statement,sentence,subj_irrel,include_irrel
0,3895343,2,3893211,2010-10-09T01:37:35.720,0,"<p>This is probably not the best time to criticize your code but:</p>

<p>All your script should be included externally, not inline.
Do not use , this is a deprecated, ancient way of including inline script.  If you must, use //
I see in your script you are using document.write().  DON'T!</p>

<p>You cannot cheat and include a script in such a way and expect it to be executed.</p>

<p>Use <a href=""http://api.jquery.com/jQuery.getScript/"" rel=""nofollow"">jQuery.getScript()</a></p>
",454533,2010-10-09T01:37:35.720,0,CC BY-SA 2.5,,1,, ,1,"Do not use , this is a deprecated, ancient way of including inline script.  ",False,False
1,3991880,2,3931881,2010-10-21T20:59:47.593,0,"<p>It stopped working on my site. I assumed deprecated meant it won't work in the future, so don't build around it. I didn't think they'd remove it completely.</p>
",462933,2010-10-21T20:59:47.593,0,CC BY-SA 2.5,,1,,.,1,"I assumed deprecated meant it won't work in the future, so don't build around it.",False,False
2,4979599,2,4976451,2011-02-12T17:52:28.150,2,"<p>The AMI's are kinda outdated now, but if you follow <a href=""http://groups.drupal.org/node/70268"" rel=""nofollow"" title=""these instructions"">these instructions from the pantheon group on g.d.o</a> using one of the Alestic Ubuntu 10.4 ami's as a base you should be fine to roll your own.</p>
",149390,2011-02-12T17:52:28.150,0,CC BY-SA 2.5,,1,,"
",0,"The AMI's are kinda outdated now, but if you follow <a href=""http://groups.drupal.org/node/70268"" rel=""nofollow"" title=""these instructions"">these instructions from the pantheon group on g.d.o</a> using one of the Alestic Ubuntu 10.4 ami's as a base you should be fine to roll your own.</p>
",False,False
3,5029447,2,5027097,2011-02-17T13:19:11.590,3,"<p>The difference is that the mapred packages are deprecated. You should use since 0.20.x the new stuff inside mapreduce package. <br>
For example the new way to implement a mapper, with mapred-package you have to implement the mapper interface. With the mapreduce-package you simply extend from a basic mapper class and override just the method you need.</p>
",540873,2011-02-17T13:19:11.590,0,CC BY-SA 2.5,,1,,.,0,The difference is that the mapred packages are deprecated.,False,False
4,5265964,2,4498192,2011-03-10T21:11:42.767,5,"<p>The ""query"" interface was the original search interface for SimpleDB.  It was set-based, non-standard and quite lovely, I thought.  However, over time AWS introduced a SQL-like query language (accessed via the Select request) and then deprecated and eventually removed the original query interface.</p>

<p>So, the reason it doesn't work in boto is because it is no longer supported by SimpleDB.  For more up-to-date boto documentation, look <a href=""http://boto.cloudhackers.com/"" rel=""noreferrer"">here</a>.</p>
",653643,2011-03-10T21:11:42.767,0,CC BY-SA 2.5,,1,,query,0,"However, over time AWS introduced a SQL-like query language (accessed via the Select request) and then deprecated and eventually removed the original query",False,False
5,5820746,2,5820120,2011-04-28T15:05:14.377,0,"<p>Man, that sounds incredibly frustrating. Looks like you're doing all of the things I would recommend, and trying lots of variations/tests. I've got one idea, and two maybe not so good ideas:</p>

<p>1) I ran into encoding issues when I used Miro, and ended up having files that would play fine locally... but ended up behaving differently when I viewed them online. To ensure that it's not an encoding issue, try subbing in an existing .ogv file that you know works - possibly from some other site/tutorial.</p>

<p>Standby reference: <a href=""http://www.bigbuckbunny.org/index.php/download/"" rel=""nofollow"">Big Buck Bunny</a></p>

<p>2) This is a longshot, and it's me showing how little I know about these formats... but maybe try using .ogg as an extension, instead of .ogv? I looked and see that .ogv is meant for video, with .ogg being deprecated... but changing the file extension might reveal something about your encoding process.</p>

<p>3) Another longshot: kill the codecs attribute for your mp4 source.</p>
",671759,2011-04-28T15:05:14.377,0,CC BY-SA 3.0,,1,,process.</p,0,being deprecated... but changing the file extension might reveal something about your encoding process.</p,False,False
6,5935495,2,4811761,2011-05-09T10:21:15.320,2,"<p>Just for completeness, I ended up using the SDK provided by Amazon for Java. It can be found here:</p>

<p><a href=""http://aws.amazon.com/sdkforjava/"" rel=""nofollow"">http://aws.amazon.com/sdkforjava/</a></p>

<p>All other client libraries were outdated or had missing functionality.</p>
",292145,2011-05-09T10:21:15.320,0,CC BY-SA 3.0,,1,libraries,functionality.</p,0,>All other client libraries were outdated or had missing functionality.</p,False,False
7,7129403,2,4521846,2011-08-20T03:21:33.600,1,"<p>As msha points out, the sending of a workerID parameter to an ExternalQuestion page seems to be deprecated, or at least taken out of the latest version of the documentation.</p>

<p>However, a fellow researcher who's been using MTurk a lot says: ""People seem to be using it in the forums. I would go ahead with it...if it ever actually disappears, I'm sure that the developer community will yell very loudly. :) ""</p>

<p>I tried it empirically today (2011-08-19), and indeed a workerID is being sent to the ExternalQuestion page I set up on my own server, after the HIT has been accepted. This was in the sandbox. My ExternalQuestion page contained a Java Web Start button (as described here: <a href=""http://download.oracle.com/javase/tutorial/deployment/deploymentInDepth/createWebStartLaunchButtonFunction.html"" rel=""nofollow"">http://download.oracle.com/javase/tutorial/deployment/deploymentInDepth/createWebStartLaunchButtonFunction.html</a> ); I don't know if that made any difference.</p>
",772978,2011-08-20T03:21:33.600,1,CC BY-SA 3.0,,1,,documentation.</p,0,"As msha points out, the sending of a workerID parameter to an ExternalQuestion page seems to be deprecated, or at least taken out of the latest version of the documentation.</p",False,False
8,7322249,2,7259601,2011-09-06T15:20:43.983,2,"<p>We found the issue. We noticed that some instances in our cluster always produced the problem and some never did. Apparently the issue is unique to instances that run more recent CPU versions combined with slightly outdated kernels.</p>

<p>The issue is explained in full here : <a href=""https://bugs.launchpad.net/ubuntu/+source/linux/+bug/727459"" rel=""nofollow noreferrer"">https://bugs.launchpad.net/ubuntu/+source/linux/+bug/727459</a></p>

<p>As the running time increases the spikes shown in the graph below become longer and longer, possible due to time shift and at some point it'll spin the one core for a long time.</p>

<p>Cpu usage graphs. Affected instance versus unaffected:</p>

<p><img src=""https://i.stack.imgur.com/sM8r6.png"" alt=""Cpu usage graphs. Affected instance versus unaffected""></p>

<p>The issue has been fixed recently so a kernel update fixed this issue for us.</p>
",813957,2011-09-06T15:20:43.983,1,CC BY-SA 3.0,,1,,kernels.</p,0,Apparently the issue is unique to instances that run more recent CPU versions combined with slightly outdated kernels.</p,False,False
9,7543698,2,6645026,2011-09-25T04:40:23.570,1,"<p>i think you are facing this problem because you are using RAILS_ROOT which has been deprecated in Rails 3. try using Rails.root instead and see if you face the same problem. Please check the correct usage for Rails.root before making the correction. </p>
",963290,2011-09-25T04:40:23.570,0,CC BY-SA 3.0,,1,,.,0,i think you are facing this problem because you are using RAILS_ROOT which has been deprecated in Rails 3.,False,False
10,7642625,2,7638646,2011-10-04T02:11:20.433,0,"<p>As described in the question both Azure and EC2 will do the job very well. This is the kind of task both systems are designed for.</p>

<p>So the question becomes really: which is <em>best</em>? That depends on two things: what the application needs to do and your own experience and preference.</p>

<p>As it's a Windows application there should probably be a leaning towards Azure. While EC2 supports Windows, the tooling and support resources for Azure are probably deeper at this point. </p>

<p>If cost is a factor then a (somewhat outdated) resource is here: <a href=""http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost-comparison/"" rel=""nofollow"">http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost-comparison/</a> -- the conclusion is that, by and large, Azure and Amazon are roughly similar for compute charges.</p>
",3546,2011-10-04T02:11:20.433,0,CC BY-SA 3.0,,1,,cost,0,">If cost is a factor then a (somewhat outdated) resource is here: <a href=""http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost-comparison/"" rel=""nofollow"">http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost",False,True
11,7651730,2,6675018,2011-10-04T17:25:34.107,4,"<p>What you really want is the equivalent of the <code>OpenListings</code> report from the <code>Product Advertising API</code> in MWS and that is the <code>RequestReport</code> call with a report type of <code>_GET_MERCHANT_LISTINGS_DATA_</code>. This returns you all the inventory a seller has listed on Amazon and from here to getting your ASIN from that list it's close.</p>

<p>You can find out more details in their <a href=""https://developer.amazonservices.com/gp/mws/docs.html"" rel=""nofollow"">documentation</a></p>

<p>Also, I advise you to not use the Product Advertising API anymore as Amazon deprecated it and it will be out of use this time next year.</p>
",49032,2011-10-04T17:25:34.107,0,CC BY-SA 3.0,,1,,next,1,"Also, I advise you to not use the Product Advertising API anymore as Amazon deprecated it and it will be out of use this time next",False,False
12,7718816,2,7704946,2011-10-10T21:19:34.097,4,"<p>Changing URLs is a safe way to invalidate outdated assets.</p>

<p>It is also a necessity if you want to allow users storing private images. Using a path deductible from the users account name/id/path would render privacy settings useless as soon as you store assets on a CDN.</p>
",589017,2011-10-10T21:19:34.097,0,CC BY-SA 3.0,,1,,outdated,0,<p>Changing URLs is a safe way to invalidate outdated,False,False
13,8106724,2,8106239,2011-11-12T18:51:24.333,2,"<p>There are lots of guides out there, some out of date, and some recent ones, below are some links. You might also be interested in <a href=""http://aws.amazon.com/elasticbeanstalk/"" rel=""nofollow"">Amazon Elastic Beanstalk</a>. Here are some links</p>

<ul>
<li><a href=""http://blog.peterdelahunty.com/2009/06/deploying-grails-app-on-ec2-in-from.html"" rel=""nofollow"">Deploying Grails App on Amazon EC2 form scratch</a></li>
<li><a href=""http://grails.1312388.n4.nabble.com/Amazon-EC2-td3055649.html"" rel=""nofollow"">Question about EC2 and Grails on the mailing list</a></li>
</ul>

<p>To answer the question about the credit card, I think you can't get away without one, since if you go over the free usage tier you need to have some sort of payment method to pay for the extra usage.</p>
",372561,2011-11-12T18:51:24.333,0,CC BY-SA 3.0,,1,,.,0,"There are lots of guides out there, some outdated, and some recent ones, below are some links.",False,False
14,8125958,2,8056962,2011-11-14T18:00:40.387,1,"<p>It used to be be possible by doing a brute force search with the Seller* API calls. However these calls have been deprecated since Nov 1 2011, so you're out of luck. If you happen to be working for this particular seller (instead of being simply a customer or a competitor), you'll want to use the APIs available to the seller (MWS) to download inventory reports.</p>
",50812,2011-11-14T18:00:40.387,0,CC BY-SA 3.0,,1,calls,.,0,"However these calls have been deprecated since Nov 1 2011, so you're out of luck.",False,False
15,8127487,2,7800800,2011-11-14T20:15:19.397,1,"<p>I'm working on the same issue and I know how to execute the GA code you want to execute, especially since you are working with the older version of the code. I am working on the same issue with an Amazon webstore -- if you are able to upload a js file, and it looks like you are able to do this in files, I would suggest using the init() function which is deprecated but it still works - here is an example where I am doing something similar. I am setting the contents of a custom variable and creating the tracking object -- but I'm not firing the page tracker until later in the code.</p>

<p>Using the code below, replace the Custom Variable setting code with the linker and set Domain code you need for cross domain tracking. Since the GA code added by amazon doesn't try to set the domain the linker functions should prevent a second cookie from being created, and use the cookie data passed in the URL.</p>

<pre><code>&lt;script type=""text/javascript""&gt; 
var gaJsHost = ((""https:"" ==     document.location.protocol) ? ""https://ssl."" : ""http://www."");
document.write(unescape(""%3Cscript src='"" + gaJsHost + ""google-analytics.com/ga.js'     type='text/javascript'%3E%3C/script%3E""));
&lt;/script&gt;
&lt;script type=""text/javascript""&gt;
try {
var pageTracker = _gat._getTracker(""UA-xxxxxxx-x""); 
pageTracker._initData();
pageTracker._setCustomVar(1, ""GWO"", utmx('combination').toString(), 1);} 
catch(err) {}&lt;/script&gt;
</code></pre>
",1046301,2011-11-14T20:15:19.397,0,CC BY-SA 3.0,,1,,.,0,"I am working on the same issue with an Amazon webstore -- if you are able to upload a js file, and it looks like you are able to do this in files, I would suggest using the init() function which is deprecated but it still works - here is an example where I am doing something similar.",False,False
16,8173888,2,8173561,2011-11-17T20:33:04.200,6,"<p>Zach, the simple answer is that there's not a simple path to there from here :)</p>

<p>When I wrote Segue I hoped that someone would soon come out with something that would make Segue obsolete. Cloudnumbers may be it one day, but probably not yet. I have toyed with making Segue a foreach backend, but since I don't use it that way, my motivation has been pretty low to take the time to learn how to build the backend. </p>

<p>One of the things that is very promising, in my opinion, is using the <code>doRedis()</code> package with workers on Amazon EC2. doRedis uses a Redis server as the job controller and then lets workers connect to the Redis server and get/return jobs and results. I've been thinking for a while that it would be nice to have a dead simple way to deploy a doRedis cluster on EC2. But nobody has written one yet that I know of. </p>
",37751,2011-11-17T20:33:04.200,11,CC BY-SA 3.0,,1,,.,0,>When I wrote Segue I hoped that someone would soon come out with something that would make Segue obsolete.,False,False
17,8927333,2,8407750,2012-01-19T14:01:34.183,2,"<p>Apparently I was looking at some badly out of date documentation.</p>

<p>in <code>AmazonS3Client</code> see:</p>

<pre><code>- (S3MultipartUpload * AmazonS3Client)initiateMultipartUploadWithKey:(NSString *)theKey withBucket:(NSString *)theBucket    
</code></pre>

<p>Which will give you a <code>S3MultipartUpload</code> which will contain an <strong>uploadId</strong>.</p>

<p>You can then put together an S3UploadPartRequest using <code>initWithMultipartUpload:        (S3MultipartUpload *)   multipartUpload</code> and send that as you usually would.</p>

<p>S3UploadPartRequest contains an int property partNumber where you can specify the part # you're uploading.</p>
",459082,2012-01-19T14:01:34.183,0,CC BY-SA 3.0,,1,,documentation.</p,0,Apparently I was looking at some badly outdated documentation.</p,False,False
18,9064647,2,5302394,2012-01-30T13:31:21.533,1,"<p>Well, as stated by Geoff Appleford, Elasticfox is an outdated extension. I dont know why it's development stopped as it is a great tool.</p>

<p>If you wish to do things that are achieved through Elasticfox, I would suggest you to take a look at Hybridfox. Hybrdifox is a fork of Elasticfox which supports other private clouds as well. It is also a great tool too.</p>

<p>It is also open source and actively updated. Check it's project home(http://code.google.com/p/hybridfox/)</p>

<p>Sources:
<a href=""http://code.google.com/p/hybridfox/"" rel=""nofollow"">Hybridfox home</a></p>
",850018,2012-01-30T13:31:21.533,0,CC BY-SA 3.0,,1,,.,0,"Well, as stated by Geoff Appleford, Elasticfox is an outdated extension.",False,False
19,9179801,2,7487292,2012-02-07T16:20:03.357,4,"<p>Note that according to Amazon, at <a href=""http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/FileSystemConfig.html"" rel=""nofollow"">http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/FileSystemConfig.html</a> ""Amazon Elastic MapReduce - File System Configuration"", the S3 Block FileSystem is deprecated and its URI prefix is now s3bfs:// and they specifically discourage using it since ""it can trigger a race condition that might cause your job flow to fail"".</p>

<p>According to the same page, HDFS is now 'first-class' file system under S3 although it is ephemeral (goes away when the Hadoop jobs ends).</p>
",98694,2012-02-07T16:20:03.357,0,CC BY-SA 3.0,,1,,"fail"".</p",0,"/DeveloperGuide/FileSystemConfig.html</a> ""Amazon Elastic MapReduce - File System Configuration"", the S3 Block FileSystem is deprecated and its URI prefix is now s3bfs:// and they specifically discourage using it since ""it can trigger a race condition that might cause your job flow to fail"".</p",False,False
20,9440295,2,9280539,2012-02-25T01:30:14.873,0,"<p>I'd like to note that AWS is deprecated when it comes to the store, you should be using MWS and here is the docs for the correct part of MWS you need V138592989.pdf"">https://images-na.ssl-images-amazon.com/images/G/01/mwsportal/doc/en_US/products/MWSProductsApiReference.<em>V138592989</em>.pdf and here is the link [hard to find] to the main MWS area. <a href=""https://developer.amazonservices.com/gp/mws/api.html/182-3268148-9799929?ie=UTF8&amp;section=products&amp;group=products&amp;version=latest"" rel=""nofollow"">https://developer.amazonservices.com/gp/mws/api.html/182-3268148-9799929?ie=UTF8&amp;section=products&amp;group=products&amp;version=latest</a></p>

<p>Keep in mind that without an active pro seller account you will not be able to get a developer key if you are trying to build apps that integrate with private parts of the amazon API. I just went through this and had to grab the free trial long enough to get a key, which still works after you cancel the trial. </p>

<p>Also, you can not receive any support from MWS without that pro seller account! I spent over an hour on the phone with my AWS rep and he didn't even know how to access the MWS site. He had to call them only to be told [by them] that they didn't know how to access their API either, this was all too new for them.</p>

<p>I did eventually get everything working for me after scouring through their docs and downloading the correct API for MWS.</p>

<p>Good Luck!</p>
",526849,2012-02-25T01:30:14.873,0,CC BY-SA 3.0,,1,,.,0,">I'd like to note that AWS is deprecated when it comes to the store, you should be using MWS and here is the docs for the correct part of MWS you need V138592989.pdf"">https://images-na.ssl-images-amazon.com/images/G/01/mwsportal/doc/en_US/products/MWSProductsApiReference.<em>V138592989</em>.pdf and here is the link [hard to find] to the main MWS area.",False,False
21,9489329,2,9489036,2012-02-28T20:38:04.773,0,"<p>Although it may be a bit outdated, <a href=""http://freemarker.sourceforge.net/"" rel=""nofollow"">FreeMarker</a> is primarily a framework for rendering template files given some set of inputs.</p>
",12604,2012-02-28T20:38:04.773,6,CC BY-SA 3.0,,1,it,inputs.</p,0,"Although it may be a bit outdated, <a href=""http://freemarker.sourceforge.net/"" rel=""nofollow"">FreeMarker</a> is primarily a framework for rendering template files given some set of inputs.</p",False,False
22,9560483,2,9559973,2012-03-05T01:06:55.170,1,"<blockquote>
  <p>Is there a way to expand its size to the size of the volume without
  losing my work?</p>
</blockquote>

<p>That depends on whether you can live with a few minutes downtime for the computation, i.e. whether stopping the instance (hence the computation process) is a problem or not - Eric Hammond has written a detailed article about <a href=""http://alestic.com/2010/02/ec2-resize-running-ebs-root"" rel=""nofollow"">Resizing the Root Disk on a Running EBS Boot EC2 Instance</a>, which addresses a different but pretty related problem:</p>

<blockquote>
  <p>[...] what if you have an EC2 instance already running and you need to
  increase the size of its root disk without running a different
  instance?</p>
  
  <p>As long as you are ok with a little down time on the EC2 instance (few
  minutes), it is possible to change out the root EBS volume with a
  larger copy, without needing to start a new instance.</p>
</blockquote>

<p>You have already done most of the steps he describes and created a new 300GB volume from the 180GB snapshot, but apparently you have missed the last required step indeed, namely resizing the file system on the volume - here are the instructions from Eric's article:</p>

<blockquote>
  <p>Connect to the instance with ssh (not shown) and resize the root file
  system to fill the new EBS volume. This step is done automatically at
  boot time on modern Ubuntu AMIs:</p>

<pre><code># ext3 root file system (most common)
sudo resize2fs /dev/sda1
#(OR)
sudo resize2fs /dev/xvda1

# XFS root file system (less common):
sudo apt-get update &amp;&amp; sudo apt-get install -y xfsprogs
sudo xfs_growfs /
</code></pre>
</blockquote>

<p>So the details depend on the file system in use on that volume, but there should be a respective resize command available for all but the most esoteric or outdated ones, none of which I'd expect in a regular Ubuntu 10 installation.</p>

<p>Good luck!</p>

<hr>

<h3>Appendix</h3>

<blockquote>
  <p>Is there a possibility that the snapshot is actually continuous with
  another drive (e.g. /dev/sdb)?</p>
</blockquote>

<p>Not just like that, this would require a <a href=""http://en.wikipedia.org/wiki/RAID"" rel=""nofollow"">RAID</a> setup of sorts, which is unlikely to be available on a stock Ubuntu 10, except if somebody provided you with a respectively customized AMI. The size of <code>/dev/sdb</code> does actually hint towards this being your <a href=""http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/InstanceStorage.html"" rel=""nofollow"">Amazon EC2 Instance Storage</a>:</p>

<blockquote>
  <p>When an instance is created from an Amazon Machine Image (AMI), in
  most cases it comes with a preconfigured block of pre-attached disk
  storage. Within this document, it is referred to as an instance store;
  it is <strong>also known as an ephemeral store</strong>. An instance store provides
  temporary block-level storage for Amazon EC2 instances. The data on
  the instance store volumes <strong>persists only during the life of the
  associated Amazon EC2 instance</strong>. The amount of this storage ranges from
  160GiB to up to 3.3TiB and varies by Amazon EC2 instance type. [...] <em>[emphasis mine]</em></p>
</blockquote>

<p>Given this storage is not persisted on instance termination (in contrast to the <a href=""http://aws.amazon.com/ebs/"" rel=""nofollow"">EBS storage</a> we all got used to enjoy - the different behavior is detailed in <a href=""http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html"" rel=""nofollow"">Root Device Storage</a>), it should be treated with respective care (i.e. never store something on instance storage you couldn't afford to loose).</p>
",45773,2012-03-05T01:06:55.170,3,CC BY-SA 3.0,,1,,Ubuntu,0,"So the details depend on the file system in use on that volume, but there should be a respective resize command available for all but the most esoteric or outdated ones, none of which I'd expect in a regular Ubuntu",False,False
23,9638670,2,9506791,2012-03-09T17:50:46.747,1,"<p>i would say API documentation is outdated:</p>

<p><a href=""http://aws.amazon.com/archives/Product%20Advertising%20API"" rel=""nofollow"">http://aws.amazon.com/archives/Product%20Advertising%20API</a>
 - says ""Last Modified: Jul 26, 2011 2:10 AM GMT""</p>

<p>this is CURRENT version of AWSECommerceService - it's from 2011-08-01 - and it does not have tagpage element:
 <a href=""http://ecs.amazonaws.com/AWSECommerceService/2011-08-01/AWSECommerceService.wsdl"" rel=""nofollow"">http://ecs.amazonaws.com/AWSECommerceService/2011-08-01/AWSECommerceService.wsdl</a></p>

<p>and here is OLDER version of AWSECommerceService - it's from 2011-04-01 - and it does have tagpage element:
 <a href=""http://ecs.amazonaws.com/AWSECommerceService/2011-04-01/AWSECommerceService.wsdl"" rel=""nofollow"">http://ecs.amazonaws.com/AWSECommerceService/2011-04-01/AWSECommerceService.wsdl</a></p>
",1246870,2012-03-09T17:50:46.747,2,CC BY-SA 3.0,,1,,outdated:</p,0,i would say API documentation is outdated:</p,False,False
24,9723442,2,9723070,2012-03-15T15:56:37.887,5,"<p>According to your logfile, your version of fog is very very old. You're using 0.3.25, and the most recent tag is at 1.1.2. Try doing this:</p>

<pre><code>bundle update fog
</code></pre>

<p>Your version of carrierwave is similarly out of date, so I'd <code>bundle update carrierwave</code> as well. That should help correct this issue.</p>
",1224374,2012-03-15T15:56:37.887,0,CC BY-SA 3.0,,1,,>,0,"Your version of carrierwave is similarly outdated, so I'd <code>",False,False
25,9810696,2,9597500,2012-03-21T18:30:18.017,2,"<p>This has meanwhile been addressed in the AWS team response to the identical question asked in the AWS forum, see <a href=""https://forums.aws.amazon.com/message.jspa?messageID=325839#325839"" rel=""nofollow"">EC2 reports AMI: Unavailable</a>:</p>

<blockquote>
  <p>This is an AWS owned AMI that is no longer publicly available as it is
  deprecated. This will not affect your currently running instance.
  Additionally, if you create an EBS AMI of your running instance you
  will create a point in time backup of your current configuration --
  which you can use to launch duplicate instances from. </p>
  
  <p>The current AWS provided Windows Server 2008 32bit AMI is:
  ami-541dcf3d</p>
</blockquote>
",45773,2012-03-21T18:30:18.017,0,CC BY-SA 3.0,,1,,.,1,"This is an AWS owned AMI that is no longer publicly available as it is
  deprecated.",False,False
26,9857697,2,9857086,2012-03-25T03:23:14.873,4,"<p>You might want to start the script with:</p>

<pre><code>apt-get update
</code></pre>

<p>as your apt cache might be out of date for the ""apt-get install"".</p>

<p>You can also debug the script by starting it with these two lines:</p>

<pre><code>#!/bin/bash -ex
exec &gt; &gt;(tee /var/log/rc.local.log|logger -t rc.local -s 2&gt;/dev/console) 2&gt;&amp;1
</code></pre>

<p>This will echo each command and its output to /var/log/rc.local.log so you can find out what is failing and with what error.</p>

<p>Make sure the file is executable:</p>

<pre><code>sudo chmod 755 /etc/rc.local
</code></pre>

<p>Note that rc.local is run on <em>every</em> boot, not just the first boot.  Make sure that you are ok with it being run again after the system has been up for a while.</p>

<p>Here's an article I wrote with more details about the ""exec"" command above: <a href=""http://alestic.com/2010/12/ec2-user-data-output"" rel=""nofollow"">http://alestic.com/2010/12/ec2-user-data-output</a></p>
",111286,2012-03-25T03:23:14.873,3,CC BY-SA 3.0,,1,,"

",0,">as your apt cache might be outdated for the ""apt-get install"".</p>

",False,False
27,9918301,2,3189979,2012-03-29T02:24:17.850,1,"<p>This discussion and the related Amazon post helped me get the client working. That being said, I felt that the solution could be improved with regards to the following:</p>

<ol>
<li>Setting WebService handlers in code is discouraged. A XML configuration file and a corresponding @HandlerChain annotation are recommended.</li>
<li>A SOAPHandler is not required in this case, LogicalHandler would do just fine. A SOAPHandler has more reach than a LogicalHandler and when it comes to code, more access is not always good. </li>
<li>Stuffing the signature generation, addition of a Node and printing the request in one handler seems like a little too much. These could be separated out for separation of responsibility and ease of testing. One approach would be to add the Node using a XSLT transformation so that the handler could remain oblivious of the transformation logic. Another handler could then be chained which just prints the request. 
<a href=""http://www.mediafire.com/?9rdhn1r8na19mft"" rel=""nofollow"" title=""Example"">Example</a></li>
</ol>
",839733,2012-03-29T02:24:17.850,0,CC BY-SA 3.0,,1,handlers,.,0,<li>Setting WebService handlers in code is discouraged.,False,False
28,10010745,2,10009638,2012-04-04T12:06:47.033,3,"<p>If you are about to register that domain anyway in case, you could simply try to call <a href=""http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/simpleworkflow/AmazonSimpleWorkflowClient.html#registerDomain%28com.amazonaws.services.simpleworkflow.model.RegisterDomainRequest%29"" rel=""nofollow"">registerDomain()</a> and catch the <a href=""http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/simpleworkflow/model/DomainAlreadyExistsException.html"" rel=""nofollow"">DomainAlreadyExistsException</a>:</p>

<blockquote>
  <p>Returned if the specified domain already exists. You will get this
  fault even if the existing domain is in deprecated status.</p>
</blockquote>

<p>Obviously this has the side effect of actually registering the domain, if it doesn't exist yet ;)</p>

<p>Otherwise <a href=""http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/simpleworkflow/AmazonSimpleWorkflowClient.html#describeDomain%28com.amazonaws.services.simpleworkflow.model.DescribeDomainRequest%29"" rel=""nofollow"">describeDomain()</a> should allow a similar approach, insofar it will throw an <a href=""http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/simpleworkflow/model/UnknownResourceException.html"" rel=""nofollow"">UnknownResourceException</a> in case of a non existing domain:</p>

<blockquote>
  <p>Returned when the named resource cannot be found with in the scope of
  this operation (region or <strong>domain</strong>). This could happen if the named
  resource was never created or is no longer available for this
  operation. <em>[emphasis mine]</em></p>
</blockquote>
",45773,2012-04-04T12:06:47.033,0,CC BY-SA 3.0,,1,,status.</p,0,"You will get this
  fault even if the existing domain is in deprecated status.</p",False,False
29,10803486,2,10770971,2012-05-29T16:56:24.620,1,"<p>The Zend_Service_Amazon is out of date and uses old WSDL.
see <a href=""http://zendframework.com/issues/browse/ZF-12046"" rel=""nofollow"">http://zendframework.com/issues/browse/ZF-12046</a></p>

<p>wait for 1.12 or use the provided batch file.</p>
",151097,2012-05-29T16:56:24.620,0,CC BY-SA 3.0,,1,Zend_Service_Amazon,"
",0,"The Zend_Service_Amazon is outdated and uses old WSDL.
",False,False
30,11148178,2,8878736,2012-06-21T23:17:07.300,0,"<p>If you have access to a Motorola Xoom running Honeycomb 3.2 you might want to test your app on that.  My app was rejected by Amazon because they experienced blank screens.  I've tested it thoroughly on Kindle Fire and have never seen that behavior.  It was only when I emphasized that point with the Amazon reviewer that I learned they were not testing on their own device but on Motorola's Xoom running an obsolete OS that I don't want to support.  I now have a Xoom and my app runs fine on it under ICS but is quite buggy with Honeycomb.</p>
",1473527,2012-06-21T23:17:07.300,0,CC BY-SA 3.0,,1,, ,1,It was only when I emphasized that point with the Amazon reviewer that I learned they were not testing on their own device but on Motorola's Xoom running an obsolete OS that I don't want to support.  ,False,False
31,11886716,2,11820166,2012-08-09T15:26:16.070,1,"<p>This <code>wasn't an issue with the AWSS3JavaClient code</code>, based on the fact that the this problem was happening both with S3 library and with other Java S3 libraries, and the fact that <code>SSL cert verification is done inside the JVM platform library code</code>, not inside our S3 library code. </p>

<p>The problem is that our JVM's keystore didn't have the most recent certificate authorities (CAs) that allow the JVM to form a chain of trust for whatever cert we're getting from the S3 SSL endpoint. This is a fairly common problem with Java and SSL, since the JVM maintains it's own keystore (i.e. it doesn't use certs from the OS). </p>

<p>If you face this problem,  try reproducing this issue with other JVMs. Whenever customers have seen this issue in the past, it's been because their local <code>JVM keystore</code> (the keystore ships with the JVM and contains the most recent certs and CAs) has been out of date. Upgrading to the latest JVM version has always fixed this in the past.</p>

<p>Try upgrading your <code>JVM version</code> to recent one, it should help because your keystore must have been expired! :)</p>
",934796,2012-08-09T15:26:16.070,0,CC BY-SA 3.0,,1,ships,.,0,(the keystore ships with the JVM and contains the most recent certs and CAs) has been outdated.,False,False
32,11996761,2,10463360,2012-08-16T23:16:20.953,0,"<p>Was researching my own (now resolved problem), and found this question via <a href=""http://www.uploadify.com/forum/#/discussion/1416/uploading-to-amazon-s3/p1"" rel=""nofollow"">http://www.uploadify.com/forum/#/discussion/1416/uploading-to-amazon-s3/p1</a></p>

<p>That very thread got me running a little over a year ago, but I have recently started experiencing problems with Chrome 21/win.</p>

<p>Your code run through <a href=""http://jsbeautifier.org/"" rel=""nofollow"">http://jsbeautifier.org/</a></p>

<pre><code>$(document).ready(function () {
    $('#file').uploadify({
        ""formData"": {
            ""AWSAccessKeyId"": ""AKIAJOVJ2J3JX5Z6AIKA"",
            ""bucket"": ""my-stupid-bucket"",
            ""acl"": ""private"",
            ""key"": ""uploads/${filename}"",
            ""signature"": ""2I0HPQ2SoZOmhAUhYEb4nANFSCQ%253D"",
            ""policy"": ""[... snip ...]""
        },
        ""fileObjName"": ""file"",
        ""uploader"": ""http://my-stupid-bucket.amazonaws.com"",
        ""swf"": ""/static/uploadify/uploadify.swf""
    });
});
</code></pre>

<p>I just upgraded to uploadify 3.1 and ran into similar invalid signature problems.  The answer seems to be that the code in that thread is simply outdated.  </p>

<p>IIRC, that thread states you should encodeURIComponent the signature/etc.  This is no longer the case, it seems.  My 2.x code is working with 3.1 without encoding the signature and policy.  Guessing this was some workaround to an uploadify/flash problem that has since been resolved.</p>

<p>Additionally, it looks like your signature is double URL encoded.</p>

<p>TL;DR;<br>
Don't encodeURIComponent your form data with uploadify 3.1 (and possibly earlier versions).</p>
",670023,2012-08-16T23:16:20.953,0,CC BY-SA 3.0,,1,, ,0,The answer seems to be that the code in that thread is simply outdated.  ,False,False
33,12008576,2,11691480,2012-08-17T15:20:08.003,0,"<p>The short answer is: NO for both</p>

<p>The long answer is: </p>

<ul>
<li>Point 1) will never be possible</li>
<li>Point 2) can be done using the <strong>CONTAINS</strong> filter of the <strong>scan</strong> operation but... only for a single match. No ""OR"" stuff. However, scan is both <em>slow</em> and <em>expensive</em> and thus heavily discouraged. </li>
</ul>

<p>UPDATE_ITEM conditions will only allow exact matches.</p>
",650133,2012-08-17T15:20:08.003,0,CC BY-SA 3.0,,1,,.,0,> and thus heavily discouraged.,False,False
34,12190789,2,12190744,2012-08-30T06:06:02.407,3,"<p>As of the documentation found <a href=""http://developer.android.com/reference/android/os/NetworkOnMainThreadException.html"" rel=""nofollow"">here</a>. </p>

<blockquote>
  <p>NetworkOnMainThreadException is thrown when an application attempts to perform a
  networking operation on its main thread.</p>
  
  <p>This is only thrown for applications targeting the Honeycomb SDK or
  higher. Applications targeting earlier SDK versions are allowed to do
  networking on their main event loop threads, but it's heavily
  discouraged.</p>
</blockquote>

<p>The solution can be using a <a href=""http://developer.android.com/reference/android/os/AsyncTask.html"" rel=""nofollow"">AsyncTask</a> for Network related operations.</p>
",1116354,2012-08-30T06:06:02.407,2,CC BY-SA 3.0,,1,,discouraged.</p,0,"Applications targeting earlier SDK versions are allowed to do
  networking on their main event loop threads, but it's heavily
  discouraged.</p",False,False
35,12405620,2,12405433,2012-09-13T11:51:12.960,3,"<p>They dropped support for obsolete APIs recently, and the newest version requires a valid Associate Tag.</p>

<p><a href=""https://affiliate-program.amazon.com/gp/advertising/api/detail/api-changes.html"" rel=""nofollow"">https://affiliate-program.amazon.com/gp/advertising/api/detail/api-changes.html</a></p>

<blockquote>
  <p>Associate Tag Parameter: Every request made to the API should include a valid Associate Tag. Any request that does not contain a valid Associate Tag will be rejected with an appropriate error message. </p>
</blockquote>

<p>ASSOC_TAG must be your real tag (one that matches the API key).</p>
",989121,2012-09-13T11:51:12.960,5,CC BY-SA 3.0,,1,,Tag.</p,0,"They dropped support for obsolete APIs recently, and the newest version requires a valid Associate Tag.</p",False,False
36,12590624,2,12588748,2012-09-25T20:35:32.993,0,"<p>The Flume NG HDFS sink doesn't implement anything special for S3 support. Hadoop has some built-in support for S3, but I don't know of anyone actively working on it. From what I have heard, it is somewhat out of date and may have some durability issues under failure.</p>

<p>That said, I know of people using it because it's ""good enough"".</p>

<p>Are you saying that ""//xyz"" (with multiple adjacent slashes) is a valid path name on S3? As you probably know, most Unixes collapse adjacent slashes.</p>
",1220179,2012-09-25T20:35:32.993,3,CC BY-SA 3.0,,1,I,failure.</p,0,"From what I have heard, it is somewhat outdated and may have some durability issues under failure.</p",False,False
37,12887557,2,11228792,2012-10-14T23:32:19.840,-1,"<p>There's no way to force V1 (path-style) bucket addressing using the Java SDK.  The only exception is when your bucket name isn't DNS addressable, in which case the SDK will automatically use V1 addressing.  This happens, e.g., when your bucket name contains a period  (which is discouraged for this reason).</p>

<p>If you want this functionality, you'll have to modify the <code>AmazonS3Client</code> class to allow it.</p>

<p><a href=""https://github.com/amazonwebservices/aws-sdk-for-java/"" rel=""nofollow"">https://github.com/amazonwebservices/aws-sdk-for-java/</a></p>

<p>However, I'm not sure I believe your claim that you ""need"" to use V1 bucket addressing.  The SDK already handles all cases where V1 addressing is necessary -- or if you have found a case where it doesn't, please let us know in the forums.</p>

<p><a href=""https://forums.aws.amazon.com/forum.jspa?forumID=70"" rel=""nofollow"">https://forums.aws.amazon.com/forum.jspa?forumID=70</a></p>
",1733179,2012-10-14T23:32:19.840,2,CC BY-SA 3.0,,1,,reason).</p,0,"This happens, e.g., when your bucket name contains a period  (which is discouraged for this reason).</p",False,False
38,13030279,2,13016797,2012-10-23T12:20:27.320,9,"<p>I've given up on this, as I think it's a bug in php - in particular the mysql_connect code, which is now deprecated. It could probably be solved by compiling php yourself with changes to the source using steps similar to those mentioned in the bug report that @eggyal mentioned: <a href=""https://bugs.php.net/bug.php?id=54158"">https://bugs.php.net/bug.php?id=54158</a></p>

<p>Instead, I'm going to work around it by doing a system() call and using the mysql command line:</p>

<pre><code>$sql = ""LOAD DATA LOCAL INFILE '$csvPathAndFile' INTO TABLE $tableName FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\""' ESCAPED BY '\\\\\\\\' LINES TERMINATED BY '\\\\r\\\\n';"";
system(""mysql -u $dbUser -h $dbHost --password=$dbPass --local_infile=1 -e \""$sql\"" $dbName"");
</code></pre>

<p>That's working for me.</p>
",151503,2012-10-23T12:20:27.320,4,CC BY-SA 3.0,,1,,.,0,"I've given up on this, as I think it's a bug in php - in particular the mysql_connect code, which is now deprecated.",False,False
39,13357555,2,12345363,2012-11-13T08:30:10.960,0,"<p>I think this has been deprecated in favor of ""mon-put-metric-alarm""</p>
",251936,2012-11-13T08:30:10.960,0,CC BY-SA 3.0,,1,,"
",0,"I think this has been deprecated in favor of ""mon-put-metric-alarm""</p>
",False,False
40,13992926,2,13860361,2012-12-21T15:36:09.507,1,"<p>The cron log shows that cron is executing the command, but it is having no apparent effect. You know that the ec2-user can run the command from the shell. So first make sure that root can run this command successfully from the shell:</p>

<pre><code>sudo bash
/opt/aws/bin/ec2-create-snapshot --region us-east-1 -K /home/ec2-user/pk.pem -C /home/ec2-user/cert.pem -d ""vol-******** snapshot"" vol-********
</code></pre>

<p>If this doesn't work then the problem isn't with cron, it's with differences between the ec2-user and root accounts. I suppose that could be permissions based, but root is very powerful. It may be due to their different environments.</p>

<p>When you run the command as ec2-user you may have the AWS_ACCESS_KEY and AWS_SECRET_KEY environment variables set to valid values. In that case the ec2 command line tool may be using them and ignoring the -K and -C options. The credentials specified using -K and -C may not be valid but you wouldn't know that because they are being ignored. The root account has a different environment, presumably without valid values for those variables.</p>

<p>This might happen because the use of certificates (the -K and -C options) instead of access keys has been deprecated by Amazon. See <a href=""http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/SettingUp_CommandLine.html#set-aws-credentials"" rel=""nofollow"">http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/SettingUp_CommandLine.html#set-aws-credentials</a>.</p>

<p>You can try to test this theory by seeing if those variables are set when you run the command from the console:</p>

<pre><code>echo $AWS_ACCESS_KEY
echo $AWS_SECRET_KEY
</code></pre>

<p>If they show values, this is probably the reason for the difference.</p>

<p>Another thing to try is to replace the command with one with valid access keys instead of a certificate (the -O and -W parameters instead of -K and -C). You can create a user via IAM that can only do the operation you want, and use its credentials for the command.</p>
",1919054,2012-12-21T15:36:09.507,1,CC BY-SA 3.0,,1,,.,0,This might happen because the use of certificates (the -K and -C options) instead of access keys has been deprecated by Amazon.,False,False
41,14119015,2,12345363,2013-01-02T08:37:18.777,0,"<p>that command is now deprecated and replaced by 
auto scaling policies and cloudwatch alarms.</p>

<p>There is some documentation on how to do this here:
<a href=""http://docs.amazonwebservices.com/AutoScaling/latest/DeveloperGuide/US_SetUpASLBApp.html"" rel=""nofollow"">http://docs.amazonwebservices.com/AutoScaling/latest/DeveloperGuide/US_SetUpASLBApp.html</a></p>
",294465,2013-01-02T08:37:18.777,0,CC BY-SA 3.0,,1,command,alarms.</p,0,"that command is now deprecated and replaced by 
auto scaling policies and cloudwatch alarms.</p",False,False
42,14192909,2,14186477,2013-01-07T09:16:43.143,3,"<p>That's an 'interesting' (and worrisome) problem - section <em>DB Engine Version Management</em> within chapter <a href=""http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/RDSFAQ.MySQL.html"" rel=""nofollow"">MySQL Database Engine</a> in the <a href=""http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/RDSFAQ.html"" rel=""nofollow"">Amazon RDS Technical FAQ</a> seems to imply that restoring an unsupported MySQL snapshot might not be possible anymore in fact (even though it is nowhere stated explicitly):</p>

<blockquote>
  <p><strong>Does Amazon RDS provide guidelines for supporting new MySQL version releases and/or deprecating MySQL versions that are currently
  supported?</strong></p>
  
  <p>[...]</p>
  
  <p>We intend to support major MySQL version releases, including MySQL
  5.1, for 3 years after they are initially supported by Amazon RDS.</p>
  
  <p><strong>We intend to support minor MySQL version releases (e.g. MySQL 5.1.45)
  for at least 1 year</strong> after they are initially supported by Amazon RDS.</p>
  
  <p>After a MySQL major or minor version has been “deprecated”, <strong>we expect
  to provide a three month grace period for you to initiate an upgrade
  to a supported version prior to an automatic upgrade being applied</strong>
  during your scheduled maintenance window.</p>
  
  <p><em>[emphasis mine]</em></p>
</blockquote>

<p>According to <a href=""https://forums.aws.amazon.com/thread.jspa?messageID=250048&amp;#250048"" rel=""nofollow"">Impossible to create a RDS instance in EU-west</a>, MySQL 5.1.42 has been deprecated as of May 24, 2011 the latest already, so this <em>three month grace period</em> has long passed.</p>

<p>Obviously the apparent effect of this deprecation you encountered (i.e. the inability to restore respectively outdated snapshots) will come to a surprise for many, so there might be options to deal with it eventually still, but I'm afraid you'll need to contact AWS for a solution, either directly or via the <a href=""https://forums.aws.amazon.com/forum.jspa?forumID=60&amp;start=0"" rel=""nofollow"">Amazon Relational Database Service Forum</a> - please post your findings as an answer here if possible, insofar I'd expect this problem to show up regularly as time goes by.</p>
",45773,2013-01-07T09:16:43.143,1,CC BY-SA 3.0,,3,,applied</strong,0,"After a MySQL major or minor version has been “deprecated”, <strong>we expect
  to provide a three month grace period for you to initiate an upgrade
  to a supported version prior to an automatic upgrade being applied</strong",False,False
43,14199262,2,6963740,2013-01-07T15:47:49.850,0,"<p>Just create a snapshot of the volume you have your modified files contained and attach it your outdated instance after detaching the outdated volume.</p>
",1507259,2013-01-07T15:47:49.850,0,CC BY-SA 3.0,,1,,"
",0,"<p>Just create a snapshot of the volume you have your modified files contained and attach it your outdated instance after detaching the outdated volume.</p>
",False,False
44,14517372,2,14517323,2013-01-25T07:33:47.267,7,"<p>From <a href=""http://dev.mysql.com/doc/refman/5.5/en/server-system-variables.html#sysvar_table_type"" rel=""noreferrer"">the documentation</a>:</p>

<blockquote>
  <p>This variable was removed in MySQL 5.5.3. Use <a href=""http://dev.mysql.com/doc/refman/5.5/en/server-system-variables.html#sysvar_storage_engine"" rel=""noreferrer"">storage_engine</a> instead.</p>
</blockquote>

<p>Which in turn says:</p>

<blockquote>
  <p>This variable is deprecated as of MySQL 5.5.3. Use <a href=""http://dev.mysql.com/doc/refman/5.5/en/server-system-variables.html#sysvar_default_storage_engine"" rel=""noreferrer"">default_storage_engine</a>  instead.  </p>
</blockquote>

<p>Therefore you should use <code>SET default_storage_engine=InnoDB</code>, which FWIW is the default since MySQL 5.5.5.</p>
",44853,2013-01-25T07:33:47.267,0,CC BY-SA 3.0,,1,variable,.,0,This variable is deprecated as of MySQL 5.5.3.,False,False
45,14661530,2,14424116,2013-02-02T11:42:20.807,1,"<p>I am not sure for this, but I seem to have followed the same guide as you, and it must be outdated. The CarrierWave Uploader API seems to have changed. Now, uploaded images are public by default, which you can change via the config.fog_public configuration option. </p>

<p>Look <a href=""https://github.com/jnicklas/carrierwave#using-amazon-s3"" rel=""nofollow"" title=""CarrierWave Readme"">here</a> and <a href=""https://github.com/jnicklas/carrierwave/blob/master/lib/carrierwave/storage/fog.rb"" rel=""nofollow"" title=""CarierrWave fog reference"">here</a> for more info. </p>

<p>I ended up with only:</p>

<pre><code>...
:provider              =&gt; 'AWS',
:aws_access_key_id     =&gt; ENV['S3_KEY'],
:aws_secret_access_key =&gt; ENV['S3_SECRET'],
:region                =&gt; ENV['S3_REGION']
...
</code></pre>

<p>In my fog initializer. Nothing more was needed. </p>
",190833,2013-02-02T11:42:20.807,0,CC BY-SA 3.0,,1,,.,1,"I am not sure for this, but I seem to have followed the same guide as you, and it must be outdated.",False,False
46,14927018,2,14926906,2013-02-17T22:48:52.360,0,"<p>The blog post is a little bit outdated. In order to use the amazon-ec2 gem, you need to <code>require 'AWS'</code>, not <code>require 'EC2'</code>. This means you have to change the second line to:</p>

<pre><code>%w(optparse rubygems AWS resolv pp).each {|l| require l}
</code></pre>

<p>You will also have to change this line:</p>

<pre><code>EC2::Base.new(options).describe_instances # etc...
</code></pre>

<p>to</p>

<pre><code>AWS::EC2::Base.new(options).describe_instances # etc...
</code></pre>
",192702,2013-02-17T22:48:52.360,0,CC BY-SA 3.0,,1,post,.,0,>The blog post is a little bit outdated.,False,False
47,15006137,2,15003022,2013-02-21T15:34:59.530,4,"<p>From your question, it is not clear whether you want to:<br>
1-avoid sending messages to malformed email addresses; or<br>
2-avoid sending messages to email addresses which are not verified under your AWS account.</p>

<p>The answer for 1 is spread in different forms accross forums, SO, etc. You either do it simple, i.e., craft a short and clear regular expression which validates roughly 80% of the cases, or you use a very complex regular expression (in order to validate against the full compliance -- good luck, check <a href=""http://ex-parrot.com/~pdw/Mail-RFC822-Address.html"" rel=""nofollow"">this example</a>), check whether the domain is not only valid but also up and running, and, last but not least, check if the account is valid under that domain. Up to you. I'd go with a simple regex.</p>

<p>The answer for 2 is available at <a href=""http://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-email-addresses.html"" rel=""nofollow"">Verifying Email Addresses in Amazon SES</a> -- the Amazon SES API and SDKs support the operations below, so you should be covered in any case:</p>

<blockquote>
  <p>Using the Amazon SES API</p>
  
  <p>You can also manage verified email addresses with the Amazon SES API. The following actions are available:</p>
  
  <p>VerifyEmailIdentity<br>
  ListIdentities<br>
  DeleteIdentity<br>
  GetIdentityVerificationAttributes  </p>
  
  <p>Note<br>
  The API actions above are preferable to the following older API actions, which are deprecated as of the May 15, 2012 release of Domain Verification.  </p>
  
  <p>VerifyEmailAddress<br>
  ListVerifiedEmailAddresses<br>
  DeleteVerifiedEmailAddress  </p>
  
  <p>You can use these API actions to write a customized front-end application for email address verification. For a complete description of the API actions related to email verification, go to the Amazon Simple Email Service API Reference.</p>
</blockquote>
",347777,2013-02-21T15:34:59.530,0,CC BY-SA 3.0,,1,, ,0,"The API actions above are preferable to the following older API actions, which are deprecated as of the May 15, 2012 release of Domain Verification.  ",False,False
48,15305590,2,15304667,2013-03-09T00:18:10.560,5,"<p>In my opinion, it's an understandable trade-off DynamoDB made.  To be highly available and redundant, they need to replicate data.  To get super-low latency, they allowed inconsistent reads.  I'm not sure of their internal implementation, but I would guess that the higher this 64KB cap is, the longer your inconsistent reads might be out of date with the actual current state of the item.  And in a super low-latency system, milliseconds may matter.</p>

<p>This pushes the problem of an inconsistent Query returning chunk 1 and 2 (but not 3, yet) to the client-side.</p>

<p>As per question comments, if you want to store larger data, I recommend storing in S3 and referring to the S3 location from an attribute on an item in DynamoDB.</p>
",886749,2013-03-09T00:18:10.560,0,CC BY-SA 3.0,,1,, ,1,"I'm not sure of their internal implementation, but I would guess that the higher this 64KB cap is, the longer your inconsistent reads might be outdated with the actual current state of the item.  ",False,False
49,16226191,2,534774,2013-04-25T23:06:38.133,9,"<p>As of Mar 8, 2012, Amazon EC2 supports 64-bit AMIs across all instance types.</p>

<p>This makes the previous answers in this listing outdated as they assume different instance types require different architecture choices.</p>

<p>I recommend always using 64-bit AMIs so that you have the most flexibility in changing the instance type of an instance and your custom AMIs created from the instance will work on any other instance types.</p>

<p>I've written more about this here: <a href=""http://alestic.com/2012/03/ec2-64-bit"" rel=""noreferrer"">http://alestic.com/2012/03/ec2-64-bit</a></p>

<p>There are some good discussion points in the reader comments on that article for specialized exceptions where 32-bit might perform better than 64-bit, but remember that this restricts what instance types you can run on.</p>
",111286,2013-04-25T23:06:38.133,1,CC BY-SA 3.0,,1,,architecture,0,This makes the previous answers in this listing outdated as they assume different instance types require different architecture,False,False
50,16285688,2,16284028,2013-04-29T18:29:07.647,1,"<p>Unfortunately the dedicated page <a href=""http://wiki.apache.org/hadoop/AmazonEC2"" rel=""nofollow"">Running Hadoop on Amazon EC2</a> (which doesn't facilitate <code>HADOOP_HOME</code> indeed) turns out to be fairly out of date in itself and doesn't seem to apply to the most recent stable version anymore (1.0.4 at the time of this writing). I'm not aware of an updated 'native' tutorial, but apparently users are quite happy with an approach via <a href=""http://whirr.apache.org/"" rel=""nofollow"">Apache Whirr</a> (which incidentally <em>started out in 2007 as some bash scripts in <a href=""https://issues.apache.org/jira/browse/HADOOP-884"" rel=""nofollow"">Apache Hadoop</a> for running Hadoop clusters on EC2</em>).</p>

<p>Accordingly there is a <a href=""http://whirr.apache.org/docs/0.8.1/quick-start-guide.html"" rel=""nofollow"">Getting Started with Whirr™</a> available, in addition there are also related 3rd party tutorials, e.g.:</p>

<ul>
<li><a href=""http://chimpler.wordpress.com/2013/01/20/deploying-hadoop-on-ec2-with-whirr/"" rel=""nofollow"">Deploying Hadoop on EC2 with Whirr</a></li>
<li><a href=""http://blog.cloudera.com/blog/2012/10/set-up-a-hadoophbase-cluster-on-ec2-in-about-an-hour/"" rel=""nofollow"">How-to: Set Up an Apache Hadoop/Apache HBase Cluster on EC2 in (About) an Hour</a></li>
</ul>

<p>I hope you'll be able to merge the information in the book about using <a href=""http://hadoop.apache.org/"" rel=""nofollow"">Apache Hadoop</a> with these about running a Hadoop cluster via Apache Whirr - good luck!</p>
",45773,2013-04-29T18:29:07.647,2,CC BY-SA 3.0,,1,,.,1,indeed) turns out to be fairly outdated in itself and doesn't seem to apply to the most recent stable version anymore (1.0.4 at the time of this writing).,False,False
51,17008875,2,16874294,2013-06-09T10:52:25.377,5,"<p>First off, you should not (never) use the <a href=""http://php.net/manual/en/book.mysql.php"">mysql_</a>* extension and its function anymore. They are outdated, deprecated and will be removed from php in a near version. Use <a href=""http://php.net/manual/en/book.mysqli.php"">mysqli_</a>, or <a href=""http://php.net/manual/en/book.pdo.php"">PDO</a>. See <a href=""http://www.php.net/manual/en/function.mysql-connect.php"">the huge red warning at the top of the mysql_connect page in the php docs</a> for more info.</p>

<ul>
<li>Check that you connect to the right server</li>
</ul>

<p>Connect to your webserver in ssh, and use the local mysql client to connect to the database server as root (you will be asked the password for the root user):</p>

<pre><code>$ mysql -uroot -p -hXXXXXXX.XXXXXXXXXX.eu-west-1.rds.amazonaws.com
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 100530
Server version: 5.1.66-0+squeeze1 (Debian)

[...]

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql&gt;
</code></pre>

<p>If this work, you should end up at a mysql prompt. If this didn't work, your issue lies in your network configuration, your webserver cannot connect to the mysql server (check ports openings).</p>

<ul>
<li><p>Make sure the database in question exists on the server</p>

<pre><code>mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| [... db names ...] |
+--------------------+
X rows in set (0.00 sec)

mysql&gt;
</code></pre></li>
</ul>

<p>If you cannot see your database name in the list, this means the server your see in phpmyadmin is not the correct one; check your phpmyadmin settings.</p>

<ul>
<li><p>Check if you can connect with this user using the cli mysql client. Note that I do not specify a database on the connection line.</p>

<pre><code>// close the root connection
mysql&gt; quit
Bye
// connect using your specific user, replace USERNAME with your db user (and type its password when asked)
$ mysql -uUSERNAME -p
Enter password:

[...]

mysql&gt; 
</code></pre></li>
</ul>

<p>If this fails and you do not get to a prompt, this can be either because the permission does not exists, or because this user cannot connect from the host you specified. Look at the error message mysql will display; something like!</p>

<pre><code>ERROR 1045 (28000): Access denied for user 'USERNAME'@'HOST' (using password: NO)
</code></pre>

<p>What is in the HOST is important and you have to make sure it is allowed in your privileges. If everything seems ok and it is still not working, try editing the privileges for that user and set the HOST to '%' (means anything is allowed). If it works with that, it means the HOST is wrongly configured in mysql's privileges.</p>

<ul>
<li><p>Check if the user can see the database in question. When you connect with a specific user, ""show databases"" will only list databases this user has privileges to see.</p>

<pre><code>mysql &gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| [... db names ...] |
+--------------------+
X rows in set (0.00 sec)

mysql&gt;
</code></pre></li>
</ul>

<p>If you cannot see your database in the list, it means the privileges for the user are wrong / not set for this database.</p>

<p>If every steps here works fine, it probably means you have an error in your php code, double check the host, password, user name, ...</p>

<ul>
<li><p>If you really can't figure out what parts of the privileges are wrong, try creating a dummy user will all permissions from any host:</p>

<pre><code>$ mysql -uroot -p -hXXXXXXX.XXXXXXXXXX.eu-west-1.rds.amazonaws.com
Enter password:
mysql &gt; GRANT ALL ON yourdbname.* to 'testuser'@'%' IDENTIFIED BY 'dummypassword';
mysql &gt; quit
Bye
// note that I give the password in the prompt directly, to make sure this isn't an input error just in case
$ mysql -utestuser -pdummypassword
mysql &gt;
</code></pre></li>
</ul>

<p>Once you made this user and can connect to it, try to connect to it using your php code. Once this work, delete the user (using <code>DROP USER 'testuser'</code>), create it again with a more limited HOST privilege and try again, until you replicated the permissions you want. </p>

<p><strong>Remember to delete that user</strong> and you're done.</p>
",105768,2013-06-09T10:52:25.377,0,CC BY-SA 3.0,,1,They,.,0,"They are outdated, deprecated and will be removed from php in a near version.",False,False
52,17487502,2,13859906,2013-07-05T11:10:14.803,2,"<p>As of Riak 1.3, riak-admin reip is deprecated and the use of riak-admin cluster replace is the recomended way of replacing a cluster's name.</p>

<p>These are the commands I had to issue:</p>

<pre><code>riak stop # stop the node
riak-admin down riak@127.0.0.1 # take it down
sudo rm -rf /var/lib/riak/ring/* # delete the riak ring
sudo sed -i ""s/127.0.0.1/`hostname -i`/g"" /etc/riak/vm.args # Change the name in config
riak-admin cluster force-replace riak@127.0.0.1 riak@""`hostname -i`"" # replace the name
riak start # start the node
</code></pre>

<p>That should set the node's name to riak@[your EC2 internal IP address].</p>
",618756,2013-07-05T11:10:14.803,0,CC BY-SA 3.0,,1,reip,name.</p,0,"As of Riak 1.3, riak-admin reip is deprecated and the use of riak-admin cluster replace is the recomended way of replacing a cluster's name.</p",False,False
53,17657951,2,17537852,2013-07-15T15:27:50.363,1,"<p>If Amazon changes the queue URL in a breaking way it will not be immediate and will be deprecated slowly, and will take effect moving up a version (i.e. when you upgrade your SDK). </p>

<p>While the documentation doesn't guarantee it, Amazon knows that it would be a massively breaking change for thousands of customers.   </p>

<p>Furthermore, lots of customers use hard coded queue URLs which they get from the console, so those customers would not get the updated queue URL format either.   </p>

<p>In the end, you will be safe either way.  If you have LOTs of queues, then you will be better off formatting them yourself.  If you have a small number of queues, then it shouldn't make much difference either way.</p>
",175308,2013-07-15T15:27:50.363,0,CC BY-SA 3.0,,1,,.,1,">If Amazon changes the queue URL in a breaking way it will not be immediate and will be deprecated slowly, and will take effect moving up a version (i.e. when you upgrade your SDK).",False,False
54,17692217,2,17692132,2013-07-17T06:01:49.083,2,"<p>And immediately after posting this I found <a href=""https://bugzilla.novell.com/show_bug.cgi?id=597230"" rel=""nofollow"">this old bug</a> that lead me to the problem</p>

<p>The problem was from my including an old Mono.Security.dll that was a requirement for Thrift. By deleting that out of date dll I was able run without error</p>
",217415,2013-07-17T06:01:49.083,0,CC BY-SA 3.0,,1,,"
",0,"By deleting that outdated dll I was able run without error</p>
",False,False
55,17755189,2,17752183,2013-07-19T20:45:21.090,2,"<p>The documentation link you are pointing to is for V1 of the Amazon DynamoDB API, which is deprecated. The V2 version, which introduced local secondary indexes, is contained in the <code>com.amazonaws.services.dynamodbv2</code> package. Here is the V2 <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/model/ResourceInUseException.html"" rel=""nofollow"">documentation</a> for <code>ResourceNotFoundException</code>.</p>
",645112,2013-07-19T20:45:21.090,0,CC BY-SA 3.0,,1,,.,0,"The documentation link you are pointing to is for V1 of the Amazon DynamoDB API, which is deprecated.",False,False
56,17849197,2,17705840,2013-07-25T04:46:27.990,0,"<p>If you are using the older version of the api (in the java package <code>com.amazonaws.services.dynamodb.model</code>) then the syntax is a little different, and you have to use <code>withRangeKeyCondition</code>.  Here's the <code>QueryRequest</code> class with that older version, you can see how it's deprecated:
<a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodb/model/QueryRequest.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodb/model/QueryRequest.html</a></p>

<p>If you are using the newer version of the API (which you should, in the java package <code>com.amazonaws.services.dynamodbv2.model</code>, then you will see that the <code>QueryRequest</code> has the <code>withKeyConditions</code> method, as shown here:
<a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/model/QueryRequest.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/model/QueryRequest.html</a></p>
",886749,2013-07-25T04:46:27.990,0,CC BY-SA 3.0,,1,,"
",0,"> class with that older version, you can see how it's deprecated:
",False,False
57,17915920,2,17608326,2013-07-29T04:10:45.097,0,"<p>This is a bug because opswork was using Chef 9 (very outdated Chef).</p>

<p>Currently they have already upgraded to Chef 11.4, so you can try again, because my script in the question now is working.</p>
",696686,2013-07-29T04:10:45.097,0,CC BY-SA 3.0,,1,,Chef).</p,0,(very outdated Chef).</p,False,False
58,18097147,2,17963734,2013-08-07T07:26:14.617,0,"<p>and it is not only deprecated, it is not supported by Amazon.
To comply with Amazon requirements, please read this page:
<a href=""https://developer.amazon.com/sdk/fire/specifications.html#AppFeature"" rel=""nofollow"">https://developer.amazon.com/sdk/fire/specifications.html#AppFeature</a></p>
",1268593,2013-08-07T07:26:14.617,0,CC BY-SA 3.0,,1,,"
",1,"and it is not only deprecated, it is not supported by Amazon.
",False,False
59,19266376,2,19264018,2013-10-09T08:07:42.550,0,"<p>The WebSocket implementation you are using seems to implement the deprecated Hixie-76 version of the WebSocket protocol, which is no longer supported by Firefox (and Chrome and others).</p>

<p>I suggest having a look at <a href=""http://socketo.me/"" rel=""nofollow"">Ratchet</a>. This is up to date and well tested.</p>
",884770,2013-10-09T08:07:42.550,2,CC BY-SA 3.0,,1,,others).</p,1,"The WebSocket implementation you are using seems to implement the deprecated Hixie-76 version of the WebSocket protocol, which is no longer supported by Firefox (and Chrome and others).</p",False,False
60,19393344,2,19165680,2013-10-16T00:26:31.640,1,"<p>Use the CloudWatch alarms - as-create-or-update-trigger has been deprecated and only there for backwards compatibility.</p>

<p>From
<a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-trigger.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-trigger.html</a></p>

<blockquote>
  <p><strong>Important</strong> <br/>
  Triggers are a deprecated feature of Auto Scaling. We
  recommend that you switch from using triggers to using Auto Scaling
  policies and alarms. For more information, see Configuring Auto
  Scaling in the Auto Scaling Developer Guide.</p>
</blockquote>
",735268,2013-10-16T00:26:31.640,0,CC BY-SA 3.0,,2,,compatibility.</p,0,>Use the CloudWatch alarms - as-create-or-update-trigger has been deprecated and only there for backwards compatibility.</p,False,False
61,19691660,2,4488793,2013-10-30T19:04:08.483,1,"<p>You can follow the official documentation of setting up Amazon ec2 instance: <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-instance_linux.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-instance_linux.html</a></p>

<p>You should start with an AMI that you are familiar with. For example, if you use Ubuntu, you can just use one of the Ubuntu AMI in the recommended page. I didn't use the BitNami server and my Django site is deployed smoothly. </p>

<p>If you are using Apache server, just follow the instructions on the official Django doc:
<a href=""https://docs.djangoproject.com/en/1.5/howto/deployment/wsgi/modwsgi/"" rel=""nofollow"">https://docs.djangoproject.com/en/1.5/howto/deployment/wsgi/modwsgi/</a></p>

<p>I tried quite a few blogs but as you said, they are outdated. Just use the official docs and it will save you a lot of time. </p>
",2905013,2013-10-30T19:04:08.483,0,CC BY-SA 3.0,,1,they,.,0,"but as you said, they are outdated.",False,False
62,19859648,2,19855409,2013-11-08T12:55:10.050,0,"<p>You need to use HTTPS, not HTTP.</p>

<blockquote>
  <p>Note that SOAP requests, both authenticated and anonymous, must be sent to Amazon S3 using SSL. Amazon S3 returns an error when you send a SOAP request over HTTP.</p>
</blockquote>

<p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/APISoap.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonS3/latest/API/APISoap.html</a></p>

<p>Note also that the SOAP interface is deprecated.</p>
",1695906,2013-11-08T12:55:10.050,1,CC BY-SA 3.0,,1,,deprecated.</p,0,Note also that the SOAP interface is deprecated.</p,False,False
63,19865359,2,11066598,2013-11-08T17:45:07.987,1,"<p>From personal expirence I can say that most documentation on elastic beanstalk customization is outdated. What about using of custom ami, you need to know the actual version of the elastic beanstalk anyway installs beanstalk scripts and performs configuration on new instance bootstrap, so if you use custom ami it also happens. Base on this I would recommend to use custom ami when you need to have some OS level customizations. </p>

<p>If you need to install some additional software or change something I would recommend to use approach described here: <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html"" rel=""nofollow"">http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>

<p>Good example described here: 
<a href=""http://www.hudku.com/blog/innocuous-looking-evil-devil/"" rel=""nofollow"">http://www.hudku.com/blog/innocuous-looking-evil-devil/</a></p>

<p>Also you can find a lot of examples on github if you try to find keyword: .ebextentions</p>
",179111,2013-11-08T17:45:07.987,0,CC BY-SA 3.0,,1,,.,0,From personal expirence I can say that most documentation on elastic beanstalk customization is outdated.,False,False
64,20599958,2,13200258,2013-12-15T21:20:49.440,0,"<p>Could be because you have exceeded the maximum allowed number of versions (500).</p>

<p>Command to check the number of versions deployed:</p>

<pre><code>elastic-beanstalk-describe-application-versions |grep '| git-'|wc -l
</code></pre>

<p>If the number of versions is exceeded, you can either ask Amazon Web Services team to increase limits for your application, or just delete obsolete versions.</p>

<p>Hope it helps.</p>
",3079042,2013-12-15T21:20:49.440,0,CC BY-SA 3.0,,1,,versions.</p,0,">If the number of versions is exceeded, you can either ask Amazon Web Services team to increase limits for your application, or just delete obsolete versions.</p",False,False
65,21029597,2,21007947,2014-01-09T19:40:13.243,0,"<p>The code above was missing a call similar to </p>

<pre><code>DescribeJobFlowsResult describeJobFlowsResult =      client.describeJobFlows(describeJobFlowsRequest);
</code></pre>

<p>This got me a solution that works, but unfortunately Amazon deprecated the method but didn't provide an alternative.  I wish I had a non deprecated solution so this is only a partial answer.</p>
",2908865,2014-01-09T19:40:13.243,0,CC BY-SA 3.0,,2,, ,1,"This got me a solution that works, but unfortunately Amazon deprecated the method but didn't provide an alternative.  ",False,False
66,21087369,2,21083772,2014-01-13T09:10:23.250,5,"<p>The documentation quote you provide comes from the SQS tutorial.  The <a href=""http://docs.pythonboto.org/en/latest/ref/sqs.html#boto.sqs.queue.Queue.write_batch"" rel=""noreferrer"">SQS API docs</a> correctly describe the current return value.  The SQS tutorial is simply out of date and needs to be corrected.  I have created an <a href=""https://github.com/boto/boto/issues/1985"" rel=""noreferrer"">issue</a> to track this.</p>

<p>If the write fails for any reason, the service will return an HTTP error code which, in turn, will cause boto to raise an SQSError exception.  If no exception is raised, the write was successful.</p>
",653643,2014-01-13T09:10:23.250,2,CC BY-SA 3.0,,1,, ,0,The SQS tutorial is simply outdated and needs to be corrected.  ,False,False
67,21267306,2,18413238,2014-01-21T19:27:26.553,0,"<p>The AWS SDK was out of date.
Updating it solved the hole trouble :)
Thanks Deefour</p>
",800389,2014-01-21T19:27:26.553,0,CC BY-SA 3.0,,1,SDK,"
",0,"The AWS SDK was outdated.
",False,False
68,21285751,2,21283214,2014-01-22T14:33:17.343,1,"<p>The doc strings are out of date in boto.  I have submitted a pull request (<a href=""https://github.com/boto/boto/pull/2017"" rel=""nofollow"">https://github.com/boto/boto/pull/2017</a>) to fix that.  However, you can still specify a value of <code>application</code> for the <code>protocol</code> parameter.  Boto is not trying to validate that parameter value.  Make sure you supply the appropriate value for the <code>endpoint</code> parameter.</p>
",653643,2014-01-22T14:33:17.343,0,CC BY-SA 3.0,,1,strings, ,0,The doc strings are outdated in boto.  ,False,False
69,21706417,2,21690037,2014-02-11T15:45:39.100,3,"<p>v0.5.1 of dompdf did not yet implement SPL autoloading, it registered <code>__autoload()</code>. This outdated method of autoloading didn't work very well because it was difficult to register more than one autoload function at a time. Plus, when using writing PHP that <em>does</em> use SPL autoloading the v0.5.1 autoloader will not be called. PHP SPL autoloading disables calling <code>__autoload()</code> (<a href=""http://www.php.net/manual/en/function.spl-autoload-register.php"" rel=""nofollow"">spl_autoload_register</a>).</p>

<p>v0.6.0 (<a href=""https://github.com/dompdf/dompdf/releases"" rel=""nofollow"">just released</a>) uses SPL autoloading and should be compatible with AWS.phar. v0.6.0 is (for the most part) a drop-in replacement for v0.5.1 so you might want to try upgrading your copy of dompdf.</p>

<p>If that's not possible for whatever reason you could register the dompdf autoloader manually using SPL after including dompdf_config.inc.php. At a minimum you could try the following:</p>

<pre><code>&lt;?php
require_once('dompdf/dompdf_config.inc.php');
spl_autoload_register('DOMDPF_autoload');
// ...
?&gt;
</code></pre>

<p>See the <a href=""https://github.com/dompdf/dompdf/blob/v0.6.0/include/autoload.inc.php"" rel=""nofollow"">dompdf v0.6.0 autoload include</a> for a more complete example.</p>
",264628,2014-02-11T15:45:39.100,0,CC BY-SA 3.0,,1,,.,1,This outdated method of autoloading didn't work very well because it was difficult to register more than one autoload function at a time.,False,False
70,21715971,2,21714001,2014-02-12T00:09:34.940,0,"<p>The Hadoop implementation of S3 file system is out of date, so writing data to S3 from hive doesn't work well. We fix the issue with reading. Now DSE can read S3 files, but writing has issue. We will check it to see whether we could fix it soon</p>
",3268070,2014-02-12T00:09:34.940,1,CC BY-SA 3.0,,1,,.,1,"The Hadoop implementation of S3 file system is outdated, so writing data to S3 from hive doesn't work well.",False,True
71,21789195,2,21730183,2014-02-14T20:44:32.017,7,"<p>Per an Amazonian, this is not possible: <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=146102&amp;tstart=0"" rel=""noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=146102&amp;tstart=0</a></p>

<p>A workaround that worked for my use case, though, was to just specify a <code>RangeKeyCondition</code> greater than the last retrieved object's timestamp.  Here's the idea:</p>

<pre><code>Condition hashKeyCondition = new Condition();
hashKeyCondition.withComparisonOperator(ComparisonOperator.EQ).withAttributeValueList(new AttributeValue().withS(hashKeyAttributeValue));

Condition rangeKeyCondition = new Condition();
rangeKeyCondition.withComparisonOperator(ComparisonOperator.GT).withAttributeValueList(new AttributeValue().withN(timestamp.toString()));

Map&lt;String, Condition&gt; keyConditions = new HashMap&lt;String, Condition&gt;();
keyConditions.put(MappedItem.INDEXED_ATTRIBUTE_NAME, hashKeyCondition);
keyConditions.put(MappedItem.TIMESTAMP, rangeKeyCondition);


QueryRequest queryRequest = new QueryRequest();
queryRequest.withTableName(tableName);
queryRequest.withIndexName(MappedItem.INDEX_NAME);
queryRequest.withKeyConditions(keyConditions);

QueryResult result = amazonDynamoDBClient.query(queryRequest);

List&lt;MappedItem&gt; mappedItems = new ArrayList&lt;MappedItem&gt;();

for(Map&lt;String, AttributeValue&gt; item : result.getItems()) {
    MappedItem mappedItem = dynamoDBMapper.marshallIntoObject(MappedItem.class, item);
    mappedItems.add(mappedItem);
}

return mappedItems;
</code></pre>

<p>Note that the <code>marshallIntoObject</code> method is deprecated in favor of a protected method in the <code>DynamoDBMapper</code> class, but it's easy enough to write a marshaller were a future upgrade to break the mapping.</p>

<p>Not as elegant as using the mapper but it accomplishes the same thing.</p>
",1416224,2014-02-14T20:44:32.017,1,CC BY-SA 3.0,,1,,>,0,Note that the <code>marshallIntoObject</code> method is deprecated in favor of a protected method in the <code>,False,False
72,21935218,2,21933724,2014-02-21T13:01:53.433,1,"<p>I am not 100% sure, but after digging the <a href=""http://docs.aws.amazon.com/AWSAndroidSDK/latest/javadoc/"" rel=""nofollow"">documentation</a>, it seems like the classes that cannot be resolved are deprecated and are no longer included in the jar files...
They do still exist in the source code folder, but seeing that since they are deprecared, I guess there is no use trying to follow the tutorial...</p>
",2253918,2014-02-21T13:01:53.433,1,CC BY-SA 3.0,,1,,"
",1,"I am not 100% sure, but after digging the <a href=""http://docs.aws.amazon.com/AWSAndroidSDK/latest/javadoc/"" rel=""nofollow"">documentation</a>, it seems like the classes that cannot be resolved are deprecated and are no longer included in the jar files...
",False,False
73,21964644,2,21873561,2014-02-23T05:35:05.953,2,"<p>The crux to the solution is to not use the (apparently deprecated) <code>bodyParser()</code>. I'm not certain what it does, but it screws up the form's parts that <code>multiparty</code> uses. So instead, if you have the same problem I had, instead of using <code>bodyParser()</code>, use the things you need explicitely (for example):</p>

<pre><code>app.use(express.urlencoded());
app.use(express.json());
</code></pre>

<p>And then for your multipart stuff, just use multiparty to parse the body yourself. The author of multiparty gives <a href=""http://andrewkelley.me/post/do-not-use-bodyparser-with-express-js.html"" rel=""nofollow"">more info</a> on the subject.</p>
",971592,2014-02-23T05:35:05.953,0,CC BY-SA 3.0,,1,,),1,The crux to the solution is to not use the (apparently deprecated),False,False
74,22038122,2,11957811,2014-02-26T10:12:38.657,1,"<p>Note:</p>

<p>The trust_proxy_headers option is deprecated and will be removed in Symfony 2.3. </p>

<p>See <a href=""http://symfony.com/doc/2.0/reference/configuration/framework.html#trusted-proxies"" rel=""nofollow"">a trusted_proxies </a> and <a href=""http://symfony.com/doc/2.0/components/http_foundation/trusting_proxies.html"" rel=""nofollow"">a Trusting Proxies</a> for details on how to properly trust proxy data.</p>
",3228106,2014-02-26T10:12:38.657,0,CC BY-SA 3.0,,1,option,.,0,The trust_proxy_headers option is deprecated and will be removed in Symfony 2.3.,False,False
75,22153493,2,22152830,2014-03-03T17:39:08.880,2,"<p>Yes, you will have to update the paths in your HTML to point to CDN. Typically if you have a deployment/build process this link changing can be done at that time (so that development time can use the local files).</p>

<p>Another important thing to also handle here is the versioning the CSS/JS etc. You might make frequent changes to your CSS/JS. When you make any change typically CDNs take 24 hrs to reflect. (Another option is invalidating files on CDN, this is but charged explicitly and is discouraged).  The suggested method is to generate a path like ""media.example.me/<strong>XYZ</strong>/stylesheets/screen.css"", and change this XYZ to a different number for each deployment (time stamp /epoch will do). This way with each deployment, you need to invalidate only the HTML and other files are any way a new path and will load fresh. This technique is generally called finger-printing the URLs.</p>
",2959100,2014-03-03T17:39:08.880,2,CC BY-SA 3.0,,1,, ,0,"(Another option is invalidating files on CDN, this is but charged explicitly and is discouraged).  ",False,False
76,22640761,2,22614106,2014-03-25T16:30:03.680,0,"<p>I actually had two problems. The first was that my bundler was outdated. I fixed that by running</p>

<pre><code>gem update bundler
</code></pre>

<p>and the second problem was that I needed to bundle install as the root user since bundler was trying to install gems in a root owned location. </p>

<p>Hope this helps someone in the future.</p>
",3224822,2014-03-25T16:30:03.680,0,CC BY-SA 3.0,,1,,.,0,The first was that my bundler was outdated.,False,False
77,22667220,2,22666177,2014-03-26T16:23:56.330,0,"<ol>
<li><p>You really, really should not use mysql_real_escape_string() or mysql_query() or any of the mysql functions. They are deprecated, insecure, and will be removed from PHP in the future, breaking your entire app. Please save yourself and your users a lot of headache by using <a href=""http://us2.php.net/mysqli"" rel=""nofollow"">mysqli</a> or <a href=""http://www.php.net/manual/en/book.pdo.php"" rel=""nofollow"">PDO</a> instead.</p></li>
<li><p>Use a foreach loop to iterate over the array. </p>

<pre><code>foreach ($instanceIds as $id){
    $sql = ""INSERT INTO server_tbl (userName, serverName, serverId, isRunning) VALUES ('$name', 'runtest', '$id', 'X')"";

    //do your insert here with the above SQL statement, depending on whether you use mysqli or PDO
</code></pre></li>
</ol>
",3169502,2014-03-26T16:23:56.330,0,CC BY-SA 3.0,,1,They,.,0,"They are deprecated, insecure, and will be removed from PHP in the future, breaking your entire app.",False,False
78,22902003,2,22899486,2014-04-07T00:58:39.563,0,"<p>I looks like your <code>update</code> endpoints might be out of date for the packages that you are installing when you are bootstrapping the instance. I would check that your <code>[tomcat]</code> recipe has the following command (as sudo):</p>

<pre><code>apt-get update
</code></pre>

<p>I would also make sure that your ubuntu user in your AMI has sudo passwordless access for installation (this is the default for Amazon ubuntu AMIs)</p>
",2989261,2014-04-07T00:58:39.563,0,CC BY-SA 3.0,,1,endpoints,.,0,> endpoints might be outdated for the packages that you are installing when you are bootstrapping the instance.,False,False
79,22902499,2,22900102,2014-04-07T02:08:04.157,-1,"<p>FTP is an obsolete and insecure protocol, you should not use it. Servers should only run SSH, and you can use SFTP or SCP to transfer files.</p>

<p>For S3, there are many tools (<a href=""http://s3tools.org/s3cmd"" rel=""nofollow"">Python</a>, <a href=""http://s3sync.net/wiki.html"" rel=""nofollow"">Ruby</a>, etc) that work well.  You could also use <code>fuse</code> to ""mount"" S3 like a filesystem, but I don't recommend these because when they have problems, they are hard to troubleshoot.</p>

<p>For GDrive, there are several (I tried out one a while back and it worked, but I don't remember which one.)</p>
",1832301,2014-04-07T02:08:04.157,1,CC BY-SA 3.0,,1,,.,1,"FTP is an obsolete and insecure protocol, you should not use it.",False,False
80,23017359,2,23016500,2014-04-11T15:59:26.683,0,"<p>I was able to solve the issue by opening up the postgresql adapter for Activerecord and commenting out all of the SET commands.</p>

<p>However, this is not a best practice at all. At the end of editing the postgresql adapter I was faced with errors due to an outdated postgres version (80002).</p>

<p>The right answer is just updating postgres, which unfortunately isn't possible amongst my shared Redshift DB that lives at Amazon.</p>
",2665588,2014-04-11T15:59:26.683,1,CC BY-SA 3.0,,1,,"

",0,"At the end of editing the postgresql adapter I was faced with errors due to an outdated postgres version (80002).</p>

",False,False
81,23056579,2,23054668,2014-04-14T09:30:45.377,0,"<p>Strike that, reverse. Nobody should <em>ever</em> still be using the <code>__autoload()</code> function. This was deprecated many years ago in favor of the stackable <code>spl_autoload_register()</code> function.</p>

<p>From the code snippet you posted, FlourishLib is doing the <em>wrong</em> thing by having a function called <code>__autoload()</code> at all.</p>
",228514,2014-04-14T09:30:45.377,1,CC BY-SA 3.0,,1,This,function.</p,0,This was deprecated many years ago in favor of the stackable <code>spl_autoload_register()</code> function.</p,False,False
82,23289372,2,23282666,2014-04-25T09:36:00.450,1,"<p>There is some outdated blog posts so first time installs inevitably end up getting terminated.. As Jeff mentioned, shiny will be looking for apps in /srv. Try this: <code>sudo cp -R /usr/local/lib/R/site-library/shiny/examples/01_hello /srv/shiny-server/</code> Also, may be useful, for Ubuntu 12.04 did you sort out the sources.list file before installing R? Can copy paste the below in Terminal.</p>

<pre><code>########################################
# UBUNTU CONFIG 
######################################## 
# useful: http://withr.me/blog/2013/07/23/configure-shiny-server-under-ubuntu/
## Unlike other Linux, need to follow the following steps to install R on Ubuntu:
gpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9
gpg -a --export E084DAB9 | sudo apt-key add -
## Open sources.list, and add the repository deb http://&lt;myFAVcran&gt;/linux/ubuntu precise/ to its end, 
deb http://cran.csiro.au/bin/linux/ubuntu precise/
## Where precise means the R version for Ubuntu 12.04. 
## Note, if you use ‘nano’ as the editor, press Ctrl+X to exit nano editor and press Y to save the change, then press Enter.
sudo nano /etc/apt/sources.list
## Update Ubuntu, and install R.
sudo apt-get update
sudo apt-get install r-base r-base-dev
</code></pre>
",1897926,2014-04-25T09:36:00.450,0,CC BY-SA 3.0,,1,,posts,0,There is some outdated blog posts,False,False
83,23371813,2,20544518,2014-04-29T17:51:15.933,11,"<p>There is a lack of clear information in this area but we can make some pretty strong inferences.  Many people assume that DynamoDB implements all of the ideas from its predecessor ""Dynamo"", but that doesn't seem to be the case and it is important to keep the two separated in your mind.  The original Dynamo system was carefully described by Amazon in the <a href=""http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html"">Dynamo Paper</a>. In thinking about these, it is also helpful if you are familiar with the distributed databases based on the Dynamo ideas, like Riak and Cassandra.  In particular, <a href=""http://cassandra.apache.org/"">Apache Cassandra</a> which provides a full range of trade-offs with respect to CAP.</p>

<p>By comparing DynamoDB which is clearly distributed to the options available in Cassandra I think we can see where it is placed in the CAP space.  According to Amazon ""DynamoDB maintains multiple copies of each item to ensure durability. When you receive an 'operation successful' response to your write request, DynamoDB ensures that the write is durable on multiple servers. However, it takes time for the update to propagate to all copies."" (<a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/APISummary.html"">Data Read and Consistency Considerations</a>).  Also, DynamoDB does not require the application to do conflict resolution the way Dynamo does. Assuming they want to provide as much availability as possible, since they say they are writing to multiple servers, writes in DyanmoDB are equivalent to Cassandra <code>QUORUM</code> level.  Also, it would seem DynamoDB does not support <a href=""http://wiki.apache.org/cassandra/HintedHandoff"">hinted handoff</a>, because that can lead to situations requiring conflict resolution.  For maximum availability, an inconsistent read would only have to be at the equivalent of Cassandras's <code>ONE</code> level.  However, to get a consistent read given the quorum writes would require a <code>QUORUM</code> level read (following the R + W > N for consistency). For more information on levels in Cassandra see <a href=""http://www.datastax.com/docs/1.1/dml/data_consistency"">About Data Consistency in Cassandra</a>.</p>

<p>In summary, I conclude that:</p>

<ul>
<li>Writes are ""Quorum"", so a majority of the nodes the row is replicated to must be available for the write to succeed</li>
<li>Inconsistent Reads are ""One"", so only a single node with the row need be available, but the data returned may be out of date</li>
<li>Consistent Reads are ""Quorum"", so a majority of the nodes the row is replicated to must be available for the read to succeed</li>
</ul>

<p>So writes have the same availability as a consistent read.</p>

<p>To specifically address your question about two simultaneous conditional writes, one or both will fail depending on how many nodes are down.  However, there will never be an inconsistency.  The availability of the writes really has nothing to do with whether they are conditional or not I think.</p>
",268898,2014-04-29T17:51:15.933,0,CC BY-SA 3.0,,1,,outdated</li,0,"Inconsistent Reads are ""One"", so only a single node with the row need be available, but the data returned may be outdated</li",False,True
84,23560825,2,20851872,2014-05-09T09:15:26.910,0,"<p>Try setting the content length field and locally declaring the S3 client:</p>

<pre><code>AmazonS3Client *s3 = [[AmazonS3Client alloc] initWithAccessKey:ACCESS_KEY_ID withSecretKey:SECRET_KEY]; //auto release deprecated, and not necessary with local declaration
s3.endpoint = [AmazonEndpoints s3Endpoint:US_WEST_2];

//post full video
por = [[S3PutObjectRequest alloc] initWithKey:generatedString inBucket:@""dormpicbucket""];
por.contentType = @""video/mp4"";
por.data = capturedMovie;
por.delegate = self;
[por setDelegate:self];
por.contentLength = [capturedMovie length];
[s3 putObject:por];
</code></pre>

<p>Also make sure to debug this: <code>[Constants pictureBucket]</code> because it looks like youre using the sample/tutorial code from aws docs almost explicitly lol.</p>
",2584565,2014-05-09T09:15:26.910,0,CC BY-SA 3.0,,1,,=,1,":SECRET_KEY]; //auto release deprecated, and not necessary with local declaration
s3.endpoint =",False,False
85,23637593,2,23619714,2014-05-13T17:10:39.017,0,"<p>You're running into two problems here: </p>

<ol>
<li>Due to <a href=""http://en.wikipedia.org/wiki/Same-origin_policy"" rel=""nofollow"">same origin policy</a>, the <code>&lt;iframe&gt;</code> in the resulting page is not going to display an arbitrary URL.</li>
<li>Even if it could display an arbitrary URL, that URL would have to be configured for SSL.</li>
</ol>

<p>So, you'll be able to load this (another amazon URL using SSL):</p>

<p><a href=""https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=https%3A%2F%2Fwww.mturk.com"" rel=""nofollow"">https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=https%3A%2F%2Fwww.mturk.com</a></p>

<p>But not this (amazon URL w/o SSL):</p>

<p><a href=""https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=http%3A%2F%2Fwww.mturk.com"" rel=""nofollow"">https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=http%3A%2F%2Fwww.mturk.com</a></p>

<p>And not an arbitrary URL (even with SSL):</p>

<p><a href=""https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=https%3A%2F%2Fwww.google.com"" rel=""nofollow"">https://s3.amazonaws.com/MTurk_test/externalpage.htm?url=https%3A%2F%2Fwww.google.com</a></p>

<p>So, my guess is that this template is horribly outdated and used to work at some point in the past but is not compliant with modern web browser technology.</p>

<p>Best solution is just to provide a link for the worker to click to visit the URL.</p>
",2338862,2014-05-13T17:10:39.017,1,CC BY-SA 3.0,,1,,browser,1,"So, my guess is that this template is horribly outdated and used to work at some point in the past but is not compliant with modern web browser",False,True
86,23766244,2,23748065,2014-05-20T17:22:31.933,3,"<p>It's a long time since I configured a squid but as far as I remember the directive <code>http_access allow all</code> in line 4 of your configuration makes the 3 lines at the very bottom obsolete… You might try to remove that one.</p>
",1741281,2014-05-20T17:22:31.933,4,CC BY-SA 3.0,,1,,"
",0,"in line 4 of your configuration makes the 3 lines at the very bottom obsolete… You might try to remove that one.</p>
",False,False
87,23985147,2,23830383,2014-06-01T22:26:13.303,2,"<p>I believe you need to use <code>verifyEmailIdentity</code> <em>not</em> <code>verifyEmailAddress</code>:</p>

<pre><code>$result = $sesClient-&gt;verifyEmailIdentity(array('EmailAddress'=&gt; $email));
</code></pre>

<p>As stated in the AWS documentation:</p>

<blockquote>
  <p>The VerifyEmailAddress action is deprecated as of the May
  15, 2012 release of Domain Verification. The VerifyEmailIdentity
  action is now preferred.</p>
</blockquote>

<p>• <a href=""http://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.Ses.SesClient.html#_verifyEmailIdentity"" rel=""nofollow""><strong>Further Reading</strong></a></p>
",499581,2014-06-01T22:26:13.303,2,CC BY-SA 3.0,,1,action,.,0,"The VerifyEmailAddress action is deprecated as of the May
  15, 2012 release of Domain Verification.",False,False
88,24030938,2,24014493,2014-06-04T06:55:26.103,40,"<p>i found a way to do that, resize2fs not working in case not sure why but it says device or resource busy. i found a very good article on <a href=""https://forums.aws.amazon.com/thread.jspa?messageID=547507"">resizedisk</a> using fdisk we can increase block size by deleting and creating it and Make the partition bootable. all it requires is a reboot. it wont effect your data if you use same start cylinder. </p>

<pre><code># df -h  &lt;&lt;1&gt;&gt;

Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1      6.0G  2.0G  3.7G  35% / 
tmpfs            15G     0   15G   0% /dev/shm

# fdisk -l  &lt;&lt;2&gt;&gt;

Disk /dev/xvda: 21.5 GB, 21474836480 bytes
97 heads, 17 sectors/track, 25435 cylinders
Units = cylinders of 1649 * 512 = 844288 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0003b587

    Device Boot      Start         End      Blocks   Id  System
/dev/xvda1   *           2        7632     6291456   83  Linux

# fdisk /dev/xvda  &lt;&lt;3&gt;&gt;

WARNING: DOS-compatible mode is deprecated. It's strongly recommended to
         switch off the mode (command 'c') and change display units to
         sectors (command 'u').

Command (m for help): u  &lt;&lt;4&gt;&gt;
Changing display/entry units to sectors

Command (m for help): p  &lt;&lt;5&gt;&gt;

Disk /dev/xvda: 21.5 GB, 21474836480 bytes
97 heads, 17 sectors/track, 25435 cylinders, total 41943040 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0003b587

    Device Boot      Start         End      Blocks   Id  System
/dev/xvda1   *        2048    12584959     6291456   83  Linux

Command (m for help): d  &lt;&lt;6&gt;&gt;
Selected partition 1

Command (m for help): n  &lt;&lt;7&gt;&gt;
Command action
   e   extended
   p   primary partition (1-4)
p  &lt;&lt;8&gt;&gt;
Partition number (1-4): 1  &lt;&lt;9&gt;&gt;
First sector (17-41943039, default 17): 2048  &lt;&lt;10&gt;&gt;
Last sector, +sectors or +size{K,M,G} (2048-41943039, default 41943039): &lt;&lt;11&gt;&gt;
Using default value 41943039

Command (m for help): p &lt;&lt;12&gt;&gt;

Disk /dev/xvda: 21.5 GB, 21474836480 bytes
97 heads, 17 sectors/track, 25435 cylinders, total 41943040 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0003b587

    Device Boot      Start         End      Blocks   Id  System
/dev/xvda1            2048    41943039    20970496   83  Linux

Command (m for help): a  &lt;&lt;13&gt;&gt;
Partition number (1-4): 1  &lt;&lt;14&gt;&gt;


Command (m for help): w  &lt;&lt;15&gt;&gt;
The partition table has been altered!

Calling ioctl() to re-read partition table.

WARNING: Re-reading the partition table failed with error 16: Device or resource busy.
The kernel still uses the old table. The new table will be used at
the next reboot or after you run partprobe(8) or kpartx(8)
Syncing disks.

# reboot  &lt;&lt;16&gt;&gt;

&lt;wait&gt;

# df -h  &lt;&lt;17&gt;&gt;
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1       20G  2.0G   17G  11% / 
tmpfs            15G     0   15G   0% /dev/shm

# resize2fs /dev/xvda1  &lt;&lt;18&gt;&gt;
resize2fs 1.41.12 (17-May-2010)
The filesystem is already 5242624 blocks long.  Nothing to do!
</code></pre>
",2172800,2014-06-04T06:55:26.103,5,CC BY-SA 3.0,,1,,.,0,WARNING: DOS-compatible mode is deprecated.,False,False
89,24148411,2,24146492,2014-06-10T18:31:54.817,0,"<p>I figured it out.. no thanks to amazon's own documentatio which is scattered and out of date..</p>

<p>you can set the permission to public read like so:</p>

<pre><code>PutObjectRequest por = new PutObjectRequest(
                        Constants.getPictureBucket(), Constants.PICTURE_NAME,
                        resolver.openInputStream(selectedImage),metadata);
                por.setCannedAcl(CannedAccessControlList.PublicRead);
</code></pre>

<p>then you end up getting a normal url: </p>

<blockquote>
  <p><a href=""https://s3-us-west-2.amazonaws.com/%5bbucket%5d/%5bNameOfThePicture%5d"" rel=""nofollow"">https://s3-us-west-2.amazonaws.com/[bucket]/[NameOfThePicture]</a></p>
</blockquote>
",1197638,2014-06-10T18:31:54.817,0,CC BY-SA 3.0,,1,,</p,0,no thanks to amazon's own documentatio which is scattered and outdated..</p,False,False
90,24208664,2,24208179,2014-06-13T15:12:46.647,0,"<p>Have you run <code>bundle update</code> and is <code>Gemfile.lock</code> in your <code>.gitignore</code> (it shouldn't be). When you set your gems the specific configuration you use gets added to your <code>Gemfile.lock</code> so that you have a consistent deployment and gem versions don't change. The error there is saying that your <code>Gemfile.lock</code> is outdated see <a href=""http://ryanbigg.com/2011/01/why-you-should-run-bundle-update/"" rel=""nofollow"">here for more details</a>.</p>
",966097,2014-06-13T15:12:46.647,0,CC BY-SA 3.0,,1,,details</a>.</p,0,"The error there is saying that your <code>Gemfile.lock</code> is outdated see <a href=""http://ryanbigg.com/2011/01/why-you-should-run-bundle-update/"" rel=""nofollow"">here for more details</a>.</p",False,False
91,24741171,2,24728634,2014-07-14T16:22:56.950,14,"<p>In terms of load, they have the same goal, but they differ in other areas:</p>

<p>Up-to-dateness of data:</p>

<ul>
<li>A read replica will continuously sync from the master. So your results will probably lag 0 - 3s (depending on the load) behind the master.</li>
<li>A cache takes the query result at a specific point in time and stores it for a certain amount of time. The longer your queries are being cached, the more lag you'll have; but your master database will experience less load. It's a trade-off you'll need to choose wisely depending on your application.</li>
</ul>

<p>Performance / query features:</p>

<ul>
<li>A cache can only return results for queries it has already seen. So if you run the same queries over and over again, it's a good match. Note that queries must not contain changing parts like <code>NOW()</code>, but must be equal in terms of the actual data to be fetched.</li>
<li>If you have many different, frequently changing, or dynamic (<code>NOW()</code>,...) queries, a read replica will be a better match.</li>
<li>ElastiCache should be much faster, since it's returning values directly from RAM. However, this also limits the number of results you can store.</li>
</ul>

<p>So you'll first need to evaluate how outdated your data can be and how cacheable your queries are. If you're using ElastiCache, you might be able to cache more than queries — like caching whole sections of a website instead of the underlying queries only, which should improve the overall load of your application.</p>

<p>PS: Have you tuned your indexes? If your main problems are writes that won't help. But if you are fighting reads, indexes are the #1 thing to check and they do make a huge difference.</p>
",573153,2014-07-14T16:22:56.950,4,CC BY-SA 3.0,,1,you,.,0,So you'll first need to evaluate how outdated your data can be and how cacheable your queries are.,False,True
92,25098266,2,16912354,2014-08-02T18:41:33.267,0,"<p>I solved this by updating the fog gem by running:</p>

<pre><code>bundle update fog
</code></pre>

<p>Aparently I was very outdated running fog 1.9.0. Now im running 1.23.0 and got to successfully deploy to EC2 by following Rubber's Railscasts (<a href=""http://railscasts.com/episodes/347-rubber-and-amazon-ec2"" rel=""nofollow"">http://railscasts.com/episodes/347-rubber-and-amazon-ec2</a>)</p>

<p>This also updated the following gems to these versions:</p>

<ul>
<li>fog-core 1.23.0 </li>
<li>net-scp 1.2.1 </li>
<li>fog-json 1.0.0</li>
<li>List item</li>
<li>inflecto 0.0.2</li>
<li>fog-brightbox 0.1.1</li>
<li>fog-softlayer 0.3.11</li>
<li>ipaddress 0.8.0</li>
<li>mini_portile 0.6.0</li>
</ul>

<p>Nokogiri depends on libxml2-2.9.0 library so be sure to have that installed</p>

<p>Hope it helps!!</p>
",382279,2014-08-02T18:41:33.267,0,CC BY-SA 3.0,,1,I,.,0,Aparently I was very outdated running fog 1.9.0.,False,False
93,25273598,2,25270372,2014-08-12T20:34:01.320,13,"<p>This will be a huge undertaking. That is not good or bad, just a statement : )</p>

<p>There are a lot of technologies you are mentioning and each one will have a bit of a learning curve. Having just watched American Ninja Warrior last night, you're learning curve is sounding a lot like The Warped Wall! Don't be discouraged, prepare to start small, and you'll be OK.</p>

<p>The three big pieces you'll have to cover are:</p>

<ul>
<li>NodeJS</li>
<li>MongoDB</li>
<li>AngularJS</li>
</ul>

<p><strong>NodeJS</strong></p>

<p>There are many tutorials online and you will need to learn Javascript and what that means on a NodeJS server. The asynchronous nature of Javascript will look very different from Java (I know, that was my transition as well). 
A tutorial I like: <a href=""http://book.mixu.net/node/index.html"">Mixu's Node Book</a>
An eBook I like: <a href=""http://www.nodebeginner.org/"">Node Beginner</a></p>

<p><strong>MongoDB</strong></p>

<p>I'm still working on this myself, but MongoDB is different than a relational SQL database. You will need to think a little differently here as well. Plenty of tutorials out there for MongoDB.</p>

<p>However, I will say it is possible to combine NodeJS with SQL. If you're doing this to learn, you can turn all the knobs at once. If you want to see something working, you can just use the DB as your normally would. I like using <a href=""http://knexjs.org"">knexJS</a> when working with a SQL database. It's awesome.</p>

<p><strong>AngularJS</strong></p>

<p>Angular is a huge framework. People love it. Easy to use once you know it. I've read its easy to get started and then more difficult to master. Lots of paths to try and tackle this one, here's a link I've book marked: <a href=""http://joelhooks.com/blog/2013/08/03/learn-angularjs-in-a-weekend/"">Learn AngularJS in a Weekend</a></p>

<p>That said, you wouldn't need to use Angular in you app. Some framework might support it, and base their NodeJS backend around an Angular frontend, but you <em>could</em> use your existing frontend skills (even, <em>gasp!</em>, jQuery) to make your web pages drive your backend server.</p>

<p><strong>My thoughts on other questions you've asked</strong></p>

<ul>
<li>Is it crazy to pretend I'll tackle in a different way the engine part and the administrative part?</li>
</ul>

<p>You could separate the two, but if this is just for learning, do everything in one. MongoDB can certainly handle your administrative stuff.</p>

<ul>
<li>Could the MEAN stack also deal with all the administrative/login features?</li>
</ul>

<p>Yep. Some frameworks have those as default packages as well. </p>

<ul>
<li>Is it crazy to pretend that I can get to learn MEAN stack with (at the moment) a limited knowledge of Javascript? The point of course is to learn it... I know there will be quiet a learning curve, but I'm ready for it.</li>
</ul>

<p>It is not crazy, but the learning curve is quite large. Having an app/goal in mind will be good drive for you. Also, don't try to do everything perfectly the first time. Find a NPM package that looks pretty good, use it, and move on. Don't worry about all the details right now. Do that on your next app, or once you have a functional baseline.</p>

<ul>
<li>Is there an easy way to integrate Node.js with Java code? has anyone tried it? For example I would like to take advantage of non-blocking capabiltiies of Node.js, but call Java method to access the third-party API for example.</li>
</ul>

<p>Sure, a Java client can call to to a NodeJS server. If you created a NodeJS API server, you could write a Java Client to test it out. You can even do <a href=""https://www.npmjs.org/search?q=java"">other crazy things from NodeJS to Java code</a>, but I would suggest leaving Java behind for now. For a lot of the web stuff (HTTP requests and the like), I think you'll be pleasantly surprised how fast it is to do something in NodeJS that what you would need to do for an equivalent in Java.</p>
",3018068,2014-08-12T20:34:01.320,2,CC BY-SA 3.0,,1,,OK.</p,1,"Don't be discouraged, prepare to start small, and you'll be OK.</p",False,False
94,25546904,2,23090900,2014-08-28T10:45:54.440,3,"<p>Oh darn. I think I found the reason for this behavior. After facing this issue, I made sure that each token was only uploaded <em>once</em> to AWS SNS. When testing this, I realized that nevertheless I ended up with multiple endpoints with the <em>same</em> token - huh???
It turned out that these duplicated tokens resulted from <em>outdated</em> tokens being uploaded to AWS SNS. After creating an endpoint using an outdated token, SNS would automagically <em>revive</em> the endpoint by updating it with the <em>current</em> device token (which afaik is delivered back from GCM as a canonical ID once you try to send push messages to outdated tokens).</p>

<p>So e.g. uploading these (made-up) tokens and custom data</p>

<pre><code>APA9...YFDw, {original_token: APA9...YFDw}
APA9...XaSd, {original_token: APA9...XaSd} &lt;-- Assume this token is outdated
APA9...sVQa, {original_token: APA9...sVQa}
</code></pre>

<p>might result in something like this - i.e. different endpoints with identical tokens:</p>

<pre><code>APA9...YFDw, {original_token: APA9...YFDw}, arn:aws:sns:eu-west-1:4711:endpoint/GCM/myapp/daf64...5c204
APA9...YFDw, {original_token: APA9...XaSd}, arn:aws:sns:eu-west-1:4711:endpoint/GCM/myapp/a980f...e3c82 &lt;-- Duplicate token!
APA9...sVQa, {original_token: APA9...sVQa}, arn:aws:sns:eu-west-1:4711:endpoint/GCM/myapp/14777...7d9ff
</code></pre>

<p>This scenario in turn seems to lead to above error on subsequent attempts to create endpoints using outdated tokens. On the hand, it seems correct that subsequent requests fail. On the other hand, intuitively I have the gut-feeling that the duplication of tokens that is taking place seems wrong, or at least difficult to handle. Maybe once SNS discovers that a token is outdated and needs to be changed, it could first check if there is already another endpoint existent with the same token...</p>

<p>I will research on this a bit more and see if I can find a way to handle this properly.</p>

<p>Cheers</p>
",342875,2014-08-28T10:45:54.440,2,CC BY-SA 3.0,,6,,>,0,outdated</em>,False,False
95,25836395,2,25687136,2014-09-14T18:07:54.880,1,"<p>One thing up front: You don't say which third party module you are using with PrestaShop, so we have to take wild guesses as to what that actually does and how to change its behavior.</p>

<p>You are sending one parent SKU (MSL110) and five children (MWS110-DGM-S, -M, -L, -XL and -XXL). MWS reports it has successfully processed one of those six. I assume it is the parent product, please check in Seller Central if that is the case.</p>

<p>Your feed doesn't validate with the XSDs I've got. Mine do not allow <code>CollectionName</code>, but may be it is outdated. The feed seems to have passed the validation stage of the MWS feed processing though, so I guess it is fine with Amazon anyways.</p>

<p>You are getting multiple errors:</p>

<ol>
<li><strong>Error 8008</strong> means Amazon couldn't establish the parent-child link. Since appearantly MWS rejected the five child products to begin with, you should focus on other errors first.</li>
<li><strong>Error 99001</strong> missing ""department_name"". This probably refers to the XML element <code>Department</code> in <code>ClassificationData</code>, which you <strong>did</strong> specify.</li>
<li><strong>Error 99001</strong> missing ""material_type"". This probably refers to the XML element <code>MaterialAndFabric</code> in <code>ClassificationData</code>, which you <strong>did not</strong> specify.</li>
<li><strong>Error 99042</strong> missing ""recommended_browse_nodes"". The XML field this refers to is <code>RecommendedBrowseNode</code> in <code>DescriptionData</code> which you <strong>did not</strong> specify</li>
</ol>

<p>Steps to solve these:</p>

<ol>
<li><p>The easiest one is the last error. Get the <a href=""http://g-ecx.images-amazon.com/images/G/01/rainier/help/btg/apparel_browse_tree_guide.xls"" rel=""nofollow"">Browse Tree Guide</a>, and pick the correct category from the list.
In your case, the appropriate category seems to be ID 1045710, ""Clothing &amp; Accessories/Men/Underwear/Boxers"". Put right before your closing <code>DescriptionData</code> tag, like this: 
<code>...&lt;RecommendedBrowseNode&gt;1045710&lt;/RecommendedBrowseNode&gt;&lt;/DescriptionData&gt;</code>. This should get rid of error #4.</p></li>
<li><p>The same Browse Tree Guide shows two additional values in the next column: <code>department_name:mens AND item_type_keyword:boxer-shorts</code>. What that means is your ClassificationData should include those two values. Your Department already reads ""Mens"", but I'd change that to lower case to match the BTG, and see if error #2 goes away. The item type keyword is called <code>StyleKeywords</code> in XML and should contain ""boxer-shorts"".</p></li>
<li><p>While you're at it, you should also add <code>MaterialAndFabric</code>. <code>&lt;MaterialAndFabric&gt;cotton&lt;/MaterialAndFabric&gt;</code>. This should get rid of error #3.</p></li>
<li><p>Now you should see all six products in Seller Central, if you do, error #1 is likely to have vanished as well, as it now can create parent-child links since they actually are in the database.</p></li>
</ol>
",2097290,2014-09-14T18:07:54.880,0,CC BY-SA 3.0,,1,,.,0,">CollectionName</code>, but may be it is outdated.",False,False
96,25877899,2,25810197,2014-09-16T20:30:20.953,1,"<p>Just to repeat what's in the comment above; i have noticed that most of the R-packages that connected to AWS service were out of date. So i have created a new package <a href=""http://github.com/lalas/awsConnect"" rel=""nofollow"">AWSConnect</a> that allows a user to do most basic operations with S3 and EC2. In that package, the function <code>s3.ls()</code> is designed to list the bucket on S3. </p>

<p>Please feel free to use it, and report any bugs/request/issues</p>
",user2329215,2014-09-16T20:30:20.953,0,CC BY-SA 3.0,,1,,.,0,<p>Just to repeat what's in the comment above; i have noticed that most of the R-packages that connected to AWS service were outdated.,False,False
97,26089273,2,26084019,2014-09-28T20:23:43.510,1,"<p>Heroku syncs your dependencies.yml file everytime you push your app to them. One of my dependencies ended up being out of date, and heroku automatically grabbed a newer version of the file, which in turn broke the Amazon dependency. Updating my Amazon to the latest version ended up fixing the problem.</p>

<p>I hadn't touched my dependenies.yml file for weeks, and also hadn't run ""play deps --sync"" for weeks, so didn't think to look there on my local machine.</p>
",1203703,2014-09-28T20:23:43.510,1,CC BY-SA 3.0,,1,,.,0,"One of my dependencies ended up being outdated, and heroku automatically grabbed a newer version of the file, which in turn broke the Amazon dependency.",False,False
98,26187284,2,26186062,2014-10-03T21:49:09.103,2,"<p>AmazonS3Resolver is deprecated, try to use AwsS3Resolver:</p>

<pre><code>#service.yml
awsS3:
    class: Aws\S3\S3Client
    factory_class: Aws\S3\S3Client
    factory_method:  factory
    arguments:
        -
            key:    %aws_key%
            secret: %aws_secret%
            region: %aws_region%

liip_imagine.cache.resolver.amazon_s3:
    class: Liip\ImagineBundle\Imagine\Cache\Resolver\AwsS3Resolver
    arguments:
        - ""@awsS3""
        - %aws_bucket%
    tags:
        - {name: 'liip_imagine.cache.resolver', resolver: 'resolver_as3'}

#config.yml
liip_imagine:
    cache: resolver_as3
    data_loader: import
    filter_sets:
        128_128_75_s3:
            quality: 75
            filters:
                thumbnail: { size: [128, 128], mode: outbound }
</code></pre>
",3249470,2014-10-03T21:49:09.103,0,CC BY-SA 3.0,,1,,AwsS3Resolver:</p,0,">AmazonS3Resolver is deprecated, try to use AwsS3Resolver:</p",False,False
99,26443593,2,19478904,2014-10-18T19:19:04.023,5,"<p>The answers here are slightly outdated, spent a great deal of my day trying to get this work in Swift and the new AWS SDK. So here's how to do it in Swift by using the new <code>AWSS3PreSignedURLBuilder</code> (available in version 2.0.7+):</p>

<pre><code>class S3BackgroundUpload : NSObject {

    // Swift doesn't support static properties yet, so have to use structs to achieve the same thing.
    struct Static {
        static var session : NSURLSession?
    }

    override init() {
        super.init()

        // Note: There are probably safer ways to store the AWS credentials.
        let configPath = NSBundle.mainBundle().pathForResource(""appconfig"", ofType: ""plist"")
        let config = NSDictionary(contentsOfFile: configPath!)
        let accessKey = config.objectForKey(""awsAccessKeyId"") as String?
        let secretKey = config.objectForKey(""awsSecretAccessKey"") as String?
        let credentialsProvider = AWSStaticCredentialsProvider .credentialsWithAccessKey(accessKey!, secretKey: secretKey!)

        // AWSRegionType.USEast1 is the default S3 endpoint (use it if you don't need specific endpoints such as s3-us-west-2.amazonaws.com)
        let configuration = AWSServiceConfiguration(region: AWSRegionType.USEast1, credentialsProvider: credentialsProvider)

        // This is setting the configuration for all AWS services, you can also pass in this configuration to the AWSS3PreSignedURLBuilder directly.
        AWSServiceManager.defaultServiceManager().setDefaultServiceConfiguration(configuration)

        if Static.session == nil {
            let configIdentifier = ""com.example.s3-background-upload""

            var config : NSURLSessionConfiguration
            if NSURLSessionConfiguration.respondsToSelector(""backgroundSessionConfigurationWithIdentifier:"") {
                // iOS8
                config = NSURLSessionConfiguration.backgroundSessionConfigurationWithIdentifier(configIdentifier)
            } else {
                // iOS7
                config = NSURLSessionConfiguration.backgroundSessionConfiguration(configIdentifier)
            }

            // NSURLSession background sessions *need* to have a delegate.
            Static.session = NSURLSession(configuration: config, delegate: self, delegateQueue: nil)
        }
    }

    func upload() {
        let s3path = ""/some/path/some_file.jpg""
        let filePath = ""/var/etc/etc/some_file.jpg""

        // Check if the file actually exists to prevent weird uncaught obj-c exceptions.
        if NSFileManager.defaultManager().fileExistsAtPath(filePath) == false {
            NSLog(""file does not exist at %@"", filePath)
            return
        }

        // NSURLSession needs the filepath in a ""file://"" NSURL format.
        let fileUrl = NSURL(string: ""file://\(filePath)"")

        let preSignedReq = AWSS3GetPreSignedURLRequest()
        preSignedReq.bucket = ""bucket-name""
        preSignedReq.key = s3path
        preSignedReq.HTTPMethod = AWSHTTPMethod.PUT                   // required
        preSignedReq.contentType = ""image/jpeg""                       // required
        preSignedReq.expires = NSDate(timeIntervalSinceNow: 60*60)    // required

        // The defaultS3PreSignedURLBuilder uses the global config, as specified in the init method.
        let urlBuilder = AWSS3PreSignedURLBuilder.defaultS3PreSignedURLBuilder()

        // The new AWS SDK uses BFTasks to chain requests together:
        urlBuilder.getPreSignedURL(preSignedReq).continueWithBlock { (task) -&gt; AnyObject! in

            if task.error != nil {
                NSLog(""getPreSignedURL error: %@"", task.error)
                return nil
            }

            var preSignedUrl = task.result as NSURL
            NSLog(""preSignedUrl: %@"", preSignedUrl)

            var request = NSMutableURLRequest(URL: preSignedUrl)
            request.cachePolicy = NSURLRequestCachePolicy.ReloadIgnoringLocalCacheData

            // Make sure the content-type and http method are the same as in preSignedReq
            request.HTTPMethod = ""PUT""
            request.setValue(preSignedReq.contentType, forHTTPHeaderField: ""Content-Type"")

            // NSURLSession background session does *not* support completionHandler, so don't set it.
            let uploadTask = Static.session?.uploadTaskWithRequest(request, fromFile: fileUrl)

            // Start the upload task:
            uploadTask?.resume()

            return nil
        }
    }
}

extension S3BackgroundUpload : NSURLSessionDelegate {

    func URLSession(session: NSURLSession, dataTask: NSURLSessionDataTask, didReceiveData data: NSData) {
        NSLog(""did receive data: %@"", NSString(data: data, encoding: NSUTF8StringEncoding))
    }

    func URLSession(session: NSURLSession, task: NSURLSessionTask, didCompleteWithError error: NSError?) {
        NSLog(""session did complete"")
        if error != nil {
            NSLog(""error: %@"", error!.localizedDescription)
        }
        // Finish up your post-upload tasks.
    }
}
</code></pre>
",1494796,2014-10-18T19:19:04.023,1,2014-10-18T19:19:04.023,CC BY-SA 3.0,1,answers,.,0,"The answers here are slightly outdated, spent a great deal of my day trying to get this work in Swift and the new AWS SDK.",False,False
100,26500697,2,4770635,2014-10-22T05:13:45.860,0,"<p>Using ntp may not work on all version of your Linux based server (e.g. an out of date Ubuntu server version that is no longer supported which will block you from downloading ntp if it is not already installed).  </p>

<p>If this is your situation, you can set independent time zones for your Linux VM:
<a href=""https://community.rackspace.com/products/f/25/t/650"" rel=""nofollow"">https://community.rackspace.com/products/f/25/t/650</a> </p>

<p>After you do this you may need to reset the time/date.  Instructions for doing this are in this article:
<a href=""http://codeghar.wordpress.com/2007/12/06/manage-time-in-ubuntu-through-command-line"" rel=""nofollow"">http://codeghar.wordpress.com/2007/12/06/manage-time-in-ubuntu-through-command-line</a></p>
",211545,2014-10-22T05:13:45.860,0,CC BY-SA 3.0,,1,, ,1,Using ntp may not work on all version of your Linux based server (e.g. an outdated Ubuntu server version that is no longer supported which will block you from downloading ntp if it is not already installed).  ,False,False
101,26962712,2,26961425,2014-11-16T22:18:36.220,1,"<p>Yes, you can do this.  You want to use the <a href=""http://docs.aws.amazon.com/IAM/latest/UserGuide/AccessPolicyLanguage_ElementDescriptions.html#Principal"" rel=""nofollow"">Principal</a> element.</p>

<p>You can find examples <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html"" rel=""nofollow"">here.</a></p>

<p>(I know links are generally frowned upon, but AWS technologies change at such a rapid pace that actual examples may be obsolete within days or months)</p>
",4782,2014-11-16T22:18:36.220,0,CC BY-SA 3.0,,1,p>(I,months)</p,0,"<p>(I know links are generally frowned upon, but AWS technologies change at such a rapid pace that actual examples may be obsolete within days or months)</p",False,False
102,27433142,2,25943805,2014-12-11T21:55:25.167,1,"<p>CuddlyBuddly is out of date and queries for the entire list of files in the bucket every time it wants to post one, which eventually slows down and stops working entirely. The django-storages app using s3Boto is more up to date and works great.</p>

<p><a href=""https://django-storages.readthedocs.org/en/latest/backends/amazon-S3.html"" rel=""nofollow"">https://django-storages.readthedocs.org/en/latest/backends/amazon-S3.html</a></p>
",1021343,2014-12-11T21:55:25.167,0,CC BY-SA 3.0,,1,CuddlyBuddly,.,0,"CuddlyBuddly is outdated and queries for the entire list of files in the bucket every time it wants to post one, which eventually slows down and stops working entirely.",False,False
103,27658878,2,15778047,2014-12-26T15:24:57.340,2,"<p>Good answer - The blog post is out of date now as their naming has changed, but it's probably still based on MD5.</p>
",2692198,2014-12-26T15:24:57.340,0,CC BY-SA 3.0,,1,post,"
",0,"The blog post is outdated now as their naming has changed, but it's probably still based on MD5.</p>
",False,False
104,27893162,2,27747455,2015-01-11T23:07:43.600,5,"<p>Homebrew relies on volunteers to keep the formulas updated.  If you notice an outdated formula, please submit a bug or pull request.</p>
",98530,2015-01-11T23:07:43.600,0,CC BY-SA 3.0,,1,,"
",0,"If you notice an outdated formula, please submit a bug or pull request.</p>
",False,False
105,28035764,2,28035721,2015-01-20T00:13:32.883,3,"<p>You can enable PHP's shortened open tags in PHP.INI by setting <code>short_open_tag</code> to <code>1</code>. Presumably this had been done on your development system</p>

<p>However, this is discouraged in the PHP reference which you can find <a href=""http://php.net/manual/en/language.basic-syntax.phptags.php"" rel=""nofollow"">here</a> and <a href=""http://php.net/manual/en/ini.core.php#ini.short-open-tag"" rel=""nofollow"">here</a></p>

<p>You alternative is to change everything to <code>&lt;?php...</code></p>
",user1864610,2015-01-20T00:13:32.883,0,CC BY-SA 3.0,,1,this,syntax.phptags.php,0,"However, this is discouraged in the PHP reference which you can find <a href=""http://php.net/manual/en/language.basic-syntax.phptags.php",False,False
106,28089460,2,10940271,2015-01-22T13:01:45.220,3,"<p>Instead of using CommonCrypto, which is deprecated in modern OS X, you can also use SecTransforms:</p>

<pre><code>CFErrorRef error = NULL;

SecTransformRef digestRef = SecDigestTransformCreate(kSecDigestHMACSHA2, 256, &amp;error);
SecTransformSetAttribute(digestRef, kSecTransformInputAttributeName, (__bridge CFDataRef)self, &amp;error);
SecTransformSetAttribute(digestRef, kSecDigestHMACKeyAttribute, (__bridge CFDataRef)key, &amp;error);

CFDataRef resultData = SecTransformExecute(digestRef, &amp;error);
NSData* hashData = (__bridge NSData*)resultData;

CFRelease(digestRef);
</code></pre>
",573035,2015-01-22T13:01:45.220,1,CC BY-SA 3.0,,1,,SecTransforms:</p,0,">Instead of using CommonCrypto, which is deprecated in modern OS X, you can also use SecTransforms:</p",False,False
107,28098998,2,28093087,2015-01-22T21:26:46.120,1,"<p>For future reference, the problem was caused by using an obsolete 2.x version of the aws-eb-cli tools package. Upgrading it to 3.x made the error obvious - building the docker image has failed on AWS. </p>

<p>What I was looking for was running an existing docker image, I found instruction for this scenario at <a href=""https://aws.amazon.com/blogs/aws/aws-elastic-beanstalk-for-docker/"" rel=""nofollow"">https://aws.amazon.com/blogs/aws/aws-elastic-beanstalk-for-docker/</a>.</p>

<p>Thanks a lot for Nick for asking the right questions which made me realize the obsolete tools package!</p>
",548915,2015-01-22T21:26:46.120,0,CC BY-SA 3.0,,2,,.,0,"For future reference, the problem was caused by using an obsolete 2.x version of the aws-eb-cli tools package.",False,False
108,28159780,2,21223048,2015-01-26T22:19:09.497,0,"<p>The vagrant-rsync is deprecated as of Vagrant 1.5. One solution out there is <a href=""https://github.com/dmatora/vagrant-unison"" rel=""nofollow"">vagrant-unison</a>. You may also check out <a href=""https://github.com/mitchellh/vagrant-aws/issues/340"" rel=""nofollow"">this discussion</a>. What should also work is a <code>vagrant reload</code>.</p>
",1200707,2015-01-26T22:19:09.497,1,CC BY-SA 3.0,,1,rsync,.,0,The vagrant-rsync is deprecated as of Vagrant 1.5.,False,False
109,28162117,2,28060017,2015-01-27T02:12:20.220,0,"<p>The cluster feature was deprecated in VSE6.7 which is why you receive the error.  </p>

<p>If you want the cluster feature you will have to use the older core release.</p>

<p>If you are interested in other virtualized functions:</p>

<p>The latest incarnation (commercial) from Brocade is the 5600 ( <a href=""http://www.brocade.com/products/all/network-functions-virtualization/index.page"" rel=""nofollow"">http://www.brocade.com/products/all/network-functions-virtualization/index.page</a> ) and they recently announced a controller, the Brocade Vyatta Controller  ( <a href=""https://github.com/BRCDcomm/BVC/wiki"" rel=""nofollow"">https://github.com/BRCDcomm/BVC/wiki</a> ).  It is based on the open source OpenDaylight (ODL) controller ( <a href=""https://wiki.opendaylight.org/view/Main_Page"" rel=""nofollow"">https://wiki.opendaylight.org/view/Main_Page</a> ).  </p>

<p>The OpenDaylight controller allows you to write programs and/or scripts to monitor, configure and control a heterogenous network of virtual (and physical) devices.</p>

<p>SDxCentral maintains a list of virtual switching and routing components:
<a href=""https://www.sdxcentral.com/comprehensive-list-virtual-switching-routing/"" rel=""nofollow"">https://www.sdxcentral.com/comprehensive-list-virtual-switching-routing/</a></p>
",2430212,2015-01-27T02:12:20.220,0,CC BY-SA 3.0,,1,feature, ,0,>The cluster feature was deprecated in VSE6.7 which is why you receive the error.  ,False,False
110,28197504,2,28176056,2015-01-28T16:34:33.580,0,"<p>I found the issue. I did a huge mistake by importing: </p>

<pre><code>com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBRangeKey
</code></pre>

<p>instead of:</p>

<pre><code>com.amazonaws.mobileconnectors.dynamodbv2.dynamodbmapper.DynamoDBRangeKey
</code></pre>

<p>for @DynamoDBRangeKey annotation since I'm using Android mobile SDK. I found the issue by downloading and adding the DynamoDBMapper.jar for 2.1.8, and this version shows the </p>

<pre><code>com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBRangeKey
</code></pre>

<p>as deprecated.   </p>
",2292615,2015-01-28T16:34:33.580,0,CC BY-SA 3.0,,1,,  ,0,>as deprecated.   ,False,False
111,28350203,2,28308210,2015-02-05T17:22:08.440,1,"<p>From your description it looks like you may be mixing ups steps of root and non-root installation of DB2. </p>

<p>In a non-root installation all DB2 binaries and other files are installed in the home directory of the user performing the install. Certain tasks still require root privileges, this is where you run <code>db2rfe</code>. Non-root installations have many limitations though, so you should only choose it if you have no other alternative.</p>

<p>A root installation copies binaries under <code>/opt</code> and creates a DB2 instance in the home directory of the instance owner user that you specify during installation. From there symlinks are created to the actual binaries. </p>

<p>Root installation is not less secure than non-root, may be it's even more secure as the binaries are owned by root.</p>

<p>Console vs. GUI installation has no relation to root vs. non-root. In the past <code>db2setup</code> was used for GUI-based installations while <code>db2_install</code> for console installations. However, <code>db2_install</code> is now deprecated; to perform a console installation use a response file option (<code>db2setup -r &lt;response file name&gt;</code>) as explained in <a href=""http://www-01.ibm.com/support/knowledgecenter/SSEPGG_10.5.0/com.ibm.db2.luw.qb.server.doc/doc/t0007298.html?cp=SSEPGG_10.5.0%2F2-0-5-4&amp;lang=en"" rel=""nofollow"" title=""the manual"">the manual</a>.</p>
",1227152,2015-02-05T17:22:08.440,0,CC BY-SA 3.0,,1,>,code,0,> is now deprecated; to perform a console installation use a response file option (<code,False,False
112,28447858,2,28364723,2015-02-11T06:26:48.950,4,"<p>Since <code>DescribeJobFlows</code> is deprecated, monitor cluster status is an alternate way to monitor job run progress.</p>

<pre><code>    RunJobFlowResult runJobResult = emr.runJobFlow(runJobFlowRequest);
    System.out.printf(""Run JobFlowId is: %s\n"", runJobResult.getJobFlowId());

    while(true) {
      DescribeClusterRequest desc = new DescribeClusterRequest()
        .withClusterId(runJobResult.getJobFlowId());
      DescribeClusterResult clusterResult = emr.describeCluster(desc);
      Cluster cluster = clusterResult.getCluster();
      String status = cluster.getStatus().getState();
      System.out.printf(""Status: %s\n"", status);
      if(status.equals(ClusterState.TERMINATED.toString()) || status.equals(ClusterState.TERMINATED_WITH_ERRORS.toString())) {
        break;
      }
      try {
        TimeUnit.SECONDS.sleep(30);
      } catch (InterruptedException e) {
        e.printStackTrace();
      }
      // maybe other handle
    }
</code></pre>
",3275167,2015-02-11T06:26:48.950,0,CC BY-SA 3.0,,1,,progress.</p,0,"> is deprecated, monitor cluster status is an alternate way to monitor job run progress.</p",False,False
113,28485802,2,28480220,2015-02-12T19:09:58.857,1,"<p><code>AmazonS3Client</code> is from the version 1 of the AWS Mobile SDK for iOS, which has been deprecated. You should use <code>AWSS3TrasnferManager</code> or <code>AWSS3</code> in the version 2 of the SDK instead. You can take a look at <a href=""http://docs.aws.amazon.com/mobile/sdkforios/developerguide/"" rel=""nofollow"">AWS Mobile SDK Guide</a> and the S3TransferManager Sample at our <a href=""https://github.com/awslabs/aws-sdk-ios-samples"" rel=""nofollow"">GitHub repo</a> for further details.</p>
",1721492,2015-02-12T19:09:58.857,1,CC BY-SA 3.0,,1,,.,0,"> is from the version 1 of the AWS Mobile SDK for iOS, which has been deprecated.",False,False
114,28579201,2,28578079,2015-02-18T08:38:57.887,1,"<p>AWS SDK for iOS is depreciated now; so I believe the documentation link also must have been taken out.</p>

<blockquote>
  <p>Version 1 of the AWS Mobile SDK is deprecated as of September 29, 2014
  and will continue to be available until December 31, 2014. If you are
  building new apps, we recommend you use Version 2. If you are working
  on existing apps that use Version 1 (1.7.x or lower) of the AWS Mobile
  SDK, you can download v1 for Android here and iOS here. The API
  reference guides are included in the respective downloads. Apps built
  using Version 1 will continue to function after December 31, 2014.
  However, we highly recommend that you update your apps to the latest
  version so you can take advantage of the latest features and bug
  fixes.
  <strong>Source</strong> : <a href=""http://aws.amazon.com/mobile/sdk/"" rel=""nofollow"">http://aws.amazon.com/mobile/sdk/</a></p>
</blockquote>

<p>I managed to find a sample code from AWS Mobile Blog [<a href=""http://mobile.awsblog.com/post/Tx15F6J3B8B4YKK/Creating-Mobile-Apps-with-Dynamic-Content-Stored-in-Amazon-S3]"" rel=""nofollow"">http://mobile.awsblog.com/post/Tx15F6J3B8B4YKK/Creating-Mobile-Apps-with-Dynamic-Content-Stored-in-Amazon-S3]</a> to get the S3 object, you can extrapolate from there.</p>

<pre><code>-(void)getRemoteImage:(AmazonS3Client*)s3
             withName:(NSString*)imageName
           fromBucket:(NSString*)bucketName
{
    S3GetObjectRequest *request = 
       [[S3GetObjectRequest alloc] initWithKey:imageName withBucket:bucketName];
    S3GetObjectResponse *response = [s3 getObject:request];

    [self storeImageLocally:response.body withName:imageName];
}
</code></pre>

<p>Download Link for v1 iOS SDK : <a href=""http://sdk-for-ios.amazonwebservices.com/aws-ios-sdk-1.7.1.zip"" rel=""nofollow"">http://sdk-for-ios.amazonwebservices.com/aws-ios-sdk-1.7.1.zip</a></p>
",649408,2015-02-18T08:38:57.887,4,CC BY-SA 3.0,,1,Version,.,0,">Version 1 of the AWS Mobile SDK is deprecated as of September 29, 2014
  and will continue to be available until December 31, 2014.",False,False
115,28781862,2,28736850,2015-02-28T12:46:21.990,1,"<p>Finally i was able to set the bootstrap option:</p>

<pre><code>Set Replication Factor  s3://elasticmapreduce/bootstrap-actions/configure-hadoop    -h, dfs.replication=2
Set Block Size  s3://elasticmapreduce/bootstrap-actions/configure-hadoop    -h, dfs.block.size=67108864
</code></pre>

<p>-s option is deprecated. This URL tell about the option to use while creating bootstrap option:
<a href=""http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-bootstrap.html#PredefinedbootstrapActions_ConfigureHadoop"" rel=""nofollow"">http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-bootstrap.html#PredefinedbootstrapActions_ConfigureHadoop</a></p>
",2163943,2015-02-28T12:46:21.990,0,CC BY-SA 3.0,,1,option,.,0,<p>-s option is deprecated.,False,False
116,28841565,2,28151518,2015-03-03T20:46:57.033,1,"<p>I recommend two things.</p>

<p>First, use instance fields and methods instead of static fields and methods. You might have a race condition because the responses are not coming in as you expect, and you're closing streams in one invocation of <code>registerInvoice</code> and then trying to read from them in another invocation. So, your class would look like</p>

<pre><code>public class BPXProxy {
    private HttpClient client;
    public void init(){
        client = new HttpClient();
    }
    public InvoiceCreationInfo registerInvoice(InvoiceData invoice) throws SQLException, HttpException, IOException {
        JSONObject invoiceJSON = new JSONObject();
        invoiceJSON.put(""invoicer"", invoice.getInvoicerIdentification().getDocument());
        invoiceJSON.put(""buyer"", invoice.getBuyerIdentification().getDocument());
        String serviceURL = ConfigurationManager.getBPXServicesPath() + ""Invoice?operation=registerExternalInvoice&amp;branchCode=""+ branchCode;
        PostMethod updatePage = new PostMethod(serviceURL);
        updatePage.addParameter(""invoice"", invoiceJSON.toJSONString());
        updatePage.addParameter(""invoiceSubject"", invoice.getInvoiceSubject());
        client.executeMethod(updatePage); // java.io.IOException: Stream closed, java.net.SocketException: Socket closed
        String response = updatePage.getResponseBodyAsString();  // java.io.IOException: chunked stream ended unexpectedly, java.io.IOException: CRLF expected at end of chunk: -1/-1
        updatePage.releaseConnection();
        JSONObject jsonResponse = JSONSimpleHelper.parseJSON(response);
        return jsonResponse;
    }
}
</code></pre>

<p>and you would invoke it like</p>

<pre><code>BPXProxy instance = new BPXProxy();
instance.init();
InvoiceCreationInfo info = instance.registerInvoice(invoice);
</code></pre>

<p>Second, it looks like you're using Apache HttpClient 3.x, which is severely outdated. Upgrade to version 4.3 (or later if it exists). You'll have to change around your code, but based on the <a href=""https://hc.apache.org/httpcomponents-client-ga/quickstart.html"" rel=""nofollow"">quick start</a>, it shouldn't be too drastic:</p>

<pre><code>public class BPXProxy {

    private CloseableHttpClient client = HttpClients.createDefault();

    public InvoiceCreationInfo registerInvoice(InvoiceData invoice) throws SQLException, HttpException, IOException {
        JSONObject invoiceJSON = new JSONObject();
        invoiceJSON.put(""invoicer"", invoice.getInvoicerIdentification().getDocument());
        invoiceJSON.put(""buyer"", invoice.getBuyerIdentification().getDocument());
        String serviceURL = ConfigurationManager.getBPXServicesPath() + ""Invoice?operation=registerExternalInvoice&amp;branchCode=""+ branchCode;
        HttpPost httpPost = new HttpPost(serviceURL);
        List &lt;NameValuePair&gt; nvps = new ArrayList &lt;NameValuePair&gt;();
        nvps.add(""invoice"", invoiceJSON.toJSONString());
        nvps.add(""invoiceSubject"", invoice.getInvoiceSubject());
        client.setEntity(new UrlEncodedFormEntity(nvps));
        CloseableHttpResponse rsp = client.execute(httpPost);
        try {
            HttpEntity entity = rsp.getEntity();
            String rspJson = EntityUtils.toString(entity);
            EntityUtils.consume(entity);
            return JSONSimpleHelper.parseJSON(rspJson);
        } finally {
            rsp.close();
        }
    }
}
</code></pre>

<p>(Warning: I have not tried to compile or test this code.)</p>
",2657036,2015-03-03T20:46:57.033,2,CC BY-SA 3.0,,1,,.,0,"Second, it looks like you're using Apache HttpClient 3.x, which is severely outdated.",False,False
117,29185978,2,28784528,2015-03-21T17:49:31.403,2,"<p>At the bottom of <a href=""http://arxiv.org/help/bulk_data_s3"" rel=""nofollow"">this page</a> arXiv explains that s3cmd gets denied because it does not support access to requester pays bucket as a non-owner and you have to apply a patch to the source code of s3cmd.  However, the version of s3cmd they used is outdated and the patch does not apply to the latest version of s3cmd.</p>

<p>Basically you need to allow s3cmd to add ""x-amz-request-payer"" header to its HTTP request to buckets.  Here is how to fix it:</p>

<ol>
<li>Download the source code of s3cmd.</li>
<li>Open S3/S3.py with a text editor.</li>
<li><p>Add this two lines of code at the bottom of <code>__init__</code> function:</p>

<pre><code>if self.s3.config.extra_headers:
    self.headers.update(self.s3.config.extra_headers)
</code></pre></li>
<li>Install s3cmd as instructed.</li>
</ol>
",1441311,2015-03-21T17:49:31.403,0,CC BY-SA 3.0,,1,version,s3cmd.</p,1,"However, the version of s3cmd they used is outdated and the patch does not apply to the latest version of s3cmd.</p",False,False
118,29303152,2,28861620,2015-03-27T14:31:16.853,5,"<p>In the code, ""Total time spent by all maps in occupied slots (ms)"" is represented by the enum SLOTS_MILLIS_MAPS (or SLOTS_MILLIS_REDUCES) in JobCounter.java.  Those constants are deprecated.  They get their numbers by multiplying the task duration by the ratio of the MB used for the map task vs the minimum MB needed for one yarn slot.</p>

<p>So, if your map task used 4 MB and the minimum slot size is 1 MB, then your task took 4*duration of time that could have been used for other tasks.  That would explain why you see different ratios for different setups.  I don't find that metric to be particularly useful (especially since it isn't clear what it even means without diving into the code).</p>
",971723,2015-03-27T14:31:16.853,1,CC BY-SA 3.0,,1,constants, ,0,Those constants are deprecated.  ,False,False
119,29355052,2,29354208,2015-03-30T19:53:38.800,0,"<p>Seems to be that S3 is deprecated in favor of s3api, this worked</p>

<pre><code>aws s3api get-object --bucket bucketname-vagrant --key or_vagrant.sql.tar.gz or_vagrant.sql.tar.gz
</code></pre>
",411141,2015-03-30T19:53:38.800,0,CC BY-SA 3.0,,1,,",",0,">Seems to be that S3 is deprecated in favor of s3api,",False,False
120,29358202,2,29354208,2015-03-30T23:39:22.587,11,"<p><code>s3</code> is not deprecated. <a href=""http://docs.aws.amazon.com/cli/latest/userguide/cli-s3.html"" rel=""noreferrer""><code>s3</code> and <code>s3api</code> are on different tiers</a>. <code>s3api</code> is the API-level, while <code>s3</code> has the high-level commands.</p>

<h1>ls</h1>

<p>The problem is that you have a typo in <code>s3://</code> in your first command.</p>

<pre><code>$ aws s3 ls s://bucketname-vagrant
A client error (NoSuchBucket) occurred when calling the ListObjects operation: The specified bucket does not exist
</code></pre>

<p>I can replicate that error with my own bucket. This works:</p>

<pre><code>$ aws s3 ls s://bucketname-vagrant
</code></pre>

<h1>cp</h1>

<pre><code>$ aws s3 cp bucketname-vagrant/or_vagrant.sql.xz /tmp/
</code></pre>

<p>The problem here is that aws-cli doesn't know if you have a local directory named <code>bucketname-vagrant</code> or not. You can fix that by using the <code>s3://</code> syntax:</p>

<pre><code>$ aws s3 cp s3://bucketname-vagrant/or_vagrant.sql.xz /tmp/
</code></pre>

<p>Again, I replicated that locally.</p>

<pre><code>$ aws s3 cp bucket/test.txt /tmp/
usage: aws s3 cp &lt;LocalPath&gt; &lt;S3Path&gt; or &lt;S3Path&gt; &lt;LocalPath&gt; or &lt;S3Path&gt; &lt;S3Path&gt;
Error: Invalid argument type

$ aws s3 cp s3://bucket/test.txt /tmp/
download: s3://bucket/test.txt to /tmp/keybase.txt
</code></pre>
",659298,2015-03-30T23:39:22.587,0,CC BY-SA 3.0,,1,>,.,1,s3</code> is not deprecated.,False,False
121,29436189,2,5187496,2015-04-03T16:39:59.150,1,"<p>Tools recommend by other answers are out of date.</p>

<p>This one is up to date: <a href=""https://github.com/schickling/git-s3"" rel=""nofollow"">https://github.com/schickling/git-s3</a></p>
",938380,2015-04-03T16:39:59.150,0,CC BY-SA 3.0,,1,,outdated.</p,0,Tools recommend by other answers are outdated.</p,False,False
122,29517794,2,29502907,2015-04-08T14:39:00.303,0,"<p>It is possibly because WebFaction as far as I know still uses an out of date mod_wsgi and possibly even Apache 2.2. There is an issue with large file uploads if they are trickled in very slowly in small chunks. Use the latest mod_wsgi version and preferably Apache 2.4, which has other fixes in it related to memory usage and you may see better results.</p>
",128141,2015-04-08T14:39:00.303,2,CC BY-SA 3.0,,1,,.,0,It is possibly because WebFaction as far as I know still uses an outdated mod_wsgi and possibly even Apache 2.2.,False,False
123,29553199,2,18968387,2015-04-10T04:06:47.670,0,"<p>The accepted answer no longer works, its outdated. This worked for me:</p>

<pre><code># Install Oracle JDK
rpm --erase --nodeps java-1.6.0-openjdk java-1.6.0-openjdk-devel
rpm -Uvh .ebextensions/jdk-6u45-linux-amd64.rpm
/usr/sbin/alternatives --install /usr/bin/java java /usr/java/default/bin/java 3
/usr/sbin/alternatives --set java /usr/java/default/bin/java
/usr/sbin/alternatives --install /usr/bin/java_sdk java_sdk /usr/java/default/bin/java 3
/usr/sbin/alternatives --set java_sdk /usr/java/default/bin/java
</code></pre>

<p>This is for java 6, since I needed it to be. Also, the jdk downloaded from oracle is actually a bin file now (oracle's custom sh script extractor), so what I did is I downloaded the bin file from oracle, extracted it to get the RPM, and then included the RPM inside the ebextensions.</p>

<p>Just include that sh script to run in an ebextensions config file (google ebextenions config if you're unsure).</p>

<p>Hope this helps somebody.</p>
",219843,2015-04-10T04:06:47.670,0,CC BY-SA 3.0,,1,,.,1,"The accepted answer no longer works, its outdated.",False,False
124,29685453,2,12096241,2015-04-16T20:35:19.193,3,"<pre><code>instances = get_only_instances(instance_ids=['i-12345678'])
</code></pre>

<p>Regarding the above answer using </p>

<pre><code>get_all_instances()
</code></pre>

<p>, from the <a href=""http://boto.readthedocs.org/en/latest/ref/ec2.html#boto.ec2.connection.EC2Connection.get_all_instances"" rel=""nofollow"">BOTO API </a>--</p>

<pre><code>get_all_instances() is deprecated in favor of get_all_reservations(). 

A future major release will change get_all_instances() to return a list of 
boto.ec2.instance.Instance objects as its name suggests. 
To obtain that behavior today, use get_only_instances().
</code></pre>
",1236537,2015-04-16T20:35:19.193,0,CC BY-SA 3.0,,1,get_all_instances,"

",0,"<pre><code>get_all_instances() is deprecated in favor of get_all_reservations(). 

",False,False
125,29774677,2,29752988,2015-04-21T14:10:55.390,2,"<p>sorry to hear you are having this issue. We added a check in excon for openssl version because openssl had a release with a bug in it (so we set it up to give a better warning/instruction when you hit that bug to update). We didn't realize that this constant might not be defined in some cases. I've updated the code to skip the check when the constant is undefined and released excon 0.45.3, if you update to that it should avoid this issue.</p>

<p>All that said, I think it is also worth noting that the openssl version you have there is quite outdated (though it maybe is not obviously so, due to backported security fixes). It would probably be a good idea to bump up to something in the 1.0.1 or even 1.0.2 series (probably using a package manager, I use homebrew for this). This might also fix your issue, but regardless is probably a good idea.</p>

<p>Hope that helps!</p>
",967276,2015-04-21T14:10:55.390,0,CC BY-SA 3.0,,1,I,.,1,"I think it is also worth noting that the openssl version you have there is quite outdated (though it maybe is not obviously so, due to backported security fixes).",False,False
126,29836295,2,29772285,2015-04-23T23:44:01.203,0,"<p>It seems that AMI deployment of MongoDB has been deprecated and replaced by <a href=""https://docs.mms.mongodb.com/tutorial/nav/automation/"" rel=""nofollow"">MMS Automation</a>; however it is stated that </p>

<blockquote>
  <p>MMS Automation is currently in Beta with a limited initial user group</p>
</blockquote>

<p>(so folks at MongoDB, before deprecating one method please try to make sure that replacement is actually available).</p>

<p>That said; it seems that MMS provides quite a few nice features:interface for provisioning machines, configuring MongoDB nodes and clusters, and upgrading your MongoDB deployment.</p>

<p>However it follows a freemium model (8 free servers; 1GB free per replica set; see <a href=""https://mms.mongodb.com/#pricing"" rel=""nofollow"">details</a>).  Providing only 1GB free for back up (on your own ec2/aws hardware) doesn't seem quite right ... </p>

<p>More details are available at <a href=""https://mms.mongodb.com/"" rel=""nofollow"">MMS site</a>.</p>
",2705777,2015-04-23T23:44:01.203,0,CC BY-SA 3.0,,1,,"""",0,"It seems that AMI deployment of MongoDB has been deprecated and replaced by <a href=""https://docs.mms.mongodb.com/tutorial/nav/automation/""",False,False
127,29898270,2,29871162,2015-04-27T14:17:02.093,0,"<p>The problem turned out to be a database access issue. Specifically, the version of PHP on the new server turned out to not support deprecated calls to php-mysql APIs (e.g. @mysql_pconnect or @mysql_connect)</p>

<p>The solution was to</p>

<p>1) Download the mysql library which supports the legacy calls. In the case of AmazonLinux, this is accomplished as follows:</p>

<pre><code>sudo yum install php-mysql
</code></pre>

<p>2) Find where the library was downloaded e.g.</p>

<pre><code>find / -name mysql.so
</code></pre>

<p>This may have been shown in the install as well, but that is a way to find it anytime. YMMV, but in my case, the library name was:</p>

<pre><code>/usr/lib64/php-zts/modules/mysql.so
</code></pre>

<p>3) Change php.ini (as shown in phpinfo when called from a php page on the httpd/apached server) as follows:</p>

<p>a) change ""extension_dir"" to show where the mysql.so is installed:</p>

<pre><code>extension_dir = ""/usr/lib64/php-zts/modules"" 
</code></pre>

<p>b) Then add the following lines:</p>

<pre><code>extension=mysqli.so
extension=mysql.so
extension=pdo_mysql.so
</code></pre>

<p>Reboot apache and you should be good to go.</p>

<p>Note: the specific legacy call not supported, which is part of of the version of CodeIgnitor I'm working with, was this:</p>

<pre><code>   return @mysql_pconnect($this-&gt;hostname, $this-&gt;username, $this-// etc. 
</code></pre>

<p>The php error log showed this error:</p>

<pre><code> PHP Fatal error:  Call to undefined function mysql_connect() in xxx.php on line xx
</code></pre>
",64895,2015-04-27T14:17:02.093,0,CC BY-SA 3.0,,1,,@mysql_connect)</p,1,"Specifically, the version of PHP on the new server turned out to not support deprecated calls to php-mysql APIs (e.g. @mysql_pconnect or @mysql_connect)</p",False,False
128,30282155,2,30270319,2015-05-17T00:37:08.027,4,"<blockquote>
  <p>Can I use HTTP Authorization method to achieve the same thing?</p>
</blockquote>

<p>Sometimes.  The key difference is that, as a developer, you don't always have enough control over the user agent to inject a header.  The most obvious example of this is a simple <code>GET</code> request launched by a web browser in response to the user clicking a link.  In that situation, you don't have the a ability to inject an <code>Authorization:</code> header for the browser to send ... so pre-signing the URL is all you can do.</p>

<p>Importantly, there's no information in a signed URL that is considered sensitive, so there's no particularly strong motivation to use the header instead of a signed URL.  Your AWS Access Key ID is not secret, and your AWS Secret can't be derived from the other elements and the signature in a computationally-feasible time frame, particularly if you use Signature Version 4, which you should.  Signature Version 2 is not officially deprecated in older regions, but newer S3 never supported it and likely never will.</p>

<p>When you do control the user agent, such as in back-end server code, adding the header may be preferable, because you don't need to do any manipulation of the URL string you already have in-hand.</p>
",1695906,2015-05-17T00:37:08.027,2,CC BY-SA 3.0,,1,Version,will.</p,1,"Signature Version 2 is not officially deprecated in older regions, but newer S3 never supported it and likely never will.</p",False,False
129,30416677,2,30399068,2015-05-23T19:21:24.243,0,"<p>A little more complicated, and assuming that you have just a few easily-identified chunks of ""too-big"" directory trees (such as <code>/opt</code> and <code>/var/log</code>) would be to create filesystem(s) on the unallocated 28GB, use rsync to copy your working files to the new filesystems, and wipe the obsolete files (to free up diskspace), mounting the new filesystems in their place (by editing <code>/etc/fstab</code>, of course).</p>

<p>If you do this for <code>/var/log</code>, you have to do a reboot immediately after migrating the files, since the system still references the original files.  Of course, you might want to practice the procedure on a test-machine.</p>

<p>It would be a lot simpler if Amazon's root volumes used LVM.</p>
",4518274,2015-05-23T19:21:24.243,0,CC BY-SA 3.0,,1,,<,0,"/log</code>) would be to create filesystem(s) on the unallocated 28GB, use rsync to copy your working files to the new filesystems, and wipe the obsolete files (to free up diskspace), mounting the new filesystems in their place (by editing <",False,True
130,30437647,2,30425447,2015-05-25T11:47:07.927,1,"<p>I think you're mixing up the Cloud Provisioner and the AWS module. </p>

<p>The <code>node_aws</code> command is from the <a href=""https://docs.puppetlabs.com/pe/latest/cloudprovisioner_aws.html"" rel=""nofollow"">Cloud Provisioner</a> (which is now deprecated).</p>
",2417812,2015-05-25T11:47:07.927,3,CC BY-SA 3.0,,1,,"
",0,"> command is from the <a href=""https://docs.puppetlabs.com/pe/latest/cloudprovisioner_aws.html"" rel=""nofollow"">Cloud Provisioner</a> (which is now deprecated).</p>
",False,False
131,30607159,2,30606751,2015-06-02T21:45:02.897,4,"<p>You create AMIs directly from EC2 instances, not from snapshots. Snapshots are for EBS volumes. Check that you created your AMI correctly from a running EC2 instance on which you have Apache/Tomcat installed and running (and configured to autostart on reboot).</p>

<p>No, you do not have to use Puppet/Chef or any other CM tool. You can do what you want in a couple of ways:</p>

<ol>
<li>The simplest way is to create an AMI from your running
EC2 instance and then configure your Auto Scaling Group to launch
new instances from that AMI based on some metric.</li>
<li>Use a base AMI without Apache/Tomcat or your software and then bootstrap new instances at launch time to download and configure everything needed.</li>
</ol>

<p>The disadvantage of #1 is that your AMIs will get out of date quickly. The disadvantage of #2 is that your instances will taken longer to come into service. I would recommend a combination of #1 and #2, specifically that you capture a new AMI every few months and that becomes your base AMI for launching and you update the instance at launch time via userdata init script.</p>
",271415,2015-06-02T21:45:02.897,3,CC BY-SA 3.0,,1,,.,0,The disadvantage of #1 is that your AMIs will get outdated quickly.,False,False
132,30672766,2,29093644,2015-06-05T17:46:51.487,2,"<p>The basic advice would be not to resize images on-the-fly as this may take a while and your users may experience a huge response times during this operation. In case you have some predefined set of styles it would be wise to generate them in advance and just return back when required.</p>

<p>Well, here is what you could do if there is no other option.</p>

<pre><code>def download_from_s3 url_to_s3, filename
  uri = URI(url_to_s3)
  response = Net::HTTP.get_response(uri)
  File.open(filename, 'wb'){|f| f.write(response.body)}
end
</code></pre>

<p>Here we basically downloaded an image located at a given URL and saved it as a file locally. Resizing may be done in a couple of different ways (it depends on whether you want to serve the downloaded file as a <code>Paperclip</code> attachment).
The most common approach here would be to use <code>image-magick</code> and its <code>convert</code> command-line script. Here is an example of resizing an image to width of <code>30</code>:</p>

<pre><code>convert  -strip -geometry 30 -quality 100 -sharpen 1 '/photos/aws_images/000/000/015/original/index.jpg' '/photos/aws_images/000/000/015/original/S_30_WIDTH__q_100__index.jpg' 2&gt;&amp;1 &gt; /dev/null
</code></pre>

<p>You can find documentation for <code>convert</code> <a href=""http://www.imagemagick.org/script/convert.php"" rel=""nofollow"">here</a>, it's suitable not only for image resizing, but also for converting between image formats, bluring, cropping and much more! Also you could be intrested in <a href=""https://github.com/drpentode/Attachment-on-the-Fly"" rel=""nofollow"">Attachment-on-the-Fly gem</a>, which seems a little bit outdated, but has some insights of how to resize images using <code>convert</code>.</p>

<p>The last step is to upload resized image to some <code>S3 bucket</code>. I assume that you've already got <code>aws-sdk</code> gem and <code>AWS::S3</code> instance (the docs can be found <a href=""http://www.rubydoc.info/gems/aws-sdk-v1/1.64.0/AWS/S3"" rel=""nofollow"">here</a>).</p>

<pre><code>def upload_to_s3 bucket_name, key, file
  s3 = AWS::S3.new(:access_key_id =&gt; 'YOUR_ACCESS_KEY_ID', :secret_access_key =&gt; 'YOUR_SECRET_ACCESS_KEY')
  bucket = s3.buckets[bucket_name]
  obj = bucket.objects[key]
  obj.write(File.open(file, 'rb'), :acl =&gt; :public_read)
end
</code></pre>

<p>So, here you obtain an <code>AWS::S3</code> object to communicate with <code>S3</code> server, provide your bucket name and desired key, and basically upload an image with an option to make it visible to everybody on the web. Note that there are lots of additional upload options (including file encryption, access permissions, metadata and much more).</p>
",1276552,2015-06-05T17:46:51.487,0,CC BY-SA 3.0,,1,,code,0,"Also you could be intrested in <a href=""https://github.com/drpentode/Attachment-on-the-Fly"" rel=""nofollow"">Attachment-on-the-Fly gem</a>, which seems a little bit outdated, but has some insights of how to resize images using <code",False,False
133,30809937,2,30802773,2015-06-12T18:29:57.290,2,"<p>You are not retaining a strong reference to the <code>transferManager</code> object in <code>- getImageData:</code>. Please remember that <code>- getObject:</code> is an asynchronous method, and it returns immediately. You need to retain a strong reference to the service client until the request finishes processing.</p>

<p>If you use the AWS Mobile SDK for iOS 2.1.2, Xcode should give you a compiler warning for the use of <code>- initWithConfiguration:</code>. The method was deprecated to mitigate the misuse of the API such as this case. Please use <code>+ defaultS3TransferManager</code> or <code>+ S3TransferManagerForKey:</code> to retrieve the <code>AWSS3TransferManager</code> object.</p>

<p>(Also, the log indicates you are using 2.0.17 instead of 2.1.2.)</p>
",1721492,2015-06-12T18:29:57.290,1,CC BY-SA 3.0,,1,method,.,0,The method was deprecated to mitigate the misuse of the API such as this case.,False,False
134,30923227,2,30909554,2015-06-18T18:40:03.547,1,"<p><code>- download:</code> is an asynchronous method and returns immediately. Since you are not retaining a strong reference to an instance of <code>S3TransferManager</code>, it can be released before the download completes. You need to keep a strong reference to <code>transferManager</code>.</p>

<p>Please note that you are using a deprecated version of the AWS SDK. It may be worthwhile to migrate to the version 2 of the AWS Mobile SDK for iOS. With the latest version of the SDK, you do not need to worry about the memory retention issue you are encountering.</p>
",1721492,2015-06-18T18:40:03.547,0,CC BY-SA 3.0,,1,,.,0,Please note that you are using a deprecated version of the AWS SDK.,False,False
135,30961451,2,30889902,2015-06-21T05:04:53.877,1,"<p>It looks like you are running a sample from Github <a href=""https://github.com/jgilfelt/android-simpl3r"" rel=""nofollow"">https://github.com/jgilfelt/android-simpl3r</a>. The SDK version 1.4.3 is released 3 years ago. It uses Android's built-in Apache HttpClient which is very buggy and has been deprecated by Android. The 'Content has been consumed' issue can occur sometimes.</p>

<p>Please update the SDK to the latest version v2.2.2. It should be backward compatible. Check it out at <a href=""http://aws.amazon.com/mobile/sdk/"" rel=""nofollow"">http://aws.amazon.com/mobile/sdk/</a>.</p>
",2026747,2015-06-21T05:04:53.877,0,CC BY-SA 3.0,,1,,.,0,It uses Android's built-in Apache HttpClient which is very buggy and has been deprecated by Android.,False,False
136,31038374,2,28706078,2015-06-24T22:41:37.210,3,"<p>I had the same issue. I was using aws.push to update my application. Then I moved to a new computer and I had to set everything up again.</p>

<p>You can use</p>

<pre><code>eb deploy
</code></pre>

<p>However, depending upon how you have your project setup, you might need to map your deployment to a branch. Use:</p>

<pre><code>eb branch
</code></pre>

<p>I was in a bind and wanted to make sure that I did not screw up a deployment by introducing any new issues into the production environment and I wanted to use:</p>

<pre><code>git aws.push
</code></pre>

<p>This can still be done.</p>

<p>Download the deprecated version of the AWS Elastic Beanstalk Command Line Tool <a href=""http://aws.amazon.com/code/6752709412171743"" rel=""nofollow"">here</a> </p>

<p>Then from within your repo run <strong>AWSDevTools-RepositorySetup.sh</strong>. You can find this file in the zip file you just downloaded, AWS-ElasticBeanstalk-CLI-2.6.4 / AWSDevTools / Linux</p>

<p>Now run</p>

<pre><code>git aws.config
</code></pre>

<p>Once configured you should be able to run git aws.push without any problems. </p>

<p><strong>I am now using eb deploy, but I was in a bind and had never used it and did not have time to test it. So this worked for me.</strong></p>
",967195,2015-06-24T22:41:37.210,0,CC BY-SA 3.0,,1,,code/6752709412171743,0,"Download the deprecated version of the AWS Elastic Beanstalk Command Line Tool <a href=""http://aws.amazon.com/code/6752709412171743",False,False
137,31160416,2,31131817,2015-07-01T11:47:35.757,0,"<p>Was able to solve the problem. Just removed and re-added the file and file-transfer plugins.</p>

<p><code>cordova plugin rm org.apache.cordova.file</code> and <code>cordova plugin rm org.apache.cordova.file-transfer</code> to remove.</p>

<p><code>cordova plugin add org.apache.cordova.file</code> and <code>cordova plugin add org.apache.cordova.file-transfer</code> to add.</p>

<p>It may tell you that these plugins are soon to be deprecated or something like that. You also have the option of using the later versions. To add you can use </p>

<p><code>cordova plugin add cordova-plugin-file</code> and <code>cordova plugin add cordova-plugin-file-transfer</code>. Just check if they are available under the respective builds in the platforms folder.</p>
",4874431,2015-07-01T11:47:35.757,0,CC BY-SA 3.0,,1,It,.,0,It may tell you that these plugins are soon to be deprecated or something like that.,False,False
138,31348128,2,31347430,2015-07-10T18:45:31.850,1,"<p>This repository <code>dockerfile/nodejs</code> has been deprecated.</p>

<p>You can instead use <code>FROM node:latest</code> instead of <code>FROM dockerfile/nodejs</code></p>

<p><code>node</code> is the official repository for nodejs.<br></p>

<p>Here is the link to the official repo page for node:
<a href=""https://registry.hub.docker.com/_/node/"" rel=""nofollow"">https://registry.hub.docker.com/_/node/</a></p>

<p>And here is the open github issue for dockerfile/nodejs not found:
<a href=""https://github.com/dockerfile/nodejs/issues/11"" rel=""nofollow"">https://github.com/dockerfile/nodejs/issues/11</a></p>
",2167517,2015-07-10T18:45:31.850,1,CC BY-SA 3.0,,1,,deprecated.</p,0,nodejs</code> has been deprecated.</p,False,False
139,31379500,2,3142388,2015-07-13T09:16:03.320,26,"<p>I had the same problem, solution by @DanielVonFange is outdated, as new version of SDK is out.</p>

<p>Adding code snippet that works for me right now with AWS Ruby SDK:</p>

<pre><code>require 'aws-sdk'

Aws.config.update({
  region: 'REGION_CODE_HERE',
  credentials: Aws::Credentials.new(
    'ACCESS_KEY_ID_HERE',
    'SECRET_ACCESS_KEY_HERE'
  )
})
bucket_name = 'BUCKET_NAME_HERE'

s3 = Aws::S3::Resource.new
s3.bucket(bucket_name).objects.each do |object|
  puts object.key
  object.acl.put({ acl: 'public-read' })
end
</code></pre>
",2336662,2015-07-13T09:16:03.320,2,CC BY-SA 3.0,,1,,out.</p,0,"solution by @DanielVonFange is outdated, as new version of SDK is out.</p",False,False
140,31387625,2,31345871,2015-07-13T15:37:56.710,0,"<p>The issue was using DynamoDB version 1 (i.e. the deprecated one). Once I switched to connecting with the new version of the table and performed the scan with the appropriate method, it worked. </p>

<p>Hope this helps other tormented souls who get confused by cross-version mismatches. </p>
",4490681,2015-07-13T15:37:56.710,0,CC BY-SA 3.0,,1,,.,0,The issue was using DynamoDB version 1 (i.e. the deprecated one).,False,False
141,31416369,2,31413718,2015-07-14T20:12:15.340,2,"<p>Finally resolve the problem.</p>

<p>On generating ssl.pem, do not follow completely the guide mentioned in mup doc. That guide is outdated.</p>

<p>The whole process is follows:</p>

<ol>
<li><p>create Aws ubuntu  ec2. Note the user is ubuntu, not ec2-user</p></li>
<li><p>• First you need to generate a CSR file and the private key
AWSuse openssl, so follow: <a href=""https://support.comodo.com/index.php?/Default/Knowledgebase/Article/View/1/19/csr-generation-using-openssl-apache-wmod_ssl-nginx-os-x"" rel=""nofollow"">https://support.comodo.com/index.php?/Default/Knowledgebase/Article/View/1/19/csr-generation-using-openssl-apache-wmod_ssl-nginx-os-x</a>
• Then purchase a SSL certificate.</p>

<p>• Then generate a SSL certificate from your SSL providers UI.</p>

<p>• Then that'll ask to provide the CSR file. Upload the CSR file we've generated. only csr, no private key.</p>

<p>• When asked to select your SSL server type, select it as nginx.</p>

<p>• Then you'll get a set of files (your domain certificate and CA files).</p></li>
</ol>

<p>I bought ssl certificate from namecheap, and got four files in one .zip:
    • Root CA Certificate - AddTrustExternalCARoot.crt
    • Intermediate CA Certificate - COMODORSAAddTrustCA.crt
    • Intermediate CA Certificate - COMODORSADomainValidationSecureServerCA.crt
    • Your PositiveSSL Certificate - www_carllo_us.crt</p>

<ol start=""3"">
<li><p>Get ssl.pem:
cat www_domain_com.crt ComodoRSADomainValidationSecureServerCA.crt COMODORSAAddTrustCA.crt AddTrustExternalCARoot.crt private.key >> ssl.pem</p></li>
<li><p>Set mup.json:</p>

<pre><code>""env"": {
""ROOT_URL"": ""https://domain.com""
},
""ssl"": {
""pem"": ""carllous-bundle.pem""
},
</code></pre></li>
<li><p>Mup setup</p></li>
</ol>

<p>Done</p>
",5107277,2015-07-14T20:12:15.340,0,CC BY-SA 3.0,,1,,outdated.</p,0,That guide is outdated.</p,False,False
142,31452261,2,31426011,2015-07-16T10:57:41.680,-1,"<p>You have to many application versions. You can manually delete application version using the AWS Console:</p>

<p>AWS Console / Elastic Beanstalk / <em>Your App</em> / Application Versions</p>

<ul>
<li>Select some obsolete versions and delete them.</li>
</ul>
",479023,2015-07-16T10:57:41.680,0,CC BY-SA 3.0,,1,,them.</li,0,Select some obsolete versions and delete them.</li,False,False
143,31522261,2,31516524,2015-07-20T17:05:40.473,5,"<p>Don't be discouraged, no DynamoDB table has the perfectly distributed access patterns like the documentation suggests.  You'll have some hot spots, it's normal and OK.  You may have to increase your read/write throughput to accommodate the hotspots, and depending on how hot they are that might make a difference in the costs.  But at the modest throughput levels you describe, it isn't going to make DynamoDB unusable or anything.</p>

<p>I recommend converting your capacity requirements into the per-second throughput metrics DynamoDB uses. Will the 100,000 per day really be evenly distributed to ~2 per second?</p>

<ul>
<li>How many are reads vs. writes?</li>
<li>How big are they in 1K capacity chunks?</li>
<li>Is there a big difference between peak and trough usage?</li>
<li>Can caching be employed to smooth out the read pattern?</li>
</ul>

<p>Yes, the hash keys will be distributed across partitions.  Partitions do not correspond to individual items, but to allocations of read/write capacity and storage (<a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html#GuidelinesForTables.Partitions"" rel=""noreferrer"">Understanding Partition Behavior</a>).</p>
",3587167,2015-07-20T17:05:40.473,0,CC BY-SA 3.0,,1,, ,0,"<p>Don't be discouraged, no DynamoDB table has the perfectly distributed access patterns like the documentation suggests.  ",False,False
144,31564979,2,31086368,2015-07-22T13:45:06.030,24,"<p>I was facing the same issue.</p>

<p>The problem for me:</p>

<p>My local mongo shell was v2.6.10. It uses an authentication method called MONGODB-CR <a href=""http://docs.mongodb.org/manual/core/authentication/"" rel=""noreferrer"">that has been deprecated</a>.</p>

<p>My server version is v3.0.4. It uses an authentication method called SCRAM-SHA-1.</p>

<p>Try to check your local shell and remote server versions with:</p>

<pre><code>mongo --version
mongod --version
</code></pre>

<p>If they are different, upgrade your local shell to v3. (I had to uninstall and install it again.)</p>
",5115166,2015-07-22T13:45:06.030,2,CC BY-SA 3.0,,1,,"

",0,""" rel=""noreferrer"">that has been deprecated</a>.</p>

",False,False
145,31581489,2,31581388,2015-07-23T08:03:39.670,3,"<p>This can happen when some api calls get outdated. I would recommend using the <a href=""https://github.com/aws/aws-sdk-php/"" rel=""nofollow"">official php libraries</a>. They work almost the same way this library works, but they are being kept up-to-date by Amazon programmers.</p>
",743016,2015-07-23T08:03:39.670,0,CC BY-SA 3.0,,1,,.,0,This can happen when some api calls get outdated.,False,False
146,31886425,2,31885443,2015-08-07T20:50:30.570,0,"<p>A helpful voice in the #elasticsearch IRC channel asked me if I had the same version running. Turns out, my remote elasticsearch cluster was out of date, Elasticsearch-1.4.1 (likely because I installed it using a script copied from a tutorial). I <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-upgrade.html"" rel=""nofollow"">upgraded Elasticsearch</a> to version 1.7.1 and now my Haystack queries are working.</p>
",1359529,2015-08-07T20:50:30.570,0,CC BY-SA 3.0,,1,cluster,.,0,"Turns out, my remote elasticsearch cluster was outdated, Elasticsearch-1.4.1 (likely because I installed it using a script copied from a tutorial).",False,False
147,31962884,2,31958582,2015-08-12T10:42:09.707,0,"<p>Options include the following:</p>

<ol>
<li><p>Have Brooklyn auto-generate a security group per VM. This is the default, however it is not the most secure as it opens the port(s) publicly rather than just to the other Ambari VMs.</p></li>
<li><p>Use a pre-existing security group for all the Ambari VMs. You can configure the location with <code>securityGroups: nameOfMySecurityGroup</code>. This security group will be used unmodified, so can be used to provide access between the Ambari VMs.</p></li>
<li><p>Configure Brooklyn to create a new security group to be shared by all the Ambari VMs. This requires using a ""location customizer"" that creates the security group and adds it to the configuration being used to create the VMs. It can use or build on <a href=""https://github.com/apache/incubator-brooklyn/blob/0.7.0-incubating/locations/jclouds/src/main/java/brooklyn/location/jclouds/networking/JcloudsLocationSecurityGroupCustomizer.java"" rel=""nofollow"">https://github.com/apache/incubator-brooklyn/blob/0.7.0-incubating/locations/jclouds/src/main/java/brooklyn/location/jclouds/networking/JcloudsLocationSecurityGroupCustomizer.java</a>. Brooklyn would benefit from making that easier to use out-of-the-box!</p></li>
</ol>

<p>Best practice depends on your security requirements. Option (3) gives you maximum flexibility to configure it securely based on your needs, so is arguably best practice. Option (2) is simple to do for infrequent deployments but annoying if you have to manually create a security group ahead of time. Option (1) is simplest, but exposes more ports than absolutely necessary so should probably be discouraged for production use.</p>
",1393883,2015-08-12T10:42:09.707,2,CC BY-SA 3.0,,1,,use.</p,0,so should probably be discouraged for production use.</p,False,False
148,32027260,2,32020472,2015-08-15T17:20:13.910,3,"<p>In practice, neither of these records are necessary.  Here's why:</p>

<ul>
<li>The two records you've listed are SPF (v=spf1) and Sender-ID (spf2.0/pra).  The latter protocol, Sender-ID, is now obsolete and this record is not required.</li>
<li>SPF works off the 'mfrom' address - that's the Return Path address.  Amazon SES uses amazonses.com in the Return Path, meaning that receivers won't even check the SPF record you're creating.  So it is not necessary to add it to the SPF record for your domain.</li>
</ul>

<p>What you need to do is set up DKIM.  Authenticating email from Amazon SES requires the use of Amazon's Easy DKIM system (<a href=""http://docs.aws.amazon.com/ses/latest/DeveloperGuide/easy-dkim.html"" rel=""nofollow"">http://docs.aws.amazon.com/ses/latest/DeveloperGuide/easy-dkim.html</a>) .</p>
",1599793,2015-08-15T17:20:13.910,1,CC BY-SA 3.0,,1,protocol,required.</li,1,"The latter protocol, Sender-ID, is now obsolete and this record is not required.</li",False,False
149,32040035,2,31999668,2015-08-16T21:16:04.760,7,"<ol>
<li><p>At present the AWS Aurora documentation is linking to an out of date SSL certificate to use, hence the problem. This has been confirmed by the AWS support staff. Use this instead: <a href=""https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem"" rel=""noreferrer"">https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem</a></p></li>
<li><p>Even when using that certificate, connecting to the cluster end-point over SSL still doesn't work for the command line using mysql -h connection. If I change from the cluster end-point to the instance end-point strangely it works.</p></li>
<li><p>Bizarrely, mysql workbench does connect over ssl, both to the instance end-point AND the cluster-end point.</p></li>
</ol>
",964634,2015-08-16T21:16:04.760,1,CC BY-SA 3.0,,1,,.,0,"At present the AWS Aurora documentation is linking to an outdated SSL certificate to use, hence the problem.",False,False
150,32084940,2,32084637,2015-08-19T01:07:38.837,2,"<p>Yup. /utc/now is is off by one hour. At the moment, it is returning </p>

<pre><code>2015-08-19T01:58:07+01:00
</code></pre>

<p>(notice the +01:00)</p>

<p><a href=""http://chronic.herokuapp.com"" rel=""nofollow"">http://chronic.herokuapp.com</a> returns </p>

<pre><code>2015-08-19 00:57:27 +00:00
</code></pre>

<p>timeapi.org is based on an out of date fork of <a href=""https://github.com/zh/timeapi"" rel=""nofollow"">https://github.com/zh/timeapi</a>, which is the basis for the heroku app linked above.</p>
",5241372,2015-08-19T01:07:38.837,0,CC BY-SA 3.0,,1,,zh,0,"timeapi.org is based on an outdated fork of <a href=""https://github.com/zh",False,False
151,32162415,2,32157262,2015-08-23T01:45:04.320,0,"<p>There is no Amazon EC2 API call to see ""inside"" the Amazon EC2 instance. However, you can <strong>obtain information about the AMI used to launch the instance</strong>.</p>

<p>Here's an example, using the <a href=""http://aws.amazon.com/cli/"" rel=""nofollow"">AWS Command-Line Interface (CLI)</a>, which makes similar API calls to Java.</p>

<pre><code>$ aws ec2 describe-instances --query 'Reservations[*].Instances[*].ImageId' --filter Name=instance-id,Values=i-xxxxxxxx --output text
ami-d9fe9be3

$ aws ec2 describe-images --image-ids ami-d9fe9be3
{
    ""Images"": [
        {
            ""VirtualizationType"": ""hvm"", 
            ""Name"": ""amzn-ami-hvm-2014.03.2.x86_64-ebs"", 
            ""Hypervisor"": ""xen"", 
            ""ImageOwnerAlias"": ""amazon"", 
            ""SriovNetSupport"": ""simple"", 
            ""ImageId"": ""ami-d9fe9be3"", 
            ""State"": ""available"", 
            ""BlockDeviceMappings"": [
                {
                    ""DeviceName"": ""/dev/xvda"", 
                    ""Ebs"": {
                        ""DeleteOnTermination"": true, 
                        ""SnapshotId"": ""snap-c90a03fd"", 
                        ""VolumeSize"": 8, 
                        ""VolumeType"": ""standard"", 
                        ""Encrypted"": false
                    }
                }
            ], 
            ""Architecture"": ""x86_64"", 
            ""ImageLocation"": ""amazon/amzn-ami-hvm-2014.03.2.x86_64-ebs"", 
            ""RootDeviceType"": ""ebs"", 
            ""OwnerId"": ""137112412989"", 
            ""RootDeviceName"": ""/dev/xvda"", 
            ""CreationDate"": ""2014-06-11T19:46:45.000Z"", 
            ""Public"": true, 
            ""ImageType"": ""machine"", 
            ""Description"": ""Amazon Linux AMI x86_64 HVM EBS""
        }
    ]
}
</code></pre>

<p>Information can be extracted from the <code>Description</code> field. However, please note that <strong>Windows AMIs are deprecated each month</strong> as updates are made available by Microsoft. This means that old Windows instances might not have information accessible about their AMIs.</p>
",174777,2015-08-23T01:45:04.320,2,CC BY-SA 3.0,,1,,month</strong,0,"However, please note that <strong>Windows AMIs are deprecated each month</strong",False,False
152,32215884,2,31709257,2015-08-25T23:48:07.003,1,"<p>It sounds like your AWS SDK associated with the document client may be out of date and does not support the new KeyConditionExpression feature. Could you please try re-installing your AWS SDK and the document SDK? Please also attach the versions you are installing if you continue to have issues after re-installing.</p>
",4131111,2015-08-25T23:48:07.003,0,CC BY-SA 3.0,,1,,.,1,It sounds like your AWS SDK associated with the document client may be outdated and does not support the new KeyConditionExpression feature.,False,False
153,32335110,2,32227782,2015-09-01T15:09:35.657,0,"<p>Ok, the issue has been fixed. A guy from PG community helped me:</p>

<p><a href=""http://community.phonegap.com/nitobi/topics/phonegap-file-transfer-how-to-prevent-multipart-data?utm_source=notification&amp;utm_medium=email&amp;utm_campaign=new_comment&amp;utm_content=reply_button&amp;reply[id]=16076747#reply_16076747"" rel=""nofollow"">http://community.phonegap.com/nitobi/topics/phonegap-file-transfer-how-to-prevent-multipart-data?utm_source=notification&amp;utm_medium=email&amp;utm_campaign=new_comment&amp;utm_content=reply_button&amp;reply[id]=16076747#reply_16076747</a></p>

<p>I used PG build and the plugin there was outdated. Once I installed the last version of cordova-plugin-media-transfer from npm and built locally the app started to upload to Amazon with no issues and no multipart data were used.</p>
",3595434,2015-09-01T15:09:35.657,0,CC BY-SA 3.0,,1,,.,0,I used PG build and the plugin there was outdated.,False,False
154,32881955,2,32881396,2015-10-01T07:37:59.290,4,"<p>You probably have just upgraded python to 3.4.1.</p>

<p>The version of the AWS CLI and botocore hosted for Ubuntu 14.04 is incompatible with python 3.4.1+. (<a href=""https://bugs.launchpad.net/ubuntu/+source/awscli/+bug/1501042"" rel=""nofollow"">source</a>)</p>

<p>The version of the CLI installed via apt-get is out of date. Python 3.4.1 introduced a breaking change with the getargspec() function, which we use in botocore. (<a href=""https://github.com/aws/aws-cli/issues/800"" rel=""nofollow"">source</a>)</p>

<p>You can try: (I don't have testbox with me to verify)</p>

<pre><code>sudo apt-get remove awscli
sudo apt-get install python-pip
sudo pip install awscli
sudo pip install upgrade botocore
</code></pre>
",1145196,2015-10-01T07:37:59.290,5,CC BY-SA 3.0,,1,version,.,0,The version of the CLI installed via apt-get is outdated.,False,False
155,32898784,2,32898692,2015-10-02T00:25:55.927,0,"<p>It runs great in my system, I just cloned the project:</p>

<pre><code>shiva@ubuntu:~/projects/amazon-ecs-sample (master)$ rails s
Digest::Digest is deprecated; use Digest
=&gt; Booting WEBrick
=&gt; Rails 3.2.5 application starting in development on http://0.0.0.0:3000
=&gt; Call with -d to detach
=&gt; Ctrl-C to shutdown server
[2015-10-02 06:05:38] INFO  WEBrick 1.3.1
[2015-10-02 06:05:38] INFO  ruby 2.1.1 (2014-02-24) [x86_64-linux]
[2015-10-02 06:05:38] INFO  WEBrick::HTTPServer#start: pid=4008 port=3000
</code></pre>

<p>May be, you need to verify changes you made in few of the latest commits. or make sure you did <code>bundle install</code></p>

<pre><code>shiva@ubuntu:~/projects/amazon-ecs-sample (master)$ bundle install
Fetching gem metadata from https://rubygems.org/...........
Fetching version metadata from https://rubygems.org/...
Fetching dependency metadata from https://rubygems.org/..
Installing rake 0.9.2.2
Installing i18n 0.6.0
Installing multi_json 1.3.6
Installing activesupport 3.2.5
Installing builder 3.0.0
Installing activemodel 3.2.5
Using erubis 2.7.0
Installing journey 1.0.3
Installing rack 1.4.1
Installing rack-cache 1.2
Installing rack-test 0.6.1
Installing hike 1.2.1
Installing tilt 1.3.3
Installing sprockets 2.1.3
Installing actionpack 3.2.5
Installing mime-types 1.18
Installing polyglot 0.3.3
Installing treetop 1.4.10
Installing mail 2.4.4
Installing actionmailer 3.2.5
Installing arel 3.0.2
Installing tzinfo 0.3.33
Installing activerecord 3.2.5
Installing activeresource 3.2.5
Installing nokogiri 1.5.4
Using ruby-hmac 0.4.0
Installing amazon-ecs 2.2.4
Installing coffee-script-source 1.3.3
Installing execjs 1.4.0
Using coffee-script 2.2.0
Installing rack-ssl 1.3.2
Installing json 1.7.3
Installing rdoc 3.12
Installing thor 0.15.2
Installing railties 3.2.5
Installing coffee-rails 3.2.2
Installing jquery-rails 2.0.2
Using bundler 1.9.2
Installing rails 3.2.5
Installing sass 3.1.19
Installing sass-rails 3.2.5
Installing sqlite3 1.3.6
Installing uglifier 1.2.4
Bundle complete! 7 Gemfile dependencies, 43 gems now installed.
Use `bundle show [gemname]` to see where a bundled gem is installed.
Post-install message from rdoc:
Depending on your version of ruby, you may need to install ruby rdoc/ri data:

&lt;= 1.8.6 : unsupported
 = 1.8.7 : gem install rdoc-data; rdoc-data --install
 = 1.9.1 : gem install rdoc-data; rdoc-data --install
&gt;= 1.9.2 : nothing to do! Yay!
</code></pre>
",3437900,2015-10-02T00:25:55.927,0,CC BY-SA 3.0,,1,,;,0,"Digest::Digest is deprecated; use Digest
=&gt;",False,False
156,32901014,2,32900928,2015-10-02T05:24:18.653,2,"<p>As your link suggests this option was available till Rails 2.3.8.</p>

<p>It was deprecated from Rails 3.</p>

<p>Here's an old day discussion showing problems with this feature: <a href=""https://groups.google.com/forum/#!topic/rubyonrails-core/BnVHfD-yO_I"" rel=""nofollow"">https://groups.google.com/forum/#!topic/rubyonrails-core/BnVHfD-yO_I</a>.</p>
",1426097,2015-10-02T05:24:18.653,1,CC BY-SA 3.0,,1,It,3.</p,0,It was deprecated from Rails 3.</p,False,False
157,33181296,2,31709257,2015-10-17T00:12:30.980,0,"<p>Previous <a href=""https://github.com/awslabs/dynamodb-document-js-sdk"" rel=""nofollow"">DynamoDB Document SDK</a> was deprecated, new client from standard Javascript SDK should be used from now on:</p>

<p><a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html"" rel=""nofollow"">http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html</a></p>
",5455783,2015-10-17T00:12:30.980,0,CC BY-SA 3.0,,1,,on:</p,0,"> was deprecated, new client from standard Javascript SDK should be used from now on:</p",False,False
158,33287596,2,15867047,2015-10-22T18:00:18.097,2,"<p>This question was filed a few years ago, and the current answer I believe is out of date.  EC2 now runs the above script with a successful response without the need for a proxy.  I came across this question while investigating my own similar issue with Google App Engine.</p>
",3037408,2015-10-22T18:00:18.097,0,CC BY-SA 3.0,,1,, ,0,"This question was filed a few years ago, and the current answer I believe is outdated.  ",False,False
159,33482970,2,33479851,2015-11-02T17:01:52.427,1,"<p>It looks like <code>invokeAsync</code> is deprecated, so I used <code>invoke</code> for this example, but it is fairly similar.  </p>

<p>The <code>Payload</code> argument in the <code>invoke</code> params becomes the <code>event</code> parameter in ChainFunc2.</p>

<p><strong>ChainFunc1</strong></p>

<pre><code>var AWS = require(""aws-sdk"");

exports.handler = function(event, context) {
    console.log('Received event:', JSON.stringify(event, null, 2));
    var params = {
        FunctionName: ""ChainFunc2"",
        InvocationType: ""RequestResponse"",
        Payload: JSON.stringify({""greeting"": ""Hello, Lambda""})
    };
    var lambdaClient = new AWS.Lambda();
    lambdaClient.invoke(params, function(err, data) {
        if (err) {
            console.log(""invoke failed:"" + err, err.stack);
            context.fail(err);
        } else {
            console.log(""invoke succeeded"", data);
            context.succeed(data);
        }
    });
};
</code></pre>

<p><strong>ChainFunc2</strong></p>

<pre><code>exports.handler = function(event, context) {
    console.log(""Received event:"", JSON.stringify(event, null, 2));
    console.log(""Greeting:"", event.greeting);
    context.succeed({""message"": ""ChainFunc2 processed this"", ""payload"": event});
};
</code></pre>
",3587167,2015-11-02T17:01:52.427,8,CC BY-SA 3.0,,1,, ,0,"invokeAsync</code> is deprecated, so I used <code>invoke</code> for this example, but it is fairly similar.  ",False,False
160,33641020,2,12908599,2015-11-10T22:39:59.500,1,"<p>The answer by Godsaur is essentially correct. However, it appears to be outdated, perhaps for SDK version 1? </p>

<p>This worked for me for version 2:</p>

<pre><code>s3 = Aws::S3::Client.new(endpoint:'https://s3-ap-southeast-1.amazonaws.com')
</code></pre>

<p>See <a href=""http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Client.html#initialize-instance_method."" rel=""nofollow"">docs</a>.</p>
",336920,2015-11-10T22:39:59.500,0,CC BY-SA 3.0,,1,,?,0,"However, it appears to be outdated, perhaps for SDK version 1?",False,False
161,33685242,2,33684609,2015-11-13T03:01:54.463,2,"<p>I would remove the <code>provider</code> key. The <code>carrierwave-aws</code> gem <a href=""https://github.com/sorentwo/carrierwave-aws#usage"" rel=""nofollow"">readme</a> (I'm guessing you are using that or something similar) does not even mention the <code>provider</code> key. That might have been an old requirement that has been deprecated. </p>
",733721,2015-11-13T03:01:54.463,0,CC BY-SA 3.0,,1,,.,0,That might have been an old requirement that has been deprecated.,False,False
162,33934630,2,28676307,2015-11-26T09:18:18.130,0,"<p>@filename has been deprecated in PHP >= 5.5.0: </p>

<p>Use <strong><em>new CurlFile</em></strong> instead, will not require any changes on the receiving end.</p>
",3387127,2015-11-26T09:18:18.130,0,CC BY-SA 3.0,,1,p>@filename,=,0,<p>@filename has been deprecated in PHP >=,False,False
163,33998898,2,33995463,2015-11-30T12:39:24.750,0,"<p>The version you are trying to use is deprecated and IAM roles are required. Follow the example as given in the documentation <a href=""http://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/calling-emr-with-java-sdk.html"" rel=""nofollow"">http://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/calling-emr-with-java-sdk.html</a>. </p>
",4394783,2015-11-30T12:39:24.750,1,CC BY-SA 3.0,,1,version,.,0,The version you are trying to use is deprecated and IAM roles are required.,False,False
164,34330629,2,34294720,2015-12-17T09:10:09.200,0,"<p>I trashed the original instance and created a new instance and this time I've installed node.js using the command that is on their website:</p>

<pre><code>sudo yum install nodejs npm --enablerepo=epel
</code></pre>

<p>Instead of installing it from source as I did the first time. By executing the command above I ensured that I installed the latest version instead of an outdated version which I believe caused my problems with CERT_UNTRUSTED.</p>
",4423934,2015-12-17T09:10:09.200,0,CC BY-SA 3.0,,1,,"
",0,"By executing the command above I ensured that I installed the latest version instead of an outdated version which I believe caused my problems with CERT_UNTRUSTED.</p>
",False,False
165,34434406,2,34434133,2015-12-23T11:13:07.067,3,"<p>The <code>===</code> is the <a href=""https://www.python.org/dev/peps/pep-0440/#arbitrary-equality"" rel=""nofollow"">arbitrary equality clause</a> and is defined in PEP-0440:</p>

<blockquote>
  <p>Arbitrary equality comparisons are simple string equality operations
  which do not take into account any of the semantic information such as
  zero padding or local versions. This operator also does not support
  prefix matching as the == operator does.</p>
  
  <p>The primary use case for arbitrary equality is to allow for specifying
  a version which cannot otherwise be represented by this PEP. This
  operator is special and acts as an escape hatch to allow someone using
  a tool which implements this PEP to still install a legacy version
  which is otherwise incompatible with this PEP.</p>
  
  <p>An example would be ===foobar which would match a version of foobar .</p>
  
  <p>This operator may also be used to explicitly require an unpatched
  version of a project such as ===1.0 which would not match for a
  version 1.0+downstream1 .</p>
  
  <p>Use of this operator is heavily discouraged and tooling MAY display a
  warning when it is used.</p>
</blockquote>

<p>You should upgrade your version of pip on the target machine (<code>pip install --upgrade pip</code>) and it should not display the error message.</p>
",790387,2015-12-23T11:13:07.067,0,CC BY-SA 3.0,,1,Use,used.</p,0,"Use of this operator is heavily discouraged and tooling MAY display a
  warning when it is used.</p",False,False
166,34479582,2,34479284,2015-12-27T10:34:07.993,2,"<p>The I2 family <a href=""https://aws.amazon.com/ec2/instance-types/"" rel=""nofollow"">are indeed</a> the I/O optimised successors to the H1 family. AWS themselves recommend them for NoSQL database operations.</p>

<p>Although a little out of date now <a href=""http://www.datastax.com/dev/blog/ec2-series-doc"" rel=""nofollow"">Datastax themselves recommend</a> as a progression in price and performance: M series, C series, and finally the I series.</p>
",3846501,2015-12-27T10:34:07.993,1,CC BY-SA 3.0,,1,,recommend</a,0,"Although a little outdated now <a href=""http://www.datastax.com/dev/blog/ec2-series-doc"" rel=""nofollow"">Datastax themselves recommend</a",False,True
167,34570519,2,34570396,2016-01-02T20:40:56.017,6,"<p>You have a very old 1.2.9 version. According to the <a href=""https://github.com/aws/aws-cli/blob/develop/CHANGELOG.rst"">changelog</a> glacier support was added in 1.7.40, and current is 1.9.15.</p>

<p>Perhaps it is better to install it via pip, your ubuntu package repo seems outdated.</p>
",13189,2016-01-02T20:40:56.017,1,CC BY-SA 3.0,,1,,"
",0,"Perhaps it is better to install it via pip, your ubuntu package repo seems outdated.</p>
",False,False
168,34775544,2,34736988,2016-01-13T19:37:40.397,0,"<p>Some services like Cognito support TLSv1.0+, while some say DynamoDB supports only TLSv1.0 (not above). If your device supports TLSv1.0, it should work then. Since you said the code works on emulator but not on the tablet, it's hard to say where the problem is. I suggest you try these:</p>

<ul>
<li>Visit <a href=""https://dynamodb.us-west-2.amazonaws.com"" rel=""nofollow"">https://dynamodb.us-west-2.amazonaws.com</a> in your browser. If your device can handle TLSv1.0, you should see <code>healthy: dynamodb.us-west-2.amazonaws.com</code>.</li>
<li>Run the same code on a different device.</li>
</ul>

<p>Some notes from comments:</p>

<ul>
<li>SSLv3 has been deprecated on all AWS services.</li>
<li>Remove aws-java-sdk and use aws-android-sdk. The latter has lots of optimizations for Android.</li>
<li>aws-android-sdk uses HttpURLConnection as the default HTTP library.</li>
</ul>

<p>To test what protocols a service supports, use this command:</p>

<pre><code>openssl s_client -connect dynamodb.us-west-2.amazonaws.com:443
</code></pre>
",2026747,2016-01-13T19:37:40.397,3,CC BY-SA 3.0,,1,SSLv3,services.</li,0,SSLv3 has been deprecated on all AWS services.</li,False,False
169,34880340,2,33613068,2016-01-19T15:13:12.903,0,"<p>I had a similar problem where the culprit was an outdated system clock. EC2 instances can sometimes drift and IAM API is very sensitive to it. Relevant information can be found here: <a href=""https://github.com/boto/boto/issues/2885"" rel=""nofollow"">https://github.com/boto/boto/issues/2885</a>.</p>
",5811137,2016-01-19T15:13:12.903,1,CC BY-SA 3.0,,1,,.,0,I had a similar problem where the culprit was an outdated system clock.,False,False
170,35068909,2,35067388,2016-01-28T18:11:31.233,7,"<p>Solved!</p>

<p>It appears that the tutorial is outdated.</p>

<p>I needed to update wercker.yml to work with Wercker v2. </p>

<p>To do this, I changed:
<code>box: wercker/ruby</code>
to
<code>box: ruby</code>.</p>
",3574603,2016-01-28T18:11:31.233,2,CC BY-SA 3.0,,1,,outdated.</p,0,It appears that the tutorial is outdated.</p,False,False
171,35100734,2,35100394,2016-01-30T10:37:40.780,1,"<p>That is because of space in your <code>:rvm_ruby_version</code> causing deprecated-like rvm commands to be generated, try replacing to:</p>

<pre><code>set :rvm_ruby_version, 'ruby-2.2.1p85'
</code></pre>
",177053,2016-01-30T10:37:40.780,0,CC BY-SA 3.0,,1,,to:</p,0,"causing deprecated-like rvm commands to be generated, try replacing to:</p",False,False
172,35174546,2,35174076,2016-02-03T10:32:45.423,4,"<p>Don't think of CloudFront as primarily <em>""hosting""</em> your site. CloudFront <em>caches</em> your site to a) reduce traffic to the origin web server (not a big concern when hosting on S3 to begin with) and b) to speed up delivery to clients since CloudFront caches are geographically distributed throughout the globe, instead of just having one centralised server.</p>

<p>Yes, you can host a website this way. Put the actual ""master copy"" on S3, configure CloudFront as a front-end cache to it. The master copy needs to stay where it is; in case you were thinking of removing it from S3 once it's cached in CloudFront, that would be a bad idea. CloudFront makes no guarantees about keeping your data available; caches may expire at any time, entire CloudFront nodes may be taken offline, replaced, or added at any time, and in all those cases the CloudFront node needs an origin server to get its copy from.</p>

<p>The only problem left then is <em>cache expiry</em>. As always, there are two approaches:</p>

<ol>
<li><p>Configure a sensible cache timeout for your content. If you set your content to expire after, say, 1 hour, then any client may see data which is outdated by up to an hour after you update your content. This may or may not be a problem, you decide. You can and should configure different expiry times for different kinds of content; images and such can probably be cached indefinitely, while the HTML of an often-updated front page should probably have much shorter life times.</p></li>
<li><p>Explicitly flush the cache with an invalidation request after you update your content. The problem with this is that you only have a limited number of free invalidation requests on CloudFront. It is not something AWS likes you to do and is mostly reserved as a tool for emergencies when incorrect data got pushed out. AWS prefers you to let content expire naturally, since this puts the least stress on the network.</p></li>
</ol>

<p>""Pre-warming"" is not typically necessary and difficult to do anyway, since you cannot <em>push</em> content to CloudFront; CloudFront <em>pulls</em> content from the origin as needed. I'm not sure what ""armfront"" you're referring to exactly or what it supposedly does. Without pre-warming, the very first request for a particular page in a particular geographic region will be ever so slightly slower, that's all.</p>
",476,2016-02-03T10:32:45.423,3,CC BY-SA 3.0,,1,,.,0,"If you set your content to expire after, say, 1 hour, then any client may see data which is outdated by up to an hour after you update your content.",False,True
173,35181781,2,34834290,2016-02-03T16:00:04.407,1,"<p>Docker Hub has deprecated pulls from Docker clients on 1.5 and earlier. Make sure that your docker client version is at least above 1.5. See <a href=""https://blog.docker.com/2015/10/docker-hub-deprecation-1-5/"" rel=""nofollow"">https://blog.docker.com/2015/10/docker-hub-deprecation-1-5/</a> for more information.</p>
",2780476,2016-02-03T16:00:04.407,0,CC BY-SA 3.0,,1,,.,0,<p>Docker Hub has deprecated pulls from Docker clients on 1.5 and earlier.,False,False
174,35186240,2,25578219,2016-02-03T19:44:39.027,3,"<p>The accepted answer is out of date. This is possible using the ""Origin Path"" setting in AWS which will rewrite the request to a sub-folder on the origin:</p>

<p><a href=""http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesOriginPath"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesOriginPath</a></p>
",1528315,2016-02-03T19:44:39.027,2,CC BY-SA 3.0,,1,answer,.,0,The accepted answer is outdated.,False,False
175,35322850,2,35248694,2016-02-10T18:12:46.483,0,"<p>I figures that this behaviour is caused by POUND Configuration because I put:</p>

<blockquote>
  <p>ListenHTTP</p>
  
  <p>Address 0.0.0.0</p>
  
  <p>Port    80</p>
</blockquote>

<p>Whereas I am supposed to replace the Address with the server IP Address.</p>

<p>This is referred as below in the config description:</p>

<blockquote>
  <p>Address address The address that Pound will listen on. This can be a
  numeric IP address, or a symbolic host name that must be resolvable at
  run-time. This is a mandatory parameter. The address 0.0.0.0 may be
  used as an alias for 'all available addresses on this machine', but
  this practice is strongly discouraged, as it will interfere with the
  rewriting mechanisms (see below).</p>
</blockquote>

<p>Refer to the following website for further info:
<a href=""http://linux.die.net/man/8/pound"" rel=""nofollow"">http://linux.die.net/man/8/pound</a></p>
",3984947,2016-02-10T18:12:46.483,0,CC BY-SA 3.0,,1,,below).</p,0,"The address 0.0.0.0 may be
  used as an alias for 'all available addresses on this machine', but
  this practice is strongly discouraged, as it will interfere with the
  rewriting mechanisms (see below).</p",False,False
176,35639822,2,35453029,2016-02-25T22:37:53.640,0,"<p>It does not seem that registerCognitoWithConfiguration() is deprecated. </p>

<p><a href=""https://github.com/aws/amazon-cognito-ios/blob/master/Cognito/AWSCognitoService.m#L118"" rel=""nofollow"">https://github.com/aws/amazon-cognito-ios/blob/master/Cognito/AWSCognitoService.m#L118</a></p>

<p><a href=""https://github.com/aws/amazon-cognito-ios/blob/master/Cognito/AWSCognitoService.h#L232"" rel=""nofollow"">https://github.com/aws/amazon-cognito-ios/blob/master/Cognito/AWSCognitoService.h#L232</a></p>
",4785056,2016-02-25T22:37:53.640,1,CC BY-SA 3.0,,1,,.,1,It does not seem that registerCognitoWithConfiguration() is deprecated.,False,False
177,35644902,2,35644640,2016-02-26T06:21:57.947,1,"<p>This books seems to be obsolete, it's better to use the doc on the <a href=""https://www.npmjs.com/package/multer"" rel=""nofollow"">npmjs page</a>. For example:</p>

<pre><code>var express = require('express');
var app = express();
var router = express.Router();

var multer = require('multer');
var upload = multer({ dest: __dirname + '/uploads' });

// single file upload
router.post('/upload', upload.single('test'), function (req, res, next) {
    var file = req.file;
    console.log(file);
});

// multiple files
router.post('/photos/upload', upload.array('photos', 12), function (req, res, next) {
  // req.files is array of `photos` files 
  // req.body will contain the text fields, if there were any 
})
</code></pre>
",5388620,2016-02-26T06:21:57.947,0,CC BY-SA 3.0,,1,books,page</a,0,"This books seems to be obsolete, it's better to use the doc on the <a href=""https://www.npmjs.com/package/multer"" rel=""nofollow"">npmjs page</a",False,False
178,36154877,2,36118668,2016-03-22T12:50:42.170,1,"<p>The <code>BroadcastReceiver</code> has been deprecated and AWS have not updated their docs yet. But you can simply create an <code>Intent</code> from the supplied data <code>Bundle</code> in the <code>GcmListenerService</code>:</p>

<pre><code>@Override
public void onMessageReceived(String from, Bundle data) {
    CognitoSyncManager manager;
    try {
      manager = CognitoSyncClientManager.getInstance();
    } catch (IllegalStateException ise) {
      Log.w(TAG, ""sync manager is not initialized"");
      return;
    }

    Intent intent = new Intent();
    intent.putExtras(data);
    PushSyncUpdate update = manager.getPushSyncUpdate(intent);
    //...
</code></pre>
",6098584,2016-03-22T12:50:42.170,0,CC BY-SA 3.0,,1,>,.,1,> has been deprecated and AWS have not updated their docs yet.,False,False
179,36212531,2,36198573,2016-03-25T01:03:16.720,0,"<p>Unless you can do all activities in shell or CLI, it is not possible to do everything in the same instance.</p>

<p>One suggestion I can give is to move on to new technologies. AWS Data Pipeline is outdated (4 years old). You should use AWS Lambda which will cost you a fraction of what you are paying and you can load the files into Redshift as soon as the files are uploaded to S3. Clean up is automatic and Lambda is much more powerful than AWS Data Pipeline. The tutorial <a href=""https://blogs.aws.amazon.com/bigdata/post/Tx24VJ6XF1JVJAA/A-Zero-Administration-Amazon-Redshift-Database-Loader"" rel=""nofollow"">A Zero-Administration Amazon Redshift Database Loader</a> is the one you want. Yes, there is some learning curve, but as the title suggest it is a zero administration load.</p>
",4237701,2016-03-25T01:03:16.720,3,CC BY-SA 3.0,,1,Pipeline,.,0,AWS Data Pipeline is outdated (4 years old).,False,False
180,36343469,2,36342920,2016-03-31T20:47:41.677,7,"<p>You are trying to use <code>boto</code> library, which is rather obsolete and not maintained. The number of
issues with this library is growing.</p>

<p>Better use currently developed <code>boto3</code>.</p>

<p>First, let us define parameters of our search:</p>

<pre><code>&gt;&gt;&gt; bucket_name = ""bucket_of_m""
&gt;&gt;&gt; prefix = ""region/cz/""
</code></pre>

<p>Do import <code>boto3</code> and create s3 representing S3 resource:</p>

<pre><code>&gt;&gt;&gt; import boto3
&gt;&gt;&gt; s3 = boto3.resource(""s3"")
</code></pre>

<p>Get the bucket:</p>

<pre><code>&gt;&gt;&gt; bucket = s3.Bucket(name=bucket_name)
&gt;&gt;&gt; bucket
s3.Bucket(name='bucket_of_m')
</code></pre>

<p>Define filter for objects with given prefix:</p>

<pre><code>&gt;&gt;&gt; res = bucket.objects.filter(Prefix=prefix)
&gt;&gt;&gt; res
s3.Bucket.objectsCollection(s3.Bucket(name='bucket_of_m'), s3.ObjectSummary)
</code></pre>

<p>and iterate over it:</p>

<pre><code>&gt;&gt;&gt; for obj in res:
...     print obj.key
...     print obj.size
...     print obj.last_modified
...
</code></pre>

<p>Each <code>obj</code> is ObjectSummary (not Object itself), but it holds enought to learn something about it</p>

<pre><code>&gt;&gt;&gt; obj
s3.ObjectSummary(bucket_name='bucket_of_m', key=u'region/cz/Ostrava/Nadrazni.txt')
&gt;&gt;&gt; type(obj)
boto3.resources.factory.s3.ObjectSummary
</code></pre>

<p>You can get Object from it and use it as you need:</p>

<pre><code>&gt;&gt;&gt; o = obj.Object()
&gt;&gt;&gt; o
s3.Object(bucket_name='bucket_of_m', key=u'region/cz/rodos/fusion/AdvancedDataFusion.xml')
</code></pre>

<p>There are not so many options for filtering, but prefix is available.</p>
",346478,2016-03-31T20:47:41.677,2,CC BY-SA 3.0,,1,which,.,1,">boto</code> library, which is rather obsolete and not maintained.",False,False
181,36368873,2,31799589,2016-04-02T03:35:38.763,0,"<p>I have faced this very issue and tried the current accepted answer but it looks like Kubernetes is changing quite fast what may make this answer also outdated soon.</p>

<p>To this date, I've tested the solution below that might become or not a definitive solution in the future:</p>

<p>There is <a href=""https://github.com/kubernetes/kubernetes/pull/23776/files"" rel=""nofollow"">this PR on Kubernetes' github project</a> that implements an easy way to ignore the SSD storage by setting <code>KUBE_AWS_STORAGE=ebs</code> before running <code>kubernetes/cluster/kube-up.sh</code>.</p>

<p>Hope it is helpful!</p>
",1463744,2016-04-02T03:35:38.763,0,CC BY-SA 3.0,,1,,outdated,0,what may make this answer also outdated,False,False
182,36585871,2,32592571,2016-04-12T23:27:47.317,70,"<p>This was an incredibly difficult issue to deal with, for two reasons:</p>

<ol>
<li><p>The fact that CloudFront is <strong>mirroring</strong> our Rails app’s response headers requires you to twist your mind around.  The CORS protocol is hard enough to understand as it is, but now you have to follow it at two levels: between the browser and CloudFront (when our Rails app uses it as a CDN), and between the browser and our Rails app (when some malicious site wants to abuse us).</p>

<p>CORS is really about a dialog between the browser and the 3rd-party resources a web page wants to access.   (In our use-case, that’s the CloudFront CDN, serving assets for our app.)   But since CloudFront gets its Access-Control response headers <em>from</em> our app, our app needs to serve those headers <em>as if</em> it is CloudFront talking, and <em>simultaneously</em> not grant permissions that would expose itself to the type of abuse that led to the Same-Origin Policy / CORS being developed in the first place.  In particular, we should not grant <code>*</code> access to <code>*</code> resources on our site.</p></li>
<li><p>I found <strong>so much</strong> outdated information out there -- an endless line of blog posts and SO threads.   CloudFront has improved its CORS support significantly since many of those posts, although it is still not perfect.   (CORS should really be handled out-of-the-box.)  And the gems themselves have evolved.</p></li>
</ol>

<p>My setup: Rails 4.1.15 running on Heroku, with assets served from CloudFront.  My app responds to both http and https, on both ""www."" and the zone apex, without doing any redirection.</p>

<p>I looked briefly at the font_assets gem mentioned in the question, but quickly dropped it in favor of rack-cors, which seemed more on point.  I did not want to simply open up all origins and all paths, as that would defeat the point of CORS and the security of the Same-Origin Policy, so I needed to be able to specify the few origins I would allow.  Finally, I personally favor configuring Rails via individual <code>config/initializers/*.rb</code> files rather than editing the standard config files (like <code>config.ru</code> or <code>config/application.rb</code>)  Putting all that together, here is my solution, which I believe is the best available, as of 2016-04-16:</p>

<ol>
<li><p><strong>Gemfile</strong></p>

<pre><code>gem ""rack-cors""
</code></pre>

<p>The rack-cors gem implements the CORS protocol in a Rack middleware.
In addition to setting Access-Control-Allow-Origin and related headers on approved origins, it adds a <code>Vary: Origin</code> response header, directing CloudFront to cache the responses (including the response headers) for each origin separately.  This is crucial when our site is accessible via multiple origins (e.g. via both http and https, and via both ""www."" and the bare domain)</p></li>
<li><p><strong>config/initializers/rack-cors.rb</strong></p>

<pre><code>## Configure Rack CORS Middleware, so that CloudFront can serve our assets.
## See https://github.com/cyu/rack-cors

if defined? Rack::Cors
    Rails.configuration.middleware.insert_before 0, Rack::Cors do
        allow do
            origins %w[
                https://example.com
                 http://example.com
                https://www.example.com
                 http://www.example.com
                https://example-staging.herokuapp.com
                 http://example-staging.herokuapp.com
            ]
            resource '/assets/*'
        end
    end
end
</code></pre>

<p>This tells the browser that it may access resources on our Rails app (and by extension, on CloudFront, since it is mirroring us) only on behalf of our Rails app (and <em>not</em> on behalf of malicious-site.com) and only for <code>/assets/</code> urls (and <em>not</em> for our controllers).  In other words, allow CloudFront to serve assets but don't open the door any more than we have to.</p>

<p>Notes:</p>

<ul>
<li>I tried inserting this <em>after</em> rack-timeout instead of at the head of the middleware chain.
It worked on dev but was not kicking in on Heroku, despite
having the same middleware (other than Honeybadger).</li>
<li><p>The origins list could also be done as Regexps.
Be careful to anchor patterns at the end-of-string.</p>

<pre><code>origins [
    /\Ahttps?:\/\/(www\.)?example\.com\z/,
    /\Ahttps?:\/\/example-staging\.herokuapp\.com\z/
]
</code></pre>

<p>but I think it’s easier just to read literal strings.</p></li>
</ul></li>
<li><p><strong>Configure CloudFront to pass the browser's Origin request header on to our Rails app.</strong></p>

<p>Strangely, it appears that CloudFront forwards the Origin header from the browser to our Rails app <em>regardless</em> whether we add it here, but that CloudFront honors our app’s <code>Vary: Origin</code> caching directive only if Origin is explicitly added to the headers whitelist (as of April 2016).</p>

<p>The request header whitelist is kind of buried.</p>

<p>If the distribution already exists, you can find it at:</p>

<ul>
<li><a href=""https://console.aws.amazon.com/cloudfront/home#distributions"">https://console.aws.amazon.com/cloudfront/home#distributions</a></li>
<li>select the distribution</li>
<li>click Distribution Settings</li>
<li>go to the Behaviors tab</li>
<li>select the behavior (there will probably be only one)</li>
<li>Click Edit</li>
<li><strong>Forward Headers: Whitelist</strong></li>
<li><strong>Whitelist Headers:</strong> Select <strong>Origin</strong> and click <strong>Add >></strong></li>
</ul>

<p><br>
If you have not created the distribution yet, create it at:</p>

<ul>
<li><a href=""https://console.aws.amazon.com/cloudfront/home#distributions"">https://console.aws.amazon.com/cloudfront/home#distributions</a></li>
<li><p>Click Create Distribution</p>

<p>(For the sake of completeness and reproducibility, I'm listing all the settings I changed from the defaults, however the Whitelist settings are the only ones that are relevant to this discussion)</p></li>
<li><p>Delivery Method: Web (not RTMP)</p></li>
<li><p>Origin Settings</p>

<ul>
<li>Origin Domain Name: example.com</li>
<li>Origin SSL Protocols: TLSv1.2 ONLY</li>
<li>Origin Protocol Policy: HTTPS only</li>
</ul></li>
<li><p>Default Cache Behavior Settings</p>

<ul>
<li>Viewer Protocol Policy: Redirect HTTP to HTTPS</li>
<li><strong>Forward Headers: Whitelist</strong></li>
<li><strong>Whitelist Headers:</strong> Select <strong>Origin</strong> and click <strong>Add >></strong></li>
<li>Compress Objects Automatically: Yes</li>
</ul></li>
</ul></li>
</ol>

<p>After changing all these things, remember that it can take some time for any old, cached values to expire from CloudFront.  You can explicitly invalidate cached assets by going to the CloudFront distribution's Invalidations tab and creating an invalidation for <code>*</code>.</p>
",577438,2016-04-12T23:27:47.317,12,CC BY-SA 3.0,,1,,  ,0,I found <strong>so much</strong> outdated information out there -- an endless line of blog posts and SO threads.   ,False,False
183,36655183,2,35601198,2016-04-15T19:14:08.667,0,"<p>I've had this problem before and it turned out the EC2 I was on had an old version of docker. There was no error, it just died trying to transfer stuff. Eventually I found an error buried in some system log that mentioned that my docker version was deprecated so I updated Docker and everything magically worked.</p>

<p>In a nutshell: try using the latest ECS optimized image for your region from <a href=""http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html</a> and see if that fixes the problem.</p>
",5748922,2016-04-15T19:14:08.667,0,CC BY-SA 3.0,,1,,deprecated,0,Eventually I found an error buried in some system log that mentioned that my docker version was deprecated,False,False
184,36701425,2,36701317,2016-04-18T18:15:40.890,2,"<p>There is a class with the same name in another package which is not <code>@Deprecated</code>:</p>

<p><a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/cloudfront/AmazonCloudFrontClient.html"" rel=""nofollow"">Docs for <code>com.amazonaws.services.cloudfront.AmazonCloudFrontClient</code></a></p>

<p>The deprecated class is in the namespace <code>com.amazonaws.services.cloudfront_2012_03_15.AmazonCloudFrontClient</code>.</p>
",4464702,2016-04-18T18:15:40.890,0,CC BY-SA 3.0,,1,,"
",0,"The deprecated class is in the namespace <code>com.amazonaws.services.cloudfront_2012_03_15.AmazonCloudFrontClient</code>.</p>
",False,False
185,36706006,2,23663753,2016-04-18T23:29:56.733,-1,"<p>Hot-swapping out the data tier within an environment is discouraged because it breaks down the integrity of the environment. What you want to do is clone the environment, with a restored snapshot of the RDS instance. This means you'll have an identical environment with a different url 'host', and if everything went without a hitch, then you can swap environment urls in order to initiate a DNS swap.</p>

<p>After the swap happens and everything is good to go, you can proceed to deflate the old environment</p>
",180616,2016-04-18T23:29:56.733,1,CC BY-SA 3.0,,1,swapping,.,0,Hot-swapping out the data tier within an environment is discouraged because it breaks down the integrity of the environment.,False,True
186,36708152,2,36706182,2016-04-19T03:42:17.277,1,"<p>It's likely that your kubectl client is out of date, because your command line works for me:</p>

<pre><code>$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""2"", GitVersion:""v1.2.0"", GitCommit:""5cb86ee022267586db386f62781338b0483733b3"", GitTreeState:""clean""}
Server Version: version.Info{Major:""1"", Minor:""2"", GitVersion:""v1.2.2"", GitCommit:""528f879e7d3790ea4287687ef0ab3f2a01cc2718"", GitTreeState:""clean""}

$ kubectl run -i --tty busybox --image=busybox --restart=Never
Waiting for pod default/busybox-dikev to be running, status is Pending, pod ready: false

Hit enter for command prompt

/ #
</code></pre>
",4215791,2016-04-19T03:42:17.277,0,CC BY-SA 3.0,,1,,me:</p,0,"It's likely that your kubectl client is outdated, because your command line works for me:</p",False,False
187,36988951,2,36964256,2016-05-02T18:15:19.697,0,"<p>This line has a bug:</p>

<pre><code>AWSSNS *snsManager = [[AWSSNS new] initWithConfiguration:configuration];
</code></pre>

<p><code>- new</code> is an equivalent of <code>- alloc</code> plus <code>- init</code>. You are calling two <code>init</code> methods. It should be changed to:</p>

<pre><code>AWSSNS *snsManager = [[AWSSNS alloc] initWithConfiguration:configuration];
</code></pre>

<p>Also, <code>- initWithConfiguration:</code> had been deprecated for a while, and now it's removed from the SDK. You need to use <a href=""http://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSSNS.html#//api/name/defaultSNS"" rel=""nofollow""><code>+ defaultSNS</code></a> or <a href=""http://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSSNS.html#//api/name/SNSForKey:"" rel=""nofollow""><code>+ SNSForKey:</code></a> instead.</p>
",1721492,2016-05-02T18:15:19.697,0,CC BY-SA 3.0,,1,initWithConfiguration:</code,.,0,"Also, <code>- initWithConfiguration:</code> had been deprecated for a while, and now it's removed from the SDK.",False,False
188,37360108,2,37335496,2016-05-21T07:23:05.403,1,"<p>The <a href=""https://www.rabbitmq.com/management.html"" rel=""nofollow"">RabbitMQ management client</a> plug-in has an api to get a list of all available queues: <code>/api/queues</code></p>

<p>Follow the link to the API on that page for more detail (haven't placed link in answer because link will fast go out of date with new API releases).</p>
",4151019,2016-05-21T07:23:05.403,0,CC BY-SA 3.0,,1,link,API,1,>Follow the link to the API on that page for more detail (haven't placed link in answer because link will fast go outdated with new API,False,False
189,37363867,2,37347003,2016-05-21T13:59:28.177,2,"<p>Your demo repo does not appear to be including the AWS SDK &amp; setting the region as noted in the <a href=""https://github.com/awslabs/dynamodb-document-js-sdk"" rel=""nofollow"">Getting Started guide</a>.  I.e.:</p>

<pre><code>var AWS = require(""aws-sdk"");
var DOC = require(""dynamodb-doc"");

AWS.config.update({region: ""us-west-1""});
var docClient = new DOC.DynamoDB();

... 
</code></pre>

<p>Note that <code>dynamo-doc</code> was deprecated almost a year ago.  You may want to try the <a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html"" rel=""nofollow"">DynamoDB DocumentClient</a> instead.  This updated API has much more clear error-handling semantics that will probably help point out where the problem is.</p>
",467512,2016-05-21T13:59:28.177,2,CC BY-SA 3.0,,1,>, ,0,> was deprecated almost a year ago.  ,False,False
190,37365182,2,37365009,2016-05-21T16:04:00.787,6,"<p>I'm looking at the latest API docs <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html"" rel=""noreferrer"">here</a>, and it looks like only <code>AWSLambdaAsyncClient.invokeAsyncAsync()</code> is deprecated. The <code>AWSLambdaAsyncClient.invokeAsync()</code> method is not marked as deprecated. It looks like they are just doing some code cleanup by removing the need for the <code>InvokeAsyncRequest</code> and <code>InvokeAsyncResult</code> classes and the extra <code>invokeAsyncAsync()</code> methods.</p>

<p>You should be able to use the <code>AWSLambdaAsyncClient.invokeAsync()</code> method which uses <code>InvokeRequest</code> and returns <code>InvokeResult</code>. You might have to set the <code>InvocationType</code> on the <code>InvokeRequest</code> to <code>InvocationType.Event</code>. It's not clear if that's needed if you are using the Async client.</p>

<p>Regarding your second question about calling Lambda functions asynchronously without using the SDK, I would look into using <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-lambda.html"" rel=""noreferrer"">API Gateway as a service proxy</a>. This is the recommended way to expose Lambda functions for asynchronous calls.</p>
",13070,2016-05-21T16:04:00.787,1,CC BY-SA 3.0,,2,>,.,1,AWSLambdaAsyncClient.invokeAsyncAsync()</code> is deprecated.,False,False
191,37556167,2,36917382,2016-05-31T21:48:00.117,2,"<p>I'm not sure if the MongoDB documentation is outdated, or just has a typo, but I think the correct file/directory is <code>/etc/yum.repos.d/mongodb.repo</code>.</p>

<p>So the full command would be:</p>

<pre><code>echo ""[MongoDB]
name=MongoDB Repository
baseurl=http://downloads-distro.mongodb.org/repo/redhat/os/x86_64
gpgcheck=0
enabled=1"" | sudo tee -a /etc/yum.repos.d/mongodb.repo
</code></pre>
",1310642,2016-05-31T21:48:00.117,0,CC BY-SA 3.0,,1,,mongodb.repo</code>.</p,1,"not sure if the MongoDB documentation is outdated, or just has a typo, but I think the correct file/directory is <code>/etc/yum.repos.d/mongodb.repo</code>.</p",False,False
192,37735884,2,37731116,2016-06-09T20:48:37.763,0,"<p>You are trying to use </p>

<pre><code>let qe = AWSDynamoDBQueryExpression()
qe.hashKeyAttribute = ""id""
qe.hashKeyValues = id
</code></pre>

<p>which is deprecated. I would suggest you use 'keyConditionExpression' and 'expressionAttributeValues' instead.</p>

<p>Thanks,
Rohan</p>
",5045692,2016-06-09T20:48:37.763,0,CC BY-SA 3.0,,1,,.,0,>which is deprecated.,False,False
193,37757550,2,37753691,2016-06-10T21:42:57.407,1,"<p>Apparently we have in out .NET 4.0 project the following references to </p>

<ol>
<li>Elasticsearch.Net - official low level elasticsearch client
packages\Elasticsearch.Net.2.3.2\lib\net45\Elasticsearch.Net.dll</li>
<li>NEST - official high level elasticsearch client
packages\NEST.2.3.2\lib\net45\Nest.dll</li>
</ol>

<p>interesting fact they still use deprecated queries. as mentioned by Frederick here</p>

<blockquote>
  <p>the client you're using but the server is 1.5.2 (since that is the
  only version provided by the aws elasticsearch service, which you
  appear to be using)</p>
</blockquote>

<p>see below - it works:</p>

<pre><code>            var query = new QueryContainer[8];//Size is based on hte number of search parameters
            var descriptor = new QueryContainerDescriptor&lt;EsNoteModel&gt;();

/*........... skipping here some code for other parameters ..........*/

 if (hasFollowUpDate.HasValue)
            {
                if ((bool) hasFollowUpDate)
                {
                    //If true shows only with a non-blank follow-up date
                    query[7] = descriptor.Filtered(p =&gt; p.Filter(f =&gt; f.Exists(r =&gt; r.Field(u =&gt; u.FollowUpDateTime))));
                }
                else
                {
                    ////If false shows only notes with a blank follow-up date
                    query[7] = descriptor.Filtered(p =&gt; p.Filter(f =&gt; f.Missing(r =&gt; r.Field(u =&gt; u.FollowUpDateTime))));
                }
            }

            var result = ElasticSearchClient.Search&lt;EsNoteModel&gt;(body =&gt; body
                .From(offset - 1)
                .Size(rows)
                .Query(q =&gt; q
                    .Bool(b =&gt; b
                          .Must(query)
                          )
                        )

                .Sort(s =&gt; s
                    .Field(f =&gt; f
                        .Field(p =&gt; p.NoteDate)
                        .Order(SortOrder.Descending)))

                );
</code></pre>

<p>So simply replacing word BOOL to Filtered - did the trick</p>
",2262047,2016-06-10T21:42:57.407,1,CC BY-SA 3.0,,1,,.,0,>interesting fact they still use deprecated queries.,False,False
194,37985081,2,37898686,2016-06-23T07:37:45.293,1,"<p>You're ember.js configuration file defines <code>rootURL</code> to be <code>""https://cdn.lorem.io""</code> which is wrong. When accessing lorem.io your <code>rootURL</code> is supposed to be <code>""https://www.lorem.io""</code>.</p>

<p>Perhaps you meant <code>baseURL</code>. See <a href=""https://til.hashrocket.com/posts/d1aff13969-the-difference-between-rooturl-and-baseurl"" rel=""nofollow"">this explanation</a> for the difference between <code>rootURL</code> and <code>baseURL</code>.</p>

<p>Warning: Keep in mind, that <code>baseURL</code> <a href=""http://emberjs.com/blog/2016/04/28/baseURL.html"" rel=""nofollow"">gets deprecated</a> in ember-cli 2.7.</p>
",2471991,2016-06-23T07:37:45.293,0,CC BY-SA 3.0,,1,,deprecated</a,0,"/baseURL.html"" rel=""nofollow"">gets deprecated</a",False,False
195,38136519,2,35977787,2016-07-01T03:20:25.763,0,"<p>According to user MichaelB@AWS (Amazon staff account):</p>

<blockquote>
  <p>For the issues with ORDS, you should skip (by entering ""2"") steps
  involving the ORDS schema and PL/SQL gateway. The ORDS schema is not
  required to support the APEX RESTful listener, and is not currently
  supported on RDS Oracle. The specific options may vary between
  versions of ORDS.
  <a href=""https://forums.aws.amazon.com/thread.jspa?messageID=711534"" rel=""nofollow"">https://forums.aws.amazon.com/thread.jspa?messageID=711534</a></p>
</blockquote>

<p>The APEX+APEX_DEV options do not create the correct ORDS users, in particular there's no ORDS_METADATA created which is where most of the important ORDS packages and data are stored.  Without SYSDBA (not available on RDS) you won't be able to install ORDS either.</p>

<p>From the documentation:
<a href=""http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.Options.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.Options.html</a>
the minimal ORDS.war configuration and associated schemas needed to get APEX working is available on Amazon's RDS as a replacement for the deprecated APEX Express Listener.  It isn't a complete install of ORDS for RESTful services.</p>

<p>The solutions are:</p>

<ol>
<li>Wait for Amazon to support ORDS on RDS</li>
<li>Install Oracle yourself on an EC2 instance and forget RDS</li>
</ol>
",148346,2016-07-01T03:20:25.763,1,CC BY-SA 3.0,,1,, ,0,the minimal ORDS.war configuration and associated schemas needed to get APEX working is available on Amazon's RDS as a replacement for the deprecated APEX Express Listener.  ,False,False
196,38312136,2,38288297,2016-07-11T16:46:11.080,1,"<p>The root cause is NetworkOnMainThreadException. In short, you need to invoke mapper.save() from a background thread, say wrapping it in an AsyncTask.</p>

<blockquote>
  <p>The exception that is thrown when an application attempts to perform a
  networking operation on its main thread.</p>
  
  <p>This is only thrown for applications targeting the Honeycomb SDK or
  higher. Applications targeting earlier SDK versions are allowed to do
  networking on their main event loop threads, but it's heavily
  discouraged. See the document Designing for Responsiveness.</p>
</blockquote>
",2026747,2016-07-11T16:46:11.080,0,CC BY-SA 3.0,,1,it,.,0,"Applications targeting earlier SDK versions are allowed to do
  networking on their main event loop threads, but it's heavily
  discouraged.",False,False
197,38313021,2,38307640,2016-07-11T17:41:18.890,-1,"<p>I think the <code>s3n://</code> URL style has been deprecated and/or removed. </p>

<p>Try defining your keys as <code>""fs.s3.awsAccessKeyId""</code>.</p>
",139490,2016-07-11T17:41:18.890,2,CC BY-SA 3.0,,1,style,.,0,URL style has been deprecated and/or removed.,False,False
198,38337569,2,38336752,2016-07-12T19:39:20.463,0,"<p>Try this and let me know if it works, I know from experience that DynamoDB is very painful to filter. </p>

<pre><code>result = dynamodb.scan(
  table_name: 'my_table',
  expression_attribute_values: { 
    ':one' =&gt; 1,
    ':two' =&gt; 2,
    ':three' =&gt; 3,
    ':four' =&gt; 4
  },
  filter_expression: 'contains(numbers, :one) OR contains(numbers, :two) OR contains(numbers, :three) OR contains(numbers, :four)'
)
</code></pre>

<p>I can't think of anything simpler currently, the method you linked is marked as deprecated, instead you should use <code>expression_attribute_values</code> and <code>filter_expression</code>.</p>
",4412925,2016-07-12T19:39:20.463,3,CC BY-SA 3.0,,1,,code,0,", the method you linked is marked as deprecated, instead you should use <code",False,False
199,38405443,2,38402419,2016-07-15T21:47:21.737,0,"<p>Your use-case is atypical. Usually you don't want the hash and range key of an object to change. In fact I would go as far as to say it's discouraged.</p>

<p>In fact, with DynamoDB an item is uniquely characterized by it's key (whether partition key only, or partition + range keys together) so changing the range key essentially creates a new item which is technically not an update.</p>

<p>I'm not sure if this accurately represents <em>the</em> reason why mixing <code>@DynamoDBAutoGeneratedTimestamp</code> and <code>@DynamoDBRangeKey</code> is not supported in the SDK but it's certainly a reason.</p>
",63074,2016-07-15T21:47:21.737,0,CC BY-SA 3.0,,1,,discouraged.</p,0,In fact I would go as far as to say it's discouraged.</p,False,False
200,38459520,2,38459327,2016-07-19T13:06:10.420,0,"<p>Docker switched to DNS based lookups a while back instead of adding entries to /etc/hosts. Linking is also discouraged in favor of using a common network for the containers.</p>
",596285,2016-07-19T13:06:10.420,4,CC BY-SA 3.0,,1,Linking,containers.</p,0,Linking is also discouraged in favor of using a common network for the containers.</p,False,False
201,38677287,2,38665929,2016-07-30T18:35:24.320,0,"<p>AWS constantly updates its Elastic Beanstalk solution stacks and makes its old stacks obsolete, so you will need to change your solution stack string to the currently-supported version. See the <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html#concepts.platforms.nodejs"" rel=""nofollow"">Supported Platforms (Node.js)</a> page for the latest supported solution stack name. This is currently (as of 7/30/2016) <code>64bit Amazon Linux 2016.03 v2.1.3 running Node.js</code>, and will likely change again in the future.</p>
",2518355,2016-07-30T18:35:24.320,1,CC BY-SA 3.0,,1,,.,0,">AWS constantly updates its Elastic Beanstalk solution stacks and makes its old stacks obsolete, so you will need to change your solution stack string to the currently-supported version.",False,False
202,38895935,2,38887061,2016-08-11T12:06:33.217,0,"<p>My guess would be that you missed a step on setup. There's one where you have to set the ""event source"". IF you don't do that, I think you get that message.</p>

<p>But the debug options are limited. I wrote EchoSim (the original one on GitHub) before the service simulator was written and, although it is a bit out of date, it does a better job of giving diagnostics.</p>

<p>Lacking debug options, the best is to do what you've done. Partition and re-test. Do static replies until you can work out where the problem is.</p>
",553903,2016-08-11T12:06:33.217,1,CC BY-SA 3.0,,1,I,p,0,"I wrote EchoSim (the original one on GitHub) before the service simulator was written and, although it is a bit outdated, it does a better job of giving diagnostics.</p>

<p",False,False
203,38986651,2,38897266,2016-08-17T01:04:32.693,1,"<p>API gateway now has native integration with 'Cognito Your User Pool', so you can pass the identity token directly - <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html"" rel=""nofollow"">api gateway docs</a>. The post you have linked is outdated</p>
",6031617,2016-08-17T01:04:32.693,2,CC BY-SA 3.0,,1,,"
",0,"The post you have linked is outdated</p>
",False,False
204,39088822,2,38214633,2016-08-22T21:13:59.777,7,"<p>If your JSON is uniformly structured I would advise you to give Spark the schema for your JSON files and this should speed up processing tremendously.</p>

<p>When you don't supply a schema Spark will read all of the lines in the file first to infer the schema which, as you have observed, can take a while.</p>

<p>See this documentation for how to create a schema: <a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema"" rel=""noreferrer"">http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema</a></p>

<p>Then you'd just have to add the schema you created to the jsonFile call:</p>

<pre><code>val df = sqlContext.jsonFile(""s3://testData/*/*/*"", mySchema)
</code></pre>

<p>At this time (I'm using Spark 1.6.2) it seems as if <code>jsonFile</code> has been deprecated, so switching to <code>sqlContext.read.schema(mySchema).json(myJsonRDD)</code> (where <code>myJsonRDD</code> is of type <code>RDD[String]</code>) might be preferable.</p>
",308013,2016-08-22T21:13:59.777,2,CC BY-SA 3.0,,1,jsonFile</code,>,0,">jsonFile</code> has been deprecated, so switching to <code>",False,False
205,39239314,2,39239035,2016-08-31T01:16:13.923,1,"<p>When you say the call succeeds with no error i am assuming you mean the call to the lambda </p>

<p>If you are using node version 4.3 the way you are returning from the function is deprecated as seen from this extract from the aws lambda documentation</p>

<blockquote>
  <p>The Node.js runtime v4.3 supports the optional callback parameter. You >can use it to explicitly return information back to the caller. The general syntax is:</p>
  
  <blockquote>
    <p>callback(Error error, Object result);</p>
  </blockquote>
  
  <p>Using the callback parameter is optional. If you don't use the optional callback parameter, the behavior is same as if you called the callback() without any parameters. You can specify the callback in your code to return information to the caller.
  If you don't use callback in your code, AWS Lambda will call it implicitly and the return value is null.</p>
  
  <blockquote>
    <p>This is the correct way to return from an aws lambda function if using node version 4.3</p>
  </blockquote>
</blockquote>

<p>first add a 3rd parameter to the handler function like so</p>

<pre><code>    exports.handler = function(event,context,callback)
</code></pre>

<p>then when returning from the function follow this form</p>

<pre><code>    function(err,data){
        if(err){
             callback(err);
        } else {
             callback(null,data);
        }
    }
</code></pre>
",6017886,2016-08-31T01:16:13.923,1,CC BY-SA 3.0,,1,,lambda,0,>If you are using node version 4.3 the way you are returning from the function is deprecated as seen from this extract from the aws lambda,False,False
206,39272105,2,39260811,2016-09-01T12:53:37.187,0,"<p>When I ran the code you provided, I got the following returned:</p>

<pre><code>$ aws --version
aws-cli/1.10.60 Python/2.7.11 Darwin/15.6.0 botocore/1.4.50 
$ aws deploy create-deployment --application-name ...snip

An error occurred (ApplicationDoesNotExistException) when calling the CreateDeployment operation: No application found for name: MyApp
</code></pre>

<p>And this(AWS server side error) is the expected behaviour(I guess).</p>

<p>One possibility that your CLI raised the validation error is that your AWS CLI is out of date.</p>

<p>Which version is your AWS CLI?</p>
",2239016,2016-09-01T12:53:37.187,5,CC BY-SA 3.0,,1,,outdated.</p,0,One possibility that your CLI raised the validation error is that your AWS CLI is outdated.</p,False,False
207,39292876,2,10528540,2016-09-02T12:50:26.450,2,"<p>Updating this for Apache Hadoop 2.7+, and ignoring Amazon EMR as they've changed things there.</p>

<ol>
<li>If you are using Hadoop 2.7 or later, use s3a over s3n. This also applies to recent versions of HDP and, AFAIK, CDH.</li>
<li>This supports 5+GB files, has other nice features, etc. It is tangibly better when reading files —and will only get better over time.</li>
<li>Apache s3:// should be considered deprecated -you don't need it any more, and shouldn't be using it.</li>
<li>Amazon EMR use ""s3://"" to refer to their own, custom, binding to S3. That's what you should be using if you are running on EMR.</li>
</ol>

<p>Improving distcp reliability and performance working with object stores is still and ongoing piece of work...contributions are, as always, welcome.</p>
",2261274,2016-09-02T12:50:26.450,0,CC BY-SA 3.0,,1,,deprecated,0,Apache s3:// should be considered deprecated,False,False
208,39359930,2,39358139,2016-09-07T01:18:33.033,2,"<p>try changing it to this:
<code>
                public void progressChanged(ProgressEvent progressEvent) {
                    LOG.info(myUpload.getProgress().getPercentTransferred() + ""%"");
                    LOG.info(""progressEvent.getEventCode():"" + progressEvent.getEventType());
                    if (progressEvent.getEventType() == ProgressEventType.TRANSFER_COMPLETED_EVENT) {
                        LOG.info(""Upload complete!!!"");
                    }
                }
</code></p>

<p>It looks like you are running some deprecated code.</p>

<p>In <code>com.amazonaws.event.ProgressEventType</code>, value 8 refers to <code>HTTP_REQUEST_COMPLETED_EVENT</code></p>

<ul>
<li><code>COMPLETED_EVENT_CODE</code> is deprecated</li>
<li><code>getEventCode</code> is deprecated</li>
</ul>

<p>refer to this -> <a href=""https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/java/com/amazonaws/event/ProgressEvent.java"" rel=""nofollow"">https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/java/com/amazonaws/event/ProgressEvent.java</a></p>
",96332,2016-09-07T01:18:33.033,1,CC BY-SA 3.0,,3,,code.</p,0,It looks like you are running some deprecated code.</p,False,False
209,39376228,2,39362847,2016-09-07T17:42:36.347,1,"<p>Unfortunately, CodeDeploy doesn't have a good/elegant way to handle those obsolete revisions at the moment. It'd be great if there is an overwrite option when bitbucket pushes to S3.</p>
",2288838,2016-09-07T17:42:36.347,0,CC BY-SA 3.0,,1,,.,1,"Unfortunately, CodeDeploy doesn't have a good/elegant way to handle those obsolete revisions at the moment.",False,False
210,39459919,2,39451096,2016-09-12T22:56:24.520,1,"<p>I had the exact same problem.  Turned out that my dependency was out of date: jackson-databind.  Update to 2.6.6 or later.</p>
",4040254,2016-09-12T22:56:24.520,0,CC BY-SA 3.0,,1,, ,0,Turned out that my dependency was outdated: jackson-databind.  ,False,True
211,39499329,2,39362847,2016-09-14T20:41:12.363,1,"<p>CodeDeploy is purely a deployment tool, it cannot handle the revisions in S3 bucket.</p>

<p>I would recommend you look into the ""lifecycle management"" for S3. Since you are using version controlled bucket (I assume), there is always one latest version and 0 to many obsolete version. You can set a lifecycle configuration of type ""NoncurrentVersionExpiration"" so that the obsolete version will be deleted after some days.</p>

<p>This method is still not possible to maintain a fixed number of deployments as AWS only allows specifying lifecycle in number of days. But it's probably the best alternative to your use-case. </p>

<p>[1] <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-set-lifecycle-configuration-intro.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-set-lifecycle-configuration-intro.html</a>
[2] <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/intro-lifecycle-rules.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonS3/latest/dev/intro-lifecycle-rules.html</a></p>
",2939614,2016-09-14T20:41:12.363,1,CC BY-SA 3.0,,2,,.,0,"Since you are using version controlled bucket (I assume), there is always one latest version and 0 to many obsolete version.",False,False
212,39543998,2,39543404,2016-09-17T07:17:32.023,0,"<p>One thing - a StringSet is a value type for a field of a DynamoDB item, and not something you can use to query for multiple things at once.</p>

<p>I looked around a bit and found some methods that would work elegantly with your design, but they're deprecated. What I would recommend is building up the query string yourself. For example...</p>

<pre><code>String[] countries = {""Japan"", ""Vietnam"", ""Thailand""};

List&lt;String&gt; queries = new ArrayList&lt;&gt;();
Map&lt;String, AttributeValue&gt; eav = new HashMap&lt;&gt;();

for (Integer i = 0; i &lt; countries.length; i++) {
    String placeholder = "":val"" + i;
    eav.put(placeholder, new AttributeValue().withS(countries[i]));
    queries.add(""Place = "" + placeholder);
}

String query = String.join("" or "", queries);

DynamoDBQueryExpression queryExpression = new DynamoDBQueryExpression()
    .withFilterExpression(query)
    .withExpressionAttributeValues(eav)
    .withHashKeyValues(someHashValue);
</code></pre>

<p>I haven't tested it, but this is the sort of thing you'd be looking at doing (unless there's a better way that I still haven't seen).</p>
",433380,2016-09-17T07:17:32.023,1,CC BY-SA 3.0,,1,,.,0,"I looked around a bit and found some methods that would work elegantly with your design, but they're deprecated.",False,False
213,39702583,2,39696264,2016-09-26T12:17:10.847,0,"<p>Maybe you should paste some code here. Also, there are two versions of AWS SDK, are you using the latest version, or the deprecated version?</p>
",2537914,2016-09-26T12:17:10.847,0,CC BY-SA 3.0,,1,,version?</p,0,"Also, there are two versions of AWS SDK, are you using the latest version, or the deprecated version?</p",False,False
214,39779480,2,39737414,2016-09-29T20:32:17.197,2,"<p>The accepted solution works, but the method <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3.html#changeObjectStorageClass-java.lang.String-java.lang.String-com.amazonaws.services.s3.model.StorageClass-"" rel=""nofollow"">changeObjectStorageClass</a> is actually deprecated.
The deprecation note suggests to use the method <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3.html#copyObject-com.amazonaws.services.s3.model.CopyObjectRequest-"" rel=""nofollow"">copyObject</a>, as in the following snippet:</p>

<pre><code>    CopyObjectRequest copyObjectRequest= 
        new CopyObjectRequest(bucket, key, bucket, key)
        .withStorageClass(StorageClass.StandardInfrequentAccess);
    s3.copyObject(copyObjectRequest);
</code></pre>

<p>I could not notice any performance difference between the two approaches.</p>
",391346,2016-09-29T20:32:17.197,2,CC BY-SA 3.0,,1,"rel=""nofollow"">changeObjectStorageClass</a","
",0,"StorageClass-"" rel=""nofollow"">changeObjectStorageClass</a> is actually deprecated.
",False,False
215,39834740,2,39813634,2016-10-03T15:06:13.050,0,"<p>First thing, <code>s3n://</code> is now deprecated, start using <code>s3://</code> for S3 paths.</p>

<p>Secondly, if you're merely copying a file into S3 from a local file on your cluster, you can use <code>aws s3 cp</code>:</p>

<pre><code>aws s3 cp /user/hive/warehouse/abc.text s3://bucket/abc.text
</code></pre>
",680578,2016-10-03T15:06:13.050,0,CC BY-SA 3.0,,1,,paths.</p,0,"is now deprecated, start using <code>s3://</code> for S3 paths.</p",False,False
216,39844490,2,39844092,2016-10-04T05:03:08.483,0,"<p>The most obvious issue in your code is that HTTPS is served from port 443.  Besides that looks like outdated code.  Why not use a recent version of Express?  If you look at the example for HTTPS they give here its pretty different from what you wrote: <a href=""http://expressjs.com/en/api.html"" rel=""nofollow"">http://expressjs.com/en/api.html</a> search for 'HTTPS'</p>

<pre><code>var express = require('express');
var https = require('https');
var http = require('http');
var app = express();

http.createServer(app).listen(80);
https.createServer(options, app).listen(443);
</code></pre>
",281920,2016-10-04T05:03:08.483,1,CC BY-SA 3.0,,1,, ,0,Besides that looks like outdated code.  ,False,False
217,39966132,2,39778587,2016-10-10T20:20:51.140,10,"<p>The solution I found for this problem is to update Hadoop to 2.7 and set </p>

<pre><code>spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2
</code></pre>

<p>In Spark 1.6 there was an alternative version of the fileoutputcommiter that wrote directly to s3, but it got deprecated in spark 2.0.0: <a href=""https://issues.apache.org/jira/browse/SPARK-10063"" rel=""noreferrer"">https://issues.apache.org/jira/browse/SPARK-10063</a></p>
",1560163,2016-10-10T20:20:51.140,2,CC BY-SA 3.0,,1,,<,0,"In Spark 1.6 there was an alternative version of the fileoutputcommiter that wrote directly to s3, but it got deprecated in spark 2.0.0: <",False,False
218,39989331,2,27677113,2016-10-12T01:53:31.893,1,"<p>I believe the GUI only shows parameters that can be changed, so no, you cannot change that on RDS.</p>

<p>Also, it seems this parameter is deprecated for current versions of MySQL. According to the official documentation on MySQL site:</p>

<blockquote>
  <p><a href=""http://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_locks_unsafe_for_binlog"" rel=""nofollow"">http://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_locks_unsafe_for_binlog</a></p>
  
  <p>As of MySQL 5.6.3, innodb_locks_unsafe_for_binlog is deprecated and
  will be removed in a future MySQL release.</p>
</blockquote>

<p>So that would be a likely reason why this parameter cannot be changed.</p>

<p>Cheers,</p>
",6088494,2016-10-12T01:53:31.893,0,CC BY-SA 3.0,,2,,.,0,"Also, it seems this parameter is deprecated for current versions of MySQL.",False,False
219,40098463,2,40083329,2016-10-18T02:14:26.410,2,"<p>What type of EC2 instance are you using?  The error is pretty descriptive and indicates that CM is unable to access memory. Maybe you are using an instance type with too little RAM. </p>

<p>Also - the docs you are referencing are out of date. The latest docs on deploying CDH5 in the cloud can be found here: <a href=""https://www.cloudera.com/documentation/director/latest/topics/director_get_started_aws.html"" rel=""nofollow"">https://www.cloudera.com/documentation/director/latest/topics/director_get_started_aws.html</a></p>

<p>These docs also recommend using Cloudera Director which will simplify much of the deployment and configuration of your cluster. </p>
",7034040,2016-10-18T02:14:26.410,2,CC BY-SA 3.0,,1,docs,.,0,>Also - the docs you are referencing are outdated.,False,False
220,40275134,2,30793481,2016-10-27T02:26:54.810,0,"<p><code>Interface DynamoDBMarshaller&lt;T extends Object&gt;</code> is deprecated already, the replacement is <code>Interface DynamoDBTypeConverter&lt;S,T&gt;</code>.</p>

<p>Inside your model class, add the annotation to your list of objects.</p>

<pre><code>@DynamoDBTypeConverted(converter = PhoneNumberConverter.class)
   public List&lt;MyObject&gt; getObjects() { return this.objects; }
</code></pre>

<p>public void setObjects(List objects) { this.objects = objects; }</p>

<p>And this is the implementation of <code>DynamoDBTypeConverter</code>.</p>

<pre><code>public class PhoneNumberConverterimplements DynamoDBTypeConverter&lt;String, PhoneNumber&gt;
{
    private static final ObjectMapper mapper = new ObjectMapper();
    private static final ObjectWriter writer = mapper.writerWithType(new TypeReference&lt;List&lt;MyObject&gt;&gt;(){});
    @Override
    public String convert(List&lt;MyObject&gt; obj) {
               try {
            return writer.writeValueAsString(obj);
        } catch (JsonProcessingException e) {
            System.out.println(
                    ""Unable to marshall the instance of "" + obj.getClass()
                    + ""into a string"");
            return null;
        }
    }

    @Override
    public List&lt;MyObject&gt; unconvert(String s) {
        TypeReference&lt;List&lt;MyObject&gt;&gt; type = new TypeReference&lt;List&lt;MyObject&gt;&gt;() {};
        try {
            List&lt;MyObject&gt; list = mapper.readValue(s, type);
            return list;
        } catch (Exception e) {
            System.out.println(""Unable to unmarshall the string "" + s
                             + ""into "" + s);
            return null;
        }
    }  
}
</code></pre>
",5353589,2016-10-27T02:26:54.810,0,CC BY-SA 3.0,,1,,code,0,"> is deprecated already, the replacement is <code",False,False
221,40480087,2,40479737,2016-11-08T06:08:00.430,4,"<p>You CLI version is outdated by 3 years and it doesn't know the new regions. Can you upgrade the CLI to <strong>1.10.x</strong> and try?</p>

<pre><code>$ aws --version
aws-cli/1.10.66 Python/2.7.12 Linux/3.14.35-28.38.amzn1.x86_64 botocore/1.4.56

$ aws ec2 describe-regions
{
    ""Regions"": [
        {
            ""Endpoint"": ""ec2.us-east-2.amazonaws.com"",
            ""RegionName"": ""us-east-2""
        },
</code></pre>
",4237701,2016-11-08T06:08:00.430,1,CC BY-SA 3.0,,1,version,.,1,You CLI version is outdated by 3 years and it doesn't know the new regions.,False,False
222,40485886,2,6271222,2016-11-08T11:35:04.423,1,"<p><a href=""https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/core-default.xml"" rel=""nofollow noreferrer"">https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/core-default.xml</a></p>

<p>fs.default.name is deprecated, and maybe fs.defaultFS is better.</p>
",6037289,2016-11-08T11:35:04.423,1,CC BY-SA 3.0,,1,fs.default.name,is,0,">fs.default.name is deprecated, and maybe fs.defaultFS is",False,False
223,40508134,2,40504704,2016-11-09T13:26:35.277,0,"<p>Yes in latest  SDK <strong>logins</strong> property get deprecated, so we need to assign <strong>IdentityProvider</strong> to <strong>logins</strong> by using <strong>AWSIdentityProviderManager</strong> delegate. So do as follows,</p>

<ol>
<li><p>Create one custom class which adopt AWSIdentityProviderManager delegate.</p>

<pre><code>import UIKit
import AWSCognitoIdentityProvider
import AWSCore
import Foundation

class DVCustomIdentityProvider: NSObject, AWSIdentityProviderManager {
 var tokens: NSDictionary = [String : String]() as NSDictionary

 init(tokens: [String : String]) {
    self.tokens = tokens as NSDictionary
 }

 func logins() -&gt; AWSTask&lt;NSDictionary&gt; { // AWSIdentityProviderManager delegate method
   return AWSTask(result: tokens)
 }

}
</code></pre></li>
<li><p>Add following code in view controller you want.</p>

<pre><code>@IBAction func loginButtonPressed(_ sender: UIButton) {
if (phoneNumberTextField.text != nil) &amp;&amp; (passwordTextField.text != nil) {
    // Change userName.getSession.... with your Facebook method to get authenticate from Facebook, in success block add what I added in my success block. 
    userName.getSession(phoneNumberTextField.text!, password: passwordTextField.text!, validationData: nil).continue(with: AWSExecutor.mainThread(), withSuccessBlock: { (task: AWSTask&lt;AWSCognitoIdentityUserSession&gt;) -&gt; Any? in // Your Facebook call will go here 

        if task.error != nil {
          // Error
        } else { 
            // SUCCESS BLOCK
            self.updateCredentials()
        }
        return nil
    })
} else {
   // Credential empty
}
}

    func updateCredentials() {
       let customcedentialProvider = DVCustomIdentityProvider(tokens: [""graph.facebook.com"" : token]))
       let credentialsProvider = AWSCognitoCredentialsProvider(regionType: ""Your region"", identityPoolId: ""Your pool id"", unauthRoleArn: ""Your unearth role name"", authRoleArn: ""Your auth role name"", identityProviderManager: customcedentialProvider)
       let configuration = AWSServiceConfiguration(region: ""Your region"", credentialsProvider:credentialsProvider)
       AWSServiceManager.default().defaultServiceConfiguration = configuration
       credentialsProvider.getIdentityId().continue(with: AWSExecutor.mainThread(), withSuccessBlock: { (taskTask: AWSTask&lt;NSString&gt;) -&gt; Any? in
         if taskTask.error == nil &amp;&amp; taskTask.exception == nil {
            kUserIdentityID = taskTask.result as String? // Im saving user identity id in constant variable called ""kUserIdentityID""

         } else {
            // Do Nothing
         }
      return nil
     })
    }
</code></pre></li>
</ol>

<p>import following in your view controller </p>

<pre><code>import AWSCognitoIdentityProvider
import AWSCore
import AWSCognito
</code></pre>

<p>Note: This code is written in <strong>swift 3</strong></p>
",6355922,2016-11-09T13:26:35.277,0,CC BY-SA 3.0,,1,,strong,0,"property get deprecated, so we need to assign <strong>IdentityProvider</strong> to <strong",False,False
224,40537717,2,40535314,2016-11-10T21:57:50.967,2,"<p>You have few issues here:</p>

<ol>
<li>Your user agent is outdated, so you get a totally different response than you expect. Use a newer one.</li>
<li><code>document.select</code> returns an <code>Element</code>, not <code>String</code>.</li>
<li><p>Your seletor is not right.
Use the following code:  </p>

<pre><code>String url = ""http://www.amazon.de/DJI-CP-PT-000498-Mavic-Drohne-grau/dp/B01M0AVO1P/"";
Document document = Jsoup.connect(url).userAgent(""Mozilla/49.0"").get();
Elements question = document.select(""#priceblock_ourprice"");
System.out.println(""Price is "" + question.html());
</code></pre></li>
</ol>
",3426328,2016-11-10T21:57:50.967,0,CC BY-SA 3.0,,1,,.,0,"Your user agent is outdated, so you get a totally different response than you expect.",False,False
225,40674961,2,40649605,2016-11-18T10:38:17.823,39,"<p>we are doing the same.
We started with Cognito but moved to Firebase because we were not satisfied with the way AWS Android SDK implements the authentication flow with Google and Facebook: the code is quite old, it makes use of deprecated methods and generally requires rewriting. On the other hand, Firebase authentication is obviously working seamlessly.
When you don't use Cognito, you need to implement your custom authenticator in AWS API Gateway which is quite easy and is described in <a href=""https://aws.amazon.com/blogs/mobile/integrating-amazon-cognito-user-pools-with-api-gateway/"" rel=""noreferrer"">https://aws.amazon.com/blogs/mobile/integrating-amazon-cognito-user-pools-with-api-gateway/</a>. Firebase instructions for token validation are in <a href=""https://firebase.google.com/docs/auth/admin/verify-id-tokens"" rel=""noreferrer"">https://firebase.google.com/docs/auth/admin/verify-id-tokens</a></p>

<p>The following is an excerpt of my authenticator's code:</p>

<pre><code>'use strict';

// Firebase initialization
// console.log('Loading function');
const admin = require(""firebase-admin"");
admin.initializeApp({
  credential: admin.credential.cert(""xxx.json""),
  databaseURL: ""https://xxx.firebaseio.com""
});
// Standard AWS AuthPolicy - don't touch !!
...
// END Standard AWS AuthPolicy - don't touch !!

exports.handler = (event, context, callback) =&gt; {
    // console.log('Client token:', event.authorizationToken);
    // console.log('Method ARN:', event.methodArn);

    // validate the incoming token
    // and produce the principal user identifier associated with the token

    // this is accomplished by Firebase Admin
    admin.auth().verifyIdToken(event.authorizationToken)
        .then(function(decodedToken) {
            let principalId = decodedToken.uid;
            // console.log(JSON.stringify(decodedToken));

            // if the token is valid, a policy must be generated which will allow or deny access to the client

            // if access is denied, the client will recieve a 403 Access Denied response
            // if access is allowed, API Gateway will proceed with the backend integration configured on the method that was called

            // build apiOptions for the AuthPolicy
            const apiOptions = {};
            const tmp = event.methodArn.split(':');
            const apiGatewayArnTmp = tmp[5].split('/');
            const awsAccountId = tmp[4];
            apiOptions.region = tmp[3];
            apiOptions.restApiId = apiGatewayArnTmp[0];
            apiOptions.stage = apiGatewayArnTmp[1];

            const method = apiGatewayArnTmp[2];
            let resource = '/'; // root resource
            if (apiGatewayArnTmp[3]) {
                resource += apiGatewayArnTmp[3];
            }


            // this function must generate a policy that is associated with the recognized principal user identifier.
            // depending on your use case, you might store policies in a DB, or generate them on the fly

            // keep in mind, the policy is cached for 5 minutes by default (TTL is configurable in the authorizer)
            // and will apply to subsequent calls to any method/resource in the RestApi
            // made with the same token

            // the policy below grants access to all resources in the RestApi
            const policy = new AuthPolicy(principalId, awsAccountId, apiOptions);
            policy.allowAllMethods();
            // policy.denyAllMethods();
            // policy.allowMethod(AuthPolicy.HttpVerb.GET, ""/users/username"");

            // finally, build the policy and exit the function
            callback(null, policy.build());
            })
        .catch(function(error) {
            // Firebase throws an error when the token is not valid
            // you can send a 401 Unauthorized response to the client by failing like so:
            console.error(error);
            callback(""Unauthorized"");
        });
};
</code></pre>

<p>We are not in production, yet, but tests on the authenticator show that it behaves correctly with Google, Facebook and password authentication and it is also very quick (60 - 200 ms).
The only drawback I can see is that you will be charged for the authenticator lambda function, while the Cognito integrated authenticator is free.</p>
",4620414,2016-11-18T10:38:17.823,5,CC BY-SA 3.0,,1,,.,1,"We started with Cognito but moved to Firebase because we were not satisfied with the way AWS Android SDK implements the authentication flow with Google and Facebook: the code is quite old, it makes use of deprecated methods and generally requires rewriting.",False,False
226,40866205,2,27658147,2016-11-29T12:41:22.683,3,"<p>The TS asked for a working SHA-1 version of the script. However, SHA-1 is outdated and Amazon has datacenters that only accept SHA-256 encryption, hereby the download script that can be used for all S3 datacenters:
It also follows HTTP 307 redirects.</p>

<pre><code>#!/bin/sh

#USAGE:
# download-aws.sh &lt;bucket&gt; &lt;region&gt; &lt;source-file&gt; &lt;dest-file&gt;

set -e

s3Key=xxxxxxxxxxxxxxxxxxxx
s3Secret=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

file=$3
bucket=$1
host=""${bucket}.s3.amazonaws.com""
resource=""/${file}""
contentType=""text/plain""
dateValue=""`date +'%Y%m%d'`""
X_amz_date=""`date +'%Y%m%dT%H%M%SZ'`""
X_amz_algorithm=""AWS4-HMAC-SHA256""
awsRegion=$2
awsService=""s3""
X_amz_credential=""$s3Key%2F$dateValue%2F$awsRegion%2F$awsService%2Faws4_request""
X_amz_credential_auth=""$s3Key/$dateValue/$awsRegion/$awsService/aws4_request""

signedHeaders=""host;x-amz-algorithm;x-amz-content-sha256;x-amz-credential;x-amz-date""
contentHash=""e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855""

HMAC_SHA256_asckey () {
        var=`/bin/echo -en $2 | openssl sha256 -hmac $1 -binary | xxd -p -c256`
        echo $var
}
HMAC_SHA256 () {
        var=`/bin/echo -en $2 | openssl dgst -sha256 -mac HMAC -macopt hexkey:$1 -binary | xxd -p -c256`
        echo $var
}
REQUEST () {
        canonicalRequest=""GET\n$resource\n\n""\
""host:$1\n""\
""x-amz-algorithm:$X_amz_algorithm""""\n""\
""x-amz-content-sha256:$contentHash""""\n""\
""x-amz-credential:$X_amz_credential""""\n""\
""x-amz-date:$X_amz_date""""\n\n""\
""$signedHeaders\n""\
""$contentHash""
        #echo $canonicalRequest
        canonicalHash=`/bin/echo -en ""$canonicalRequest"" | openssl sha256 -binary | xxd -p -c256`
        stringToSign=""$X_amz_algorithm\n$X_amz_date\n$dateValue/$awsRegion/s3/aws4_request\n$canonicalHash""
        #echo $stringToSign


        s1=`HMAC_SHA256_asckey ""AWS4""""$s3Secret"" $dateValue`
        s2=`HMAC_SHA256 ""$s1"" ""$awsRegion""`
        s3=`HMAC_SHA256 ""$s2"" ""$awsService""`
        signingKey=`HMAC_SHA256 ""$s3"" ""aws4_request""`
        signature=`/bin/echo -en $stringToSign | openssl dgst -sha256 -mac HMAC -macopt hexkey:$signingKey -binary | xxd -p -c256`
        #echo signature

        authorization=""$X_amz_algorithm Credential=$X_amz_credential_auth,SignedHeaders=$signedHeaders,Signature=$signature""
        result=$(curl --silent -H ""Host: $1"" -H ""X-Amz-Algorithm: $X_amz_algorithm"" -H ""X-Amz-Content-Sha256: $contentHash"" -H ""X-Amz-Credential: $X_amz_credential"" -H ""X-Amz-Date: $X_amz_date"" -H ""Authorization: $authorization"" https://${1}/${file} -o ""$2"" --write-out ""%{http_code}"")
        if [ $result -eq 307 ]; then
                redirecthost=`cat $2 | sed -n 's:.*&lt;Endpoint&gt;\(.*\)&lt;/Endpoint&gt;.*:\1:p'`
                REQUEST ""$redirecthost"" ""$2""
        fi
}
REQUEST ""$host"" ""$4""
</code></pre>

<p>Tested on Ubuntu</p>

<p>If someone knows a solution to remove the HMAC-ASCII step, you're welcome to reply. I got this only working in this way.</p>
",2906692,2016-11-29T12:41:22.683,1,CC BY-SA 3.0,,1,SHA-1,"
",0,"However, SHA-1 is outdated and Amazon has datacenters that only accept SHA-256 encryption, hereby the download script that can be used for all S3 datacenters:
",False,True
227,40905002,2,40868530,2016-12-01T07:37:04.687,0,"<p>It sounds like maybe the issue might be an outdated certificate on CloudFront. </p>

<p>If so, you can either upload your RapidSSL certificate using aws iam upload-server-certificate, or (I'd prefer) request a new certificate that you'll only use with CloudFront from AWS Certificate Manager. The latter is free, and AWS will autoupdate the certificate before it expires. </p>

<p>You can start this simple and quick process by going to your CloudFront distribution in AWS Console, and clicking <em>Edit</em> > <em>Request or Import a Certificate with ACM</em></p>

<p>Once that is done, you will be able to choose the certificate from the Custom Certificate dropdown at the same location <a href=""https://i.stack.imgur.com/aRuss.png"" rel=""nofollow noreferrer"">(screenshot)</a>.</p>

<p><strong>Note: If you upload your RapidSSL certificate, it must be uploaded to eu-east-1 (N. Virginia) in order to be used with CloudFront. Requesting ACM Certificates must also be done in this region</strong></p>

<hr>

<p>If you provide an URL to your website, it's easier to confirm this issue.</p>

<p>I scribbled some notes on this <a href=""http://notes.webutvikling.org/howto-aws-cloudfront-with-ssl-and-your-domain/"" rel=""nofollow noreferrer"">here</a> with more details on uploading your own/RapidSSL certificate.</p>
",2403169,2016-12-01T07:37:04.687,0,CC BY-SA 3.0,,1,,.,0,It sounds like maybe the issue might be an outdated certificate on CloudFront.,False,False
228,41079360,2,41078151,2016-12-10T19:23:08.403,1,"<p>Buddy, you are using the deprecated version of AWS SDK. The new SDK is integrated directly in Unity without jars and java files. It's a .NET library. Check it out on github:</p>

<p><a href=""https://github.com/aws/aws-sdk-net"" rel=""nofollow noreferrer"">https://github.com/aws/aws-sdk-net</a></p>

<p>and here you can see how you can build it for Unity:</p>

<p><a href=""https://github.com/aws/aws-sdk-net/blob/master/Unity.README.md"" rel=""nofollow noreferrer"">https://github.com/aws/aws-sdk-net/blob/master/Unity.README.md</a></p>
",3183423,2016-12-10T19:23:08.403,2,CC BY-SA 3.0,,1,,.,0,"Buddy, you are using the deprecated version of AWS SDK.",False,False
229,41083194,2,18839216,2016-12-11T05:07:46.730,0,"<p>The answer is somewhat outdated with the new SDK. The following works with v3 SDK.</p>

<pre class=""lang-php prettyprint-override""><code>$client-&gt;registerStreamWrapper();
$result = $client-&gt;headObject([
    'Bucket' =&gt; $bucket,
    'Key'    =&gt; $key
]);

$headers = $result-&gt;toArray();

header('Content-Type: ' . $headers['ContentType']);
header('Content-Disposition: attachment');

// Stop output buffering
if (ob_get_level()) {
    ob_end_flush();
}

flush();

// stream the output
readfile(""s3://{$bucket}/{$key}"");
</code></pre>
",1234452,2016-12-11T05:07:46.730,0,CC BY-SA 3.0,,1,answer,.,0,The answer is somewhat outdated with the new SDK.,False,False
230,41190825,2,31654994,2016-12-16T19:00:10.967,0,"<p>Looks like you are using AWS SDK V2 code with AWS SDK V3.
<code>Aws\Common\Enum\Region</code> is obsolete in V3.</p>
",3562288,2016-12-16T19:00:10.967,0,CC BY-SA 3.0,,1,>,"
",0,"> is obsolete in V3.</p>
",False,False
231,41191255,2,41190793,2016-12-16T19:29:47.533,0,"<p>Here is a complete sample using the <a href=""http://docs.aws.amazon.com/cognito/latest/developerguide/tutorial-integrating-user-pools-android.html#tutorial-integrating-user-pools-sign-up-users-android"" rel=""nofollow noreferrer"">link</a> sugested by Jeff: </p>

<pre><code>    CognitoUserAttributes attributes = new CognitoUserAttributes();
    attributes.addAttribute(""phone_number"",""+15555555555"");
    attributes.addAttribute(""email"",""email@mydomain.com"");

    cognitoUserPool.signUp('username', 'password', attributes, null, new SignUpHandler() {
        @Override
        public void onSuccess(CognitoUser cognitoUser, boolean b, CognitoUserCodeDeliveryDetails cognitoUserCodeDeliveryDetails) {
            // If the sign up was successful, ""user"" is a CognitoUser object of the user who was signed up.
            // ""codeDeliveryDetails"" will contain details about where the confirmation codes will be delivered.
        }

        @Override
        public void onFailure(Exception e) {
            // Sign up failed, code check the exception for cause and perform remedial actions.
        }
    });
</code></pre>

<p>The sections <em>Examples of Using User Pools with the Mobile SDK for Android</em> seems to be outdated.</p>

<p>Hope to help somebody else ;)</p>
",4848308,2016-12-16T19:29:47.533,0,CC BY-SA 3.0,,1,,outdated.</p,0,Examples of Using User Pools with the Mobile SDK for Android</em> seems to be outdated.</p,False,False
232,41271068,2,41201750,2016-12-21T20:06:54.137,2,"<p>I think the solution here is likely very easy.</p>

<p>Instead of <code>s3://</code> use <code>s3a://</code> as a scheme for your job accessing the bucket.
See <a href=""https://wiki.apache.org/hadoop/AmazonS3"" rel=""nofollow noreferrer"">here</a>, the <code>s3://</code> scheme is deprecated and requires the bucket in question to be exclusive to your Hadoop data. Quote from the above doc link:</p>

<blockquote>
  <p>This filesystem requires you to dedicate a bucket for the filesystem -
  you should not use an existing bucket containing files, or write other
  files to the same bucket. The files stored by this filesystem can be
  larger than 5GB, but they are not interoperable with other S3 tools.</p>
</blockquote>
",5509152,2016-12-21T20:06:54.137,4,CC BY-SA 3.0,,1,scheme,.,0,", the <code>s3://</code> scheme is deprecated and requires the bucket in question to be exclusive to your Hadoop data.",False,True
233,41289711,2,41057173,2016-12-22T18:36:38.750,0,"<p>TLDR: Use Chef recipes and cookbooks to configure an <code>AWS::OpsWorks::Instance</code>, not <code>cfn-init</code>.</p>

<p>Internally, OpsWorks uses <code>cloud-init</code> to install the AWS OpsWorks Stacks agent on the instance (see <a href=""http://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-custom-ami.html#workinginstances-custom-ami-work-startup"" rel=""nofollow noreferrer"">Using Custom AMIs: Startup Behavior</a>). Presumably for this reason, setting a custom user-data script is not supported on OpsWorks instances, so you can't run <code>cfn-init</code> on startup the same way you can on regular <code>AWS::EC2::Instance</code> resources.</p>

<p>For a workaround, you could <a href=""http://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-custom-ami.html#workinginstances-custom-ami-work-startup"" rel=""nofollow noreferrer"">use a custom AMI</a> for your instance and configure the AMI to invoke <code>cfn-init</code> on startup.</p>

<p>However, a workaround like that is strongly discouraged- you really shouldn't ever need to use <code>cfn-init</code> (or its corresponding <code>AWS::CloudFormation::Init</code> metadata) in an OpsWorks stack to begin with. <code>cfn-init</code> is entirely orthogonal to the much more robust Chef-based configuration management that OpsWorks provides, which is presumably the reason you're using OpsWorks in the first place. Take advantage of it! Use Chef recipes and cookbooks to configure your instance.</p>

<p>If you really need to use <code>cfn-init</code> and custom user-data (to support legacy code, for example), then I'd recommend sticking to standard EC2 instances until you can port your application logic over to custom Chef cookbooks.</p>
",2518355,2016-12-22T18:36:38.750,1,CC BY-SA 3.0,,1,,discouraged-,0,"However, a workaround like that is strongly discouraged-",False,False
234,41304804,2,41302414,2016-12-23T16:27:13.753,1,"<p>If you would like to keep the <code>channel</code> attribute as <code>List&lt;Map&lt;String, Object&gt;&gt;</code>, the simple answer is that you can't save the values as List of Map objects. The only solution available is to convert the data as String and store in DynamoDB.</p>

<p><strong>Couple of options to convert to String or JSON:-</strong></p>

<p>1) <code>@DynamoDBTypeConvertedJson</code> - This annotation can be used to convert the data into JSON and store it as String attribute in DynamoDB</p>

<p><code>@DynamoDBMarshalling</code> - is deprecated</p>

<p>2) Write a custom converter. Again, this will save the data as String in DynamoDB.</p>

<p><a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBTypeConverted.html"" rel=""nofollow noreferrer"">DynamoDBTypeConverted</a></p>

<p><strong>Note:-</strong></p>

<p>Though DynamoDB supports Document data types such as List and Map, it will be impossible to query the data if you have Map inside List. The DynamoDB API doesn't support filtering the data for this data model.</p>

<p>If you need to support any use case querying/updating the <code>channels</code> attribute, I would strongly recommend to rethink about the data model to flatten the data structure.</p>

<p>Another <strong>drawback</strong> is that index can't be created on Document data types (List and Map).</p>
",6337748,2016-12-23T16:27:13.753,0,CC BY-SA 3.0,,1,,"

",0,"deprecated</p>

",False,False
235,41386467,2,31329495,2016-12-29T20:04:16.787,10,"<p><strong>1)</strong> Configure your API Gateway resource to use <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html"" rel=""noreferrer"">Lambda Proxy Integration</a> by checking the checkbox labeled <strong>""Use Lambda Proxy integration""</strong> on the ""Integration Request"" screen of the API Gateway resource definition. (Or define it in your cloudformation/terraform/serverless/etc config)</p>

<p><strong>2)</strong> Change your lambda code in 2 ways</p>

<ul>
<li>Process the incoming <code>event</code> (1st function argument) appropriately. It is no longer just the bare payload, it represents the entire HTTP request including headers, query string, and body. Sample below. Key point is that JSON bodies will be strings requiring explicit <code>JSON.parse(event.body)</code> call (don't forget <code>try/catch</code> around that). Example is below.</li>
<li>Respond by calling the callback with null then a response object that provides the HTTP details including <code>statusCode</code>, <code>body</code>, and <code>headers</code>.

<ul>
<li><code>body</code> should be a string, so do <code>JSON.stringify(payload)</code> as needed</li>
<li><code>statusCode</code> can be a number</li>
<li><code>headers</code> is an object of header names to values</li>
</ul></li>
</ul>

<h2>Sample Lambda Event Argument for Proxy Integration</h2>

<pre><code>{
    ""resource"": ""/example-path"",
    ""path"": ""/example-path"",
    ""httpMethod"": ""POST"",
    ""headers"": {
        ""Accept"": ""*/*"",
        ""Accept-Encoding"": ""gzip, deflate"",
        ""CloudFront-Forwarded-Proto"": ""https"",
        ""CloudFront-Is-Desktop-Viewer"": ""true"",
        ""CloudFront-Is-Mobile-Viewer"": ""false"",
        ""CloudFront-Is-SmartTV-Viewer"": ""false"",
        ""CloudFront-Is-Tablet-Viewer"": ""false"",
        ""CloudFront-Viewer-Country"": ""US"",
        ""Content-Type"": ""application/json"",
        ""Host"": ""exampleapiid.execute-api.us-west-2.amazonaws.com"",
        ""User-Agent"": ""insomnia/4.0.12"",
        ""Via"": ""1.1 9438b4fa578cbce283b48cf092373802.cloudfront.net (CloudFront)"",
        ""X-Amz-Cf-Id"": ""oCflC0BzaPQpTF9qVddpN_-v0X57Dnu6oXTbzObgV-uU-PKP5egkFQ=="",
        ""X-Forwarded-For"": ""73.217.16.234, 216.137.42.129"",
        ""X-Forwarded-Port"": ""443"",
        ""X-Forwarded-Proto"": ""https""
    },
    ""queryStringParameters"": {
        ""bar"": ""BarValue"",
        ""foo"": ""FooValue""
    },
    ""pathParameters"": null,
    ""stageVariables"": null,
    ""requestContext"": {
        ""accountId"": ""666"",
        ""resourceId"": ""xyz"",
        ""stage"": ""dev"",
        ""requestId"": ""5944789f-ce00-11e6-b2a2-dfdbdba4a4ee"",
        ""identity"": {
            ""cognitoIdentityPoolId"": null,
            ""accountId"": null,
            ""cognitoIdentityId"": null,
            ""caller"": null,
            ""apiKey"": null,
            ""sourceIp"": ""73.217.16.234"",
            ""accessKey"": null,
            ""cognitoAuthenticationType"": null,
            ""cognitoAuthenticationProvider"": null,
            ""userArn"": null,
            ""userAgent"": ""insomnia/4.0.12"",
            ""user"": null
        },
        ""resourcePath"": ""/example-path"",
        ""httpMethod"": ""POST"",
        ""apiId"": ""exampleapiid""
    },
    ""body"": ""{\n  \""foo\"": \""FOO\"",\n  \""bar\"": \""BAR\"",\n  \""baz\"": \""BAZ\""\n}\n"",
    ""isBase64Encoded"": false
}
</code></pre>

<h2>Sample Callback Response Shape</h2>

<pre><code>callback(null, {
  statusCode: 409,
  body: JSON.stringify(bodyObject),
  headers: {
    'Content-Type': 'application/json'
  }
})
</code></pre>

<hr>

<p><strong>Notes</strong>
- I believe the methods on <code>context</code> such as <code>context.succeed()</code> are deprecated. They are no longer documented although they do still seem to work. I think coding to the callback API is the correct thing going forward.</p>
",266795,2016-12-29T20:04:16.787,1,CC BY-SA 3.0,,1,>,.,0,> are deprecated.,False,False
236,41506701,2,37542778,2017-01-06T13:23:19.463,0,"<p>I've got question: do you run your pipeline from web console or there is a program?
 The reason I'm asking, please check all fields are filled correctly. It could be you've missed region and it cant find method signature with empty param where supposed to be a <code>String (ex. eu-west-1).</code></p>

<p>From <a href=""https://github.com/awslabs/emr-dynamodb-connector/blob/master/emr-dynamodb-tools/src/main/java/org/apache/hadoop/dynamodb/tools/DynamoDBExport.java"" rel=""nofollow noreferrer"">https://github.com/awslabs/emr-dynamodb-connector/blob/master/emr-dynamodb-tools/src/main/java/org/apache/hadoop/dynamodb/tools/DynamoDBExport.java</a> you could chase your code flow. However keep in mind this class could be out of date so lines could be not matching. But it's giving you rough idea what happens there.</p>
",1693021,2017-01-06T13:23:19.463,0,CC BY-SA 3.0,,1,,.,1,However keep in mind this class could be outdated so lines could be not matching.,False,False
237,41741645,2,41741100,2017-01-19T12:18:59.270,1,"<p>When using NVidia please consider the information obsolete that is given in the related question you linked.</p>

<p>For about a year now the NVidia drivers support true headless operation without an X server running. See this exhaustive article given on the Nvidia developer blog: <a href=""https://devblogs.nvidia.com/parallelforall/egl-eye-opengl-visualization-without-x-server/"" rel=""nofollow noreferrer"">https://devblogs.nvidia.com/parallelforall/egl-eye-opengl-visualization-without-x-server/</a></p>
",524368,2017-01-19T12:18:59.270,4,CC BY-SA 3.0,,1,,question,0,>When using NVidia please consider the information obsolete that is given in the related question,False,False
238,41801876,2,12132350,2017-01-23T08:25:45.447,0,"<p>According <a href=""https://www.magentocommerce.com/magento-connect/pay-with-amazon-for-magento-1.html"" rel=""nofollow noreferrer"">Magentocommerce</a> the latest version is 1.4.0 </p>

<p>But this is outdated. You'll find a newer version (1.4.3 at this time) on <a href=""https://github.com/amzn/amazon-payments-magento-plugin"" rel=""nofollow noreferrer"">GitHub</a></p>

<p>So I recommend to check GitHub in future (too).</p>
",2635490,2017-01-23T08:25:45.447,0,CC BY-SA 3.0,,1,this,.,0,But this is outdated.,False,False
239,41818987,2,41816583,2017-01-24T02:22:58.613,1,"<p>The policy provided looks correct and not out of date to me. Are you sure that the IAM user in question doesn't have any additional groups/policies applied beyond the <code>AWS::IAM::Group</code> specified that would be granting them the unexpected permissions?</p>

<p>One way to confirm this would be to create a new IAM user from scratch and attempt to reproduce the issue there.</p>
",2518355,2017-01-24T02:22:58.613,1,CC BY-SA 3.0,,1,,.,1,The policy provided looks correct and not outdated to me.,False,False
240,41832945,2,41832343,2017-01-24T16:10:38.507,1,"<p>Perhaps you need pass the <code>--httpinterface</code> flag to the mongodb container: </p>

<pre><code>docker run --name some-mongo -d mongo --httpinterface 
</code></pre>

<p>This flag are deprecated since version 3.2: HTTP interface for MongoDB</p>

<p>Check <a href=""https://docs.mongodb.com/manual/reference/program/mongod/#cmdoption--httpinterface"" rel=""nofollow noreferrer"">Mongodb Documentation</a></p>
",5636405,2017-01-24T16:10:38.507,1,CC BY-SA 3.0,,1,flag,:,0,This flag are deprecated since version 3.2:,False,False
241,41862821,2,41851027,2017-01-25T22:27:28.183,10,"<p>While opinion-based questions are discouraged on StackOverflow, and answers always depend upon the particular situation, it is <strong>highly likely</strong> that Amazon S3 is your better choice.</p>

<p>You didn't say whether only wish to <strong>store</strong> the data, or whether you also wish to <strong>serve</strong> the data out to users. I'll assume both.</p>

<p><strong>Benefits of using Amazon S3 to store static assets</strong> such as pictures and videos:</p>

<ul>
<li>S3 is <strong>pay-as-you-go</strong> (only pay for the storage consumed, with different options depending upon how often/fast you wish to retrieve the objects)</li>
<li>S3 is <strong>highly available:</strong> You don't need to run any servers</li>
<li>S3 is <strong>highly durable:</strong> Your data is duplicated across three data centres, so it is more resilient to failure</li>
<li>S3 is <strong>highly scalable:</strong> It can handle massive volumes of requests. If you served content from Amazon EC2, you'd have to scale-out to meet requests</li>
<li>S3 has <strong>in-built security</strong> at the object, bucket and user level.</li>
</ul>

<p>Basically, Amazon S3 is a fully-managed storage service that can serve static assets out to the Internet.</p>

<p><strong>If you were to store data on an Amazon EC2 instance</strong>, and serve the content from the EC2 instance:</p>

<ul>
<li>You would need to <strong>pre-provision storage</strong> using Amazon EBS volumes (and you pay for the entire volume even if it isn't all used)</li>
<li>You would need to <strong>Snapshot the EBS volumes</strong> to improve durability (EBS Snapshots are stored in Amazon S3, replicated between data centres)</li>
<li>You would need to <strong>scale</strong> your EC2 instances (make them bigger, or add more) to handle the workload</li>
<li>You would need to <strong>replicate</strong> data between instances if you are running multiple EC2 instances to meet request volumes</li>
<li>You would need to install and configure the software on the EC2 instance(s) to manage security, content serving, monitoring, etc.</li>
</ul>

<p>The only benefit of storing this static data directly on an Amazon EC2 instance rather than Amazon S3 is that it is <strong>immediately accessible</strong> to software running on the instance. This makes the code simpler and access faster.</p>

<p>There is also the option of using <strong>Amazon Elastic File System (EFS)</strong>, which is NAS-like storage. You can mount an EFS volume simultaneously on multiple EC2 instances. Data is replicated between multiple Availability Zones. It is charged on a pay-as-you-go basis. However, it is only the storage layer - you'd still need to use Amazon EC2 instance(s) to serve the content to the Internet.</p>
",174777,2017-01-25T22:27:28.183,0,CC BY-SA 3.0,,1,,>,0,">While opinion-based questions are discouraged on StackOverflow, and answers always depend upon the particular situation, it is <strong>highly likely</strong>",False,False
242,41929722,2,40139140,2017-01-30T05:50:01.317,1,"<p>First, beware that modifying CloudFormation-created resources outside of CloudFormation is <em>explicitly discouraged</em>, according to <a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#donttouch"" rel=""nofollow noreferrer"">AWS CloudFormation Best Practices</a>:</p>

<blockquote>
  <h3>Manage All Stack Resources Through AWS CloudFormation</h3>
  
  <p>After you launch a stack, use the AWS CloudFormation <a href=""https://console.aws.amazon.com/cloudformation/home"" rel=""nofollow noreferrer"">console</a>, <a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/"" rel=""nofollow noreferrer"">API</a>, or <a href=""http://docs.aws.amazon.com/cli/latest/reference/cloudformation/index.html"" rel=""nofollow noreferrer"">AWS CLI</a> to update resources in your stack. Do not make changes to stack resources outside of AWS CloudFormation. Doing so can create a mismatch between your stack's template and the current state of your stack resources, which can cause errors if you update or delete the stack.</p>
</blockquote>

<p>However, if you've modified a CloudFormation-managed resource accidentally and need to recover, you may have some limited options beyond simply deleting and re-creating the stack altogether (which may not be an acceptable option):</p>

<ol>
<li><p>It is not possible for CloudFormation to <em>automatically</em> update its internal state based on the current state of an externally-modified resource.</p>

<p>However, depending on the exact resource type, in some cases you can <em>manually</em> update CloudFormation afterwards by applying a stack update that matches the current state of the resource.</p></li>
<li><p>Similarly, it is not possible for CloudFormation to <em>automatically</em> revert an externally-modified resource back to its original unmodified CloudFormation state.</p>

<p>However, depending on the exact resource type, in some cases you can either:</p>

<ul>
<li>Revert a resource by <em>manually</em> updating the resource back to its original state;</li>
<li>Update the resource by applying a stack update, bringing both the CloudFormation stack and the managed resource to an altogether new state that will once again be in sync.</li>
</ul></li>
</ol>
",2518355,2017-01-30T05:50:01.317,0,CC BY-SA 3.0,,1,,"

",0,"First, beware that modifying CloudFormation-created resources outside of CloudFormation is <em>explicitly discouraged</em>, according to <a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#donttouch"" rel=""nofollow noreferrer"">AWS CloudFormation Best Practices</a>:</p>

",False,False
243,42013451,2,41282207,2017-02-02T22:25:04.403,0,"<p>Sigv2 is deprecated in favor of Sigv4. The only service that still requires sigv2 is SDB, which you probably shouldn't be writing new code against. </p>

<p>Is this for SDB or another service?</p>
",734691,2017-02-02T22:25:04.403,0,CC BY-SA 3.0,,1,Sigv2,.,0,<p>Sigv2 is deprecated in favor of Sigv4.,False,False
244,42102133,2,42099898,2017-02-07T23:45:44.533,4,"<p>From your screenshot, it appears that the CloudFormation stack along with the <code>EcsInstanceAsg</code> Auto Scaling Group were previously created, and you're attempting to update the Auto Scaling Group to reference a newly-created Load Balancer.</p>

<p>The most common issue with CloudFormation resources failing to stabilize on updates is due to referenced resources being modified and/or deleted outside of the CloudFormation stack. This causes CloudFormation to modify a resource that it can no longer find, which can cause random errors or timeouts, and is discouraged according to <a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#donttouch"" rel=""nofollow noreferrer"">AWS CloudFormation Best Practices</a>. If this is the case, the best way to proceed is to make a fresh start with a brand new stack, if possible.</p>

<p>If this is not the case for you, there might be an unknown limitation or issue with in-place updates to the <a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html#cfn-as-group-loadbalancernames"" rel=""nofollow noreferrer""><code>LoadBalancerNames</code></a> property in the <code>AWS::AutoScaling::AutoScalingGroup</code> (support for in-place updates to this property was only <a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/ReleaseHistory.html"" rel=""nofollow noreferrer"">just added on Jan 17 2017</a>, so there might still be issues with it). Try re-creating your Auto Scaling Group (changing the name of the <code>EcsInstanceAsg</code> resource in your template would cause it to be re-created) and see if that solves the issue.</p>
",2518355,2017-02-07T23:45:44.533,0,CC BY-SA 3.0,,1,,<,1,"This causes CloudFormation to modify a resource that it can no longer find, which can cause random errors or timeouts, and is discouraged according to <",False,False
245,42184058,2,42183433,2017-02-12T03:55:40.103,0,"<p>Look at this first line of code:</p>

<pre><code>client = AmazonEC2ClientBuilder.standard().withCredentials(credentials).build(); 
</code></pre>

<p>The variable <code>client</code> is an <code>AmazonEC2Client</code> object (you should have declared it as such higher up in your code). On the second line you are trying to call a method on the <code>AmazonEC2Client</code> object. You aren't using the <code>AmazonEC2ClientBuilder</code> anymore. You've already built your client object in the first line of code, and now you want to call a method on that object in the next line, like so:</p>

<pre><code>client.setRegion(""US-WEST2"");
</code></pre>

<p>However that method is deprecated and it is recommended you call the <code>setRegion</code> method on the <code>AmazonEC2ClientBuilder</code> instead. So you would remove the second line entirely, and change the first line to this:</p>

<pre><code>client = AmazonEC2ClientBuilder.standard()
     .withCredentials(credentials)
     .withRegion(Regions.US_WEST_2)
     .build();
</code></pre>
",13070,2017-02-12T03:55:40.103,0,CC BY-SA 3.0,,1,method,code,0,However that method is deprecated and it is recommended you call the <code,False,False
246,42217589,2,40849112,2017-02-14T03:53:18.340,4,"<p>After banging my head around on this for a day, it turns out the issue is caused by an out of date AWS CLI. <a href=""http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html#task-iam-roles-minimum-sdk"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html#task-iam-roles-minimum-sdk</a></p>

<p>In my case I was installing aws-cli with <code>apt-get install aws-cli</code> which installs version 1.4.2. This version does not handle the <code>AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</code> environment variable needed to get the correct IAM. So it defaults to getting the instance's IAM.</p>

<p>The solution was to install the AWS CLI through pip or the bundled installation to ensure I had the latest version. The same will apply for the AWS SDKs - required versions are described in the link above.</p>
",773479,2017-02-14T03:53:18.340,0,CC BY-SA 3.0,,1,,.,0,">After banging my head around on this for a day, it turns out the issue is caused by an outdated AWS CLI.",False,False
247,42298949,2,42288219,2017-02-17T13:16:14.020,2,"<p>It's a common misconception that you ""fetch"" pre-signed URLs from the service, but that isn't true.  </p>

<p>They're generated locally in your code (or locally in the SDK code).  Your secret key is used to generate an HMAC digest of a canonical representation of the request that the URL represents.  When the service receives the request (e.g., when the URL is clicked) the service canonicalizes the request, generates the hash from your secret (which is not in the URL, but is known to you and to the service), and if the results match, the request is considered authenticated and allowed to continue to the authorization stage (to ensure that the access key ID presented actually has permission to allow the request).  If the results don't match, then the key or secret are wrong, the signing code on your side has made an error, or the signed URL has been modified since it was signed.  Access denied.</p>

<p>There are, then, no service endpoints that return signed URLs, although you could relatively easily create one for your server to call, using Lambda and API gateway.  Lambda for Node, for example, already has the SDK installed, so you could pass the URL in and return the URL back out, over HTTPS.  Simple but a little bit silly.  You'd have to authenticate the requests from your app using api keys or secret headers, of course, since you wouldn't be able to use native auth, because that would require the SDK. :)</p>

<p>Generating your own code to create signed URLs is not that difficult.   The process is <a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html"" rel=""nofollow noreferrer"">fully documented</a> though you'll note on that page that they suggest you use the SDKs.  The reason for this, I suspect, is that even if you are not sufficiently skilled at coding to write your own implementation of the algorithm, it won't be a barrier to entry.  </p>

<p>I have written my own implementatiions of this... in fact, I've even written an implementation of the older <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html"" rel=""nofollow noreferrer"">V2 signing algorithm</a> (which still works in any region that has been online since before 2014) entirely in SQL, as a MySQL stored function (e.g. <code>SELECT get_signed_url('/bucket/key');</code> returns a signed url to <code>GET</code> an object.  The credentials are stored as variables inside the function)... and given that SQL is probably the least likely language you'd think of for such an operation, this works perfectly.  </p>

<p>So you can write it yourself.</p>

<p>But the objection to including the SDK for maintenance reasons is misplaced, I think.  </p>

<p>AWS doesn't make breaking changes to their service APIs. </p>

<p>They just don't.  When S3 introduced version 2 of the ListObjects action, V1 still remained available.  Using V2 is recommended, but V1 is <em>not</em> deprecated, nor subject to removal.  </p>

<p>When Signature Version 4 (mentioned above) was introduced, they added it to all the old regions, left Signature Version 2 in place where it was already available, and deployed new regions only with V4.  Sig V2 is also not deprecated, but V4 is recommended.  Object versioning in S3 was written in such a way that it is 100% backwards compatible with code that doesn't understand object versioning.  The list goes on.</p>

<p>Short of a bug that impacts you (unlikely), or something related to security (also unlikely) the current version of any SDK should not need to be replaced in your project unless you wanted to take advantage of new features, new services, or new AWS regions, which probably doesn't happen all that often in a stable project.</p>
",1695906,2017-02-17T13:16:14.020,0,CC BY-SA 3.0,,2,, ,1,"> deprecated, nor subject to removal.  ",False,False
248,42510741,2,34884802,2017-02-28T13:59:59.197,3,"<p>The above answer is outdated. 
AWS has added Cognito-Groups recently. That provides more flexibility</p>

<p>You can use technique described in the article to achieve that:
<a href=""https://aws.amazon.com/blogs/aws/new-amazon-cognito-groups-and-fine-grained-role-based-access-control-2/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/aws/new-amazon-cognito-groups-and-fine-grained-role-based-access-control-2/</a></p>
",4074545,2017-02-28T13:59:59.197,0,CC BY-SA 3.0,,1,answer,"
",0,"The above answer is outdated. 
",False,False
249,42530951,2,42506880,2017-03-01T11:49:19.950,2,"<p>The problem was that I was using <strong>ImageUtils</strong> instead of <strong>textureLoader</strong>. I think ImageUtils is deprecated...</p>
",4165012,2017-03-01T11:49:19.950,1,CC BY-SA 3.0,,1,,...,0,I think ImageUtils is deprecated...,False,False
250,42583411,2,28115250,2017-03-03T16:01:49.473,0,"<blockquote>
  <p>macOS users: If you are using the Python 3.6 from the python.org
  binary installer linked on this page, please carefully read the
  Important Information displayed during installation; this information
  is also available after installation by clicking on
  /Applications/Python 3.6/ReadMe.rtf. There is important information
  there about changes in the 3.6.0 installer-supplied Python,
  particularly with regard to SSL certificate validation.</p>
</blockquote>

<p><a href=""https://www.python.org/downloads/release/python-360/"" rel=""nofollow noreferrer"">https://www.python.org/downloads/release/python-360/</a></p>

<p>From ReadMe.rtf at the time of this writing:</p>

<blockquote>
  <p>Certificate verification and OpenSSL</p>
  
  <p><strong>NEW</strong> This variant of Python 3.6 now includes its own private copy of OpenSSL 1.0.2.  Unlike previous releases, the deprecated
  Apple-supplied OpenSSL libraries are no longer used.  This also means
  that the trust certificates in system and user keychains managed by
  the Keychain Access application and the security command line utility
  are no longer used as defaults by the Python ssl module.  For 3.6.0, a
  sample command script is included in /Applications/Python 3.6 to
  install a curated bundle of default root certificates from the
  third-party certifi package (<a href=""https://pypi.python.org/pypi/certifi"" rel=""nofollow noreferrer"">https://pypi.python.org/pypi/certifi</a>). 
  If you choose to use certifi, you should consider subscribing to the
  project's email update service to be notified when the certificate
  bundle is updated.</p>
  
  <p>The bundled pip included with the Python 3.6 installer has its own
  default certificate store for verifying download connections.</p>
</blockquote>
",93345,2017-03-03T16:01:49.473,0,CC BY-SA 3.0,,1,, ,1,"Unlike previous releases, the deprecated
  Apple-supplied OpenSSL libraries are no longer used.  ",False,False
251,42762861,2,42684034,2017-03-13T11:46:12.713,2,"<p>S3n doesn't support IAM roles, and 2.4 is a very outdated version anyway. Not as buggy as 2.5 when it comes to s3n, but still less than perfect.</p>

<p>If you want to use IAM roles, you are going to have to switch to S3a, and yes, for you, that does mean upgrading Hadoop. sorry.</p>
",2261274,2017-03-13T11:46:12.713,0,CC BY-SA 3.0,,1,,.,1,">S3n doesn't support IAM roles, and 2.4 is a very outdated version anyway.",False,False
252,42854114,2,42733164,2017-03-17T09:46:46.893,0,"<p>Okay so I finally figured it out. The reason why it wasn't working was because my botocore was on version <code>1.4.62</code>. I only realized it because another script that ran fine on my colleague's machine was throwing exceptions on mine. We had the same boto3 version but different botocore versions. After I <code>pip install botocore==1.5.26</code> both the other script and my kinesis <code>put_record</code> started working.</p>

<p>tldr: <code>botocore 1.4.62</code> is horribly broken in many ways so upgrade NOW. I can't believe how much of my life is wasted by outdated broken libraries. I wonder if Amazon dev can unpublish broken versions of the client?</p>
",429288,2017-03-17T09:46:46.893,0,CC BY-SA 3.0,,1,,.,1,I can't believe how much of my life is wasted by outdated broken libraries.,False,False
253,42906663,2,28399041,2017-03-20T14:47:07.853,1,"<p>The ec2-ami-tools package in the Ubuntu 14.04 repository is outdated. The version in the repository is only version 1.4.0. Frankfurt support was only added in <a href=""https://aws.amazon.com/items/368?externalID=368&amp;categoryID=88"" rel=""nofollow noreferrer"">version 1.5.6</a>. The newer version of the tool should work in the Frankfurt region.</p>

<p>This tool is only necessary if you plan on creating instance store-backed AMIs. If your instance is EBS-backed and you need to make an AMI from it, using the AWS CLI will be much easier.</p>
",7740115,2017-03-20T14:47:07.853,0,CC BY-SA 3.0,,1,package,.,0,>The ec2-ami-tools package in the Ubuntu 14.04 repository is outdated.,False,False
254,43056995,2,43056007,2017-03-27T21:45:29.613,0,"<p>I read that the use of rc.local is getting deprecated. One thing to try is a line in /etc/crontab like this: </p>

<p>@reboot  full-path-of-script</p>

<p>If there's a specific user you want to run the script as, you can list it after @reboot.</p>
",7724239,2017-03-27T21:45:29.613,10,CC BY-SA 3.0,,1,,.,0,I read that the use of rc.local is getting deprecated.,False,False
255,43065697,2,40493924,2017-03-28T09:26:10.217,1,"<p>Any email address will have two parts <em>localmailbox@domain.com</em>.</p>

<ol>
<li>domain.com: This is your domain name and this part will always be case insensitive </li>
<li>localmailbox: This part is case sensitive - theoretically different cases would point to different mail boxes. But to avoid confusion email providers like gmail enforce case insensitive mailboxes.</li>
</ol>

<p>Why amazon has it as case-sensitive - this can be because of the <a href=""https://tools.ietf.org/html/rfc5321"" rel=""nofollow noreferrer"">RFC-5321</a>  which states </p>

<blockquote>
  <p>SMTP implementations MUST take care to preserve the case of mailbox local-parts. In particular, for some hosts, the user ""smith"" is different from the user ""Smith"". However, exploiting the case sensitivity of mailbox local-parts impedes interoperability and is discouraged.</p>
</blockquote>
",813103,2017-03-28T09:26:10.217,0,CC BY-SA 3.0,,1,,discouraged.</p,0,"However, exploiting the case sensitivity of mailbox local-parts impedes interoperability and is discouraged.</p",False,False
256,43207871,2,43207442,2017-04-04T12:54:37.510,2,"<p>The plugin you linked to looks very outdated, Last updated: <code>24 March 2010</code> and seems odd to depend on another plugin like quartz to use s3. </p>

<p>I would instead suggest Karman (maintained by the same person as asset-pipeline) and updated for Grails 2 &amp; 3 <a href=""https://grails.org/plugin/karman-aws?skipRedirect=true"" rel=""nofollow noreferrer"">https://grails.org/plugin/karman-aws?skipRedirect=true</a></p>

<p>Or you may just use the aws java sdk directly - it is straight forward api: <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadObjSingleOpJava.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadObjSingleOpJava.html</a></p>
",2209885,2017-04-04T12:54:37.510,1,CC BY-SA 3.0,,1,plugin,:,0,"The plugin you linked to looks very outdated, Last updated:",False,False
257,43207907,2,43207442,2017-04-04T12:56:04.723,1,"<p><a href=""http://grails.org/plugin/amazon-s3"" rel=""nofollow noreferrer"">This plugin</a> is out of date.</p>

<p>I think it will be better to use <a href=""https://grails.org/plugin/aws-sdk"" rel=""nofollow noreferrer"">https://grails.org/plugin/aws-sdk</a>. This plugin supports S3 integration. You can find docs on github.</p>
",3886830,2017-04-04T12:56:04.723,0,CC BY-SA 3.0,,1,,outdated.</p,0,is outdated.</p,False,False
258,43286947,2,42679445,2017-04-07T20:53:43.417,0,"<p>SOLVED: So it turns out that my friend had a different version of the pods and when I pulled his version I got the out of date ones too. We struggled for about a week before we found the solution, <code>pod update</code>. </p>

<p>TL/DR: <code>pod update</code>  </p>
",6522822,2017-04-07T20:53:43.417,0,CC BY-SA 3.0,,1,,.,0,: So it turns out that my friend had a different version of the pods and when I pulled his version I got the outdated ones too.,False,False
259,43307654,2,43281729,2017-04-09T14:06:27.677,0,"<p>The AWS documentation is outdated.</p>

<p>You need to implement a different interface.</p>

<p>Passing a map:</p>

<pre><code>public class MyHandler implements RequestHandler&lt;Map&lt;String,Object&gt;,String&gt; 
</code></pre>

<p>Passing into a custom POJO object:</p>

<pre><code>public class MyHandler implements RequestHandler&lt;CustomRequest,CustomResponse&gt; {
</code></pre>

<p>Consuming a stream:</p>

<pre><code>public class MyHandler implements RequestStreamHandler {
</code></pre>

<p>Make sure to grab the right dependency:</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;
    &lt;artifactId&gt;aws-lambda-java-core&lt;/artifactId&gt;
    &lt;version&gt;1.1.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>I developed a <a href=""https://github.com/udoheld/aws-lambda-hello-log"" rel=""nofollow noreferrer"">sample application</a> implementing each of the interfaces. You can just clone it from <a href=""https://github.com/udoheld/aws-lambda-hello-log"" rel=""nofollow noreferrer"">github</a>. One of the handlers will print out all available environment variables as well.</p>

<p>Lambda expects JSON as input.</p>
",1022141,2017-04-09T14:06:27.677,0,CC BY-SA 3.0,,1,,outdated.</p,0,The AWS documentation is outdated.</p,False,False
260,43316867,2,28906981,2017-04-10T07:10:08.283,0,"<p>I had the same issue. The managed policies were correct in my case, but I had to update the trust relationships for both the DataPipelineDefaultRole and DataPipelineDefaultResourceRole roles using the documentation Gonfva linked to above as they were out of date.</p>
",865309,2017-04-10T07:10:08.283,0,CC BY-SA 3.0,,1,,"
",0,"outdated.</p>
",False,False
261,43442679,2,26533245,2017-04-16T22:09:26.793,1,"<p>For Android SDK, setEndpoint solves the problem, although it's been deprecated. </p>

<pre><code>CognitoCachingCredentialsProvider credentialsProvider = new CognitoCachingCredentialsProvider(
                context, ""identityPoolId"", Regions.US_EAST_1);
AmazonS3 s3 = new AmazonS3Client(credentialsProvider);
s3.setEndpoint(""s3.us-east-2.amazonaws.com"");
</code></pre>
",6664008,2017-04-16T22:09:26.793,0,CC BY-SA 3.0,,1,,.,0,"For Android SDK, setEndpoint solves the problem, although it's been deprecated.",False,False
262,43450379,2,43449710,2017-04-17T11:17:19.660,4,"<p>Ah! That documented process won't work if the AMI used to originally launch the instance has been deprecated (expired).</p>

<p>For step 5, simply select your AMI from the <strong>AMIs</strong> section of the EC2 management console, then choose the <strong>Launch</strong> command from the Actions menu. This will let you launch a new machine using the AMI you created. Make sure you choose a new keypair for which you have the <code>.pem</code> file.</p>

<p>Then, just continue from step 6. The general steps are:</p>

<ul>
<li>Stop your original instance</li>
<li>Detach the boot disk (""Disk A"")</li>
<li>Launch another Windows instance (or use one you already have access to)</li>
<li>Attach Disk A to the 2nd instance</li>
<li>Update the <code>\Program Files\Amazon\Ec2ConfigService\Settings\config.xml</code> file on Disk A and update the <code>Ec2SetPassword</code> parameter to <code>Enabled</code> (see Step 9 on that documentation page)</li>
<li>Detach Disk A from the 2nd instance and reattach it to the original instance (from Step 5 on the documentation page)</li>
<li>Start the original instance and try to login</li>
</ul>
",174777,2017-04-17T11:17:19.660,1,CC BY-SA 3.0,,1,,expired).</p,1,That documented process won't work if the AMI used to originally launch the instance has been deprecated (expired).</p,False,False
263,43476581,2,43466637,2017-04-18T15:39:58.767,2,"<p>The <code>table_cache</code> system variable was deprecated and renamed <code>table_open_cache</code> back in MySQL 5.1, and was still called <code>table_open_cache</code> in <a href=""https://dev.mysql.com/doc/refman/5.6/en/table-cache.html"" rel=""nofollow noreferrer"">5.6</a>.</p>

<p>It's in the RDS parameter group.</p>

<p>However, it's very rare that this value is an appropriate value to tweak.  It has long been known to <a href=""https://www.percona.com/blog/2009/11/16/table_cache-negative-scalability/"" rel=""nofollow noreferrer"">scale negatively</a> -- the more ""optimum"" your configuration, the worse the server will perform.  </p>

<p>If you're using a tuning script, the odds are extremely high that you're operating on bad advice if changing that value has been recommended.  Tuning scripts in general are notorious for their well-intentioned, but ill-conceived, bad advice.</p>
",1695906,2017-04-18T15:39:58.767,2,CC BY-SA 3.0,,1,variable,>,0,>The <code>table_cache</code> system variable was deprecated and renamed <code>,False,False
264,43480255,2,43464820,2017-04-18T19:03:07.197,3,"<p>I found the solution! @SwarajGiri pointed in the right direction by suggesting aws-sdk version 2.2.15.</p>

<p>My problem was that I had a <a href=""https://docs.npmjs.com/cli/shrinkwrap"" rel=""nofollow noreferrer"">npm-shrinkwrap.json</a> in my folder and was not aware that it was overruling the package.json. The aws-sdk version inside the npm-shrinkwrap.json was 2.1.39 which does not include the documentClient(). <a href=""https://aws.amazon.com/de/blogs/developer/announcing-the-amazon-dynamodb-document-client-in-the-aws-sdk-for-javascript/"" rel=""nofollow noreferrer"">Document Client was introduced in version 2.2.0</a></p>

<p>Note that the <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/nodejs-dynamodb-tutorial.html"" rel=""nofollow noreferrer"">node.js Elastic Beantalk with DynamoDB tutorial</a> I followed includes this somewhat outdated npm-shrinkwrap.json!</p>

<p>I hope I do not break any rules by answering my own question.</p>
",4475379,2017-04-18T19:03:07.197,0,CC BY-SA 3.0,,1,,shrinkwrap.json!</p,0,I followed includes this somewhat outdated npm-shrinkwrap.json!</p,False,False
265,43543870,2,43542746,2017-04-21T13:21:28.747,1,"<p>I am not using the <a href=""https://github.com/awslabs/amazon-sqs-java-extended-client-lib"" rel=""nofollow noreferrer"">Amazon SQS Extended Client Library for Java</a> myself, so I can not really judge the details, but it looks like it has gotten out of sync with the AWS Java SDK.</p>

<p>See <a href=""https://github.com/awslabs/amazon-sqs-java-extended-client-lib/issues/12"" rel=""nofollow noreferrer"">Exeption on AmazonSQSExtendedClient.deleteMessage</a> for a (so far unresolved) issue on this, as well as <a href=""https://github.com/awslabs/amazon-sqs-java-extended-client-lib/issues/14"" rel=""nofollow noreferrer"">SQS Extended Client Lib not compatible with Core SDK 1.11.xx</a> for a more general one.</p>

<p>The latter points to the <em>eventually</em> helpful <a href=""https://github.com/awslabs/amazon-sqs-java-extended-client-lib/pull/11"" rel=""nofollow noreferrer"">support aws-sdk-java 1.11.8</a> pull request, though that one is pretty outdated by now as well.</p>
",46149,2017-04-21T13:21:28.747,1,CC BY-SA 3.0,,1,,well.</p,0,"> pull request, though that one is pretty outdated by now as well.</p",False,False
266,43620365,2,17309559,2017-04-25T20:27:51.790,3,"<p>Since this post is so old and I believe streaming directly is now supported, I spent a lot of time reading out of date answers on this topic...</p>

<p>If it helps anyone I was able to stream from the client to s3 directly without the need for installing packages:</p>

<p><a href=""https://gist.github.com/mattlockyer/532291b6194f6d9ca40cb82564db9d2a"" rel=""nofollow noreferrer"">https://gist.github.com/mattlockyer/532291b6194f6d9ca40cb82564db9d2a</a></p>

<p>The server assumes <code>req</code> is a stream object, in my case a File object was used in xhr(send) which will send binary data in modern browsers.</p>

<pre><code>const fileUploadStream = (req, res) =&gt; {
  //get ""body"" args from header
  const { id, fn } = JSON.parse(req.get('body'));
  const Key = id + '/' + fn; //upload to s3 folder ""id"" with filename === fn
  const params = {
    Key,
    Bucket: bucketName, //set somewhere
    Body: req, //req is a stream
  };
  s3.upload(params, (err, data) =&gt; {
    if (err) {
      res.send('Error Uploading Data: ' + JSON.stringify(err) + '\n' + JSON.stringify(err.stack));
    } else {
      res.send(Key);
    }
  });
};
</code></pre>

<p>Yes it breaks convention but if you look at the gist it's much cleaner than anything else I found relying on other packages.</p>

<p>+1 for pragmatism and thanks to @SalehenRahman for his help.</p>
",1060487,2017-04-25T20:27:51.790,2,CC BY-SA 3.0,,1,,...,0,"and I believe streaming directly is now supported, I spent a lot of time reading outdated answers on this topic...",False,False
267,43645031,2,32082115,2017-04-26T21:44:05.163,1,"<p>After many hours of searching through outdated SO answers, here's my working setup:</p>

<p>.ebextensions/01-migrate-db.config</p>

<pre><code>container_commands:
  01_node_binary:
    command: ""ln -sf `ls -td /opt/elasticbeanstalk/node-install/node-* | head -1`/bin/node /bin/node""
    leader_only: true
  02_npm_binary:
    command: ""ln -sf `ls -td /opt/elasticbeanstalk/node-install/node-* | head -1`/bin/npm /bin/npm""
    leader_only: true
  03_migrate_db:
    command: ""sudo DB_HOST=${DB_HOST} DB_PORT=${DB_PORT} DB_NAME=${DB_NAME} DB_USER=${DB_USER} DB_PASSWORD=${DB_PASSWORD} npm run db:migration:run""
    leader_only: true
</code></pre>

<p>package.json</p>

<pre><code>...
scripts: {
  ""db:migration:run"": ""knex migrate:latest""
}
</code></pre>
",1249098,2017-04-26T21:44:05.163,0,CC BY-SA 3.0,,1,,working,0,"After many hours of searching through outdated SO answers, here's my working",False,False
268,43793859,2,43793382,2017-05-04T22:41:45.410,0,"<p>The issue was caused by my CLI being out of date.</p>
",3166160,2017-05-04T22:41:45.410,0,CC BY-SA 3.0,,1,,outdated.</p,0,The issue was caused by my CLI being outdated.</p,False,False
269,43797138,2,43797035,2017-05-05T05:26:56.360,0,"<p>Found the error. The code in my question is from Amazon tutorials - <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadObjSingleOpJava.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadObjSingleOpJava.html</a></p>

<p>However I am sure it is incorrect or deprecated. </p>

<p>Pay attention to the below line</p>

<pre><code>s3client.putObject(new PutObjectRequest(bucketName, keyName, file));
</code></pre>

<p>For this to work, instead of <code>keyName</code>, you have to pass the filename with extension. For an example <code>websitepage.html</code>.</p>
",1379286,2017-05-05T05:26:56.360,1,CC BY-SA 3.0,,1,,.,0,However I am sure it is incorrect or deprecated.,False,False
270,43819599,2,43565698,2017-05-06T10:56:55.857,1,"<p>I went through a similar instead some months ago, basically I have an ec2 instance with Kafka in N. Virginia and I configured topbeat on my local machine to send metrics to that ec2 instance. 
I was able to get it working by adding </p>

<pre><code>advertised.host.name=public-ip
</code></pre>

<p>as configuration in the server.properties of kafka, but according to the <a href=""http://kafka.apache.org/documentation/#brokerconfigs"" rel=""nofollow noreferrer"">documentation</a> this properties is deprecated. </p>

<p>Reading further <a href=""http://kafka.apache.org/documentation/#brokerconfigs"" rel=""nofollow noreferrer"">documentation</a>, it is said that if you are in an IaaS environment, you have to configure the advertised.listeners different from the interface to which the broker binds. </p>
",6115442,2017-05-06T10:56:55.857,0,CC BY-SA 3.0,,1,properties,.,0,this properties is deprecated.,False,False
271,43889585,2,43883077,2017-05-10T10:16:38.887,0,"<p>Not sure, how did you manage to upload .part files to s3 without authentication even if you have tweaked s3 policies. I guess you might have added aws keys in the system environment as properties or in conf files.
In order to access aws resource, atleast its required to supply access key and secret key. Also, s3 scheme is deprecated now.
Following code works with hadoop-aws-2.8.0.jar and spark 2.1.
(note: I should have used s3a scheme as its the preferred over s3n (native scheme).</p>

<pre><code>val spark = SparkSession
              .builder
              .appName(""SparkS3Integration"")
              .master(""local[*]"")
              .getOrCreate()
            spark.sparkContext.hadoopConfiguration.set(""fs.s3n.awsAccessKeyId"", awsAccessKey)
            spark.sparkContext.hadoopConfiguration.set(""fs.s3n.awsSecretAccessKey"", awsSecretKey)

 val rdd = spark.sparkContext.parallelize(Seq(1,2,3,4))
 rdd.saveAsTextFile(""s3n://&lt;bucket_name&gt;/&lt;path&gt;"")
</code></pre>
",6093375,2017-05-10T10:16:38.887,1,CC BY-SA 3.0,,1,scheme,"
",0,"Also, s3 scheme is deprecated now.
",False,False
272,43905495,2,43894093,2017-05-11T02:26:05.170,1,"<p>The ridiculousness of this requirement notwithstanding, here's how you tell the browser what to call the file it's downloading when the file is saved locally: you need S3 to add this response header:</p>

<pre><code>Content-Disposition: attachment; filename=""the-desired-name.ext""
</code></pre>

<p>This can be done in 2 ways:</p>

<p><strong>Option 1.</strong></p>

<p>When you upload the file to S3, send that header.  S3 will return it with each download, and the browser will use that filename.</p>

<p><strong>Option 2.</strong></p>

<p>In your signed URL, tell S3 to add this header to the response.  This will add <code>&amp;response-content-disposition=attac...</code> to the signed URL and S3 will see this and add the needed response header.</p>

<p>How you do this depends on what you're using to generate the pre-signed URL (i.e. which SDK, or if you wrote your own, as I did).  </p>

<p>Additionally, when generating a pre-signed URL, there's a little-known feature that allows you to identify who you gave the signed URL to, right in the URL.  Since signed URLs cannot be altered or tampered with without completely invalidating the URL, a user is discouraged from ""sharing"" URLs they may discover because the URL identifies them.  :)  How is this done?  You add <code>x-amz-meta-{anything}={anything-else}</code> to the URL before signing. Example: <code>...&amp;x-amz-meta-downloaded-by=michael-sqlbot&amp;...</code>. The part of S3 that validates the URL will require these parameters to be present so that the URL will match the signature, but the part of S3 that actually returns what is being downloaded... disregards it.</p>

<p>Of course... if you're not using pre-signed URLs, then that is the <em>real</em> problem you have... and hiding the URL is just an attempt to solve the wrong problem.  Any minimally-sophisticated user can determine the URL and I would assert that there is no possible genuine security benefit to be achieved by trying to hide it.</p>
",1695906,2017-05-11T02:26:05.170,0,CC BY-SA 3.0,,1,user, ,1,"Since signed URLs cannot be altered or tampered with without completely invalidating the URL, a user is discouraged from ""sharing"" URLs they may discover because the URL identifies them.  ",False,False
273,44163299,2,44159204,2017-05-24T15:57:39.990,6,"<p>According to the docs <a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlifecycle.html"" rel=""noreferrer"">here</a> you need to add Filter element, which is required as per Amazon API, and confusingly enough, not required by boto. I added the deprecated Prefix argument instead of the Filter and it seems to be working too.</p>
",5581236,2017-05-24T15:57:39.990,1,CC BY-SA 3.0,,1,,too.</p,0,I added the deprecated Prefix argument instead of the Filter and it seems to be working too.</p,False,False
274,44178910,2,44158930,2017-05-25T11:01:19.550,1,"<p>So, it seems like I was barking at the wrong tree here, trying to use Scan instead of Query : </p>

<blockquote>
  <p>The customer should not use ScanFilter, which is deprecated. List/Map datatypes are better suited to the new FilterExpression syntax. I suspect the problem is with the multi-value attribute with ""N:"" in the name; this doesn't seem close to the wire format. To use the ""new"" filter expressions, they should create ExpressionAttributeNames and ExpressionAttributeValues to tell the AWS SDK about the attributes for the filter, addExpressionAttributeNamesEntry and addExpressionAttributeValuesEntry.<a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBScanExpression.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBScanExpression.html</a> Also, I would be remiss if I didn't point out that Scan is absolutely not the right API action for this query; Query is the correct action. testName should be used with Query with a KeyConditionExpression to narrow the search down to the correct partition, and revision should be added to the same with the BETWEEN syntax. Scan is intended for backups and background operations, and in this case the customer's index schema can be used with Query to narrow the search. Caution them about using Scan; this is a fundamental best practice:<a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/QueryAndScanGuidelines.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/QueryAndScanGuidelines.html</a></p>
</blockquote>

<p>So I've used Query instead :</p>

<pre><code>    Map&lt;String, AttributeValue&gt; eav = new HashMap&lt;String, AttributeValue&gt;();
    eav.put("":v1"", new AttributeValue().withS(testName));
    eav.put("":v2"", new AttributeValue().withS(version));
    eav.put("":v3"", new AttributeValue().withN(String.valueOf(minRev)));
    eav.put("":v4"", new AttributeValue().withN(String.valueOf(maxRev)));

    final DynamoDBQueryExpression&lt;ValidationReport&gt; query = new DynamoDBQueryExpression&lt;&gt;();
    query.withKeyConditionExpression(""testName = :v1"");
    query.withFilterExpression(""buildVersion = :v2 and revision BETWEEN :v3 AND :v4"");
    query.setConsistentRead(false);
    query.withExpressionAttributeValues(eav);
    return getMapper().query(ValidationReport.class, query);`
</code></pre>

<p>Please note that when using the AttributeValue there's quite a difference between the following :</p>

<pre><code>new AttributeValue().withS(""SomeStringValue"")
new AttributeValue().withN(""SomeNumber"")
</code></pre>

<p>Which is what I was missing</p>
",1249794,2017-05-25T11:01:19.550,0,CC BY-SA 3.0,,1,,.,1,"The customer should not use ScanFilter, which is deprecated.",False,False
275,44244284,2,34107898,2017-05-29T13:56:52.477,0,"<p>It seems the DynamoDB endpoint URL hasn't been specified yet. Please set the endpoint value in client object. Endpoint URL must be specified, even if we are using local DynamoDB (e.g. localhost:8000).</p>

<pre><code>AmazonDynamoDBClient client = new AmazonDynamoDBClient(credentials);
client.setEndpoint(""http://localhost:8000"");
client.setRegion(Region.getRegion(Regions.US_WEST_2));
</code></pre>

<p>Additional note, it seems that you are using deprecated way of instantiating a DynamoDB client object. The following way is preferred, esp. if you are using the latest AWS DynamoDB SDK.</p>

<pre><code>AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard()
                .withCredentials(new AWSStaticCredentialsProvider(
                        new BasicAWSCredentials(""*********"", ""************"")))
                .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(
                        ""http://localhost:8000"",
                        Regions.US_WEST_2.getName()))
                .build();
</code></pre>
",1527997,2017-05-29T13:56:52.477,0,CC BY-SA 3.0,,1,,.,0,">Additional note, it seems that you are using deprecated way of instantiating a DynamoDB client object.",False,False
276,44341051,2,44325418,2017-06-03T06:29:49.530,1,"<p>Signature Version 2 has been deprecated in favor of the more secure version 4. We don't support it, and the only reason it is supported in other sdks is for backwards compatibility. For Amazon S3, you can turn off body signing for v4. This is the default option for the S3 Client in the C++ SDK.</p>
",734691,2017-06-03T06:29:49.530,3,CC BY-SA 3.0,,1,Version,.,0,<p>Signature Version 2 has been deprecated in favor of the more secure version 4.,False,False
277,44443872,2,44443417,2017-06-08T19:15:23.850,13,"<p>Unless you plan on maintaining it by yourself, no, you should not use TitanDB. Titan has not seen updates in almost 2 years. It is far out of date on its core dependencies.</p>

<p>You should use <a href=""https://janusgraph.org"" rel=""noreferrer"">JanusGraph</a>. It was established as a project at the Linux Foundation in January 2017, and 2 releases have already been made. Many individuals and companies are involved in the open community.</p>

<p>If you check out the <a href=""https://github.com/awslabs/dynamodb-titan-storage-backend/tree/master"" rel=""noreferrer"">AWS Labs DynamoDB Titan Storage adapter</a>, you'll see that <a href=""https://github.com/amcp"" rel=""noreferrer"">Alex Patrikalakis</a> has already integrated JanusGraph 0.1.1 on the master branch. Alex is also a committer on JanusGraph.</p>
",5025129,2017-06-08T19:15:23.850,2,CC BY-SA 3.0,,1,It,dependencies.</p,0,It is far outdated on its core dependencies.</p,False,False
278,44452017,2,44430121,2017-06-09T07:38:59.963,0,"<p>I was using 3rd party library ""Simpl3r""  which uploads file to my bucket. Since i inserted this library quiet some time ago, so the aws-sdk-s3 version it was using was outdated.</p>

<p>Solution which works for me:</p>

<ol>
<li><p>open simpler gradle file.</p>

<p>removed line: compile fileTree(dir: 'libs', include: ['*.jar'])</p>

<p>added line : compile 'com.amazonaws:aws-android-sdk-core:2.4.4'</p></li>
<li><p>update aws sdk ver over there i.e 2.4.4 (latest at the moment)</p></li>
<li><p>integrated aws sdk ver 2.4.4 (similar with Simpl3r library version) in    build.gradle(app), to use this aws library function throughout my app module .    </p></li>
</ol>

<p>Atlast, my duplicate entry of classes.class (which builds at run time) issue gets resolved. </p>

<p>Tip: Try to make version of your sub-module(gradle files) similar to app gradle file.<br>
Also, make sure that you use compile fileTree(dir:'libs', include: ['*jar']) in sub module gradle file only if , any particular jar file needs to be run with that particular module .</p>
",7434126,2017-06-09T07:38:59.963,0,CC BY-SA 3.0,,1,,outdated.</p,0,"Since i inserted this library quiet some time ago, so the aws-sdk-s3 version it was using was outdated.</p",False,False
279,44487474,2,44482351,2017-06-11T18:47:22.543,2,"<p>That cookbook (<code>python</code>) is deprecated and does not work with Chef 13. Use <code>poise-python</code> instead.</p>
",78722,2017-06-11T18:47:22.543,0,CC BY-SA 3.0,,1,python</code,.,1,>python</code>) is deprecated and does not work with Chef 13.,False,False
280,44616825,2,44615176,2017-06-18T15:55:35.533,0,"<p>Amazon do not make the whole transcript available only the parts that match an intent and the extracted slots.</p>

<p>You used to be able to get this data but it was deprecated  for US users and never made available to users in other regions.</p>
",504554,2017-06-18T15:55:35.533,0,CC BY-SA 3.0,,1,it,regions.</p,1,but it was deprecated  for US users and never made available to users in other regions.</p,False,False
281,44686986,2,44682499,2017-06-21T22:02:52.153,0,"<p>The first argument to <code>subprocess.call</code> has to be a program or executable. In your case, it is not. Looks like you want to execute the call in a shell, so set this argument <code>shell=True</code>. Note: using <code>shell=True</code> is a security hazard. </p>

<blockquote>
  <p><strong>Warning</strong> Executing shell commands that incorporate unsanitized input from an untrusted source makes a program vulnerable to shell
  injection, a serious security flaw which can result in arbitrary
  command execution. For this reason, the use of shell=True is strongly
  discouraged in cases where the command string is constructed from
  external input.</p>
</blockquote>

<pre><code>subprocess.call([""export instance_profile=`curl http://169.254.169.254/latest/meta-data/iam/security-credentials`"",
""export AWS_ACCESS_KEY_ID=`curl http://169.254.169.254/latest/meta- data/iam/security-credentials/${instance_profile} | grep AccessKeyId | cut -d':' -f2 | sed 's/[^0-9A-Z]*//g'`"",
""export AWS_SECRET_ACCESS_KEY=`curl http://169.254.169.254/latest/meta-data/iam/security- credentials/${instance_profile} | grep SecretAccessKey | cut -d':' - f2 | sed 's/[^0-9A-Za-z/+=]*//g'`"",
""export AWS_SECURITY_TOKEN=`curl http://169.254.169.254/latest/meta-data/iam/security-credentials/${instance_profile} | grep Token | cut -d':' -f2 | sed 's/[^0-9A-Za-z/+=]*//g'`"",
""export http_proxy=proxy.xxx.xxxxxxxxx.com:8099"",
""export https_proxy=${http_proxy}""], shell=True)
</code></pre>
",4237701,2017-06-21T22:02:52.153,0,CC BY-SA 3.0,,1,True,input.</p,0,"True is strongly
  discouraged in cases where the command string is constructed from
  external input.</p",False,False
282,44832956,2,44678025,2017-06-29T18:56:38.540,0,"<p>Thanks for pointing that out. Can you give the links of the documentation that has deprecated classes? We will try to keep the latest ones and remove if there are any redundant deprecated references.</p>

<ol>
<li><p>load() API is used to retrieve an item: Using an object's primary key load the corresponding item from the database. Look for an example here: <a href=""http://docs.aws.amazon.com/mobile/sdkforios/developerguide/dynamodb-object-mapper.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/mobile/sdkforios/developerguide/dynamodb-object-mapper.html</a> under ""Retrieve an item"" section.
query() API can be used to return any number of records that match the query. The query API enables you to query a table or a secondary index.
To answer your question, if you know the primary key of the record that you are trying to retrieve, you can use load() API, otherwise use query() API.</p></li>
<li><p>DynamoDBStreams work well for your use case. Otherwise you can intgerate AWS Lambda with DynamoDB table to do polling which will be cleaner than a timer based approach. This question is partially answered here: <a href=""https://stackoverflow.com/questions/36636166/hooks-for-aws-dynamodb-streams"">Hooks for AWS DynamoDB streams</a></p></li>
</ol>
",5169068,2017-06-29T18:56:38.540,1,CC BY-SA 3.0,,2,,?,0,Can you give the links of the documentation that has deprecated classes?,False,False
283,45037883,2,44738071,2017-07-11T14:47:44.793,0,"<p>Thanks for this solution!</p>

<p>I'm using lambci/docker-lambda with Docker to test my lambda functions, and just like real lambda, the botocore is currently outdated. To add botocore to your lambda project:</p>

<p><code>pip install botocore -t /your/project/dir</code> </p>

<p>In case you're working on Mac OSX and installed pip using brew, the <code>-t</code> won't work. Execute the following command where your lambda_function.py is located, and you're good to go. </p>

<p><code>docker run -v ""$PWD"":/localdir python:2.7-alpine pip install botocore -t /localdir</code></p>
",8290239,2017-07-11T14:47:44.793,0,CC BY-SA 3.0,,1,botocore,.,0,">I'm using lambci/docker-lambda with Docker to test my lambda functions, and just like real lambda, the botocore is currently outdated.",False,False
284,45142379,2,45140929,2017-07-17T10:55:14.010,1,"<p>If you want to ""replace"" the data each time you run, and not to ""append"" it, then you have to configure for such a scenario in your Spark Elasticsearch properties.</p>

<p>First thing you need to do is to have an ID in your document, and tell elastisearch what is your id ""column"" (if you come from a dataframe) or key (in json terms).</p>

<p>This is documented here : <a href=""https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html</a></p>

<blockquote>
  <p>For cases where the id (or other metadata fields like ttl or timestamp) of the document needs to be specified, one can do so by setting the appropriate mapping namely es.mapping.id. Following the previous example, to indicate to Elasticsearch to use the field id as the document id, update the RDD configuration (it is also possible to set the property on the SparkConf though due to its global effect it is discouraged):</p>

<pre><code>EsSpark.saveToEs(rdd, ""spark/docs"", Map(""es.mapping.id"" -&gt; ""id""))
</code></pre>
</blockquote>

<p>A second configuration key is available to control what kind of job elasticsearch tries to do upon writing data, but the default is correct for your user case :  </p>

<blockquote>
  <p>es.write.operation (default index)</p>
  
  <p>The write operation elasticsearch-hadoop should peform - can be any of:</p>
  
  <p>index (default)
          new data is added while existing data (based on its id) is replaced (reindexed).   </p>
  
  <p>create
          adds new data - if the data already exists (based on its id), an exception is thrown. </p>
  
  <p>update
          updates existing data (based on its id). If no data is found, an exception is thrown. </p>
  
  <p>upsert
          known as merge or insert if the data does not exist, updates if the data exists (based on its id). </p>
</blockquote>
",2131074,2017-07-17T10:55:14.010,1,CC BY-SA 3.0,,1,,"
",0,"discouraged):</p>

<pre><code>EsSpark.saveToEs(rdd, ""spark/docs"", Map(""es.mapping.id"" -&gt; ""id""))
",False,False
285,45190588,2,45190364,2017-07-19T12:27:17.427,3,"<p>Deprecation warnings is not an error, it's just the compiler warning you that something has been deprecated and may be removed in the future - your code will still work even if you're using <code>new AmazonKinesisClient()</code>, until that constructor is removed from the SDK sometime in the future.</p>

<p>The new way of creating clients in the AWS SDK is to use the builder API like this:</p>

<pre><code>final AmazonKinesisClientBuilder builder = AmazonKinesisClient.builder();
final AmazonKinesis client = builder.build();
</code></pre>

<p>This way, you can use <code>builder</code> to customize the client, like setting region or using STS credentials.</p>

<p>If you just want to get an instance using the default settings you can do:</p>

<pre><code>final AmazonKinesis client = AmazonKinesisClient.builder().build();
</code></pre>
",3182188,2017-07-19T12:27:17.427,1,CC BY-SA 3.0,,1,,future.</p,1,"Deprecation warnings is not an error, it's just the compiler warning you that something has been deprecated and may be removed in the future - your code will still work even if you're using <code>new AmazonKinesisClient()</code>, until that constructor is removed from the SDK sometime in the future.</p",False,False
286,45292262,2,45288311,2017-07-25T01:04:06.677,1,"<p><em>Public Buckets</em> are very rare. In fact, they are highly discouraged from a <strong>security</strong> perspective and also from a <strong>cost</strong> perspective -- somebody could upload illegal files and use it for file sharing, and the bucket owner would pay for it!</p>

<p>I would normally say that the chance of somebody being able to successfully upload to a random bucket is practically zero, but I suspect you are thinking of a case where an <strong>evil party</strong> might create a similarly-named bucket in the hope of collecting confidential data (similar to domain-name camping).</p>

<p>In that case, you can create a Deny policy on the user to <strong>prohibit access to ALL S3 buckets, except for the ones you specifically nominate</strong>:</p>

<pre><code>{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Sid"": ""Allow"",
            ""Effect"": ""Allow"",
            ""Action"": [
                ""s3:*""
            ],
            ""Resource"": [
                ""arn:aws:s3:::good-bucket1"",
                ""arn:aws:s3:::good-bucket2""
            ]
        },
        {
            ""Sid"": ""NotOthers"",
            ""Effect"": ""Deny"",
            ""Action"": [
                ""s3:*""
            ],
            ""NotResource"": [
                ""arn:aws:s3:::good-bucket1"",
                ""arn:aws:s3:::good-bucket2""                ]
        }
    ]
}
</code></pre>

<p>This will work because the <code>Deny</code> against the IAM User will override any <code>Allow</code> in a Bucket Policy. The only downside is that <strong>you will need to specifically list the buckets you wish to include/exclude</strong> because there is no way to specify that rules apply to ""a public bucket"".</p>
",174777,2017-07-25T01:04:06.677,0,CC BY-SA 3.0,,1,,it!</p,0,"In fact, they are highly discouraged from a <strong>security</strong> perspective and also from a <strong>cost</strong> perspective -- somebody could upload illegal files and use it for file sharing, and the bucket owner would pay for it!</p",False,False
287,45335927,2,44374705,2017-07-26T19:32:27.433,0,"<p>To expand a little on Muhammad Abdullah's answer to his own question.</p>

<p>Using an outdated SDK is indeed the issue. I went wrong on this when looking at the developer guide (<a href=""http://docs.aws.amazon.com/mobile/sdkforandroid/developerguide/setup.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/mobile/sdkforandroid/developerguide/setup.html</a>) which is not kept up to date on the SDKs. </p>

<p>On their github page (<a href=""https://github.com/aws/aws-sdk-android"" rel=""nofollow noreferrer"">https://github.com/aws/aws-sdk-android</a>) you can find the latest version of the SDKs without having to download them. Once you have found the current version number you can simply update the build.gradle file to reflect this newer version.</p>

<p>For example change <code>compile 'com.amazonaws:aws-android-sdk-core:2.2.+'</code> to <code>compile 'com.amazonaws:aws-android-sdk-core:2.4.+'</code> note that the third part of the version number is covered by the + so only the first two parts are relevant here.</p>
",7868000,2017-07-26T19:32:27.433,0,CC BY-SA 3.0,,1,,.,0,Using an outdated SDK is indeed the issue.,False,False
288,45355425,2,45352856,2017-07-27T15:44:03.863,1,"<p>You do not query <code>s3</code> information through the Spark API, but rather through the AWS S3 SDK. You can do it like this:</p>

<pre><code>import com.amazonaws.services.s3.AmazonS3Client

val lastModified = new AmazonS3Client().getObject(""myBucket"",""path/to/file"").getObjectMetadata.getLastModified
</code></pre>

<p>Obviously, you will have to download the AWS S3 SDK through Maven and include the dependency. Also I think they might have deprecated the <code>AmazonS3Client</code> in newer versions of the SDK, so you may need to make slight changes depending on which version of the SDK you download:)</p>
",5291448,2017-07-27T15:44:03.863,0,CC BY-SA 3.0,,1,,AmazonS3Client</code,0,Also I think they might have deprecated the <code>AmazonS3Client</code,False,False
289,45390471,2,44150156,2017-07-29T14:36:32.183,39,"<p>Long story short: Dynamo does not support this. It's not build for this use-case. It's intended for quick data access with low-latency. It simply does not support any aggregating functionality.</p>

<p>You have three main options:</p>

<ul>
<li><p>Export DynamoDB data to <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/RedshiftforDynamoDB.html"" rel=""noreferrer"">Redshift</a> or <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EMRforDynamoDB.Tutorial.html"" rel=""noreferrer"">EMR Hive</a>. Then you can execute SQL queries on a stale data. The benefit of this approach is that it consumes RCUs just once, but you will stick with outdated data.</p></li>
<li><p>Use <a href=""https://github.com/awslabs/emr-dynamodb-connector"" rel=""noreferrer"">DynamoDB connector</a> for Hive and directly query DynamoDB. Again you can write arbitrary SQL queries, but in this case it will access data in DynamoDB directly. The downside is that it will consume read capacity on every query you do.</p></li>
<li><p>Maintain aggregated data in a separate table using <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"" rel=""noreferrer"">DynamoDB streams</a>. For example you can have a table UserId as a partition key and a nested map with tags and counts as an attribute. On every update in your original data DynamoDB streams will execute a Lambda function or some code on your hosts to update aggregate table. This is the most cost efficient method, but you will need to implement additional code for each new query.</p></li>
</ul>

<p>Of course you can extract data at the application level and aggregate it there, but I would not recommend to do it. Unless you have a small table you will need to think about throttling, using just part of provisioned capacity (you want to consume, say, 20% of your RCUs for aggregation and not 100%), and how to distribute your work among multiple workers.</p>

<p>Both Redshift and Hive already know how to do this. Redshift relies on multiple worker nodes when it executes a query, while Hive is based on top of Map-Reduce. Also, both Redshift and Hive can use predefined percentage of your RCUs throughput.</p>
",1112503,2017-07-29T14:36:32.183,0,CC BY-SA 3.0,,1,,data.</p></li,0,"The benefit of this approach is that it consumes RCUs just once, but you will stick with outdated data.</p></li",False,True
290,45420948,2,44027709,2017-07-31T16:25:28.073,3,"<p>This is due to outdated dependencies in the current version of the <code>awsebcli</code> tool. They pinned version ""docker-py (>=1.1.0,&lt;=1.7.2)"" which does not support the newer credential helper formats. The latest version of <code>docker-py</code> is the first one to properly support the latest credential helper format and until the AWS EB CLI developers update <code>docker-py</code> to use 2.4.0 (<a href=""https://github.com/docker/docker-py/releases/tag/2.4.0"" rel=""nofollow noreferrer"">https://github.com/docker/docker-py/releases/tag/2.4.0</a>) this will remain broken.</p>
",1950432,2017-07-31T16:25:28.073,0,CC BY-SA 3.0,,1,,.,0,This is due to outdated dependencies in the current version of the <code>awsebcli</code> tool.,False,False
291,45696811,2,45657251,2017-08-15T16:04:19.753,0,"<p>Always use boto3. boto is deprecated. </p>

<ol>
<li><p>As long as you setup AWS CLI credential, you don't need to pass the hard-coded credential. Read <a href=""http://boto3.readthedocs.io/en/latest/guide/configuration.html"" rel=""nofollow noreferrer"">boto3 credential setup</a> throughly.</p></li>
<li><p>There is no reason to initiate boto3.session unless you are using different region and user profile. </p></li>
<li><p>Take your time and study difference between service client(boto3.client) vs service resources(boto3.resources).</p></li>
<li><p>Low level boto3.client is easier to use for experiments. Use high level boto3.resource if you need to pass around arbitrary object.</p></li>
</ol>

<p>Here is the simple code for <a href=""http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.download_file"" rel=""nofollow noreferrer"">boto3.client(""s3"").download_file</a>.</p>

<pre><code>import boto3 
# initiate the proper AWS services client, i.e. S3 
s3 = boto3.client(""s3"")
s3.download_file('your_bucket_name', 'Date_Table.csv', '/your/local/path/and/filename')
</code></pre>
",6017840,2017-08-15T16:04:19.753,0,CC BY-SA 3.0,,1,boto,.,0,boto is deprecated.,False,False
292,45698759,2,45547556,2017-08-15T18:02:11.867,0,"<p>Typically for B/G testing you wouldn't use different dns for new functions, but define rules, such as every 100th user gets send to the new function or only ips from a certain region or office have access to the new functionality, etc. </p>

<p>Assuming you're using AWS, you should be able to create an ALB in front of the ELBs for context based routing in which you should be able define rules for your routing to either B or G. In this case you have to separate environments functioning independently (possibly using the same DB though).</p>

<p>For more complicated rules, you can use tools such as leanplum or omniture inside your spring boot application. With this approach you have one single environment hosting old and new functionality and later you'd remove the code that is outdated. </p>
",3794466,2017-08-15T18:02:11.867,0,CC BY-SA 3.0,,1,,.,0,With this approach you have one single environment hosting old and new functionality and later you'd remove the code that is outdated.,False,False
293,45731516,2,19088649,2017-08-17T09:36:05.123,2,"<p>If anyone is looking for this answer, its outdated and you can find the new documentation here: <a href=""https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/commands.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/commands.html</a></p>

<pre><code>use Aws\S3\S3Client;
use Aws\CommandPool;

// Create the client.
$client = new S3Client([
    'region'  =&gt; 'us-standard',
    'version' =&gt; '2006-03-01'
]);

$bucket = 'example';
$commands = [
    $client-&gt;getCommand('HeadObject', ['Bucket' =&gt; $bucket, 'Key' =&gt; 'a']),
    $client-&gt;getCommand('HeadObject', ['Bucket' =&gt; $bucket, 'Key' =&gt; 'b']),
    $client-&gt;getCommand('HeadObject', ['Bucket' =&gt; $bucket, 'Key' =&gt; 'c'])
];

$pool = new CommandPool($client, $commands);

// Initiate the pool transfers
$promise = $pool-&gt;promise();

// Force the pool to complete synchronously
$promise-&gt;wait();
</code></pre>

<p>Same thing can be done for SES commands</p>
",4722097,2017-08-17T09:36:05.123,1,CC BY-SA 3.0,,1,,/,0,">If anyone is looking for this answer, its outdated and you can find the new documentation here: <a href=""https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/commands.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/",False,False
294,45746614,2,45734959,2017-08-18T00:01:59.347,2,"<p>This is not a reportable bug... it is a documented limitation in the REST interface of S3.</p>

<blockquote>
  <p>User-defined metadata is a set of key-value pairs. Amazon S3 stores user-defined metadata keys in lowercase. <strong>Each key-value pair must conform to US-ASCII when using REST</strong> and UTF-8 when using SOAP or browser-based uploads via POST. (Emphasis added.)</p>
  
  <p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-metadata"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-metadata</a></p>
</blockquote>

<p>SOAP is deprecated (not to mention terrible), and even though POST uploads will allow you to store UTF-8 characters in metadata, don't do it, because you may be unable to work with the object or read back the metadata.</p>

<p>In contrast with object metadata, object <em>tagging</em>   <strong>does</strong> support UTF-8.</p>

<blockquote>
  <p>A tag key can be up to 128 Unicode characters in length and tag values can be up to 256 Unicode characters in length.</p>
  
  <p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/object-tagging.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/dev/object-tagging.html</a></p>
</blockquote>
",1695906,2017-08-18T00:01:59.347,0,CC BY-SA 3.0,,1,SOAP,metadata.</p,1,"SOAP is deprecated (not to mention terrible), and even though POST uploads will allow you to store UTF-8 characters in metadata, don't do it, because you may be unable to work with the object or read back the metadata.</p",False,True
295,46173381,2,37868404,2017-09-12T09:57:21.223,2,"<p>Ideally you should use s3a rather than s3n, as s3n is deprecated.</p>

<p>With s3a, there is a parameter:</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;fs.s3a.buffer.dir&lt;/name&gt;
  &lt;value&gt;${hadoop.tmp.dir}/s3a&lt;/value&gt;
  &lt;description&gt;Comma separated list of directories that will be used to buffer file
uploads to. No effect if fs.s3a.fast.upload is true.&lt;/description&gt;
&lt;/property&gt;
</code></pre>

<p>When you are getting the local file error, it most likely because the buffer directory has no space.</p>

<p>While you can change this setting to point at a directory with more space, a better solution may be to set (again in S3a):</p>

<p>fs.s3a.fast.upload=true</p>

<p>This avoids buffering the data on local disk and should actually be faster too.</p>

<p>The S3n buffer directory parameter should be:</p>

<pre><code>fs.s3.buffer.dir
</code></pre>

<p>So if you stick with s3n, ensure it has plenty of space and it should hopefully resolve this issue.</p>
",88839,2017-09-12T09:57:21.223,1,CC BY-SA 3.0,,1,,deprecated.</p,0,"Ideally you should use s3a rather than s3n, as s3n is deprecated.</p",False,False
296,46180625,2,46178220,2017-09-12T15:43:38.777,2,"<p>Turns out that for some reason outdated AWS documentation and examples are among the first results when doing a search. A few google results pages later, a more up to date example came up. Basically the form I was using was wrong. The correct one is as follows: </p>

<pre><code>&lt;html&gt;
  &lt;head&gt;

    &lt;meta http-equiv=""Content-Type"" content=""text/html; charset=UTF-8"" /&gt;

  &lt;/head&gt;
  &lt;body&gt;

  &lt;form action=""http://sigv4examplebucket.s3.amazonaws.com/"" method=""post"" enctype=""multipart/form-data""&gt;
    Key to upload: 
    &lt;input type=""input""  name=""key"" value=""user/user1/${filename}"" /&gt;&lt;br /&gt;
    &lt;input type=""hidden"" name=""acl"" value=""public-read"" /&gt;
    &lt;input type=""hidden"" name=""success_action_redirect"" value=""http://sigv4examplebucket.s3.amazonaws.com/successful_upload.html"" /&gt;
    Content-Type: 
    &lt;input type=""input""  name=""Content-Type"" value=""image/jpeg"" /&gt;&lt;br /&gt;
    &lt;input type=""hidden"" name=""x-amz-meta-uuid"" value=""14365123651274"" /&gt; 
    &lt;input type=""hidden"" name=""x-amz-server-side-encryption"" value=""AES256"" /&gt; 
    &lt;input type=""text""   name=""X-Amz-Credential"" value=""AKIAIOSFODNN7EXAMPLE/20151229/us-east-1/s3/aws4_request"" /&gt;
    &lt;input type=""text""   name=""X-Amz-Algorithm"" value=""AWS4-HMAC-SHA256"" /&gt;
    &lt;input type=""text""   name=""X-Amz-Date"" value=""20151229T000000Z"" /&gt;

    Tags for File: 
    &lt;input type=""input""  name=""x-amz-meta-tag"" value="""" /&gt;&lt;br /&gt;
    &lt;input type=""hidden"" name=""Policy"" value='&lt;Base64-encoded policy string&gt;' /&gt;
    &lt;input type=""hidden"" name=""X-Amz-Signature"" value=""&lt;signature-value&gt;"" /&gt;
    File: 
    &lt;input type=""file""   name=""file"" /&gt; &lt;br /&gt;
    &lt;!-- The elements after this will be ignored --&gt;
    &lt;input type=""submit"" name=""submit"" value=""Upload to Amazon S3"" /&gt;
  &lt;/form&gt;

&lt;/html&gt;
</code></pre>

<p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html"" rel=""nofollow noreferrer"">Here</a> is the link from where I got the form and where additional examples can be found.</p>
",3424245,2017-09-12T15:43:38.777,0,CC BY-SA 3.0,,1,,.,0,>Turns out that for some reason outdated AWS documentation and examples are among the first results when doing a search.,False,False
297,46208072,2,46205362,2017-09-13T23:02:21.537,1,"<p>Closest I could find is:</p>

<blockquote>
  <p>ASCII is a deprecated leader node–only function.</p>
</blockquote>

<p>You could write a <a href=""http://docs.aws.amazon.com/redshift/latest/dg/udf-creating-a-scalar-udf.html"" rel=""nofollow noreferrer"">Python User-Defined Function</a> that returns a character given the ASCII value.</p>
",174777,2017-09-13T23:02:21.537,2,CC BY-SA 3.0,,1,,function.</p,0,>ASCII is a deprecated leader node–only function.</p,False,False
298,46245728,2,36071026,2017-09-15T18:42:13.153,4,"<p>Existing answer is outdated. Associating existing Elastic IPs is now possible thanks to this change: <a href=""https://github.com/hashicorp/terraform/pull/5236"" rel=""nofollow noreferrer"">https://github.com/hashicorp/terraform/pull/5236</a></p>

<p>Docs: <a href=""https://www.terraform.io/docs/providers/aws/r/eip_association.html"" rel=""nofollow noreferrer"">https://www.terraform.io/docs/providers/aws/r/eip_association.html</a></p>

<p>Excerpt:</p>

<blockquote>
  <p>aws_eip_association </p>
  
  <p>Provides an AWS EIP Association as a top level
  resource, to associate and disassociate Elastic IPs from AWS Instances
  and Network Interfaces.</p>
  
  <p>NOTE: aws_eip_association is useful in scenarios where EIPs are either
  pre-existing or distributed to customers or users and therefore cannot
  be changed.</p>
</blockquote>
",3728792,2017-09-15T18:42:13.153,0,CC BY-SA 3.0,,1,answer,.,0,<p>Existing answer is outdated.,False,False
299,46266842,2,46260831,2017-09-17T17:22:16.207,1,"<p><strong>localhost</strong> inside a container doesn't point to real machine host. Literally when you use <strong>localhost</strong> inside any docker container it will be pointing to the container itself. </p>

<p>Docker provides two ways of handling this sitution:</p>

<p><strong>Links.</strong> This is deprecated way and <a href=""https://docs.docker.com/engine/userguide/networking/default_network/dockerlinks/"" rel=""nofollow noreferrer"">should be avoided</a> in new projects.</p>

<p><strong>Networks.</strong> Docker can establish a network where you can access your containers by DNS names (container ID, or container name).</p>

<p>Create a network in a <a href=""https://docs.docker.com/engine/userguide/networking/work-with-networks/"" rel=""nofollow noreferrer"">way described here</a>, replace <code>localhost</code> by the name of DB container and you'll be ready to go.</p>
",2065796,2017-09-17T17:22:16.207,2,CC BY-SA 3.0,,1,This,dockerlinks/,0,"This is deprecated way and <a href=""https://docs.docker.com/engine/userguide/networking/default_network/dockerlinks/",False,False
300,46297214,2,46274499,2017-09-19T09:48:59.923,0,"<p>Generally, it's a bad idea. If somebody has access to port on server that is used for agent-server communication (8443 if I'm not mistaken), he can register as agent and get all your cluster configs&amp;passwords. Or classic man-in-the-middle attack would allow to do the same by reading your unencrypted traffic. A bit more difficult attack would allow to send commands to agents (probably with root permissions).</p>

<p>Your issue sounds like you reprovisioned your <code>ambari-server</code> host, and left old <code>ambari-agent</code> instances running, or maybe your certificates became outdated? At first connection to <code>ambari-server</code>, agents generate certificates and send to server. Server signs these certificates with it's own key, so now server-agent connection is encrypted. Did you try to remove old certificates and restart server&amp;agents as suggested <a href=""https://community.hortonworks.com/articles/68799/steps-to-fix-ambari-server-agent-expired-certs.html"" rel=""nofollow noreferrer"">here</a>?</p>
",1421254,2017-09-19T09:48:59.923,0,CC BY-SA 3.0,,1,instances,?,0,"> instances running, or maybe your certificates became outdated?",False,False
301,46350623,2,46094535,2017-09-21T18:11:19.427,0,"<p>I suggest you refer to the <a href=""https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html"" rel=""nofollow noreferrer"">Build Specification Reference</a>.</p>

<p>Specifically, you should remove <code>addons:</code> as well as <code>s3_region:</code> as neither of these are valid CodeBuild flags. You should also be using <code>version: 0.2</code>, as <code>version: 0.1</code> has been deprecated.</p>

<p>Here is what your <code>buildspec.yml</code> should look like:</p>

<pre><code>version: 0.2
phases:
  build:
    commands:
      - echo Nothing to do yet

artifacts:
  files:
    - '**/*'
</code></pre>
",629493,2017-09-21T18:11:19.427,0,CC BY-SA 3.0,,1,,deprecated.</p,0,> has been deprecated.</p,False,False
302,46374359,2,46374358,2017-09-22T23:12:15.527,16,"<p>Here is my solution to get a GUI running on Amazon's AMI.  I used this <a href=""https://devopscube.com/setup-gui-for-amazon-ec2-linux/"" rel=""noreferrer"">post</a> as a starting point, but had to make many changes to get it working on Amazon's AMI.  I also added additional info to make this work in a reasonably automated way so an individual who needs to bring up this environment more than once could do it without too much hassle.</p>

<p>Note: I include a lot of commentary in this post.  I apologize in advance, but I thought it might be helpful to someone needing to make modfications if they could understand why made the various choices along the way.</p>

<p>The scripts included below install some files along the way.  See section 4 for a list of the files and the directory structure used by these scripts.</p>

<h2>Step 1. Install the Desktop</h2>

<p>After performing a 'yum update', most solutions include a line like</p>

<pre><code>sudo yum groupinstall -y ""Desktop""
</code></pre>

<p>This deceivingly simple step requires significantly more effort on the Amazon AMI.  This group is not configured in the Amazon AMI (AAMI from here on out).  The AAMI has Amazon's own repositories installed and enabled by default.  Also installed is the epel repo, but it is disabled by default.  After enabling epel I found the Desktop group but it was not populated with packages.  I also found Xfce (another desktop alternative) which was populated.  Eventually I decided to install Xfce rather than Desktop.  Still, that was not straight forward, but it eventually led to the solution.</p>

<p>Here it's worth noting that the first thing I tried was to install the centos repository and install the Desktop group from there.  Initially this seemed promising.  The group was fully populated with packages.  However, after some effort I eventually decided there were simply too many version conflicts between the dependencies and packages that were already installed on the AAMI.</p>

<p>This led me choose Xfce from the epel repo.  Since the epel repo was already installed on AAMI I figured there would be better dependency version coordination with the Amazon repos.  This was generally true.  Many dependencies were found either in the epel repo or the Amazon repos.  For the ones that weren't, I was able to find them in the centos repo, and in most cases those were leaf dependencies.  So most of the trouble came from the few dependencies in the centos repo that had sub-dependencies which conflicted with the amazon or epel repo.  In the end a few hacks were required to bypass the dependency conflicts.  I tried to minimize those as much as possible.  Here is the script for installing Xfce</p>

<p>installGui.sh</p>

<pre><code>#!/bin/bash

# echo each command
set -x

# assumes RSRC_DIR and IS_EMR set by parent script
YUM_RSRC_DIR=$RSRC_DIR/yum

sudo yum -y update

# Most info I've found on installing a GUI on AWS suggests to install using
#&gt; sudo yum groupinstall -y ""Desktop""
# This group is not available by default on the Amazon Linux AMI.  The group
# is listed if the epel repo is enabled, but it is empty.  I tried installing
# the centos repo, which does have support for this group, but it simply end
# up having to many dependency version conflicts with packages already installed
# by the Amazon repos.
#
# I found the path of least resistance to be installing the group Xfce from
# the epel repo. The epel repo is already included in amazon image, just not enabled.
# So I'm guessing there was at least some consideration by Amazon to align
# the dependency versions of this repo with the Amazon repos.
#
# My general approach to this problem was to start with the last command:
#&gt; sudo yum groupinstall -y Xfce
# which will generate a list of missing dependencies.  The script below
# essentially works backwards through that list to eliminate all the
# missing dependencies.
#
# In general, many of the dependencies required by Xfce are found in either
# the epel repo or the Amazon repos.  Most of the remaining dependencies can be
# found in the centos repo, and either don't have any further dependencies, or if they
# do those dependencies are satisfied with the centos repo with no collisions
# in the epel or amazon repo.  Then there are a couple of oddball dependencies
# to clean up.

# if yum-config-manager is not found then install yum-utils
#&gt; sudo yum install yum-utils
sudo yum-config-manager --enable epel

# install centos repo
# place the repo config @  /etc/yum.repos.d/centos.repo
sudo cp $YUM_RSRC_DIR/yum.repos.d/centos.repo /etc/yum.repos.d/

# The config centos.repo specifies the key with a URL.  If for some reason the key
# must be in a local file, it can be found here: https://www.centos.org/keys/RPM-GPG-KEY-CentOS-6
# It can be installed to the right location in one step:
#&gt; wget -O /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 https://www.centos.org/keys/RPM-GPG-KEY-CentOS-6
# Note, a key file must also be installed in the system key ring.  The docs are a bit confusing
# on this, I found that I needed to run both gpg AND then followed by rpm, eg:
#&gt; sudo gpg --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6
#&gt; sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6

# I found there are a lot of version conflicts between the centos, Amazon and epel repos.
# So I did not enable the centos repo generally.  Instead I used the --enablerepo switch
# enable it explicitly for each yum command that required it.  This only works for yum.  If
# rpm must be used, then yum-config-manager must be used to enable/disable repos as a
# separate step.
#
# Another problem I ran into was yum installing the 32-bit (*.i686) package rather than
# the 64-bit (*.x86_64) verision of the package.  I never figured out why.  So I had
# to specify the *.x86_64 package explicitly.  The search tools (eg. 'whatprovides')
# did not list the 64 bit package either even though a manual search through the
# package showed the 64 bit components were present.
#
# Sometimes it is difficult to determine which package must be in installed to satisfy
# a particular dependency.  'whatprovides' is a very useful tool for this
#&gt; yum --enablerepo centos whatprovides libgdk_pixbuf-2.0.so.0
#&gt; rpm -q --whatprovides libgdk_pixbuf

sudo yum --enablerepo centos install -y gdk-pixbuf2.x86_64
sudo yum --enablerepo centos install -y gtk2.x86_64
sudo yum --enablerepo centos install -y libnotify.x86_64
sudo yum --enablerepo centos install -y gnome-icon-theme
sudo yum --enablerepo centos install -y redhat-menus
sudo yum --enablerepo centos install -y gstreamer-plugins-base.x86_64

# problem when we get to libvte, installing libvte requires expat, which conflicts with amazon lib
# the centos package version was older and did not install right lib version
# but … the expat dependency was coming from a dependency on python-libs.
# the easiest workaround was to install python using the amazon repo, that in turn
# installs a version of python libs that is compatible with the version of libexpat on the system.

sudo yum install -y python
sudo yum --enablerepo centos install -y vte.x86_64

sudo yum --enablerepo centos install -y libical.x86_64
sudo yum --enablerepo centos install -y gnome-keyring.x86_64

# another sticky point, xfdesktop requires desktop-backgrounds-basic, but ‘whatprovides’ does not 
# provide any packages for this query (not sure why).  It turns out this is provided by the centos 
# repo, installing ‘desktop-backgrounds-basic’ will try to install the package redhat-logos, but 
# unfortunately this is obsoleted by Amazon’s generic-logos package
# The only way I could find to get around this was to erase the generic logos package.
# This doesn't seem too risky since this is just images for the desktop and menus.
#
sudo yum erase -y generic-logos

# Amazon repo must be disabled to prevent interference with the install
# of redhat-logos
sudo yum --disablerepo amzn-main --enablerepo centos install -y redhat-logos

# next problem is a dependency on dbus.  The dependency comes from dbus-x11 in 
# centos repo.  It requires dbus version 1.2.24, the amazon image already has
# version 1.6.12 installed.  Since the dbus-x11 is only used by the GUI package,
# easiest way around this is to install dbus-x11 with no dependency checks.
# So it will use the newer version of dbus (should be OK).  The main thing that could be a problem
# here is if it skips some other dependency.  When doing manually, its possible to run the install until
# the only error left is the dbus dependency.  It’s a bit risky running in a script since, basically it’s assuming
# all the dependencies are already in place.
yumdownloader --enablerepo centos dbus-x11.x86_64
sudo rpm -ivh --nodeps dbus-x11-1.2.24-8.el6_6.x86_64.rpm
rm dbus-x11-1.2.24-8.el6_6.x86_64.rpm

sudo yum install -y xfdesktop.x86_64

# We need the version of poppler-glib from centos repo, but it is found in several repos.
# Disable the other repos for this step.
# On EMR systems a newer version of poppler is already installed.  So move up 1 level
# in dependency chain and force install of tumbler.

if [ $IS_EMR -eq 1 ]
then
    yumdownloader --enablerepo centos tumbler.x86_64
    sudo rpm -ivh --nodeps tumbler-0.1.21-1.el6.x86_64.rpm
else
    sudo yum --disablerepo amzn-main --disablerepo amzn-updates --disablerepo epel --enablerepo centos install -y poppler-glib
fi


sudo yum install  --enablerepo centos -y polkit-gnome.x86_64
sudo yum install  --enablerepo centos  -y control-center-filesystem.x86_64

sudo yum groupinstall -y Xfce
</code></pre>

<p>Here are the contents for the centos repository config file:</p>

<p>centos.repo</p>

<pre><code>[centos]
name=CentOS mirror
baseurl=http://repo1.ash.innoscale.net/centos/6/os/x86_64/
failovermethod=priority
enabled=0
gpgcheck=1
gpgkey=https://www.centos.org/keys/RPM-GPG-KEY-CentOS-6
</code></pre>

<p>If all you needed was a recipe to get a desktop package installed on the Amazon AMI, then you're done.  The rest of this post covers how to configure VNC to access the desktop via an SSH tunnel, and how to package all of this so that the instance can be easily started from a script.</p>

<h2>Step 2. Install and Configure VNC</h2>

<p>Below is my top level script for installing the GUI.  After configuring a few variables the first thing it does is call the script from step 1 above. This script has some extra baggage since I've built it to work on a regular ec2 instance, or emr and as root or as ec2-user.  The essential steps are</p>

<ol>
<li>install libXfont</li>
<li>install tiger-vnc-server</li>
<li>install the VNC server config file </li>
<li>create a .vnc directory in the user home directory </li>
<li>install the xstartup file in the .vnc directory</li>
<li>install a dummy passwd file in the .vnc directory </li>
<li>start the VNC server</li>
</ol>

<p>A few key points to note:</p>

<p>This assumes you will access the VNC server through an SSH tunnel.  In the end this really seemed like the easiest and most reliably secure way to go.  Since you probably have a port for SSH open in your security group specification, you won't have to make any changes to it.  Also, the encryption config for VNC clients/servers is not straight forward.  It seemed easy to make a mistake and leave your communications unencrypted.  The settings for this are in the vncservers file.  The -localhost switch tells vnc only to accept local connections.  The '-nolisten tcp' tells associated xserver modules to also not accept connections from the network.  Finally the '-SecurityTypes None' switch allows you to open your VNC session without typing a passwd, since the only way into the machine is through ssh, the additional password check seems redundant.</p>

<p>The xstartup file determines what will start when your VNC session is initiated the first time.  I've noticed many posts on this subject skip this point.  If you don't tell it to start the Xfce desktop, you will just get a blank window when you start VNC.  The config I have here is very simple.  </p>

<p>Even though I mentioned above that the VNC server is configured to not prompt for a password, it nevertheless requires a passwd file in the .vnc directory in order for the server to start.  The first time you run the script it will fail when it tries to start the server.  Login to the machine via ssh and run 'vncpasswd'.  It will create a passwd file in the .vnc directory that you can save to use as part of these scripts during install.  Note, I've read that VNC does not do anything sophisticated to protect the passwd file.  So I would not recommend using a passwd that you use for other, more important accounts.</p>

<p>installGui.sh</p>

<pre><code>#!/bin/bash

# echo each command
set -x

BIN_DIR=""${BASH_SOURCE%/*}""
ROOT_DIR=$(dirname $BIN_DIR)
RSRC_DIR=$ROOT_DIR/rsrc
VNC_DIR=$RSRC_DIR/vnc

# Install user config files into ec2-user home directory
# if it is available.  In practice, this should always
# be true

if [ -d ""/home/ec2-user"" ]
then
   USER_ACCT=ec2-user
else
   USER_ACCT=hadoop
fi

HOME_DIR=""/home""

# Use existence of hadoop home directory as proxy to determine if
# this is an EMR system.  Can be used later to differentiate
# steps on EC2 system vs EMR.
if [ -d ""/home/hadoop"" ]
then
    IS_EMR=1
else
    IS_EMR=0
fi


# execute Xfce desktop install
. ""$BIN_DIR/installXfce.sh""

# now roughly follow the following from step 3: https://devopscube.com/setup-gui-for-amazon-ec2-linux/

sudo yum install -y pixman pixman-devel libXfont

sudo yum -y install tigervnc-server


# install the user account configuration file.
# This setup assumes the user will always connect to the VNC server
# through an SSH tunnel.  This is generally more secure, easier to
# configure and easier to get correct than trying to allow direct
# connections via TCP.
# Therefore, config VNC server to only accept local connections, and
# no password required.
sudo cp $VNC_DIR/vncservers-$USER_ACCT /etc/sysconfig/vncservers

# install the user account, vnc config files

sudo mkdir $HOME_DIR/$USER_ACCT/.vnc
sudo chown $USER_ACCT:$USER_ACCT $HOME_DIR/$USER_ACCT/.vnc

# need xstartup file to tell vncserver to start the window manager
sudo cp $VNC_DIR/xstartup $HOME_DIR/$USER_ACCT/.vnc/
sudo chown $USER_ACCT:$USER_ACCT $HOME_DIR/$USER_ACCT/.vnc/xstartup

# Even though the VNC server is config'd to not require a passwd, the
# server still looks for the passwd file when it starts the session.
# It will fail if the passwd file is not found.
# The first time these scripts are run, the final step will fail.
# Then manually run
#&gt; vncpasswd
# It will create the file ~/.vnc/passwd.  Then save this file to persistent
# storage so that it can be installed to the user account during
# server initialization.

sudo cp $ROOT_DIR/home/user/.vnc/passwd $HOME_DIR/$USER_ACCT/.vnc/
sudo chown $USER_ACCT:$USER_ACCT $HOME_DIR/$USER_ACCT/.vnc/passwd

# This script will be running as root if called from the EC2 launch
# command.  VNC server needs to be started as the user that
# you will connect to the server as (eg. ec2-user, hadoop, etc.)
sudo su -c ""sudo service vncserver start"" -s /bin/sh $USER_ACCT

# how to stop vncserver
# vncserver -kill :1

# On the remote client
# 1. start the ssh tunner
#&gt; ssh -i ~/.ssh/&lt;YOUR_KEY_FILE&gt;.pem -L 5901:localhost:5901 -N ec2-user@&lt;YOUR_SERVER_PUBLIC_IP&gt;
#    for debugging connection use -vvv switch
# 2. connect to the vnc server using client on the remote machine.  When
#    prompted for the IP address, use 'localhost:5901'
#    This connects to port 5901 on your local machine, which is where the ssh
#    tunnel is listening.
</code></pre>

<p>vncservers</p>

<pre><code># The VNCSERVERS variable is a list of display:user pairs.
#
# Uncomment the lines below to start a VNC server on display :2
# as my 'myusername' (adjust this to your own).  You will also
# need to set a VNC password; run 'man vncpasswd' to see how
# to do that.  
#
# DO NOT RUN THIS SERVICE if your local area network is
# untrusted!  For a secure way of using VNC, see this URL:
# http://kbase.redhat.com/faq/docs/DOC-7028

# Use ""-nolisten tcp"" to prevent X connections to your VNC server via TCP.

# Use ""-localhost"" to prevent remote VNC clients connecting except when
# doing so through a secure tunnel.  See the ""-via"" option in the
# `man vncviewer' manual page.

# Use ""-SecurityTypes None"" to allow session login without a password.
# This should only be used in combination with ""-localhost""
# Note: VNC server still looks for the passwd file in ~/.vnc directory
# when the session starts regardless of whether the user is
# required to enter a passwd.

# VNCSERVERS=""2:myusername""
# VNCSERVERARGS[2]=""-geometry 800x600 -nolisten tcp -localhost""
VNCSERVERS=""1:ec2-user""
VNCSERVERARGS[1]=""-geometry 1280x1024 -nolisten tcp -localhost -SecurityTypes None""
</code></pre>

<p>xstartup</p>

<pre><code>#!/bin/sh

unset SESSION_MANAGER
unset DBUS_SESSION_BUS_ADDRESS
# exec /etc/X11/xinit/xinitrc
/usr/share/vte/termcap/xterm &amp;
/usr/bin/startxfce4 &amp;
</code></pre>

<h2>Step 3. Connect to Your Instance</h2>

<p>Once you've got the VNC server running on EC2 you can try connecting to it.  First open an SSH tunnel to your instance.  5901 is the port where the VNC server listens for display 1 from the vncservers file.  It will listen for display 2 on port 5902, etc.  This command creates a tunnel from port 5901 on your local machine to port 5901 on the instance.</p>

<pre><code>ssh -i ~/.ssh/&lt;YOUR_KEY_FILE&gt;.pem -L 5901:localhost:5901 -N ec2-user@&lt;YOUR_SERVER_PUBLIC_IP&gt;
</code></pre>

<p>Now open your preferred VNC client.  Where it prompts for the IP address of the server enter:</p>

<blockquote>
  <p>localhost:5901</p>
</blockquote>

<p>If nothing happens at all, then either there was a problem starting the vnc server, or there is a connectivity problem preventing the client from reaching the server, or possibly a problem in vncservers config file</p>

<p>If a window comes up, but it is just blank then check that the Xfce install completed successfully and that the xstartup file is installed.</p>

<h2>Step 4. Simplify</h2>

<p>If you just need to do this once then sftp'ing the scripts over to your instance and running manually is fine.  Otherwise you're going to want to automate this as much as possible to make it faster and less error prone when you do need to fire up an instance with a GUI.</p>

<p>The first step to automating is to create an EFS volume containing the scripts and config files that can be mounted when the instance is started.  Amazon has plenty of <a href=""https://aws.amazon.com/getting-started/tutorials/create-network-file-system/"" rel=""noreferrer"">info</a> on creating a network file system.  A couple points to pay attention to when creating the volume.  If you don't want your volume to be open to the world you may want to create a custom security group to use for your EFS volume.  I created security group for my EFS volume (call it NFS_Mount) that only allows inbound TCP traffic on port 2049 coming from one of my other security groups, call it MasterVNC.  Then when you create an instance, make sure to associate the MasterVNC security group with that instance.  Otherwise the EFS volume won't allow your instance to connect with it.</p>

<p>Now mount the EFS volume:</p>

<pre><code>sudo mkdir /mnt/YOUR_MOUNT_POINT_DIR
sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 fs-YOUR_EFS_ID.efs.us-east-1.amazonaws.com:/ /mnt/YOUR_MOUNT_POINT_DIR
</code></pre>

<p>Now populate /mnt/YOUR_MOUNT_POINT_DIR with the 6 files mentioned in steps 1 and 2 using the following directory structure.  Recall that you must create the passwd file the first time using the command 'vncpasswd'.  It will create the file at ~/.vnc/passwd.</p>

<blockquote>
  <p>/mnt/YOUR_MOUNT_POINT_DIR/bin/installGui.sh
  /mnt/YOUR_MOUNT_POINT_DIR/bin/installXfce.sh</p>
  
  <p>/mnt/YOUR_MOUNT_POINT_DIR/rsrc/vnc/vncservers-ec2-user
  /mnt/YOUR_MOUNT_POINT_DIR/rsrc/vnc/xstartup
  /mnt/YOUR_MOUNT_POINT_DIR/rsrc/vnc/passwd</p>
  
  <p>/mnt/YOUR_MOUNT_POINT_DIR/rsrc/yum/yum.repos.d/centos.repo</p>
</blockquote>

<p>At this point, setting up an instance with a GUI should be pretty easy.  Create your instance as you normally would (make sure to include the MasterVNC security group), ssh to the instance, mount the EFS volume, and run the installGui.sh script.  </p>

<h2>Step 5. Automate</h2>

<p>You can take things a step further and launch your instance in 1 step using the AWS CLI tools on your local machine.  To do this you will need to mount the EFS volume and run the installGui.sh script using arguments to the AWS CLI commands. This just requires creating a top level script and passing it to the CLI command.</p>

<p>Of course there are a couple complications.  EC2 and EMR use different switches and mechanisms to attach the script.  And furthermore, on EMR I only want the GUI to be installed on the master node (not the core or task nodes).</p>

<p>Launching an EC2 instance requires embedding the script in the command with the --user-data switch.  This is done easily by specifying the absolute path to the script file on your local machine.</p>

<pre><code>aws ec2 run-instances --user-data file:///PATH_TO_YOUR_SCRIPT/top.sh  ... other options
</code></pre>

<p>The EMR launch does not support embedding scripts from a local file.  Instead you can specify an S3 URI in the bootstrap actions.</p>

<pre><code>aws emr create-cluster --bootstrap-actions '[{""Path"":""s3://YOUR_BUCKET/YOUR_DIR/top.sh"",""Name"":""Custom action""}]' ... other options
</code></pre>

<p>Finally, you'll see in top.sh below most of the script is a function to determine if the machine is a basic EC2 instance or an EMR master.  If not for that the script could be 3 lines.  You may wonder why not just use the built in 'run-if' bootstrap action rather than writing my own function.  The built in 'run-if' script has a bug and does not properly run scripts located in S3.</p>

<p>Debugging things once you put them in the init sequence can be a challenge.  One thing that can help is the log file: /var/log/cloud-init-output.log. This captures all the console output from the scripts run during bootstrap initialization.</p>

<p>top.sh</p>

<pre><code>#!/bin/bash

# note: conditional bootstrap function run-if has a bug, workaround ...
# this function adapted from https://forums.aws.amazon.com/thread.jspa?threadID=222418
# Determine if we are running on the master node.
# 0 - running on master, or non EMR node
# 1 - running on a task or core node

check_if_master_or_non_emr() {
    python - &lt;&lt;'__SCRIPT__'
import sys
import json

instance_file = ""/mnt/var/lib/info/instance.json""

try:
    with open(instance_file) as f:
        props = json.load(f)
    is_master_or_non_emr = props.get('isMaster', False)

except IOError as ex:
    is_master_or_non_emr = True   # file will not exist when testing on a non-emr machine

if is_master_or_non_emr:
    sys.exit(1)
else:
    sys.exit(0)
__SCRIPT__
}

check_if_master_or_non_emr
IS_MASTER_OR_NON_EMR=$?

# If this machine is part of EMR cluster, then ONLY install on the MASTER node

if [ $IS_MASTER_OR_NON_EMR -eq 1 ]
then
    sudo mkdir /mnt/YOUR_MOUNT_POINT_DIR

    sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 fs-YOUR_EFS_ID.efs.us-east-1.amazonaws.com:/ /mnt/YOUR_MOUNT_POINT_DIR

    . /mnt/YOUR_MOUNT_POINT_DIR/bin/installGui.sh
fi

exit 0
</code></pre>
",6471212,2017-09-22T23:12:15.527,0,CC BY-SA 3.0,,1,,"
",0,"# unfortunately this is obsoleted by Amazon’s generic-logos package
",False,False
303,46423676,2,46423207,2017-09-26T10:14:34.630,1,"<p>Send from a cron script or other scheduled task that does not have a timeout - search on here for how to do that.</p>

<p>Send more efficiently - see <a href=""https://github.com/PHPMailer/PHPMailer/blob/master/examples/mailing_list.phps"" rel=""nofollow noreferrer"">the mailing list example provided with PHPMailer</a>.</p>

<p>Get your local mail server to work for you - submit messages to it (which will be very fast) and let it deal with slow deliveries - it's what mail servers are for.</p>

<p>I can see you've based your code on an obsolete example and are using an old version of PHPMailer, so <a href=""https://github.com/PHPMailer/PHPMailer"" rel=""nofollow noreferrer"">get the latest version</a>.</p>
",333340,2017-09-26T10:14:34.630,0,CC BY-SA 3.0,,1,,"noreferrer"">get",0,"I can see you've based your code on an obsolete example and are using an old version of PHPMailer, so <a href=""https://github.com/PHPMailer/PHPMailer"" rel=""nofollow noreferrer"">get",False,False
304,46455940,2,46455502,2017-09-27T19:37:39.700,1,"<p>Try this: </p>

<p><code>sudo -H pip install --upgrade awscli --ignore-installed six</code></p>

<p>This should work around the ""Uninstalling a distutils installed project (six) has been deprecated"" problem. It looks like the underlying issue has been fixed recently, as I haven't had to use the <code>--ignore-installed</code> option for the last few upgrades.</p>
",4638378,2017-09-27T19:37:39.700,0,CC BY-SA 3.0,,1,,.,0,"This should work around the ""Uninstalling a distutils installed project (six) has been deprecated"" problem.",False,False
305,46533878,2,46533670,2017-10-02T21:19:12.587,4,"<p>In general all of the old client constructors are deprecated in the newer AWS libraries.  You'll need to do something like:</p>

<pre><code>AWSCognitoIdentityProvider provider = 
        AWSCognitoIdentityProviderClientBuilder.standard().defaultClient();
</code></pre>

<p>This is the bare bones version - if you need to pass a different credentials provider or region you'll need to add some more parameters.  See <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/cognitoidp/AWSCognitoIdentityProviderClientBuilder.html"" rel=""nofollow noreferrer"">AWSCognitoIdentityProviderClientBuilder</a> and <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/cognitoidp/AWSCognitoIdentityProvider.html"" rel=""nofollow noreferrer"">AWSCognitoIdentityProvider</a> for more details.</p>
",2933977,2017-10-02T21:19:12.587,3,CC BY-SA 3.0,,1,all, ,0,In general all of the old client constructors are deprecated in the newer AWS libraries.  ,False,False
306,46554671,2,35589641,2017-10-03T23:00:54.747,0,"<p>I was struggling with this issue for hours. I was using AmazonS3EncryptionClient and nothing I did helped. Then I noticed that the client is actually deprecated, so I thought I'd try switching to the builder model they have:</p>

<pre><code>var builder = AmazonS3EncryptionClientBuilder.standard()
  .withEncryptionMaterials(new StaticEncryptionMaterialsProvider(encryptionMaterials))
if (accessKey.nonEmpty &amp;&amp; secretKey.nonEmpty) builder = builder.withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey.get, secretKey.get)))
builder.build()
</code></pre>

<p>And... that solved it. Looks like Lambda has trouble injecting the credentials in the old model, but works well in the new one.</p>
",3830413,2017-10-03T23:00:54.747,0,CC BY-SA 3.0,,1,,model,0,"Then I noticed that the client is actually deprecated, so I thought I'd try switching to the builder model",False,False
307,46625030,2,38956130,2017-10-07T20:56:03.547,4,"<p>Starting from the <a href=""http://citi.umich.edu/projects/nfsv4/windows/"" rel=""nofollow noreferrer"">NFSv4.1 client made by folks at CITI @ University of Michigan</a>, with a few relatively minor changes, you can get a working connection to an AWS EFS filesystem. </p>

<p>As @kafka points out: AWS EFS disallows / fails when any client specifies a share deny value other than <code>OPEN4_SHARE_DENY_NONE</code>. Luckily the CITI folks discovered this as a possible problem and added a definition that, when commented out, will only ever use <code>OPEN4_SHARE_DENY_NONE</code> for the share deny value. </p>

<p>Once this definition is commented out, then you need to recompile it for your system – relatively trivial if you use the versions of Visual Studio and WDK that the readme specifies. One gotcha was that the self-signed certificate process needs to <em>not</em> use the outdated Root Agency certificate (since it's only 512-bit). Use <code>certreq</code> instead.</p>

<p>I'm working on collecting this knowledge into <a href=""https://github.com/contentfree/ms-nfs41-client"" rel=""nofollow noreferrer"">a fork of the CITI code at Github</a>. (I'm sure you either solved your problem or moved on, but good luck to those folks who landed here from Google!)</p>
",137641,2017-10-07T20:56:03.547,0,CC BY-SA 3.0,,1,,.,0,> use the outdated Root Agency certificate (since it's only 512-bit).,False,False
308,46718013,2,46697687,2017-10-12T20:05:54.873,1,"<p>The issue is that when you use the old consumer (and use the <code>--zookeeper</code> argument) the ZooKeeper port should be provided (<code>2181</code>).</p>

<p>However, please note that the old consumer is now deprecated, and using the new consumer is highly recommended. Please see the answer by Mickael Maison for more info.</p>
",3557290,2017-10-12T20:05:54.873,0,CC BY-SA 3.0,,1,,.,0,"However, please note that the old consumer is now deprecated, and using the new consumer is highly recommended.",False,False
309,46770108,2,45758902,2017-10-16T12:14:59.497,0,"<p>You probably need to restart the MySQL workbench, not reboot the RDS.</p>

<p>As I described <a href=""https://stackoverflow.com/a/46770024/1285846"">here</a>, the database handles have changed, but the code doesn't notice. So the  MySQL workbench is using outdated handles that are being rejected.</p>
",1285846,2017-10-16T12:14:59.497,0,CC BY-SA 3.0,,1,,rejected.</p,0,So the  MySQL workbench is using outdated handles that are being rejected.</p,False,False
310,46797968,2,46797086,2017-10-17T19:41:53.523,0,"<p>When you call spark-submit it will add it's own python folder and the py4j library to PYTHONPATH for you.  They live in the folder spark is installed to, they are not directly installed into the site-packages of the python you are using.  For example, if I pyspark an print out the python path:</p>

<pre><code>$ pyspark
Python 2.6.6 (r266:84292, Aug 18 2016, 15:13:37) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
17/10/17 15:34:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/10/17 15:34:04 WARN Utils: Your hostname, rwidmaier resolves to a loopback address: 127.0.0.1; using 10.4.248.126 instead (on interface em1)
17/10/17 15:34:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
/home/rwidmaier/apps/spark/python/pyspark/context.py:195: UserWarning: Support for Python 2.6 is deprecated as of Spark 2.0.0
  warnings.warn(""Support for Python 2.6 is deprecated as of Spark 2.0.0"")
17/10/17 15:34:09 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/10/17 15:34:09 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/10/17 15:34:09 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.2.0
      /_/

Using Python version 2.6.6 (r266:84292, Aug 18 2016 15:13:37)
SparkSession available as 'spark'.
&gt;&gt;&gt; import sys
&gt;&gt;&gt; from pprint import pprint
&gt;&gt;&gt; pprint(sys.path)
['',
 u'/tmp/spark-0a08efa7-714a-498b-b5e9-bba48684d52d/userFiles-52012e25-d7af-4599-a214-9637141ed4ec',
 '/usr/lib64/python2.6/site-packages/deluge-1.3.11-py2.6-linux-x86_64.egg',
 '/home/ryan/apps/spark/python/lib/py4j-0.10.4-src.zip',
 '/home/ryan/apps/spark/python',
 '/usr/lib64/python26.zip',
 '/usr/lib64/python2.6',
 '/usr/lib64/python2.6/plat-linux2',
 '/usr/lib64/python2.6/lib-tk',
 '/usr/lib64/python2.6/lib-old',
 '/usr/lib64/python2.6/lib-dynload',
 '/usr/lib64/python2.6/site-packages',
 '/usr/lib64/python2.6/site-packages/gst-0.10',
 '/usr/lib64/python2.6/site-packages/gtk-2.0',
 '/usr/lib64/python2.6/site-packages/webkit-1.0',
 '/usr/lib/python2.6/site-packages']
</code></pre>

<p>You can see in the list that it has manually added: </p>

<ul>
<li>/home/ryan/apps/spark/python/lib/py4j-0.10.4-src.zip</li>
<li>/home/ryan/apps/spark/python</li>
</ul>

<p>To ensure that the interpreter is always setup with the right libraries, you just need to set PYSPARK_PYTHON to point to the python install you would like to use, and then run ""pyspark"" instead of running python directly.  Or alternatively you can use spark-submit if you have want to provide a driver.</p>
",1461187,2017-10-17T19:41:53.523,0,CC BY-SA 3.0,,1,Support,"
",0,"Support for Python 2.6 is deprecated as of Spark 2.0.0
  warnings.warn(""Support for Python 2.6 is deprecated as of Spark 2.0.0"")
",False,False
311,46879241,2,46856731,2017-10-22T21:14:54.750,3,"<p>After a lot of trial and error, i've managed to get it working. I'm not sure if AWS docs are outdated but they are certainly over kill.</p>

<p>Here goes:</p>

<ol>
<li><p>From Apple Developer website visit navigate to Certificates, IDs &amp; Profiles > Identifiers > App IDs</p></li>
<li><p>Create a new App ID and enable the Push Notifications service.</p></li>
<li><p>Click Create a new Production SSL certificate, then create a new CSR file in the local keychain on Mac, upload it to Developer site (during the certificate generation process) then download the generated .cer file.</p></li>
<li><p>Double click the .cer file to add it to the Keychain on mac.</p></li>
<li><p>Open Keychain, select 'My Certificates' highlight the certificate that got added in step 3, probably starts with 'Apple Push Services'.</p></li>
<li><p>Right-click the cert and export it (extension is .p12). If it asks you to set a password you can leave blank.</p></li>
<li><p>In AWS SNS, navigate into Applications, click into your APNS application (or add a new application). Under 'platform application actions' click update credentials and upload your exported .p12 file.</p></li>
<li><p>Finally, click 'Load credentials from file' and update to exit the application settings.</p></li>
<li><p>Enjoy push notifications in your app.</p></li>
</ol>

<p>This worked for me, I'm not sure why AWS docs suggest the commands to convert files, it looks like they're not necessary and have caused great confusion.</p>

<p>Hope this else somebody.</p>
",1042951,2017-10-22T21:14:54.750,0,CC BY-SA 3.0,,1,,kill.</p,1,I'm not sure if AWS docs are outdated but they are certainly over kill.</p,False,False
312,46961074,2,14969273,2017-10-26T18:09:15.890,4,"<p>The answer by <em>jamesis</em> is using <code>boto</code> which is an older version and will be deprecated. 
The current supported version is <code>boto3</code>.</p>

<p>The same expiration policy on the logs folder can be done as follows:</p>

<pre><code>import boto3
from botocore.exceptions import ClientError

client = boto3.client('s3')
try:
    policy_status = client.put_bucket_lifecycle_configuration(
               Bucket='boto-lifecycle-test',
               LifecycleConfiguration={
                    'Rules': 
                           [
                             {
                             'Expiration':
                                {
                                 'Days': 30,
                                 'ExpiredObjectDeleteMarker': True
                                },
                             'Prefix': 'logs/',
                             'Filter': {
                               'Prefix': 'logs/',
                             },
                             'Status': 'Enabled',
                            }
                        ]})
except ClientError as e:
     print(""Unable to apply bucket policy. \nReason:{0}"".format(e))
</code></pre>

<p><strong>This will override any existing lifecycle configuration policy on <code>logs</code>.</strong></p>

<p>A good thing to do would be to check if the bucket exists and if you have the permissions to access it before applying the expiration configuration i.e. before the <code>try-except</code> </p>

<pre><code>bucket_exists = client.head_bucket(
   Bucket='boto-lifecycle-test'
)
</code></pre>

<p>Since the <code>logs</code> folder itself isn't a bucket but rather an object within the bucket <code>boto-lifecycletest</code>, the bucket itself can have a different expiration policy.
You can check this from the result in <code>policy_exists</code> as below.</p>

<pre><code>policy_exists = client.get_bucket_lifecycle_configuration(
    Bucket='boto-lifecycle-test')
bucket_policy = policy_exists['Rules'][0]['Expiration']
</code></pre>

<p>More information about setting the expiration policy can be checked at <a href=""https://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.put_bucket_lifecycle_configuration"" rel=""nofollow noreferrer"">Expiry policy</a></p>
",1934182,2017-10-26T18:09:15.890,0,CC BY-SA 3.0,,1,,"
",0,"which is an older version and will be deprecated. 
",False,False
313,46985780,2,46970210,2017-10-28T02:55:14.783,0,"<p>Make sure you are not using a deprecated constructor of the CognitoUserPool class. This worked for me:</p>

<pre><code>CognitoUserPool userPool = new CognitoUserPool(this, awsConfiguration);
</code></pre>
",5326518,2017-10-28T02:55:14.783,0,CC BY-SA 3.0,,1,,.,1,>Make sure you are not using a deprecated constructor of the CognitoUserPool class.,False,False
314,46995002,2,46994914,2017-10-28T22:10:30.217,2,"<p>There will be a cost if it creates something other than a t2.micro instance. If it can use a t2.micro then you could shutdown your current t2.micro (if you have one running) while the CloudFormer server runs.</p>

<p>It needs to create an EC2 instance because all it is is a Ruby script that queries your AWS account. The last time I tried it it was very out of date and missing support for lots of AWS services. It was also slow and buggy and was a pain to get it to complete without errors. It's a shame they can't just release the script and you could run it however you saw fit. Or better yet they should build this feature directly into the CloudFormation console.</p>
",13070,2017-10-28T22:10:30.217,2,CC BY-SA 3.0,,1,it,.,0,it was very outdated and missing support for lots of AWS services.,False,False
315,47077318,2,47067212,2017-11-02T14:08:30.293,5,"<p>I just tested this with an application running behind an Application Load Balancer. With gzip compression enabled on the server, the browser receives gzipped resources. The ALB correctly passes along the <code>content-encoding: gzip</code> HTTP header. Note that with ALB you also get HTTP/2 support which further reduces the time it takes for browsers to load your website's files.</p>

<p>I think that article you linked is incorrect, or out of date, or maybe it's an issue specific to Classic ELBs.</p>
",13070,2017-11-02T14:08:30.293,2,CC BY-SA 3.0,,1,,ELBs.</p,0,"I think that article you linked is incorrect, or outdated, or maybe it's an issue specific to Classic ELBs.</p",False,False
316,47153880,2,30793481,2017-11-07T09:09:09.900,2,"<p>DynamoDBMarshaller is now deprecated but I get exactly the same problem with DynamoDBTypeConvertedJson. If you want to store a collection as JSON within a DynamoDBMapper class, use DynamoDBTypeConverted and write a custom converter class (do not use DynamoDBTypeConvertedJson which will not return your collection on unconvert).</p>

<p>Here is the solution using DynamoDBTypeConverted </p>

<pre><code>// Model.java
@DynamoDBTable(tableName = ""..."")
public class Model {
  private String id;
  private List&lt;MyObject&gt; objects;

  public Model(String id, List&lt;MyObject&gt; objects) {
    this.id = id;
    this.objects = objects;
  }

  @DynamoDBHashKey(attributeName = ""id"")
  public String getId() { return this.id; }
  public void setId(String id) { this.id = id; }

  @DynamoDBTypeConverted(converter = MyObjectConverter.class)
  public List&lt;MyObject&gt; getObjects() { return this.objects; }
  public void setObjects(List&lt;MyObject&gt; objects) { this.objects = objects; }
}
</code></pre>

<p>-</p>

<pre><code>public class MyObjectConverter implements DynamoDBTypeConverter&lt;String, List&lt;MyObject&gt;&gt; {

    @Override
    public String convert(List&lt;Object&gt; objects) {
        //Jackson object mapper
        ObjectMapper objectMapper = new ObjectMapper();
        try {
            String objectsString = objectMapper.writeValueAsString(objects);
            return objectsString;
        } catch (JsonProcessingException e) {
            //do something
        }
        return null;
    }

    @Override
    public List&lt;Object&gt; unconvert(String objectssString) {
        ObjectMapper objectMapper = new ObjectMapper();
        try {
            List&lt;Object&gt; objects = objectMapper.readValue(objectsString, new TypeReference&lt;List&lt;Object&gt;&gt;(){});
            return objects;
        } catch (JsonParseException e) {
            //do something
        } catch (JsonMappingException e) {
            //do something
        } catch (IOException e) {
            //do something
        }
        return null;
    }
}
</code></pre>
",4985580,2017-11-07T09:09:09.900,0,CC BY-SA 3.0,,1,DynamoDBMarshaller,.,0,<p>DynamoDBMarshaller is now deprecated but I get exactly the same problem with DynamoDBTypeConvertedJson.,False,False
317,47195647,2,24896743,2017-11-09T06:53:09.857,2,"<p>The accepted answer is now outdated. For future viewers, there is no need to include anything as extra header as now AWS includes a <code>Signature</code> field in every signed url which is different everytime you make a request.</p>
",7739188,2017-11-09T06:53:09.857,0,CC BY-SA 3.0,,1,answer,.,0,The accepted answer is now outdated.,False,False
318,47202941,2,47142212,2017-11-09T13:21:09.463,0,"<p>Thank you for your answer ! So after trying the same code on Python and Java, I found out that the issue was related to the driver. the one I was using was apparently outdated. I installed the last driver, restarted Rstudio, now works perfectly ! Thanks ! </p>
",8895384,2017-11-09T13:21:09.463,0,CC BY-SA 3.0,,1,one,.,0,the one I was using was apparently outdated.,False,False
319,47324631,2,37685114,2017-11-16T08:14:39.157,1,"<p>From the description, it looks like what you are looking for is 
1) Avro data write to S3</p>

<p>2) Data to be partitioned in S3</p>

<p>3) Exactly-once support while writing.</p>

<p>Qubole <a href=""https://github.com/qubole/streamx"" rel=""nofollow noreferrer"">StreamX</a> supports rich variety of format conversions, avro being one of them, along with data partition. 
And, exactly once is in our pipeline which will be out soon. </p>

<p>Whereas secor is getting deprecated(mentioned in one of their responses on google group) and it also do not support avro.</p>

<p>So you can use qubole streamx to start with.</p>
",1526887,2017-11-16T08:14:39.157,1,CC BY-SA 3.0,,1,,avro.</p,1,Whereas secor is getting deprecated(mentioned in one of their responses on google group) and it also do not support avro.</p,False,False
320,47345328,2,47329936,2017-11-17T07:24:06.427,1,"<p>As stated, your question is not very clear: ""What is the conflict resolution strategy for DynamoDB"" - what conflicts? Are you referring to potentially inconsistent reads?</p>

<p>DynamoDB, for GetItem queries, allows both eventual consistent and strongly consistent reads, configurable with a parameter on the request (as described in the docs here: <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a>). For strongly consistent reads the value returned is the most recent value at the time the query was executed. For eventual consistent reads it is possible to read a slightly out of date version of an item but there is no ""conflict resolution"" per se.</p>

<p>You may be thinking about conditional updates which allow for requests to fail if an expected condition is not met at the time the query is executed. </p>
",63074,2017-11-17T07:24:06.427,10,CC BY-SA 3.0,,1,,se.</p,0,"For eventual consistent reads it is possible to read a slightly outdated version of an item but there is no ""conflict resolution"" per se.</p",False,False
321,47671516,2,47652281,2017-12-06T10:02:17.067,4,"<p>I'm going to have to correct the post by himanshuIIITian slightly, (sorry).</p>

<ol>
<li><p>Use the s3a connector, not the older, obsolete, unmaintained, s3n. S3A is: faster, works with the newer S3 clusters (Seoul, Frankfurt, London, ...), scales better. S3N has fundamental performance issues which have only been fixed in the latest version of Hadoop by deleting that connector entirely. Move on.</p></li>
<li><p>You cannot safely use s3 as a direct destination of a Spark query., not with the classic ""FileSystem"" committers available today. Write to your local file:// and then copy up the data afterwards, using the AWS CLI interface. You'll get better performance as well as the guarantees of reliable writing which you would normally expect from IO</p></li>
</ol>
",2261274,2017-12-06T10:02:17.067,2,CC BY-SA 3.0,,1,,.,1,"Use the s3a connector, not the older, obsolete, unmaintained, s3n.",False,False
322,47731473,2,47730479,2017-12-09T17:55:01.747,0,"<p>The ""Manage"" tab in the MTurk Requester Website is for managing Batches created with the MTurk Requester Website (using the Create) tab. If you need/want to view HITs that you create with the API, you can use the <a href=""http://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_ListHITsOperation.html"" rel=""nofollow noreferrer"">ListHITs API method</a> either with the API directly (using your Java code) or using the AWS Command Line Interface (CLI). </p>

<p>Here's a blog explaining how to do this with the AWS CLI: 
<a href=""https://blog.mturk.com/tutorial-managing-mturk-hits-with-the-aws-command-line-interface-56eaabb7fd4c"" rel=""nofollow noreferrer"">https://blog.mturk.com/tutorial-managing-mturk-hits-with-the-aws-command-line-interface-56eaabb7fd4c</a></p>

<p>The blog shows how to use aws-shell, which is a more interactive shell that sits atop the AWS CLI. It has autocomplete and shows you inline ""man"" pages on each command. I personally prefer this. </p>

<p>The CLI and aws-shell will also let you write filters and formatters for results. So you can do things like this:</p>

<pre><code>aws mturk list-hits --output table --query 'HITs[].{""1. HITId"": HITId, ""2. Title"": Title, ""3. Status"":HITStatus}' --endpoint-url https://mturk-requester-sandbox.us-east-1.amazonaws.com --max-results 5
</code></pre>

<p>This calls <a href=""http://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_ListHITsOperation.html"" rel=""nofollow noreferrer"">ListHITs</a>, on the Sandbox (--endpoint), getting only 5 results (--max-results), formats the output as a table instead of the default JSON (--output) and filters that JSON for the HITs object (HITs[]) pulling down only the fields HITId, Title, and Status while also setting titles for those fields as ""1. HITId"", ""2. Title"", and ""3. Status"". </p>

<p>There used to be a link in the MTurk Requester Website for a GUI to manage HITs individually which would show HITs from the API, but it was deprecated this month. There's a brief thread on it here: <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=267769&amp;tstart=0"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=267769&amp;tstart=0</a></p>
",404123,2017-12-09T17:55:01.747,1,CC BY-SA 3.0,,1,,.,0,"There used to be a link in the MTurk Requester Website for a GUI to manage HITs individually which would show HITs from the API, but it was deprecated this month.",False,False
323,47772403,2,47765493,2017-12-12T12:15:37.277,1,"<ul>
<li>Nobody should be using S3n as the connector. It is obsolete and removed from Hadoop 3. If you have the Hadoop 2.7.x JARs on the classpath, use s3a</li>
<li>The issue with rename() is not just the consistency, but the bigger the file, the longer it takes.</li>
</ul>

<p>Really checkpointing to object stores needs to be done differently. If you look closely, there is no <code>rename()</code>, yet so much existing code expects it to be an O(1) atomic operation.</p>
",2261274,2017-12-12T12:15:37.277,0,CC BY-SA 3.0,,1,It,.,0,It is obsolete and removed from Hadoop 3.,False,False
324,47859094,2,47859052,2017-12-17T20:23:21.490,1,"<p>You're looking for <a href=""http://boto3.readthedocs.io/en/latest/reference/services/dynamodb.html#DynamoDB.Table.update_item"" rel=""nofollow noreferrer""><code>update_item()</code></a>. You should use <code>UpdateExpression</code> because <code>AttributeUpdates</code> is deprecated, but this simple example should get you started:</p>

<pre><code>response = table.update_item(
   Key={
     'Device': device,
   },
   AttributeUpdates={
     'RequestList': {
       'Alias': aliasInput,
       'Date': date
     },
     'AvailableQuanity': 0,
     'ReserveQuanity': 0,
   },
)
</code></pre>
",492773,2017-12-17T20:23:21.490,1,CC BY-SA 3.0,,1,>,you,0,"> is deprecated, but this simple example should get you",False,False
325,47873654,2,47566748,2017-12-18T17:37:25.077,0,"<p>Personally I don't like the amazon documentation - I'm not sure if this is your case but, for me, most of the time the material there is outdated.</p>

<p>I'm using the aws client and the Lavelle community dynamo library to make some queries into DynamoDB via annotations / querydsl. </p>

<p>Below follows my pom.xml for a spring-boot/spring-data/dynamodb/swagger micro service.
I hope it helps.</p>

<pre><code>&lt;project
xmlns=""http://maven.apache.org/POM/4.0.0""
xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;

&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

&lt;groupId&gt;sandbox-aws-DynamoDB&lt;/groupId&gt;
&lt;artifactId&gt;sandbox-aws-DynamoDB&lt;/artifactId&gt;
&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
&lt;packaging&gt;jar&lt;/packaging&gt;

&lt;name&gt;sandbox-aws-DynamoDB&lt;/name&gt;
&lt;url&gt;http://maven.apache.org&lt;/url&gt;

&lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
    &lt;java.version&gt;1.8&lt;/java.version&gt;

    &lt;spring-boot.version&gt;1.5.9.RELEASE&lt;/spring-boot.version&gt;

    &lt;!--
    Spring-boot 1.5.9.RELEASE DOES NOT WORK WITH spring-data-dynamodb 5.0.0
    missing org.springframework.data.querydsl.QuerydslUtils
    --&gt;
    &lt;spring-data-dynamodb&gt;4.5.0&lt;/spring-data-dynamodb&gt;
    &lt;aws-java-sdk-dynamodb&gt;1.11.136&lt;/aws-java-sdk-dynamodb&gt;

    &lt;log4j.version&gt;1.3.8.RELEASE&lt;/log4j.version&gt;

    &lt;swagger2.version&gt;2.7.0&lt;/swagger2.version&gt;
    &lt;swagger.ui.version&gt;2.7.0&lt;/swagger.ui.version&gt;
&lt;/properties&gt;

&lt;dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt;
            &lt;version&gt;${spring-boot.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;

&lt;dependencies&gt;
    &lt;!-- Spring Boot --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;de.codecentric&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-admin-starter-client&lt;/artifactId&gt;
        &lt;version&gt;1.5.5&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- AWS DynamoDB --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.github.derjust&lt;/groupId&gt;
        &lt;artifactId&gt;spring-data-dynamodb&lt;/artifactId&gt;
        &lt;version&gt;${spring-data-dynamodb}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;
        &lt;artifactId&gt;aws-java-sdk-dynamodb&lt;/artifactId&gt;
        &lt;version&gt;${aws-java-sdk-dynamodb}&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- Swagger --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.springfox&lt;/groupId&gt;
        &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt;
        &lt;version&gt;${swagger2.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.springfox&lt;/groupId&gt;
        &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt;
        &lt;version&gt;${swagger.ui.version}&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- Commons --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;
        &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;
        &lt;version&gt;3.6&lt;/version&gt;
    &lt;/dependency&gt;

&lt;/dependencies&gt;

&lt;build&gt;
    &lt;finalName&gt;sandbox-aws-dynamodb&lt;/finalName&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;repackage&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;
            &lt;configuration&gt;
                &lt;delimiters&gt;
                    &lt;delimiter&gt;@&lt;/delimiter&gt;
                &lt;/delimiters&gt;
                &lt;useDefaultDelimiters&gt;false&lt;/useDefaultDelimiters&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
            &lt;configuration&gt;
                &lt;source&gt;1.8&lt;/source&gt;
                &lt;target&gt;1.8&lt;/target&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt;
            &lt;version&gt;2.9&lt;/version&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;id&gt;attach-javadocs&lt;/id&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;jar&lt;/goal&gt;
                    &lt;/goals&gt;
                    &lt;configuration&gt;
                        &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt;
                        &lt;failOnError&gt;false&lt;/failOnError&gt;
                        &lt;doclet&gt;org.umlgraph.doclet.UmlGraphDoc&lt;/doclet&gt;
                        &lt;docletPath&gt;/home/jorge/project/umlgraph-5.6.6.jar&lt;/docletPath&gt;

                        &lt;docletArtifact&gt;
                            &lt;groupId&gt;org.umlgraph&lt;/groupId&gt;
                            &lt;artifactId&gt;doclet&lt;/artifactId&gt;
                            &lt;version&gt;5.1&lt;/version&gt;
                        &lt;/docletArtifact&gt;
                        &lt;additionalparam&gt;-views&lt;/additionalparam&gt;
                        &lt;useStandardDocletOptions&gt;true&lt;/useStandardDocletOptions&gt;
                    &lt;/configuration&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-release-plugin&lt;/artifactId&gt;
            &lt;version&gt;2.5.1&lt;/version&gt;
            &lt;dependencies&gt;
                &lt;dependency&gt;
                    &lt;groupId&gt;org.apache.maven.scm&lt;/groupId&gt;
                    &lt;artifactId&gt;maven-scm-provider-gitexe&lt;/artifactId&gt;
                    &lt;version&gt;1.9.2&lt;/version&gt;
                &lt;/dependency&gt;
            &lt;/dependencies&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
    &lt;resources&gt;
        &lt;resource&gt;
            &lt;directory&gt;src/main/resources&lt;/directory&gt;
            &lt;filtering&gt;true&lt;/filtering&gt;
        &lt;/resource&gt;
    &lt;/resources&gt;
&lt;/build&gt;
</code></pre>

<p></p>
",958131,2017-12-18T17:37:25.077,0,CC BY-SA 3.0,,1,,outdated.</p,0,"but, for me, most of the time the material there is outdated.</p",False,False
326,47999025,2,47998843,2017-12-27T21:46:37.663,1,"<p>The <a href=""http://docs.aws.amazon.com/athena/latest/ug/athena-sql-workbench.html"" rel=""nofollow noreferrer"">documentation at #1 is deprecated</a>, as you might gather from the different filenames in the two guides. Amazon hasn't fully cleaned up their docs, since switching from their homegrown JDBC driver to the driver they've OEMed from Simba.</p>

<p>You did not provide the JDBC URL you're using, so I cannot provide a specific correction, but the error message you got seems pretty clear -- you apparently didn't build your JDBC URL correctly.  It's missing the mandatory <code>AwsRegion</code> setting.</p>

<p>Note the URL syntax from the <a href=""https://s3.amazonaws.com/athena-downloads/drivers/JDBC/docs/Simba+Athena+JDBC+Driver+Install+and+Configuration+Guide.pdf"" rel=""nofollow noreferrer"">PDF guide for the JDBC driver</a> you're using --</p>

<p><code>jdbc:awsathena://AwsRegion=[Region];UID=[AccessKey];PWD=[SecretKey];S3OutputLocation=[Output];[Property1]=[Value1];[Property2]=[Value2];...</code></p>
",241164,2017-12-27T21:46:37.663,0,CC BY-SA 3.0,,1,,deprecated</a,0,">The <a href=""http://docs.aws.amazon.com/athena/latest/ug/athena-sql-workbench.html"" rel=""nofollow noreferrer"">documentation at #1 is deprecated</a",False,False
327,48153828,2,48146202,2018-01-08T15:58:46.547,1,"<p>Once everything is provisioned, the CloudFormation part is done and the CloudFormation logs will only tell you about provisioning information. So the issue here is with the actual software and not with CloudFormation. </p>

<p>That being said, I happen to be quite experienced with Fleet and to get the logs for these services there's a couple things you can do.</p>

<p>First, try to query the logs using: </p>

<p><code>fleetctl journal httpd.service</code>, <code>fleetctl journal worker.1.service</code>, etc.</p>

<p>If that doesn't work, try to SSH into one host and use:
<code>journalctl -u httpd.service</code></p>

<p>As final note, I don't know if you want to run this inside a production environment but I think you should know that <strong>fleetd</strong> is being deprecated.</p>

<blockquote>
  <p>fleet is no longer developed or maintained by CoreOS. After February
  1, 2018, a fleet container image will continue to be available from
  the CoreOS Quay registry, but will not be shipped as part of Container
  Linux. CoreOS instead recommends Kubernetes for all clustering needs.</p>
</blockquote>

<p>If I were you, I'd look for a similar solution running on either Kubernetes or AWS ECS.</p>
",970247,2018-01-08T15:58:46.547,0,CC BY-SA 3.0,,1,,deprecated.</p,0,> is being deprecated.</p,False,False
328,48317094,2,48314880,2018-01-18T08:47:08.897,7,"<p>Parquet file written by <code>pyarrow</code> (long name: Apache Arrow) are compatible with Apache Spark. But you have to be careful which datatypes you write into the Parquet files as Apache Arrow supports a wider range of them then Apache Spark does. There is currently a flag <code>flavor=spark</code> in <code>pyarrow</code> that you can use to automatically set some compatibility options so that Spark can read these files in again. Sadly in the latest release, this option is not sufficient (expect to change with <code>pyarrow==0.9.0</code>). You should take care to write out timestamps using the deprecated INT96 type (<code>use_deprecated_int96_timestamps=True</code>) as well as avoiding unsigned integer columns. For the unsigned integer columns, convert them simply to a signed integer. Sadly Spark errors out if you have a unsigned type in your schema instead of just loading them as signed (they are actually always stored as signed, but only marked with a flag as unsigned). Respecting these two things, the files should be readable in Apache Spark and AWS Athena (which is just Presto under the hood).</p>
",1689261,2018-01-18T08:47:08.897,0,CC BY-SA 3.0,,2,,code,0,You should take care to write out timestamps using the deprecated INT96 type (<code,False,False
329,48433056,2,48341064,2018-01-24T22:54:57.280,1,"<p>Use <code>callback()</code> instead of <code>context.succeed()</code> or <code>context.fail()</code>. Those context methods are deprecated.</p>

<p>According to <a href=""https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html#transition-to-new-nodejs-runtime"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html#transition-to-new-nodejs-runtime</a>,</p>

<blockquote>
  <p>Node.js runtime v0.10.42 does not support the callback parameter for your Lambda function that runtimes v4.3 and v6.10 support. When using runtime v0.10.42, you use the following context object methods to properly terminate your Lambda function. The context object supports the done(), succeed(), and fail() methods that you can use to terminate your Lambda function. These methods are also present in runtimes v4.3 and v6.10 for backward compatibility.</p>
</blockquote>

<p>So, in your callback for <code>sendMail()</code>, it becomes...</p>

<pre><code>if(err) {
    console.log(err);
    callback(err);
} else {
    console.log(""===EMAIL SENT==="");
    console.log(""EMAIL CODE END"");
    console.log('EMAIL: ', email);
    console.log(data);
    callback(null, event);
}
</code></pre>

<p>Another thing, you don't need <code>context.callbackWaitsForEmptyEventLoop = false;</code>.</p>

<p>Also, since you connect to the database <code>inside</code> your handler, you also need to disconnect from it after you get your results.</p>
",1252647,2018-01-24T22:54:57.280,6,CC BY-SA 3.0,,1,,deprecated.</p,0,Those context methods are deprecated.</p,False,False
330,48469199,2,48397691,2018-01-26T20:39:30.397,0,"<p>What you could do is create a parameter that provides a list to pick from. </p>

<p>My below example allows you to pick 3 OS types within a specific region. I hard-coded the AMI identifiers so this may be out of date if AWS changes the AMI ID. The template uses a map to select the proper AMI ID from the value specified in the parameter and the region that the template is ran in. You could easily scale this up for many different regions or OS types.</p>

<pre><code>{
    ""AWSTemplateFormatVersion"": ""2010-09-09"",
    ""Parameters"": {
        ""osType"": {
            ""Description"": ""OS Type"",
            ""Type"": ""String"",
            ""AllowedValues"": [
                ""Server2016"",
                ""SUSE"",
                ""RHEL""
            ],
            ""ConstraintDescription"": ""must be a prod or test""
        }
    },
    ""Mappings"": {
        ""RegionAndInstanceTypeToAMIID"": {
            ""us-east-1"": {
                ""Server2016"": ""ami-8ff710e2"",
                ""SUSE"": ""ami-f5f41398"",
                ""RHEL"": ""ami-26ebbc5c""
            },
            ""us-west-2"": {
                ""Server2016"": ""ami-8ff710e2"",
                ""SUSE"": ""ami-f5f41398"",
                ""RHEL"": ""ami-26ebbc5c""
            }
        }
    },
    ""Resources"": {
        ""testInstance"": {
            ""Type"": ""AWS::EC2::Instance"",
            ""Properties"": {
                ""ImageId"": {
                    ""Fn::FindInMap"": [
                        ""RegionAndInstanceTypeToAMIID"",
                        {
                            ""Ref"": ""AWS::Region""
                        },
                        {
                            ""Ref"": ""osType""
                        }
                    ]
                }
            }
        }
    }
}
</code></pre>
",9274337,2018-01-26T20:39:30.397,0,CC BY-SA 3.0,,1,,.,0,I hard-coded the AMI identifiers so this may be outdated if AWS changes the AMI ID.,False,False
331,48587332,2,44975588,2018-02-02T16:50:42.577,1,"<p>You can do it by <code>client.setRegion(region)</code> something like:</p>

<p><code>AmazonS3Client client = 
new AmazonS3Client(new BasicAWSCredentials(accessKey, secretKey));
client.setRegion(region);
return client;</code></p>

<p>Alternatively, <code>new AmazonS3Client(new BasicAWSCredentials(accessKey, secretKey))</code> picks the Region information from the following, if you haven't provided (it tries searching from 1 to 3) :
<p>1.) EnvironmentVariableCredentialsProvider()
<p>2.) SystemPropertiesCredentialsProvider()
<p>3.) EC2 instance</p>

<p>But this method is deprecated now, you should use now:</p>

<p><code>AmazonS3 amazonS3 = AmazonS3ClientBuilder.standard()
                      .withRegion(Regions.US_EAST_1)
                      .build();</code></p>
",5860781,2018-02-02T16:50:42.577,0,CC BY-SA 3.0,,1,,now:</p,0,"But this method is deprecated now, you should use now:</p",False,False
332,48589579,2,48588402,2018-02-02T19:25:09.197,2,"<p>The default constructors have fixed endpoints, which (at least for the services I've used) are in <code>us-east-1</code>. You can change to a different region by calling the <code>setRegion()</code> or <code>setEndpoint()</code> method.</p>

<p>However, the default constructors have been deprecated since version 1.11.11. You should now use a client builder (such as <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3ClientBuilder.html"" rel=""nofollow noreferrer"">AmazonS3ClientBuilder</a>), which will look for a configured region. Unless you have a good reason to do otherwise, call <code>AmazonS3ClientBuilder.defaultClient()</code>.</p>
",9305910,2018-02-02T19:25:09.197,0,CC BY-SA 3.0,,1,constructors,.,0,"However, the default constructors have been deprecated since version 1.11.11.",False,False
333,48622583,2,31466916,2018-02-05T12:26:22.877,2,"<p>Some methods have been deprecated in AWS SDK. </p>

<pre><code>BasicAWSCredentials awsCredentials = new BasicAWSCredentials(""CLIENT-ID"", ""SECRET-KEY"");
AmazonSNS snsClient = AmazonSNSClientBuilder.standard()
                     .withRegion(Regions.fromName(""YOUR_REGION""))
     .withCredentials(new AWSStaticCredentialsProvider(awsCredentials)).build();

Map&lt;String, MessageAttributeValue&gt; smsAttributes = new HashMap&lt;String, MessageAttributeValue&gt;();
smsAttributes.put(""AWS.SNS.SMS.SenderID"",new MessageAttributeValue().withStringValue(""SENDER-ID"").withDataType(""String"");
smsAttributes.put(""AWS.SNS.SMS.SMSType"",new MessageAttributeValue().withStringValue(""Transactional"").withDataType(""String""));

PublishRequest request = new PublishRequest();
request.withMessage(""YOUR MESSAGE"")
.withPhoneNumber(""E.164-PhoneNumber"")
.withMessageAttributes(smsAttributes);
PublishResult result=snsClient.publish(request);
</code></pre>
",4356145,2018-02-05T12:26:22.877,0,CC BY-SA 3.0,,1,methods,.,0,Some methods have been deprecated in AWS SDK.,False,False
334,48709151,2,38578937,2018-02-09T15:32:52.413,8,"<p>The accepted answer is using a deprecated APIs. Here's an updated revision.</p>

<p>First, update your maven dependencies:</p>

<pre><code> &lt;dependency&gt;
        &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;
        &lt;artifactId&gt;aws-java-sdk&lt;/artifactId&gt;
        &lt;version&gt;1.11.274&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>AWSConfiguration.java</p>

<pre><code>import com.amazonaws.auth.AWSCredentials;
import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicAWSCredentials;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class AWSConfiguration {

    @Value(""${cloud.aws.credentials.accessKey}"")
    private String accessKey;

    @Value(""${cloud.aws.credentials.secretKey}"")
    private String secretKey;

    @Value(""${cloud.aws.region}"")
    private String region;

    @Bean
    public BasicAWSCredentials basicAWSCredentials() {
        return new BasicAWSCredentials(accessKey, secretKey);
    }

    @Bean
    public AmazonS3 amazonS3Client(AWSCredentials awsCredentials) {
        AmazonS3ClientBuilder builder = AmazonS3ClientBuilder.standard();
        builder.withCredentials(new AWSStaticCredentialsProvider(awsCredentials));
        builder.setRegion(region);
        AmazonS3 amazonS3 = builder.build();
        return amazonS3;
    }
}
</code></pre>

<p>S3Service.java</p>

<pre><code>import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3Client;
import com.amazonaws.services.s3.model.*;
import org.apache.commons.io.IOUtils;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpHeaders;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.stereotype.Service;
import org.springframework.util.StringUtils;
import org.springframework.web.multipart.MultipartFile;

import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.net.URLEncoder;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

@Service
public class S3Service {

    @Autowired
    private AmazonS3 amazonS3;

    @Value(""${cloud.aws.s3.bucket}"")
    private String bucket;

    private PutObjectResult upload(String filePath, String uploadKey) throws FileNotFoundException {
        return upload(new FileInputStream(filePath), uploadKey);
    }

    private PutObjectResult upload(InputStream inputStream, String uploadKey) {
        PutObjectRequest putObjectRequest = new PutObjectRequest(bucket, uploadKey, inputStream, new ObjectMetadata());

        putObjectRequest.setCannedAcl(CannedAccessControlList.PublicRead);

        PutObjectResult putObjectResult = amazonS3.putObject(putObjectRequest);

        IOUtils.closeQuietly(inputStream);

        return putObjectResult;
    }

    public List&lt;PutObjectResult&gt; upload(MultipartFile[] multipartFiles) {
        List&lt;PutObjectResult&gt; putObjectResults = new ArrayList&lt;&gt;();

        Arrays.stream(multipartFiles)
                .filter(multipartFile -&gt; !StringUtils.isEmpty(multipartFile.getOriginalFilename()))
                .forEach(multipartFile -&gt; {
                    try {
                        putObjectResults.add(upload(multipartFile.getInputStream(), multipartFile.getOriginalFilename()));
                    } catch (IOException e) {
                        e.printStackTrace();
                    }
                });

        return putObjectResults;
    }

    public ResponseEntity&lt;byte[]&gt; download(String key) throws IOException {
        GetObjectRequest getObjectRequest = new GetObjectRequest(bucket, key);

        S3Object s3Object = amazonS3.getObject(getObjectRequest);

        S3ObjectInputStream objectInputStream = s3Object.getObjectContent();

        byte[] bytes = IOUtils.toByteArray(objectInputStream);

        String fileName = URLEncoder.encode(key, ""UTF-8"").replaceAll(""\\+"", ""%20"");

        HttpHeaders httpHeaders = new HttpHeaders();
        httpHeaders.setContentType(MediaType.APPLICATION_OCTET_STREAM);
        httpHeaders.setContentLength(bytes.length);
        httpHeaders.setContentDispositionFormData(""attachment"", fileName);

        return new ResponseEntity&lt;&gt;(bytes, httpHeaders, HttpStatus.OK);
    }

    public List&lt;S3ObjectSummary&gt; list() {
        ObjectListing objectListing = amazonS3.listObjects(new ListObjectsRequest().withBucketName(bucket));

        List&lt;S3ObjectSummary&gt; s3ObjectSummaries = objectListing.getObjectSummaries();

        return s3ObjectSummaries;
    }
}
</code></pre>
",1019952,2018-02-09T15:32:52.413,2,CC BY-SA 3.0,,1,,.,0,The accepted answer is using a deprecated APIs.,False,False
335,48715352,2,47398375,2018-02-09T23:00:10.543,1,"<p>Unless I am misunderstanding... Even though terraform refresh would update the state file it would not update your *.tf files which would still be out of date. (Which would break the concept of IAC).</p>

<p>On AWS you can use AWS Config to see any changes that have happened to your infrastructure after your initial deployment to reconcile the differences.</p>

<p>You can also use cloudformer to get a current cloudformation template of your infrastructure. <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html"" rel=""nofollow noreferrer"">CloudFormer</a></p>
",4453351,2018-02-09T23:00:10.543,0,CC BY-SA 3.0,,1,,.,1,Even though terraform refresh would update the state file it would not update your *.tf files which would still be outdated.,False,False
336,48742904,2,48740949,2018-02-12T09:15:39.140,0,"<p>Firstly, I notice that your <code>IntegrationHttpMethod</code> is <code>ANY</code>. For Lambdas, unless you're using the <code>{proxy+}</code> configuration, try using <code>POST</code>. I'm pretty sure the CloudFormation docs are still out of date for this, but you'll find some useful info in <a href=""https://stackoverflow.com/a/39772260/4206678"">this answer</a>.</p>

<p>The second thing that I notice is the malformed proxy response, which in your case could just be a misconfiguration. Just to rule this out, dealing with a malformed proxy response is <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/malformed-502-api-gateway/"" rel=""nofollow noreferrer"">answered</a> on the AWS support centre. Basically, your lambda response should be in the following format, including not adding any extra keys.</p>

<pre><code>{
    ""isBase64Encoded"": true|false,
    ""statusCode"": httpStatusCode,
    ""headers"": { ""headerName"": ""headerValue"", ... },
    ""body"": ""...""
}
</code></pre>

<p>You could use the example function on the support doc in place of your regular function so that you can isolate the problem.</p>
",4206678,2018-02-12T09:15:39.140,1,CC BY-SA 3.0,,1,,in,0,"the CloudFormation docs are still outdated for this, but you'll find some useful info in",False,False
337,48844091,2,48842054,2018-02-17T17:40:43.397,0,"<p>Its working now. The above issue is because I was using a deprecated Lambda API and there were few mistakes in usage of Lambda.</p>

<p>With below code I am able to save a JSON payload in to Atlas MongoDB instance. </p>

<p>If you run this inside an VPC, make sure necessary network policies are in place. Otherwise you will receive a Socket error in logs and below error in Eclipse console.</p>

<blockquote>
  <p>==================== INVOCATION ERROR ==================== com.amazonaws.SdkClientException: Unable to execute HTTP request: Read
  timed out</p>
</blockquote>

<p>My complete code, I am sure this will help others who are new to Lambda and MongoDB Atlas.</p>

<pre><code>package com.amazonaws.lambda.demo;

import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.RequestHandler;

public class LambdaCreateUser implements RequestHandler&lt;RegistrationRequest, String&gt; {


    public String handleRequest(RegistrationRequest input, Context context) {

        context.getLogger().log(""Input: "" + input.getFirstName());
        Boolean userStatus = false;

        try {

        CognitoSaveUser newUser = new CognitoSaveUser();
        userStatus = newUser.saveNewUserInDB(input, context);

        } catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }


        return ""Hello from Lambda-""+userStatus.toString();
    }

} 
</code></pre>

<p>Don't forget to create the 2 constructors at the end of this bean class. I lost hours without those.
package com.amazonaws.lambda.demo;</p>

<pre><code>public class RegistrationRequest {
    String firstName;
    String lastName;
    String department;
    String userName;
    String userPassword;
    String confirmPassword;
    String email;
    String contactNo;

    public String getFirstName() {
        return firstName;
    }
    public void setFirstName(String firstName) {
        this.firstName = firstName;
    }
    public String getLastName() {
        return lastName;
    }
    public void setLastName(String lastName) {
        this.lastName = lastName;
    }
    public String getDepartment() {
        return department;
    }
    public void setDepartment(String department) {
        this.department = department;
    }
    public String getUserName() {
        return userName;
    }
    public void setUserName(String userName) {
        this.userName = userName;
    }
    public String getUserPassword() {
        return userPassword;
    }
    public void setUserPassword(String userPassword) {
        this.userPassword = userPassword;
    }
    public String getConfirmPassword() {
        return confirmPassword;
    }
    public void setConfirmPassword(String confirmPassword) {
        this.confirmPassword = confirmPassword;
    }
    public String getEmail() {
        return email;
    }
    public void setEmail(String email) {
        this.email = email;
    }
    public String getContactNo() {
        return contactNo;
    }
    public void setContactNo(String contactNo) {
        this.contactNo = contactNo;
    }
    public RegistrationRequest(String firstName, String lastName, String department, String userName, String userPassword, String confirmPassword, String email, String contactNo) {
        this.firstName = firstName;
        this.lastName = lastName;
        this.department = department;
        this.userName = userName;
        this.userPassword = userPassword;
        this.confirmPassword = confirmPassword;
        this.email = email;
        this.contactNo = contactNo;
    }
    public RegistrationRequest() {
    }

}
</code></pre>

<p>The final class which will do all the magic. Don't forget to put the correct username, password and dbname for MongoDB URI</p>

<pre><code>package com.amazonaws.lambda.demo;

import com.amazonaws.services.lambda.runtime.Context;
import com.fasterxml.jackson.core.JsonFactory;
import com.fasterxml.jackson.core.JsonParseException;
import com.fasterxml.jackson.core.JsonParser;
import com.fasterxml.jackson.core.JsonToken;
import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.ObjectMapper;

import java.io.IOException;

import org.bson.Document;

import com.mongodb.MongoClient;
import com.mongodb.MongoClientURI;
import com.mongodb.client.MongoCollection;
import com.mongodb.client.MongoDatabase;
public class CognitoSaveUser {


    public Boolean saveNewUserInDB (RegistrationRequest jsonUserDetails,Context context) {

        Boolean processStatus = false;
        context.getLogger().log(""Inside the method saveNewUserInDB"");
        ObjectMapper mapper = new ObjectMapper();

        try {

            String jsonString = mapper.writeValueAsString(jsonUserDetails);
            context.getLogger().log(""Converted JSOn String-&gt;""+jsonString);

            JsonFactory factory = new JsonFactory();
            JsonParser  parser  = factory.createParser(jsonString);

            Document doc = new Document();
            String docKey = """";
            String docValue = """";

            while(!parser.isClosed()){
                JsonToken jsonToken = parser.nextToken();

                if(JsonToken.FIELD_NAME.equals(jsonToken)){
                    docKey = parser.getCurrentName();
                    jsonToken = parser.nextToken();
                    docValue = parser.getValueAsString();
                    doc.append(docKey, docValue);
                    context.getLogger().log(""Key-&gt;""+docKey+""-value-&gt;""+docValue);
                    } 
                }

            MongoClientURI uri = new MongoClientURI(
                       ""mongodb+srv://username:password@clusterdemo-07d17.mongodb.net/dbname"");
            MongoClient mongoClient = new MongoClient(uri);
            MongoDatabase database = mongoClient.getDatabase(""EclipseCognitoTest"");
            MongoCollection&lt;Document&gt; collection = database.getCollection(""UserProfile"");

            context.getLogger().log(""DB Connection is OK"");
            collection.insertOne(doc);

            processStatus = true;
            context.getLogger().log(""processStatus=""+processStatus);

        } catch (JsonParseException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        } catch (JsonMappingException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        } catch (IOException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }catch (Exception e) {
            e.printStackTrace();
        }
        return processStatus;
    }


}
</code></pre>
",5404024,2018-02-17T17:40:43.397,0,CC BY-SA 3.0,,1,,Lambda.</p,0,The above issue is because I was using a deprecated Lambda API and there were few mistakes in usage of Lambda.</p,False,False
338,48879926,2,22588733,2018-02-20T07:30:37.320,2,"<p>Since AmazonDynamoDBClient(credentials) is deprecated i use this.</p>

<pre><code>init {
        val cp= AWSStaticCredentialsProvider(BasicAWSCredentials(ACCESS_KEY, SECRET_KEY))
        val client = AmazonDynamoDBClientBuilder.standard().withCredentials(cp).withRegion(Regions.US_EAST_1).build()
        dynamoDB = DynamoDB(client)
    }
</code></pre>
",2102794,2018-02-20T07:30:37.320,0,CC BY-SA 3.0,,1,,deprecated,0,Since AmazonDynamoDBClient(credentials) is deprecated,False,False
339,48882087,2,48881195,2018-02-20T09:41:58.447,1,"<p>A few things you might want to look into:</p>

<p>You'll need to make sure you're using either Node.js runtime v6.10 or v4.3. (Node v0.10.42 is currently marked as deprecated. AWS recommends migrating existing functions to the newer Node.js runtime versions as soon as possible)</p>

<p>The IAM role for your lambda function needs to have an <em>Allow</em> rule for the <code>sns:Publish</code> action.</p>

<p>AWS recommends that specify the phone number using the E.164 format. For example: +44xxxxxxxxxx. (<a href=""https://docs.aws.amazon.com/sns/latest/dg/sms_publish-to-phone.html"" rel=""nofollow noreferrer"">more info</a>)</p>

<p>Also, AWS <strong>strongly</strong> recommends updating any use of the <code>context</code> method and replacing it with the <code>callback</code> approach (<a href=""https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html"" rel=""nofollow noreferrer"">more info</a>). For example:</p>

<pre><code>const AWS = require(""aws-sdk"");
const sns = new AWS.SNS({apiVersion: ""2010-03-31""});

exports.handler = (event, context, callback) =&gt; {
  const params = {
    PhoneNumber: ""+44xxxxxxxxxx"", // E.164 format.
    Message: ""STRING_VALUE"",
    MessageStructure: ""STRING_VALUE""
  }
  sns.publish(params, (err, data) =&gt; {
    if (err) {
      console.error(`Error ${err.message}`);
      callback(err);
    } else {
      console.log(""Success"");
      callback(null, data); // callback instead of context.
    }
  }
};
</code></pre>
",3770040,2018-02-20T09:41:58.447,5,CC BY-SA 3.0,,1,,.,0,(Node v0.10.42 is currently marked as deprecated.,False,False
340,48913918,2,41935525,2018-02-21T19:25:25.827,0,"<p>I know that this is now somewhat out of date, but after hours of searching and having the same problem (Also, following the AWS Tutorials!), they didn't mention anything about this.</p>

<p>The package is there but named incorrectly, you can find it by using;</p>

<pre><code>yum search httpd
</code></pre>

<p>For me this returned a package labeled as</p>

<pre><code>yum info httpd.x86_64
</code></pre>

<p>I followed up using</p>

<pre><code>sudo yum install -y httpd.x86_64
</code></pre>

<p>The package then successfully installed, next step is to set permissions and launch the service!</p>

<p>Hope this helps.</p>

<p>Ash</p>
",4363004,2018-02-21T19:25:25.827,0,CC BY-SA 3.0,,1,I,",",0,"I know that this is now somewhat outdated,",False,False
341,48934983,2,48934511,2018-02-22T18:41:34.963,0,"<p>The <code>DynamoDB</code> class implements the <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.SDKs.Interfaces.Document.html"" rel=""nofollow noreferrer"">document interface</a>, which is a layer on top of the low-level interface (there's also the <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.SDKs.Interfaces.Mapper.html"" rel=""nofollow noreferrer"">object persistence interface</a>, which is another layer on top of the low-level interface).</p>

<p>You create a <code>DynamoDB</code> instance around an <code>AmazonDynamoDB</code> instance.</p>

<p>The latter is an interface, and there are two objects that implement this interface. The first is <code>AmazonDynamoDBClient</code>, which has been deprecated in favor of the client object returned by <code>AmazonDynamoDBBuilder</code>.</p>

<p>However, unless you need to configure your client object, it's better to call <code>AmazonDynamoDBBuilder.defaultClient()</code>, rather than construct an instance and call <code>build()</code>.</p>
",9398157,2018-02-22T18:41:34.963,0,CC BY-SA 3.0,,1,,code,0,">AmazonDynamoDBClient</code>, which has been deprecated in favor of the client object returned by <code",False,False
342,49000770,2,25721048,2018-02-27T03:45:10.347,0,"<p><code>[NSURL initWithScheme: host: path:]</code> is deprecated in iOS10.
should use <code>NSURLComponents</code> instead.</p>

<p>my updated solution:</p>

<pre><code>SDWebImageManager.sharedManager.cacheKeyFilter = ^NSString *(NSURL *url) {
     if([[url absoluteString] isEqualToString:@""""]){
        return @"""";
    }
    NSURLComponents *urlComponents = [[NSURLComponents alloc] initWithURL:url resolvingAgainstBaseURL:NO];
    urlComponents.query = nil;
    return [[urlComponents URL] absoluteString];
};
</code></pre>
",2609878,2018-02-27T03:45:10.347,0,CC BY-SA 3.0,,1,host,"
",0,": host: path:]</code> is deprecated in iOS10.
",False,False
343,49014211,2,49011366,2018-02-27T17:07:49.740,2,"<p>Looks like my S3 API was out of date. After updating to the latest version (1.11.257) I now have access to com.amazonaws.services.s3.model.SetBucketEncryptionRequest</p>
",3727399,2018-02-27T17:07:49.740,0,CC BY-SA 3.0,,1,,.,0,Looks like my S3 API was outdated.,False,False
344,49128386,2,49116349,2018-03-06T10:17:16.517,0,"<p>I've finally solved the problem. ""It was easy"", of course!</p>

<p>Here is the code in lambda:</p>

<pre><code>S3.getObject(params).promise()
  .then((configYaml) =&gt; {
    // Get the content of the file as a string
    // It works on any text file, should your config is in another format
    const data = configYaml.Body.toString('utf-8');
    console.log(data);

    // Parse the string, which is a yaml in this case
    // Please note that `YAML.parse()` is deprecated
    return YAML.load(data);
  })
  .then((config) =&gt; {
    // Do some wonderful staff with the config object
    console.log(`• Config: ${JSON.stringify(config)}`);
    return null;
  })
  .then(callback)
  .catch(callback);
</code></pre>

<p>All I was asking for is: <code>YAML.load(configYaml.Body.toString('utf-8'))</code>.</p>
",2107667,2018-03-06T10:17:16.517,0,CC BY-SA 3.0,,1,,),0,"Please note that `YAML.parse()` is deprecated
    return YAML.load(data);
  })
  .then((config)",False,True
345,49181531,2,49180428,2018-03-08T20:01:51.150,2,"<p>You cannot use non-ASCII characters for S3 user-defined metadata when using either REST API or the AWS SDK (since AWS SDK is basically wrapper libraries that wrap the underlying Amazon S3 REST API):</p>

<blockquote>
  <p>User-defined metadata is a set of key-value pairs. Amazon S3 stores
  user-defined metadata keys in lowercase. Each key-value pair must
  conform to <strong>US-ASCII when you are using REST</strong> and to UTF-8 when you are
  using SOAP or browser-based uploads via POST.</p>
</blockquote>

<p>While UTF-8 is supported for both; SOAP and browser-based uploads using POST, AWS <a href=""https://docs.aws.amazon.com/AmazonS3/latest/API/APISoap.html"" rel=""nofollow noreferrer"">recommends</a> that you use either the REST API or the AWS SDKs saying that new features will not be supported for SOAP:</p>

<blockquote>
  <p>SOAP support over HTTP is deprecated, but it is still available over
  HTTPS. New Amazon S3 features will not be supported for SOAP. We
  recommend that you use either the REST API or the AWS SDKs.</p>
</blockquote>
",3770040,2018-03-08T20:01:51.150,1,CC BY-SA 3.0,,1,support,.,0,"SOAP support over HTTP is deprecated, but it is still available over
  HTTPS.",False,False
346,49285699,2,49259499,2018-03-14T18:54:48.763,1,"<p>Terraform Plugin released in JetBrains plugin community is used for HashiCorp Terraform / HCL language support for doing stuffs like:</p>

<ul>
<li>Syntax highlighting.</li>
<li>Code formatter with the 'Reformat code'.</li>
<li>Report usage of deprecated properties.</li>
</ul>

<p>And <strong>NOT</strong> as a substitute for executing terraform binary.</p>

<p>Reference: <a href=""https://plugins.jetbrains.com/plugin/7808-hashicorp-terraform--hcl-language-support"" rel=""nofollow noreferrer"">https://plugins.jetbrains.com/plugin/7808-hashicorp-terraform--hcl-language-support</a></p>
",6684192,2018-03-14T18:54:48.763,1,CC BY-SA 3.0,,1,,properties.</li,0,<li>Report usage of deprecated properties.</li,False,False
347,49291577,2,48964473,2018-03-15T04:26:38.997,1,"<p>I think the documentation may be out of date. At AWS re:Invent 2017, there was an excellent session called ""AWS CLI: 2017 and Beyond"" which is <a href=""https://www.youtube.com/watch?v=W8IyScUGuGI"" rel=""nofollow noreferrer"">currently available on YouTube</a>. The presentation goes into detail about some of the new features, including dynamic credentials for the AWS CLI, and there is a corresponding GitHub repository <a href=""https://github.com/awslabs/awsprocesscreds"" rel=""nofollow noreferrer"">awslabs/awsprocesscreds</a> which may have some useful examples.</p>
",8972418,2018-03-15T04:26:38.997,0,CC BY-SA 3.0,,1,,.,0,I think the documentation may be outdated.,False,False
348,49313550,2,49313351,2018-03-16T05:05:47.593,4,"<p>I believe you are receiving the ""No updates"" message because technically nothing is changing in your CloudFormation template. When attempting to build the changeset, the contents of the S3 file are not examined. It just sees that none of the CloudFormation resource properties have changed since the previous deployment.</p>

<p>Instead, you may use a local relative path for the CodeUri, and <code>aws cloudformation package</code> can upload the file to a unique S3 path for you. This ensures that the template changes each time and the new Lambda code is recognized/deployed. For example:</p>

<pre><code>aws cloudformation package --template-file samTemplate.yml --output-template-file sam-output-dev.yml --s3-bucket ""$CodePipelineS3Bucket"" --s3-prefix ""$CloudFormationPackageS3Prefix""
</code></pre>

<p>This command can be put into the build step before your create/execute changeset steps.</p>

<p>To see an demonstration of this entire flow in practice, you can look at <a href=""https://github.com/lafiosca/lard-example-crud-api"" rel=""nofollow noreferrer"">this repository</a>, although I will warn that it's a bit outdated thanks to the new features released at the end of 2017. (For example, I was publishing Lambda aliases manually via extra steps because it was written pre-<code>AutoPublishAlias</code>.)</p>
",8972418,2018-03-16T05:05:47.593,3,CC BY-SA 3.0,,1,,.,0,"repository</a>, although I will warn that it's a bit outdated thanks to the new features released at the end of 2017.",False,False
349,49372527,2,49330745,2018-03-19T21:21:36.487,3,"<p>This looks like you are mixing the 1.11.286 version of aws-java-sdk with an older version (1.11.149) of aws-java-sdk-core. The newer client is using a new field added to the core module but since your core module is out of date you are seeing the no such field error. Can you ensure all of your dependencies are in sync with one another?</p>
",5932496,2018-03-19T21:21:36.487,1,CC BY-SA 3.0,,1,,.,0,The newer client is using a new field added to the core module but since your core module is outdated you are seeing the no such field error.,False,False
350,49373369,2,49327624,2018-03-19T22:22:02.267,1,"<p>Is your application actually accessing your AWS credentials before making the call? try putting them inline in your code even though that's discouraged just as a test. If it works correctly, something is wrong with accessing your creds file.</p>
",9336087,2018-03-19T22:22:02.267,1,CC BY-SA 3.0,,1,,.,0,try putting them inline in your code even though that's discouraged just as a test.,False,False
351,49401378,2,49306178,2018-03-21T08:28:36.950,1,"<p>Actually, I have two different aws cli's installed. And one was too old to support Fargate. I have updated cli installed on root level while jenkins user uses out of dated cli which does not support Fargate.</p>
",2121089,2018-03-21T08:28:36.950,0,CC BY-SA 3.0,,1,,"
",1,"I have updated cli installed on root level while jenkins user uses outdatedd cli which does not support Fargate.</p>
",False,False
352,49708118,2,42848346,2018-04-07T13:56:43.040,1,"<p>The question is a bit outdated, but I just released <a href=""https://github.com/fvinas/multi_sqs_listener"" rel=""nofollow noreferrer"" title=""Multi SQS listener"">multi_sqs_listener</a> that provides a high level, multi-threaded way to listen to multiple SQS queues from Python code.</p>

<pre class=""lang-python prettyprint-override""><code>import time
from multi_sqs_listener import QueueConfig, EventBus, MultiSQSListener


class MyListener(MultiSQSListener):
    def low_priority_job(self, message):
        print('Starting low priority, long job: {}'.format(message))
        time.sleep(5)
        print('Ended low priority job: {}'.format(message))
    def high_priority_job(self, message):
        print('Starting high priority, quick job: {}'.format(message))
        time.sleep(.2)
        print('Ended high priority job: {}'.format(message))
    def handle_message(self, queue, bus, priority, message):
        if bus == 'high-priority-bus':
            self.high_priority_job(message.body)
        else:
            self.low_priority_job(message.body)

low_priority_bus = EventBus('low-priority-bus', priority=1)
high_priority_bus = EventBus('high-priority-bus', priority=5)
EventBus.register_buses([low_priority_bus, high_priority_bus])

low_priority_queue = QueueConfig('low-priority-queue', low_priority_bus)
high_priority_queue = QueueConfig('high-priority-queue', high_priority_bus)
my_listener = MyListener([low_priority_queue, high_priority_queue])
my_listener.listen()
</code></pre>
",9611797,2018-04-07T13:56:43.040,0,CC BY-SA 3.0,,1,question,"title=""Multi",0,"The question is a bit outdated, but I just released <a href=""https://github.com/fvinas/multi_sqs_listener"" rel=""nofollow noreferrer"" title=""Multi",False,False
353,50016580,2,50016185,2018-04-25T07:38:50.537,8,"<p>May i ask you why you are not using the built-in approach of <code>AWS CodeBuild</code>? You are able to get parameters out of <code>SSM</code> through the build spec of your <code>AWS CodeBuild</code> project. The additional call through the Java SDK is obsolete in this case.</p>

<pre><code>version: 0.2

env:
  parameter-store:
    key: ""value""
    key: ""value""

phases:
  build:
    commands:
      - command
      - command
</code></pre>

<blockquote>
  <p>parameter-store: Required if env is specified, and you want to
  retrieve custom environment variables stored in Amazon EC2 Systems
  Manager Parameter Store. Contains a mapping of key/value scalars,
  where each mapping represents a single custom environment variable
  stored in Amazon EC2 Systems Manager Parameter Store. key is the name
  you will use later in your build commands to refer to this custom
  environment variable, and value is the name of the custom environment
  variable stored in Amazon EC2 Systems Manager Parameter Store.</p>
</blockquote>

<p>For more informations please check the <a href=""https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html"" rel=""noreferrer"">Build Specification Reference for AWS CodeBuild</a></p>
",3115607,2018-04-25T07:38:50.537,0,CC BY-SA 3.0,,1,call,case.</p,0,The additional call through the Java SDK is obsolete in this case.</p,False,False
354,50026439,2,50016185,2018-04-25T15:48:10.817,0,"<p>Your AWS Java SDK is likely out of date. The minimum version for retrieving credentials in CodeBuild is 1.11.16.
<a href=""https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html#troubleshooting-versions"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html#troubleshooting-versions</a></p>
",5095975,2018-04-25T15:48:10.817,2,CC BY-SA 3.0,,1,SDK,.,0,Your AWS Java SDK is likely outdated.,False,False
355,50032619,2,50032521,2018-04-25T23:16:15.143,3,"<blockquote>
  <p>path must be absolute or specify root to res.sendFile</p>
</blockquote>

<p>You can use the <code>root</code> option for it:</p>

<pre><code>res.sendFile('index.html', { root: '.' })
</code></pre>

<p>By the way, <code>sendfile</code> is deprecated, use <code>sendFile</code>.</p>
",6401895,2018-04-25T23:16:15.143,1,CC BY-SA 3.0,,1,sendfile</code,>,0,"By the way, <code>sendfile</code> is deprecated, use <code>",False,False
356,50036442,2,50036271,2018-04-26T06:43:58.110,12,"<p>Basically, if you NEED to have the latest values, use a <strong>fully consistent read</strong>. You'll get the guaranteed current value.</p>

<p>If your app is okay with potentially outdated information (mere seconds or less out of date), then use <strong>eventually consistent reads</strong>.</p>

<p>Examples of fully-consistent:</p>

<ul>
<li>Bank balance (Want to know the latest amount)</li>
<li>Location of a locomotive on a train network (Need absolute certainty to guarantee safety)</li>
<li>Stock trading (Need to know the latest price)</li>
</ul>

<p>Use-cases for eventually consistent reads:</p>

<ul>
<li>Number of Facebook friends (Does it matter if another was added in the last few seconds?)</li>
<li>Number of commuters who used a particular turnstile in the past 5 minutes (Not important if it is out by a few people)</li>
<li>Stock research (Doesn't matter if it's out by a few seconds)</li>
</ul>
",174777,2018-04-26T06:43:58.110,0,CC BY-SA 3.0,,1,,"

",0,">If your app is okay with potentially outdated information (mere seconds or less outdated), then use <strong>eventually consistent reads</strong>.</p>

",False,False
357,50050912,2,50048987,2018-04-26T19:54:43.457,0,"<p>You could use a dnslookup command. The following will give you the ip</p>

<pre><code>dig +nocmd your.cloud any +multiline +noall +answer
</code></pre>

<p>Then you can use the following command to get the region:</p>

<pre><code>curl freegeoip.net/xml/xx.xxx.xxx.xxx
</code></pre>

<p>this will get you the geolocation of the address. You can also use their <a href=""https://geoiptool.com/"" rel=""nofollow noreferrer"">website</a> if that helps </p>

<p>tested with: <code>curl freegeoip.net/xml/54.231.184.255</code></p>

<p>response: </p>

<p><code>&lt;Response&gt;
    &lt;DeprecationMessage&gt;This API endpoint is deprecated and will stop working on July 1st, 2018. For more information please visit: https://github.com/apilayer/freegeoip#readme&lt;/DeprecationMessage&gt;
    &lt;IP&gt;54.231.184.255&lt;/IP&gt;
    &lt;CountryCode&gt;US&lt;/CountryCode&gt;
    &lt;CountryName&gt;United States&lt;/CountryName&gt;
    &lt;RegionCode&gt;OR&lt;/RegionCode&gt;
    &lt;RegionName&gt;Oregon&lt;/RegionName&gt;
    &lt;City&gt;Boardman&lt;/City&gt;
    &lt;ZipCode&gt;97818&lt;/ZipCode&gt;
    &lt;TimeZone&gt;America/Los_Angeles&lt;/TimeZone&gt;
    &lt;Latitude&gt;45.7788&lt;/Latitude&gt;
    &lt;Longitude&gt;-119.529&lt;/Longitude&gt;
    &lt;MetroCode&gt;810&lt;/MetroCode&gt;
&lt;/Response&gt;</code></p>
",1546706,2018-04-26T19:54:43.457,1,CC BY-SA 3.0,,1,endpoint,.,0,">

<p><code>&lt;Response&gt;
    &lt;DeprecationMessage&gt;This API endpoint is deprecated and will stop working on July 1st, 2018.",False,False
358,50112614,2,44823012,2018-05-01T07:07:23.587,0,"<p>The MobileHubHelper framework is deprecated and the AWS iOS SDK has the Auth SDK to manage authentication with AWS. It is recommended you use that. You can find the instructions here: <a href=""https://docs.aws.amazon.com/aws-mobile/latest/developerguide/add-aws-mobile-user-sign-in.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/aws-mobile/latest/developerguide/add-aws-mobile-user-sign-in.html</a></p>

<p>Thanks,
Rohan</p>
",5045692,2018-05-01T07:07:23.587,0,CC BY-SA 3.0,,1,framework,and,0,The MobileHubHelper framework is deprecated and,False,False
359,50225318,2,50222478,2018-05-08T03:20:58.143,0,"<p>I think the stated focus on performance belies real trade-offs.</p>

<p>Consider that someone will have to maintain your code -- if you use an API, the test area is small, but AWS APIs might change or be deprecated; if you an SDK, next programmer will plug in new SDK version and hope that it works, but if it doesn't they'd be bogged down by sheer weight of the SDK.</p>

<p>Likewise, imagine someone needs to do a security review of this app, or to introduce something not yet covered by SDK (let's imagine propagating accounting group from caller role to underlying storage).</p>

<p>I don't think there is a clear answer.</p>

<p>Here are my <strong>suggestions</strong>:</p>

<ul>
<li>keep it consistent -- either API or SDK (within given app)</li>
<li>consider the bigger picture (how many apps do you plan to write?)</li>
<li>don't be afraid to switch to the other approach later</li>
</ul>

<p>I've had to decide on something similar <strong>in the past</strong>, with Docker (much nicer APIs and SDKs/libs). Here's how it played out:</p>

<p>For testing, we ended up using beta version of Docker Python bindings: prod version was not enough, and bindings (your SDK) were overall pretty good and clear.</p>

<p>For log scraping, I used HTTP calls (your API), ""because performance"", in reality comparative mental load using API vs SDK, and because bindings (SDK) did not support asyncio.</p>
",705086,2018-05-08T03:20:58.143,0,CC BY-SA 4.0,,1,,SDK.</p,1,">Consider that someone will have to maintain your code -- if you use an API, the test area is small, but AWS APIs might change or be deprecated; if you an SDK, next programmer will plug in new SDK version and hope that it works, but if it doesn't they'd be bogged down by sheer weight of the SDK.</p",False,False
360,50236173,2,47939238,2018-05-08T14:35:48.390,1,"<p>This error for me was because I had previously logged in with a different user storing their JWT tokens in local storage, then logged in with a different user which confused things.</p>

<p>There seems to be a bug with the latest but now deprecated (non-AWSAmplify) version of the amazon-cognito-identity-js library where it doesn't clear the JWT tokens when you call its signout() method.  This can be fixed by manually clearing the keys from local storage.</p>
",213719,2018-05-08T14:35:48.390,0,CC BY-SA 4.0,,1,, ,1,There seems to be a bug with the latest but now deprecated (non-AWSAmplify) version of the amazon-cognito-identity-js library where it doesn't clear the JWT tokens when you call its signout() method.  ,False,False
361,50420788,2,48770235,2018-05-19T00:47:00.157,2,"<p>If you are trying to use FreeTDS on ElasticBeanstalk, it's worth noting that the version on their yum repository is currently 0.91, which is ancient (from 2011).</p>

<p>Once I installed the latest version, all of my problems went away. I don't know if that would solve your performance issues or not, but if not, hopefully it helps someone else who comes along.</p>

<p>Here was the solution I came up with (which downloads and installs freetds directly from the source instead of the outdated yum repo):</p>

<ol>
<li>Create a file in your project called '/.ebextensions/001_install_freetds.config'</li>
<li>Paste in these contents:</li>
</ol>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""false"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>commands:
  000_download_freetds:
    command: ""[ ! -e /home/ec2-user/freetds-1.00.86.tar.gz ] &amp;&amp; wget -nc ftp://ftp.freetds.org/pub/freetds/stable/freetds-1.00.86.tar.gz -O /home/ec2-user/freetds-1.00.86.tar.gz || true""
  001_extract_freetds:
    command: ""[ ! -e /home/ec2-user/freetds-1.00.86 ] &amp;&amp; tar -xvf /home/ec2-user/freetds-1.00.86.tar.gz -C /home/ec2-user/ || true""
  002_configure_freetds:
    command: ""[ ! -e /usr/local/etc/freetds.conf ] &amp;&amp; cd /home/ec2-user/freetds-1.00.86 &amp;&amp; sudo ./configure --prefix=/usr/local --with-tdsver=7.4 || true""
  003_build_freetds_and_install:
    command: ""[ ! -e /usr/local/etc/freetds.conf ] &amp;&amp; ( cd /home/ec2-user/freetds-1.00.86 &amp;&amp; sudo make &amp;&amp; sudo make install ) || true""</code></pre>
</div>
</div>
</p>

<ol start=""3"">
<li>Redeploy your app</li>
</ol>

<p>Note: You may want to change the various file names to reflect the latest stable version. This is just what is the most recent stable release, as of when I'm writing this post.</p>
",1546785,2018-05-19T00:47:00.157,0,CC BY-SA 4.0,,1,,repo):</p,0,which downloads and installs freetds directly from the source instead of the outdated yum repo):</p,False,False
362,50470118,2,50469127,2018-05-22T14:23:44.327,3,"<p>AWS load balancers don't support ""backup"" nodes that only take traffic when the primary is down.</p>

<p>Beyond that, you are proposing a complicated scenario.</p>

<blockquote>
  <p><em>was thinking I could setup a second EC2 server with a MySQL slave</em></p>
</blockquote>

<p>If you do that, you can only fail over once, then you can't fail back, because the master database will then be obsolete.  For a configuration like this to work and be useful, your two MySQL servers need to be configured with master/master (circular) replication, so that each is a replica of the other.  This is an advanced configuration that requires expertise and caution.</p>

<p>For the MySQL component, an RDS instance with multi-AZ enabled will provide you with hands-off fault tolerance of the database.</p>

<p>Of course, the client may be unwilling to pay for this as well.</p>

<p>A reasonable shortcut for small systems might be <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html"" rel=""nofollow noreferrer"">EC2 instance recovery</a> which will bring the site back up if the underlying hardware fails.  This feature replaces a failed instance with a new instance, reattaches the EBS volumes, and starts it back up.  If the system is stable and you have a solid backup strategy for all data, this might be sufficient.  Effective redundancy as a retrofit is non-trivial.</p>
",1695906,2018-05-22T14:23:44.327,5,CC BY-SA 4.0,,1,you, ,1,">If you do that, you can only fail over once, then you can't fail back, because the master database will then be obsolete.  ",False,True
363,50475782,2,50474710,2018-05-22T20:13:41.187,0,"<p>It looks pretty simple with <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html#changeObjectStorageClass-java.lang.String-java.lang.String-com.amazonaws.services.s3.model.StorageClass-"" rel=""nofollow noreferrer"">changeObjectStorageClass</a>:</p>

<pre><code>AmazonS3Client s3Client = (AmazonS3Client)AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();

PutObjectRequest request = new PutObjectRequest(bucketName,
                                                fileObjKeyName, new File(fileName));
ObjectMetadata metadata = new ObjectMetadata();
metadata.setContentType(""binary/octet-stream"");
request.setMetadata(metadata);
s3Client.putObject(request);

s3Client.changeObjectStorageClass(bucketName, fileObjKeyName,
                                  StorageClass.ReducedRedundancy );
</code></pre>

<p>The only strange part is that you need to use the <code>changeObjectStorageClass</code> on an instance of <code>AmazonS3Client</code> - the version on the interface <code>AmazonS3</code> is deprecated.</p>
",2933977,2018-05-22T20:13:41.187,0,CC BY-SA 4.0,,1,,deprecated.</p,0,> is deprecated.</p,False,False
364,50518140,2,50516212,2018-05-24T21:04:03.440,0,"<p><strong>My Reality</strong></p>

<p>I have been developing an ORM for some time now, and there's a lot of the JDBC API offers to you in terms of metadata information about the database. It actually offers you the list of functions for numeric, varchar and dates values.</p>

<p>Unfortunately, the JDBC driver (and I imagine other drivers too) most of the time do not implement such part of the specification, and the call ends up returning an empty list.</p>

<p>In other cases, it actually returns a list of functions but it's blatantly outdated. Maybe the method was implemented 10 years ago, and no one updated it in any recent version of the driver or the database.</p>

<p>I would advise against relying on any database metadata that goes beyond the minimal set of operations. Yes, unfortunately that is my experience with 10 main stream databases so far, and I guess minor databases may be worse in this respect.</p>

<p><strong>A Workaround</strong></p>

<p>Now, if you really need to query that information at runtime, you can still assemble a SQL statement that uses the <em>candidate</em> function, run it, and see if it runs or crashes. Not the ideal option, but I guess this will clearly tell you if it's implemented or not. Running it for hundreds of functions should take no time.</p>
",6436191,2018-05-24T21:04:03.440,1,CC BY-SA 4.0,,1,it,.,0,it's blatantly outdated.,False,False
365,50857199,2,50856581,2018-06-14T12:07:20.170,2,"<p>You will need to check at the error you get while trying to execute that statement. Let's see the problems and possible problems:</p>

<h2>Rights</h2>

<p>You will need to make sure the MySQL user you try to execute the query with has the necessary rights to do so. Try to hard-code an <code>insert</code> statement along with all parameters. Are you able to do so? Or do you get an error that you do not have the rights to do so?</p>

<h2>Deprecation</h2>

<p><a href=""http://php.net/manual/ro/function.mysql-query.php"" rel=""nofollow noreferrer"">mysql_<em></a> functions are deprecated. You will need to use either <a href=""http://php.net/manual/ro/mysqli.query.php"" rel=""nofollow noreferrer"">mysqli_</em></a> functions or <a href=""http://php.net/manual/ro/book.pdo.php"" rel=""nofollow noreferrer"">PDO</a>.</p>

<h2>SQL Injection</h2>

<p>Your code has high risks of security due to possibility of <a href=""http://www.tizag.com/mysqlTutorial/mysql-php-sql-injection.php"" rel=""nofollow noreferrer"">SQL Injection</a>. You will need to escape your query via <a href=""http://php.net/manual/ro/mysqli.real-escape-string.php"" rel=""nofollow noreferrer"">mysqli_real_escape_string</a> or parameterize your query via PDO. If you do not do so, users will be able to damage your database if they want to hack your site, or even steal data.</p>

<h2>XSS Injection</h2>

<p>Your code has high risks of security due to possibility of <a href=""https://www.owasp.org/index.php/Cross-site_Scripting_(XSS)"" rel=""nofollow noreferrer"">XSS injection</a> as well. You will need to make sure no scripts will be injected into your fields unless you explicitly want to allow that. XSS injection is a possible means to steal data from other users.</p>

<h2>Is it message or messages</h2>

<p>Check what is inside your <code>$_POST[""messages""]</code>. Is it an array? If so, you try to use an array as a string and hence you get an exception.</p>

<h2>Check your logs</h2>

<p>You will need to check the server logs to find the exact problem you face. If server logging is not enabled, then you will need to enable it and run the PHP code again.</p>
",436560,2018-06-14T12:07:20.170,0,CC BY-SA 4.0,,1,functions,.,0,> functions are deprecated.,False,False
366,50926351,2,35753573,2018-06-19T10:37:13.067,1,"<p><a href=""https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html"" rel=""nofollow noreferrer"">AWS Lambda Best Practices notes</a> talks about DI:</p>

<blockquote>
  <p><strong>Minimize the complexity of your dependencies.</strong> Prefer simpler
  frameworks that load quickly on <a href=""https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html"" rel=""nofollow noreferrer"">Execution Context</a> startup. For
  example, prefer simpler Java dependency injection (IoC) frameworks
  like <a href=""http://square.github.io/dagger/"" rel=""nofollow noreferrer"">Dagger</a> or <a href=""https://github.com/google/guice"" rel=""nofollow noreferrer"">Guice</a>, over more complex ones like <a href=""https://github.com/spring-projects/spring-framework"" rel=""nofollow noreferrer"">Spring Framework</a>.</p>
</blockquote>

<p>So I'd like to suggest you to use <a href=""https://github.com/google/dagger"" rel=""nofollow noreferrer"">Dagger 2</a> (because Square's Dagger 1.x is already deprecated). It provides such benefits:</p>

<ul>
<li>light-weight framework with very few integrations, Java interface/annotation configuration and compile-time code generated bindings;</li>
<li>very small size;</li>
<li>fail as early as possible ( compile-time, not runtime);</li>
<li>performance - as fast as hand-written code and no coding around the framework.</li>
</ul>
",1145792,2018-06-19T10:37:13.067,0,CC BY-SA 4.0,,1,Dagger,.,0,(because Square's Dagger 1.x is already deprecated).,False,False
367,50964107,2,50583520,2018-06-21T08:41:57.087,0,"<p>The problem here is that some dependency pulled in <code>akka-actor_2.11</code> version <code>2.2.20</code> which is very old and does not have the method mentioned in the exception.</p>

<p>Take a look at the output of <code>mvn dependency:analyze</code> and <code>mvn dependency:tree</code> and see which dependency brings in outdated Akka. Then update that dependency or force the usage of the latest Akka in your project by adding</p>

<pre><code>&lt;dependencyManagement&gt;
  &lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt;
        &lt;artifactId&gt;akka-actor_2.11&lt;/artifactId&gt;
        &lt;version&gt;2.5.13&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;
</code></pre>

<p>to your POM.</p>
",77102,2018-06-21T08:41:57.087,0,CC BY-SA 4.0,,1,,.,0,and see which dependency brings in outdated Akka.,False,False
368,51053361,2,50887599,2018-06-27T01:32:21.867,0,"<p>Use client builder instead of directly using client constructors since it has been deprecated by the latest SDK version.</p>

<pre><code>AmazonSNS sns = AmazonSNSClient.builder()
                    .withRegion(""&lt;your region&gt;"")
                    .withClientConfiguration(&lt;configuration&gt;)
                    .withCredentials(&lt;your credentials&gt;)
                    .build();
</code></pre>
",6284370,2018-06-27T01:32:21.867,0,CC BY-SA 4.0,,1,,version.</p,0,<p>Use client builder instead of directly using client constructors since it has been deprecated by the latest SDK version.</p,False,False
369,51272109,2,36604024,2018-07-10T18:41:40.450,0,"<p>The AWSSecurityTokenServiceClient is deprecated.  The following code also works.</p>

<pre><code>BasicAWSCredentials theAWSCredentials= new BasicAWSCredentials("""","""");
AWSCredentialsProvider theAWSCredentialsProvider = new AWSStaticCredentialsProvider(theAWSCredentials);
AWSSecurityTokenService theSecurityTokenService = AWSSecurityTokenServiceClientBuilder.standard().withCredentials(theAWSCredentialsProvider).build();
</code></pre>
",10060983,2018-07-10T18:41:40.450,2,CC BY-SA 4.0,,1,AWSSecurityTokenServiceClient, ,0,The AWSSecurityTokenServiceClient is deprecated.  ,False,False
370,51432753,2,28534695,2018-07-19T22:52:53.997,12,"<p>The accepted answer is out of date. It is now possible to create stacks across accounts and regions using <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started.html"" rel=""noreferrer"">CloudFormation StackSets</a>. </p>
",7020467,2018-07-19T22:52:53.997,1,CC BY-SA 4.0,,1,answer,.,0,The accepted answer is outdated.,False,False
371,5265964,2,4498192,2011-03-10T21:11:42.767,5,"<p>The ""query"" interface was the original search interface for SimpleDB.  It was set-based, non-standard and quite lovely, I thought.  However, over time AWS introduced a SQL-like query language (accessed via the Select request) and then deprecated and eventually removed the original query interface.</p>

<p>So, the reason it doesn't work in boto is because it is no longer supported by SimpleDB.  For more up-to-date boto documentation, look <a href=""http://boto.cloudhackers.com/"" rel=""noreferrer"">here</a>.</p>
",653643,2011-03-10T21:11:42.767,0,CC BY-SA 2.5,,1,,query,0,"However, over time AWS introduced a SQL-like query language (accessed via the Select request) and then deprecated and eventually removed the original query",False,False
372,5820746,2,5820120,2011-04-28T15:05:14.377,0,"<p>Man, that sounds incredibly frustrating. Looks like you're doing all of the things I would recommend, and trying lots of variations/tests. I've got one idea, and two maybe not so good ideas:</p>

<p>1) I ran into encoding issues when I used Miro, and ended up having files that would play fine locally... but ended up behaving differently when I viewed them online. To ensure that it's not an encoding issue, try subbing in an existing .ogv file that you know works - possibly from some other site/tutorial.</p>

<p>Standby reference: <a href=""http://www.bigbuckbunny.org/index.php/download/"" rel=""nofollow"">Big Buck Bunny</a></p>

<p>2) This is a longshot, and it's me showing how little I know about these formats... but maybe try using .ogg as an extension, instead of .ogv? I looked and see that .ogv is meant for video, with .ogg being deprecated... but changing the file extension might reveal something about your encoding process.</p>

<p>3) Another longshot: kill the codecs attribute for your mp4 source.</p>
",671759,2011-04-28T15:05:14.377,0,CC BY-SA 3.0,,1,,process.</p,0,being deprecated... but changing the file extension might reveal something about your encoding process.</p,False,False
373,5935495,2,4811761,2011-05-09T10:21:15.320,2,"<p>Just for completeness, I ended up using the SDK provided by Amazon for Java. It can be found here:</p>

<p><a href=""http://aws.amazon.com/sdkforjava/"" rel=""nofollow"">http://aws.amazon.com/sdkforjava/</a></p>

<p>All other client libraries were outdated or had missing functionality.</p>
",292145,2011-05-09T10:21:15.320,0,CC BY-SA 3.0,,1,libraries,functionality.</p,0,>All other client libraries were outdated or had missing functionality.</p,False,False
374,7129403,2,4521846,2011-08-20T03:21:33.600,1,"<p>As msha points out, the sending of a workerID parameter to an ExternalQuestion page seems to be deprecated, or at least taken out of the latest version of the documentation.</p>

<p>However, a fellow researcher who's been using MTurk a lot says: ""People seem to be using it in the forums. I would go ahead with it...if it ever actually disappears, I'm sure that the developer community will yell very loudly. :) ""</p>

<p>I tried it empirically today (2011-08-19), and indeed a workerID is being sent to the ExternalQuestion page I set up on my own server, after the HIT has been accepted. This was in the sandbox. My ExternalQuestion page contained a Java Web Start button (as described here: <a href=""http://download.oracle.com/javase/tutorial/deployment/deploymentInDepth/createWebStartLaunchButtonFunction.html"" rel=""nofollow"">http://download.oracle.com/javase/tutorial/deployment/deploymentInDepth/createWebStartLaunchButtonFunction.html</a> ); I don't know if that made any difference.</p>
",772978,2011-08-20T03:21:33.600,1,CC BY-SA 3.0,,1,,documentation.</p,0,"As msha points out, the sending of a workerID parameter to an ExternalQuestion page seems to be deprecated, or at least taken out of the latest version of the documentation.</p",False,False
375,7322249,2,7259601,2011-09-06T15:20:43.983,2,"<p>We found the issue. We noticed that some instances in our cluster always produced the problem and some never did. Apparently the issue is unique to instances that run more recent CPU versions combined with slightly outdated kernels.</p>

<p>The issue is explained in full here : <a href=""https://bugs.launchpad.net/ubuntu/+source/linux/+bug/727459"" rel=""nofollow noreferrer"">https://bugs.launchpad.net/ubuntu/+source/linux/+bug/727459</a></p>

<p>As the running time increases the spikes shown in the graph below become longer and longer, possible due to time shift and at some point it'll spin the one core for a long time.</p>

<p>Cpu usage graphs. Affected instance versus unaffected:</p>

<p><img src=""https://i.stack.imgur.com/sM8r6.png"" alt=""Cpu usage graphs. Affected instance versus unaffected""></p>

<p>The issue has been fixed recently so a kernel update fixed this issue for us.</p>
",813957,2011-09-06T15:20:43.983,1,CC BY-SA 3.0,,1,,kernels.</p,0,Apparently the issue is unique to instances that run more recent CPU versions combined with slightly outdated kernels.</p,False,False
376,7543698,2,6645026,2011-09-25T04:40:23.570,1,"<p>i think you are facing this problem because you are using RAILS_ROOT which has been deprecated in Rails 3. try using Rails.root instead and see if you face the same problem. Please check the correct usage for Rails.root before making the correction. </p>
",963290,2011-09-25T04:40:23.570,0,CC BY-SA 3.0,,1,,.,0,i think you are facing this problem because you are using RAILS_ROOT which has been deprecated in Rails 3.,False,False
377,7642625,2,7638646,2011-10-04T02:11:20.433,0,"<p>As described in the question both Azure and EC2 will do the job very well. This is the kind of task both systems are designed for.</p>

<p>So the question becomes really: which is <em>best</em>? That depends on two things: what the application needs to do and your own experience and preference.</p>

<p>As it's a Windows application there should probably be a leaning towards Azure. While EC2 supports Windows, the tooling and support resources for Azure are probably deeper at this point. </p>

<p>If cost is a factor then a (somewhat outdated) resource is here: <a href=""http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost-comparison/"" rel=""nofollow"">http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost-comparison/</a> -- the conclusion is that, by and large, Azure and Amazon are roughly similar for compute charges.</p>
",3546,2011-10-04T02:11:20.433,0,CC BY-SA 3.0,,1,,cost,0,">If cost is a factor then a (somewhat outdated) resource is here: <a href=""http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost-comparison/"" rel=""nofollow"">http://blog.mccrory.me/2010/10/30/public-cloud-hourly-cost",False,True
378,7651730,2,6675018,2011-10-04T17:25:34.107,4,"<p>What you really want is the equivalent of the <code>OpenListings</code> report from the <code>Product Advertising API</code> in MWS and that is the <code>RequestReport</code> call with a report type of <code>_GET_MERCHANT_LISTINGS_DATA_</code>. This returns you all the inventory a seller has listed on Amazon and from here to getting your ASIN from that list it's close.</p>

<p>You can find out more details in their <a href=""https://developer.amazonservices.com/gp/mws/docs.html"" rel=""nofollow"">documentation</a></p>

<p>Also, I advise you to not use the Product Advertising API anymore as Amazon deprecated it and it will be out of use this time next year.</p>
",49032,2011-10-04T17:25:34.107,0,CC BY-SA 3.0,,1,,next,1,"Also, I advise you to not use the Product Advertising API anymore as Amazon deprecated it and it will be out of use this time next",False,False
379,7718816,2,7704946,2011-10-10T21:19:34.097,4,"<p>Changing URLs is a safe way to invalidate outdated assets.</p>

<p>It is also a necessity if you want to allow users storing private images. Using a path deductible from the users account name/id/path would render privacy settings useless as soon as you store assets on a CDN.</p>
",589017,2011-10-10T21:19:34.097,0,CC BY-SA 3.0,,1,,outdated,0,<p>Changing URLs is a safe way to invalidate outdated,False,False
380,8106724,2,8106239,2011-11-12T18:51:24.333,2,"<p>There are lots of guides out there, some out of date, and some recent ones, below are some links. You might also be interested in <a href=""http://aws.amazon.com/elasticbeanstalk/"" rel=""nofollow"">Amazon Elastic Beanstalk</a>. Here are some links</p>

<ul>
<li><a href=""http://blog.peterdelahunty.com/2009/06/deploying-grails-app-on-ec2-in-from.html"" rel=""nofollow"">Deploying Grails App on Amazon EC2 form scratch</a></li>
<li><a href=""http://grails.1312388.n4.nabble.com/Amazon-EC2-td3055649.html"" rel=""nofollow"">Question about EC2 and Grails on the mailing list</a></li>
</ul>

<p>To answer the question about the credit card, I think you can't get away without one, since if you go over the free usage tier you need to have some sort of payment method to pay for the extra usage.</p>
",372561,2011-11-12T18:51:24.333,0,CC BY-SA 3.0,,1,,.,0,"There are lots of guides out there, some outdated, and some recent ones, below are some links.",False,False
381,8125958,2,8056962,2011-11-14T18:00:40.387,1,"<p>It used to be be possible by doing a brute force search with the Seller* API calls. However these calls have been deprecated since Nov 1 2011, so you're out of luck. If you happen to be working for this particular seller (instead of being simply a customer or a competitor), you'll want to use the APIs available to the seller (MWS) to download inventory reports.</p>
",50812,2011-11-14T18:00:40.387,0,CC BY-SA 3.0,,1,calls,.,0,"However these calls have been deprecated since Nov 1 2011, so you're out of luck.",False,False
382,8127487,2,7800800,2011-11-14T20:15:19.397,1,"<p>I'm working on the same issue and I know how to execute the GA code you want to execute, especially since you are working with the older version of the code. I am working on the same issue with an Amazon webstore -- if you are able to upload a js file, and it looks like you are able to do this in files, I would suggest using the init() function which is deprecated but it still works - here is an example where I am doing something similar. I am setting the contents of a custom variable and creating the tracking object -- but I'm not firing the page tracker until later in the code.</p>

<p>Using the code below, replace the Custom Variable setting code with the linker and set Domain code you need for cross domain tracking. Since the GA code added by amazon doesn't try to set the domain the linker functions should prevent a second cookie from being created, and use the cookie data passed in the URL.</p>

<pre><code>&lt;script type=""text/javascript""&gt; 
var gaJsHost = ((""https:"" ==     document.location.protocol) ? ""https://ssl."" : ""http://www."");
document.write(unescape(""%3Cscript src='"" + gaJsHost + ""google-analytics.com/ga.js'     type='text/javascript'%3E%3C/script%3E""));
&lt;/script&gt;
&lt;script type=""text/javascript""&gt;
try {
var pageTracker = _gat._getTracker(""UA-xxxxxxx-x""); 
pageTracker._initData();
pageTracker._setCustomVar(1, ""GWO"", utmx('combination').toString(), 1);} 
catch(err) {}&lt;/script&gt;
</code></pre>
",1046301,2011-11-14T20:15:19.397,0,CC BY-SA 3.0,,1,,.,0,"I am working on the same issue with an Amazon webstore -- if you are able to upload a js file, and it looks like you are able to do this in files, I would suggest using the init() function which is deprecated but it still works - here is an example where I am doing something similar.",False,False
383,8173888,2,8173561,2011-11-17T20:33:04.200,6,"<p>Zach, the simple answer is that there's not a simple path to there from here :)</p>

<p>When I wrote Segue I hoped that someone would soon come out with something that would make Segue obsolete. Cloudnumbers may be it one day, but probably not yet. I have toyed with making Segue a foreach backend, but since I don't use it that way, my motivation has been pretty low to take the time to learn how to build the backend. </p>

<p>One of the things that is very promising, in my opinion, is using the <code>doRedis()</code> package with workers on Amazon EC2. doRedis uses a Redis server as the job controller and then lets workers connect to the Redis server and get/return jobs and results. I've been thinking for a while that it would be nice to have a dead simple way to deploy a doRedis cluster on EC2. But nobody has written one yet that I know of. </p>
",37751,2011-11-17T20:33:04.200,11,CC BY-SA 3.0,,1,,.,0,>When I wrote Segue I hoped that someone would soon come out with something that would make Segue obsolete.,False,False
384,8927333,2,8407750,2012-01-19T14:01:34.183,2,"<p>Apparently I was looking at some badly out of date documentation.</p>

<p>in <code>AmazonS3Client</code> see:</p>

<pre><code>- (S3MultipartUpload * AmazonS3Client)initiateMultipartUploadWithKey:(NSString *)theKey withBucket:(NSString *)theBucket    
</code></pre>

<p>Which will give you a <code>S3MultipartUpload</code> which will contain an <strong>uploadId</strong>.</p>

<p>You can then put together an S3UploadPartRequest using <code>initWithMultipartUpload:        (S3MultipartUpload *)   multipartUpload</code> and send that as you usually would.</p>

<p>S3UploadPartRequest contains an int property partNumber where you can specify the part # you're uploading.</p>
",459082,2012-01-19T14:01:34.183,0,CC BY-SA 3.0,,1,,documentation.</p,0,Apparently I was looking at some badly outdated documentation.</p,False,False
385,9064647,2,5302394,2012-01-30T13:31:21.533,1,"<p>Well, as stated by Geoff Appleford, Elasticfox is an outdated extension. I dont know why it's development stopped as it is a great tool.</p>

<p>If you wish to do things that are achieved through Elasticfox, I would suggest you to take a look at Hybridfox. Hybrdifox is a fork of Elasticfox which supports other private clouds as well. It is also a great tool too.</p>

<p>It is also open source and actively updated. Check it's project home(http://code.google.com/p/hybridfox/)</p>

<p>Sources:
<a href=""http://code.google.com/p/hybridfox/"" rel=""nofollow"">Hybridfox home</a></p>
",850018,2012-01-30T13:31:21.533,0,CC BY-SA 3.0,,1,,.,0,"Well, as stated by Geoff Appleford, Elasticfox is an outdated extension.",False,False
386,9179801,2,7487292,2012-02-07T16:20:03.357,4,"<p>Note that according to Amazon, at <a href=""http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/FileSystemConfig.html"" rel=""nofollow"">http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/FileSystemConfig.html</a> ""Amazon Elastic MapReduce - File System Configuration"", the S3 Block FileSystem is deprecated and its URI prefix is now s3bfs:// and they specifically discourage using it since ""it can trigger a race condition that might cause your job flow to fail"".</p>

<p>According to the same page, HDFS is now 'first-class' file system under S3 although it is ephemeral (goes away when the Hadoop jobs ends).</p>
",98694,2012-02-07T16:20:03.357,0,CC BY-SA 3.0,,1,,"fail"".</p",0,"/DeveloperGuide/FileSystemConfig.html</a> ""Amazon Elastic MapReduce - File System Configuration"", the S3 Block FileSystem is deprecated and its URI prefix is now s3bfs:// and they specifically discourage using it since ""it can trigger a race condition that might cause your job flow to fail"".</p",False,False
387,9440295,2,9280539,2012-02-25T01:30:14.873,0,"<p>I'd like to note that AWS is deprecated when it comes to the store, you should be using MWS and here is the docs for the correct part of MWS you need V138592989.pdf"">https://images-na.ssl-images-amazon.com/images/G/01/mwsportal/doc/en_US/products/MWSProductsApiReference.<em>V138592989</em>.pdf and here is the link [hard to find] to the main MWS area. <a href=""https://developer.amazonservices.com/gp/mws/api.html/182-3268148-9799929?ie=UTF8&amp;section=products&amp;group=products&amp;version=latest"" rel=""nofollow"">https://developer.amazonservices.com/gp/mws/api.html/182-3268148-9799929?ie=UTF8&amp;section=products&amp;group=products&amp;version=latest</a></p>

<p>Keep in mind that without an active pro seller account you will not be able to get a developer key if you are trying to build apps that integrate with private parts of the amazon API. I just went through this and had to grab the free trial long enough to get a key, which still works after you cancel the trial. </p>

<p>Also, you can not receive any support from MWS without that pro seller account! I spent over an hour on the phone with my AWS rep and he didn't even know how to access the MWS site. He had to call them only to be told [by them] that they didn't know how to access their API either, this was all too new for them.</p>

<p>I did eventually get everything working for me after scouring through their docs and downloading the correct API for MWS.</p>

<p>Good Luck!</p>
",526849,2012-02-25T01:30:14.873,0,CC BY-SA 3.0,,1,,.,0,">I'd like to note that AWS is deprecated when it comes to the store, you should be using MWS and here is the docs for the correct part of MWS you need V138592989.pdf"">https://images-na.ssl-images-amazon.com/images/G/01/mwsportal/doc/en_US/products/MWSProductsApiReference.<em>V138592989</em>.pdf and here is the link [hard to find] to the main MWS area.",False,False
388,9489329,2,9489036,2012-02-28T20:38:04.773,0,"<p>Although it may be a bit outdated, <a href=""http://freemarker.sourceforge.net/"" rel=""nofollow"">FreeMarker</a> is primarily a framework for rendering template files given some set of inputs.</p>
",12604,2012-02-28T20:38:04.773,6,CC BY-SA 3.0,,1,it,inputs.</p,0,"Although it may be a bit outdated, <a href=""http://freemarker.sourceforge.net/"" rel=""nofollow"">FreeMarker</a> is primarily a framework for rendering template files given some set of inputs.</p",False,False
389,9560483,2,9559973,2012-03-05T01:06:55.170,1,"<blockquote>
  <p>Is there a way to expand its size to the size of the volume without
  losing my work?</p>
</blockquote>

<p>That depends on whether you can live with a few minutes downtime for the computation, i.e. whether stopping the instance (hence the computation process) is a problem or not - Eric Hammond has written a detailed article about <a href=""http://alestic.com/2010/02/ec2-resize-running-ebs-root"" rel=""nofollow"">Resizing the Root Disk on a Running EBS Boot EC2 Instance</a>, which addresses a different but pretty related problem:</p>

<blockquote>
  <p>[...] what if you have an EC2 instance already running and you need to
  increase the size of its root disk without running a different
  instance?</p>
  
  <p>As long as you are ok with a little down time on the EC2 instance (few
  minutes), it is possible to change out the root EBS volume with a
  larger copy, without needing to start a new instance.</p>
</blockquote>

<p>You have already done most of the steps he describes and created a new 300GB volume from the 180GB snapshot, but apparently you have missed the last required step indeed, namely resizing the file system on the volume - here are the instructions from Eric's article:</p>

<blockquote>
  <p>Connect to the instance with ssh (not shown) and resize the root file
  system to fill the new EBS volume. This step is done automatically at
  boot time on modern Ubuntu AMIs:</p>

<pre><code># ext3 root file system (most common)
sudo resize2fs /dev/sda1
#(OR)
sudo resize2fs /dev/xvda1

# XFS root file system (less common):
sudo apt-get update &amp;&amp; sudo apt-get install -y xfsprogs
sudo xfs_growfs /
</code></pre>
</blockquote>

<p>So the details depend on the file system in use on that volume, but there should be a respective resize command available for all but the most esoteric or outdated ones, none of which I'd expect in a regular Ubuntu 10 installation.</p>

<p>Good luck!</p>

<hr>

<h3>Appendix</h3>

<blockquote>
  <p>Is there a possibility that the snapshot is actually continuous with
  another drive (e.g. /dev/sdb)?</p>
</blockquote>

<p>Not just like that, this would require a <a href=""http://en.wikipedia.org/wiki/RAID"" rel=""nofollow"">RAID</a> setup of sorts, which is unlikely to be available on a stock Ubuntu 10, except if somebody provided you with a respectively customized AMI. The size of <code>/dev/sdb</code> does actually hint towards this being your <a href=""http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/InstanceStorage.html"" rel=""nofollow"">Amazon EC2 Instance Storage</a>:</p>

<blockquote>
  <p>When an instance is created from an Amazon Machine Image (AMI), in
  most cases it comes with a preconfigured block of pre-attached disk
  storage. Within this document, it is referred to as an instance store;
  it is <strong>also known as an ephemeral store</strong>. An instance store provides
  temporary block-level storage for Amazon EC2 instances. The data on
  the instance store volumes <strong>persists only during the life of the
  associated Amazon EC2 instance</strong>. The amount of this storage ranges from
  160GiB to up to 3.3TiB and varies by Amazon EC2 instance type. [...] <em>[emphasis mine]</em></p>
</blockquote>

<p>Given this storage is not persisted on instance termination (in contrast to the <a href=""http://aws.amazon.com/ebs/"" rel=""nofollow"">EBS storage</a> we all got used to enjoy - the different behavior is detailed in <a href=""http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html"" rel=""nofollow"">Root Device Storage</a>), it should be treated with respective care (i.e. never store something on instance storage you couldn't afford to loose).</p>
",45773,2012-03-05T01:06:55.170,3,CC BY-SA 3.0,,1,,Ubuntu,0,"So the details depend on the file system in use on that volume, but there should be a respective resize command available for all but the most esoteric or outdated ones, none of which I'd expect in a regular Ubuntu",False,False
390,9638670,2,9506791,2012-03-09T17:50:46.747,1,"<p>i would say API documentation is outdated:</p>

<p><a href=""http://aws.amazon.com/archives/Product%20Advertising%20API"" rel=""nofollow"">http://aws.amazon.com/archives/Product%20Advertising%20API</a>
 - says ""Last Modified: Jul 26, 2011 2:10 AM GMT""</p>

<p>this is CURRENT version of AWSECommerceService - it's from 2011-08-01 - and it does not have tagpage element:
 <a href=""http://ecs.amazonaws.com/AWSECommerceService/2011-08-01/AWSECommerceService.wsdl"" rel=""nofollow"">http://ecs.amazonaws.com/AWSECommerceService/2011-08-01/AWSECommerceService.wsdl</a></p>

<p>and here is OLDER version of AWSECommerceService - it's from 2011-04-01 - and it does have tagpage element:
 <a href=""http://ecs.amazonaws.com/AWSECommerceService/2011-04-01/AWSECommerceService.wsdl"" rel=""nofollow"">http://ecs.amazonaws.com/AWSECommerceService/2011-04-01/AWSECommerceService.wsdl</a></p>
",1246870,2012-03-09T17:50:46.747,2,CC BY-SA 3.0,,1,,outdated:</p,0,i would say API documentation is outdated:</p,False,False
391,9723442,2,9723070,2012-03-15T15:56:37.887,5,"<p>According to your logfile, your version of fog is very very old. You're using 0.3.25, and the most recent tag is at 1.1.2. Try doing this:</p>

<pre><code>bundle update fog
</code></pre>

<p>Your version of carrierwave is similarly out of date, so I'd <code>bundle update carrierwave</code> as well. That should help correct this issue.</p>
",1224374,2012-03-15T15:56:37.887,0,CC BY-SA 3.0,,1,,>,0,"Your version of carrierwave is similarly outdated, so I'd <code>",False,False
392,9810696,2,9597500,2012-03-21T18:30:18.017,2,"<p>This has meanwhile been addressed in the AWS team response to the identical question asked in the AWS forum, see <a href=""https://forums.aws.amazon.com/message.jspa?messageID=325839#325839"" rel=""nofollow"">EC2 reports AMI: Unavailable</a>:</p>

<blockquote>
  <p>This is an AWS owned AMI that is no longer publicly available as it is
  deprecated. This will not affect your currently running instance.
  Additionally, if you create an EBS AMI of your running instance you
  will create a point in time backup of your current configuration --
  which you can use to launch duplicate instances from. </p>
  
  <p>The current AWS provided Windows Server 2008 32bit AMI is:
  ami-541dcf3d</p>
</blockquote>
",45773,2012-03-21T18:30:18.017,0,CC BY-SA 3.0,,1,,.,1,"This is an AWS owned AMI that is no longer publicly available as it is
  deprecated.",False,False
393,9857697,2,9857086,2012-03-25T03:23:14.873,4,"<p>You might want to start the script with:</p>

<pre><code>apt-get update
</code></pre>

<p>as your apt cache might be out of date for the ""apt-get install"".</p>

<p>You can also debug the script by starting it with these two lines:</p>

<pre><code>#!/bin/bash -ex
exec &gt; &gt;(tee /var/log/rc.local.log|logger -t rc.local -s 2&gt;/dev/console) 2&gt;&amp;1
</code></pre>

<p>This will echo each command and its output to /var/log/rc.local.log so you can find out what is failing and with what error.</p>

<p>Make sure the file is executable:</p>

<pre><code>sudo chmod 755 /etc/rc.local
</code></pre>

<p>Note that rc.local is run on <em>every</em> boot, not just the first boot.  Make sure that you are ok with it being run again after the system has been up for a while.</p>

<p>Here's an article I wrote with more details about the ""exec"" command above: <a href=""http://alestic.com/2010/12/ec2-user-data-output"" rel=""nofollow"">http://alestic.com/2010/12/ec2-user-data-output</a></p>
",111286,2012-03-25T03:23:14.873,3,CC BY-SA 3.0,,1,,"

",0,">as your apt cache might be outdated for the ""apt-get install"".</p>

",False,False
394,9918301,2,3189979,2012-03-29T02:24:17.850,1,"<p>This discussion and the related Amazon post helped me get the client working. That being said, I felt that the solution could be improved with regards to the following:</p>

<ol>
<li>Setting WebService handlers in code is discouraged. A XML configuration file and a corresponding @HandlerChain annotation are recommended.</li>
<li>A SOAPHandler is not required in this case, LogicalHandler would do just fine. A SOAPHandler has more reach than a LogicalHandler and when it comes to code, more access is not always good. </li>
<li>Stuffing the signature generation, addition of a Node and printing the request in one handler seems like a little too much. These could be separated out for separation of responsibility and ease of testing. One approach would be to add the Node using a XSLT transformation so that the handler could remain oblivious of the transformation logic. Another handler could then be chained which just prints the request. 
<a href=""http://www.mediafire.com/?9rdhn1r8na19mft"" rel=""nofollow"" title=""Example"">Example</a></li>
</ol>
",839733,2012-03-29T02:24:17.850,0,CC BY-SA 3.0,,1,handlers,.,0,<li>Setting WebService handlers in code is discouraged.,False,False
395,51555601,2,51554772,2018-07-27T10:05:01.867,0,"<p>sorry i didnt launch the VNC SERVER ....
 but now the connection is ok but nothing appear on my screen :</p>

<pre><code>    Fri Jul 27 09:57:29 2018
 vncext:      VNC extension running!
 vncext:      Listening for VNC connections on local interface(s), port 5901
 vncext:      created VNC server for screen 0
/usr/bin/startxfce4: X server already running on display :1
gpg-agent[862]: WARNING: ""--write-env-file"" is an obsolete option - it has no effect
gpg-agent: a gpg-agent is already running - not starting a new one

(xfce4-session:849): xfce4-session-WARNING **: gpg-agent returned no PID in the variables

(xfce4-session:849): xfce4-session-WARNING **: xfsm_manager_load_session: Something wrong with /home/jamal/.cache/sessions/xfce4-session-kali:1, Does it exist? Permissions issue?

(xfsettingsd:878): xfsettingsd-WARNING **: Failed to get the _NET_NUMBER_OF_DESKTOPS property.

(xfwm4:866): xfwm4-WARNING **: Error opening /dev/dri/card0: Permission denied

** (light-locker:875): ERROR **: Environment variable XDG_SESSION_PATH not set. Is LightDM running?

Fri Jul 27 09:59:01 2018
 Connections: accepted: [::1]::41716
 SConnection: Client needs protocol version 3.8
 SConnection: Client requests security type VncAuth(2)

Fri Jul 27 09:59:05 2018
 VNCSConnST:  Server default pixel format depth 24 (32bpp) little-endian rgb888
 VNCSConnST:  Client pixel format depth 6 (8bpp) rgb222
 VNCSConnST:  Client pixel format depth 24 (32bpp) little-endian rgb888
</code></pre>

<p>above the vnc log.</p>

<p>my user has all admin privilege in sudoers files ( ALL:ALL:ALL ).</p>
",9988527,2018-07-27T10:05:01.867,0,CC BY-SA 4.0,,1,,:,1,": WARNING: ""--write-env-file"" is an obsolete option - it has no effect
gpg-agent: a gpg-agent is already running - not starting a new one

(xfce4-session:849):",False,False
396,51806562,2,51801971,2018-08-12T07:08:29.840,0,"<p>I solved the problem by doing the following:</p>

<ul>
<li>using a p2.xlarge EC2 instance</li>
<li>selecting <a href=""https://aws.amazon.com/marketplace/pp/B077GCH38C"" rel=""nofollow noreferrer"">Deep Learning AMI (Ubuntu) v12.0</a> as starting AMI</li>
<li>use <code>conda env list</code> to see the list of available environments and then activating the one I needed with <code>source activate tensorflow_p36</code></li>
</ul>

<p>This last point was probably the thing I never realized to do in my previous tests.</p>

<p>After that, everything was working as expected</p>

<pre><code>&gt;&gt;&gt; from keras import backend as K
/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
&gt;&gt;&gt; K.tensorflow_backend._get_available_gpus()
['/job:localhost/replica:0/task:0/device:GPU:0']
</code></pre>

<p>Also, running <code>nvidia-smi</code> was showing the use of gpu resources during model training, the same with <code>nvidia-smi -i 0 -q -d MEMORY,UTILIZATION,POWER</code>.</p>

<p>In my sample case, training a single epoch went from 42s to 13s.</p>
",2702370,2018-08-12T07:08:29.840,0,CC BY-SA 4.0,,1,Conversion,.,0,Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.,False,False
397,51834988,2,51828454,2018-08-14T06:29:28.603,0,"<p>If you don't mind the data being slightly out of date, you could use <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html"" rel=""nofollow noreferrer"">Amazon S3 Inventory</a>, which can provide a daily CSV file listing all of your objects in the Amazon S3 bucket:</p>

<blockquote>
  <p>Amazon S3 inventory provides comma-separated values (CSV) or Apache optimized row columnar (ORC) output files that list your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or a shared prefix (that is, objects that have names that begin with a common string).</p>
</blockquote>

<p>You could parse this file to obtain Keys and Last Modified dates, then sort by date.</p>
",174777,2018-08-14T06:29:28.603,0,CC BY-SA 4.0,,1,,/,1,">If you don't mind the data being slightly outdated, you could use <a href=""https://docs.aws.amazon.com/AmazonS3/",False,True
398,51892437,2,15050146,2018-08-17T09:33:24.090,1,"<p>If you want to use Java program to do it you can do:</p>

<pre><code>public  void uploadFolder(String bucket, String path, boolean includeSubDirectories) {
    File dir = new File(path);
    MultipleFileUpload upload = transferManager.uploadDirectory(bucket, """", dir, includeSubDirectories);
    try {
        upload.waitForCompletion();
    } catch (InterruptedException e) {
        e.printStackTrace();
    }
}
</code></pre>

<hr>

<p>Creation of s3client and transfer manager to connect to local S3 if you wish to test is as below:</p>

<pre><code>    AWSCredentials credentials = new BasicAWSCredentials(accessKey, token);
    s3Client = new AmazonS3Client(credentials); // This is deprecated but you can create using standard beans provided by spring/aws
    s3Client.setEndpoint(""http://127.0.0.1:9000"");//If you wish to connect to local S3 using minio etc...
    TransferManager transferManager = TransferManagerBuilder.standard().withS3Client(s3Client).build();
</code></pre>
",2207562,2018-08-17T09:33:24.090,0,CC BY-SA 4.0,,1,This,"s3Client.setEndpoint(""http://127.0.0.1:9000"");//If",0,"This is deprecated but you can create using standard beans provided by spring/aws
    s3Client.setEndpoint(""http://127.0.0.1:9000"");//If",False,False
399,51944162,2,51551067,2018-08-21T07:59:05.643,1,"<p>Don't try to install <code>google-cloud</code>, that package <a href=""https://pypi.org/project/google-cloud/"" rel=""nofollow noreferrer"">is deprecated</a>. </p>

<p>As you have tagged this as Storage also, I assume you intend to use GCS from Python: rather install <a href=""https://pypi.org/project/google-cloud-storage/"" rel=""nofollow noreferrer""><code>google-cloud-storage</code></a> with <code>pip install --upgrade google-cloud-storage</code>. It is also recommended to use <a href=""https://cloud.google.com/python/setup#linux"" rel=""nofollow noreferrer"">virtualenv</a>.</p>
",3058302,2018-08-21T07:59:05.643,0,CC BY-SA 4.0,,1,,deprecated</a,0,"noreferrer"">is deprecated</a",False,False
400,52047518,2,17713387,2018-08-27T21:59:55.700,2,"<p>I had this issue too. For me, it was because I had outdated credentials in my ~/.aws/config file. Fixing that solved the problem.</p>
",583366,2018-08-27T21:59:55.700,1,CC BY-SA 4.0,,1,,.,0,"For me, it was because I had outdated credentials in my ~/.aws/config file.",False,False
401,52061386,2,41796355,2018-08-28T15:21:50.337,19,"<p>The top voted answer by Ryan put me on the right track, but since AmazonS3Client is now deprecated, this code has resolved the problem for me</p>

<pre><code>    AmazonS3 s3 = AmazonS3ClientBuilder.standard()
                  .withCredentials(DefaultAWSCredentialsProviderChain.getInstance())
                  .build();
</code></pre>

<p>This code appears to correctly pick up the active IAM role, say in Lambda.</p>
",10285578,2018-08-28T15:21:50.337,0,CC BY-SA 4.0,,1,,me</p,0,"The top voted answer by Ryan put me on the right track, but since AmazonS3Client is now deprecated, this code has resolved the problem for me</p",False,False
402,52064938,2,52052946,2018-08-28T19:23:10.470,1,"<p>Your C++ compiler crashed. Compare the compiler version on your working system with your failing system. Your solution might be an out of date compiler / tool chain.</p>
",8016720,2018-08-28T19:23:10.470,0,CC BY-SA 4.0,,1,,chain.</p,0,Your solution might be an outdated compiler / tool chain.</p,False,False
403,52146845,2,45468799,2018-09-03T09:27:08.253,1,"<p>The answer above is either wrong or outdated. I'm not sure. You can increase how long your credentials last. They can last up to 12 hours. The maximum is not 1 hour. Go to IAM > Roles > Your specified role > Edit the session duration.</p>

<p><a href=""https://i.stack.imgur.com/hItd5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hItd5.png"" alt=""enter image description here""></a></p>

<p>As per the <a href=""https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html"" rel=""nofollow noreferrer"">IAM documentation</a>, the maximum is defined by the Maximum CLI/API session duration, the max of which is 12 hours.</p>

<p>Editing might fix your issue, considering your operation may take more than 1 hour and less than 12. If more than 12, consider editing your script to refresh credentials. To be honest, I'm not sure how to do this or if it is possible, but this <a href=""https://stackoverflow.com/questions/36894947/boto3-uses-old-credentials"">SO answer</a> might help, as well as the <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html"" rel=""nofollow noreferrer"">docs</a>.</p>
",8808193,2018-09-03T09:27:08.253,0,CC BY-SA 4.0,,1,,.,0,The answer above is either wrong or outdated.,False,False
404,52340887,2,45311259,2018-09-15T01:30:39.677,0,"<p>It's not possible. The closest you can get there is by using the built-in slot type <a href=""https://developer.amazon.com/docs/custom-skills/literal-slot-type-reference.html"" rel=""nofollow noreferrer"">AMAZON.Literal</a> (US only and deprecated) or <a href=""https://developer.amazon.com/docs/custom-skills/slot-type-reference.html#amazonsearchquery"" rel=""nofollow noreferrer"">AMAZON.SearchQuery</a> (available in all locales). I say ""closest"" because SearchQuery for example requires a carrier phrase in the utterance besides the slot (it can't be there alone capturing everything).</p>

<p>Note that the free form capture provided by these types is less accurate than if you define a custom slot type (i.e. you know more or less what you want to capture)</p>
",267265,2018-09-15T01:30:39.677,0,CC BY-SA 4.0,,1,,reference.html#amazonsearchquery,0,"The closest you can get there is by using the built-in slot type <a href=""https://developer.amazon.com/docs/custom-skills/literal-slot-type-reference.html"" rel=""nofollow noreferrer"">AMAZON.Literal</a> (US only and deprecated) or <a href=""https://developer.amazon.com/docs/custom-skills/slot-type-reference.html#amazonsearchquery",False,False
405,52376521,2,52370736,2018-09-17T22:33:50.727,1,"<p>So a couple of things:</p>

<ol>
<li>OpsWorks Stacks is dangerously out of date and using it should be considered highly suspect.</li>
<li>I don't actually recognize that <code>define</code> block thing in there, maybe that's an older OpsWorks syntax?</li>
<li>You can definitely run an Ansible playbook from Chef code, but I would probably go a little simpler than you have there. Probably just run <code>ansible-playbook</code> locally and aim it at localhost.</li>
</ol>
",78722,2018-09-17T22:33:50.727,3,CC BY-SA 4.0,,1,Stacks,suspect.</li,0,OpsWorks Stacks is dangerously outdated and using it should be considered highly suspect.</li,False,False
406,52462657,2,52462585,2018-09-23T03:16:29.210,0,"<p>DynamoDB Streams enables a read-only view of the actions that have been performed against DynamoDB tables. Think of streams as analogous to a Kinesis stream that provides an audit of all writes to the tables. Streams does not provide the ability to enforce atomicity on writes to tables, but could be used for monitoring after the fact that the sources are out of sync. The best I can see for transactional storage within DynamoDB is in the <a href=""https://aws.amazon.com/blogs/developer/performing-conditional-writes-using-the-amazon-dynamodb-transaction-library/"" rel=""nofollow noreferrer"">AWS Developer Blog</a>, but it is from 2014 so it may be outdated.</p>

<p><a href=""https://aws.amazon.com/about-aws/whats-new/2014/11/10/introducing-dynamodb-streams/"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aws/whats-new/2014/11/10/introducing-dynamodb-streams/</a>
<a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>
",1546662,2018-09-23T03:16:29.210,0,CC BY-SA 4.0,,1,,outdated.</p,0,"The best I can see for transactional storage within DynamoDB is in the <a href=""https://aws.amazon.com/blogs/developer/performing-conditional-writes-using-the-amazon-dynamodb-transaction-library/"" rel=""nofollow noreferrer"">AWS Developer Blog</a>, but it is from 2014 so it may be outdated.</p",False,False
407,52482519,2,46454876,2018-09-24T15:20:03.400,-1,"<p>Check out <a href=""https://github.com/EddyVerbruggen/nativescript-nodeify"" rel=""nofollow noreferrer"">nativescript-nodeify</a> for instructions. The cognito-specific stuff is at the very bottom. The nativescript-aws-sdk has now been deprecated so that's not a good option anymore</p>
",6238533,2018-09-24T15:20:03.400,0,CC BY-SA 4.0,,1,sdk,option,1,The nativescript-aws-sdk has now been deprecated so that's not a good option,False,False
408,52494343,2,52484539,2018-09-25T09:01:44.813,1,"<p>You can't really implement 'SQL MERGE' method in s3 since it's not possible to update existing data objects. </p>

<p>A workaround is to load existing rows in a Glue job, merge it with incoming dataset, drop obsolete records and overwrite all objects on s3. If you have a lot of data it would be more efficient to partition it by some columns and then override those partitions that should contain new data only.</p>

<p>If you goal is preventing duplicates then you can do similar: load existing, drop those records from incoming dataset that already exist in s3 (loaded on previous step) and then write to s3 new records only.</p>
",1791510,2018-09-25T09:01:44.813,0,CC BY-SA 4.0,,1,,.,0,"A workaround is to load existing rows in a Glue job, merge it with incoming dataset, drop obsolete records and overwrite all objects on s3.",False,True
409,52548475,2,52548330,2018-09-28T04:45:33.950,0,"<p>Read the following PHP documentation linked below, particularly the backward incompatibility changes and deprecated features.  If you use any of those, you need to change your code. If you don't, then you should be fine.  You need to test it.</p>

<p><a href=""http://php.net/manual/en/migration56.php"" rel=""nofollow noreferrer"">http://php.net/manual/en/migration56.php</a></p>
",3720435,2018-09-28T04:45:33.950,0,CC BY-SA 4.0,,1,, ,0,">Read the following PHP documentation linked below, particularly the backward incompatibility changes and deprecated features.  ",False,False
410,52563919,2,52495435,2018-09-29T00:06:16.753,2,"<p>The error message refers to <code>part_number</code> which is the name of a CSV column (called ""flat files"" in MWS). In XML you need to specify a <code>StandardProductID</code> instead, like this:</p>

<pre><code>        &lt;StandardProductID&gt;
            &lt;Type&gt;EAN&lt;/Type&gt;
            &lt;Value&gt;1234567890123&lt;/Value&gt;
        &lt;/StandardProductID&gt;    
</code></pre>

<p>Please also note, that my <code>Beauty.xsd</code> does not have <code>HairCareProduct</code>. I had to change this to <code>BeautyMisc</code> to validate the feed - but that may be due to an outdated set of XSDs.</p>
",2097290,2018-09-29T00:06:16.753,0,CC BY-SA 4.0,,1,,"
",0,"BeautyMisc</code> to validate the feed - but that may be due to an outdated set of XSDs.</p>
",False,False
411,52617621,2,52600867,2018-10-02T23:11:43.827,0,"<p>Out of curiosity, have you read <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">How Amazon DynamoDB adaptive capacity accommodates uneven data access patterns (or, why what you know about DynamoDB might be outdated)</a> yet?</p>
",10448632,2018-10-02T23:11:43.827,0,CC BY-SA 4.0,,2,,outdated/,0,"Out of curiosity, have you read <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/",False,True
412,52631083,2,52628056,2018-10-03T15:51:13.923,3,"<p>According to the <a href=""https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html#RESTBucketGET-responses"" rel=""nofollow noreferrer"">documentation</a> for NextMarker:</p>

<blockquote>
  <p>Note</p>
  
  <p>This element is returned only if you specify a delimiter request parameter. If the  response does not include the NextMarker and it is truncated, you can use the value of the last Key in the response as the marker in the subsequent request to get the next set of object keys.</p>
</blockquote>

<p>Your code should therefore be:</p>

<pre><code>if(!isDone) {
  objects_request.SetMarker(outcome.GetResult().GetContents().back().GetKey());
}
</code></pre>

<p>Also note that the V1 ListObjects method is deprecated, you should be using <a href=""https://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_s3_1_1_s3_client.html#ae6bcac3c32efa6939df8b507112ba2ce"" rel=""nofollow noreferrer"">ListObjectsV2</a> which uses explicit continuation tokens which are a bit easier to use.</p>
",5494370,2018-10-03T15:51:13.923,2,CC BY-SA 4.0,,1,,class_aws_1_1_s3_1_1_s3_client.html#ae6bcac3c32efa6939df8b507112ba2ce,0,">Also note that the V1 ListObjects method is deprecated, you should be using <a href=""https://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_s3_1_1_s3_client.html#ae6bcac3c32efa6939df8b507112ba2ce",False,False
413,52640164,2,52639945,2018-10-04T06:15:16.910,0,"<p>The problem is that you are using EB CLI version 2.x. This is deprecated and no longer supported. The base set of commands changed with version 3.x.</p>

<p><a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3.html"" rel=""nofollow noreferrer"">The Elastic Beanstalk Command Line Interface (EB CLI)</a></p>

<p>The solution is to install the latest version.</p>

<p><a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install.html"" rel=""nofollow noreferrer"">Install the Elastic Beanstalk Command Line Interface (EB CLI)</a></p>
",8016720,2018-10-04T06:15:16.910,3,CC BY-SA 4.0,,1,This,.,1,This is deprecated and no longer supported.,False,False
414,52729688,2,51106504,2018-10-09T21:37:37.247,0,"<p>For any interested parties, <code>cs-import-documents</code> no longer exists. This was part of an older version of the AWC CLI, but has since been deprecated and not replaced. </p>
",4043845,2018-10-09T21:37:37.247,0,CC BY-SA 4.0,,1,,.,1,"This was part of an older version of the AWC CLI, but has since been deprecated and not replaced.",False,False
415,52835056,2,50722876,2018-10-16T12:04:38.230,0,"<p>The unofficial backport should soon be obsolete. Apache httpd team planned to release 2.4.36 with openssl 1.1.1 and tlsv1.3 support.
Due to an issue with h2 this has been stopped, you can read more about it here:
<a href=""https://lists.apache.org/thread.html/1ebfadad8f34cdf8520e2b9771cd8b8ca1a11b8d6d416b41e73c2687@%3Cdev.httpd.apache.org%3E"" rel=""nofollow noreferrer"">lists.apache.org/</a></p>

<p>If you still want to compile by yourself, you can</p>

<ul>
<li>either check out the source via <a href=""https://github.com/apache/httpd/tree/2.4.36"" rel=""nofollow noreferrer"">github</a> (tag: 2.4.36)</li>
<li>or download the <a href=""https://dist.apache.org/repos/dist/dev/httpd/"" rel=""nofollow noreferrer"">source</a> directly from apache dist server (content not static, will no longer be present as soon as an update was uploaded)</li>
</ul>

<p>Maybe you want to apply the h2 fix from <a href=""http://svn.apache.org/viewvc?view=revision&amp;revision=1843954"" rel=""nofollow noreferrer"">rev 1843954</a></p>
",10074916,2018-10-16T12:04:38.230,0,CC BY-SA 4.0,,1,backport,.,0,The unofficial backport should soon be obsolete.,False,False
416,52959606,2,52947430,2018-10-24T00:42:34.160,3,"<p>I think that in this case, the simplest method will be the best: you could just keep a pointer to a <code>Transform</code> component in components that read/write it.</p>

<p>I don't think that using events (or some other indirection, like observers) solves any real problem in here.</p>

<ol>
<li><p><code>Transform</code> component is very simple - it's not something that will be changed during development. Abstracting access to it will actually make code more complex and harder to maintain.</p></li>
<li><p><code>Transform</code> is a component that will be frequently changed for many objects, maybe even most of your objects will update it each frame. Sending events each time there's a change has a cost - probably much higher than simply copying matrix/vector/quaternion from one location to another.</p></li>
<li><p>I think that using events, or some other abstraction, won't solve other problems, like multiple components updating the same <code>Transform</code> component, or components using outdated transform data.</p></li>
<li><p>Typically, renderers just copy all matrices of rendered objects each frame. There's no point in caching them in a rendering system.</p></li>
</ol>

<p>Components like <code>Transform</code> are used often. Making them overly complex might be a problem in many different parts of an engine, while using the simplest solution, a pointer, will give you greater freedom.</p>

<hr>

<p>BTW, there's also very simple way to make sure that <code>RenderComponent</code> will read transform <em>after</em> it has been updated (e. g. by <code>PhysicsComponent</code>) - you can split work into two steps:</p>

<ol>
<li><p><code>Update()</code> in which systems may modify components, and</p></li>
<li><p><code>PostUpdate()</code> in which systems can only read data from components</p></li>
</ol>

<p>For example <code>PhysicsSystem::Update()</code> might copy transform data to respective <code>TransformComponent</code> components, and then <code>RenderSystem::PostUpdate()</code> could just read from <code>TransformComponent</code>, without the risk of using outdated data.</p>
",8964493,2018-10-24T00:42:34.160,3,CC BY-SA 4.0,,2,,transform,1,"I think that using events, or some other abstraction, won't solve other problems, like multiple components updating the same <code>Transform</code> component, or components using outdated transform",False,False
417,53056505,2,53056504,2018-10-30T02:23:54.947,6,"<p><strong>DynamoDB attribute of type <code>Number</code> can store 126-bit integers (or 127-bit unsigned integers, with serious caveats).</strong> </p>

<p>According to Amazon's <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.NamingRulesDataTypes.html#HowItWorks.DataTypes.Number"" rel=""noreferrer"">documentation</a>: </p>

<blockquote>
  <p>Numbers can have up to 38 digits precision. Exceeding this results in an exception.</p>
</blockquote>

<p>This means (verified by testing in the AWS console) that the largest positive integer and smallest negative integers, respectively, that DynamoDB can store in a <code>Number</code> attribute are: </p>

<blockquote>
  <p>99,999,999,999,999,999,999,999,999,999,999,999,999 (aka 10^38-1)
  -99,999,999,999,999,999,999,999,999,999,999,999,999 (aka -10^38+1)</p>
</blockquote>

<p>These numbers require 126 bits of storage, using this formula: </p>

<pre><code>bits = floor (ln(number) / ln (2))
     = floor (87.498 / 0.693)
     = floor (126.259)
     = 126
</code></pre>

<p>So you can safely store a 126-bit signed int in a DynamoDB.</p>

<p>If you want to live dangerously, you can store a 127-bit <em>unsigned</em> int too, but there are some caveats: </p>

<ul>
<li>You'd need to avoid (or at least be very careful) using such a number as a sort key, because values with a most-significant-bit of 1 will sort as negative numbers.</li>
<li>Your app will need to convert unsigned ints to signed ints when storing them or querying for them in DynamoDB, and will also need to convert them back to unsigned after reading data from DynamoDB. </li>
</ul>

<p>If it were me, I wouldn't take these risks for one extra bit without a very, very good reason.</p>

<p>One logical question is whether 126 (or 127 given the caveats above) is good enough to store a UUID.  The answer is: it depends. If you are in control of the UUID generation, then you can always shave a bit or two from the UUID and store it. If you shave from the 4 ""version"" bits (see format <a href=""https://en.wikipedia.org/wiki/Universally_unique_identifier#Format"" rel=""noreferrer"">here</a>) then you may not be losing any entropy at all if you are always generating UUIDs with the same version.</p>

<p>However, if someone else is generating those UUIDs AND is expecting lossless storage, then you may not be able to use a <code>Number</code> to store the UUID. But you may be able to store it if you restrict clients to a whitelist of 4-8 <a href=""https://en.wikipedia.org/wiki/Universally_unique_identifier#Versions"" rel=""noreferrer"">UUID versions</a>. The largest version now is 5 out of a 0-15 range, and some of the older versions are discouraged for privacy reasons, so this limitation may be reasonable depending on your clients and whether they adhere to the version bits as defined in <a href=""https://tools.ietf.org/html/rfc4122#page-7"" rel=""noreferrer"">RFC 4122</a>. </p>

<p><em>BTW, I was surprised that this bit-limit question wasn't already online... at least not in an easily-Google-able place. So contributing this Q&amp;A pair so future searchers can find it.</em> </p>
",126352,2018-10-30T02:23:54.947,0,CC BY-SA 4.0,,1,,rfc4122#page-7,0,"The largest version now is 5 out of a 0-15 range, and some of the older versions are discouraged for privacy reasons, so this limitation may be reasonable depending on your clients and whether they adhere to the version bits as defined in <a href=""https://tools.ietf.org/html/rfc4122#page-7",False,False
418,53062912,2,33455244,2018-10-30T11:03:50.570,0,"<p>Upgrading a MySQL DB instance can be tricky but easy to achieve through the following steps.</p>

<p><strong><em>1. Ensure that running an updated version of MySQL Engine(not deprecated).</em></strong>
Trying to upgrade a deprecated MySQL Version via the AWS Console(UI) results in an error message.""</p>

<blockquote>
  <p>""Cannot find version 5.... for mysql (Service: AmazonRDS; Status Code:
  400; Error Code: InvalidParameterCombination; Request ID:........""</p>
</blockquote>

<p><strong>Even Snapshot Restore is most likely to run for several mins/hours without any success.</strong> </p>

<p>2.<strong>Use AWS CLI</strong> </p>

<p>Use 'modify-db-instance' command to scale the storage size[1] and applied the version upgrade on your DB instance. 
Here's the example command:</p>

<pre><code>aws rds modify-db-instance \ 
    --db-instance-identifier &lt;RDS_identifier&gt; \ 
    --allocated-storage &lt;storage_size&gt; \ 
    --apply-immediately 
</code></pre>

<p>You may also refer to this guide on to install AWS CLI toll: Installing the AWS Command Line Interface - [<a href=""https://docs.aws.amazon.com/cli/latest/userguide/installing.html][1]"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cli/latest/userguide/installing.html][1]</a></p>

<p>3.After the successful upgrade, optionally modify/upgrade your MySQL version to a non depreciated Version via the AWS Console(UI).</p>
",2841570,2018-10-30T11:03:50.570,0,CC BY-SA 4.0,,2,,"
",0,"Ensure that running an updated version of MySQL Engine(not deprecated).</em></strong>
",False,False
419,53080135,2,53073711,2018-10-31T09:27:13.497,1,"<p>It looks like that documentation is out of date with the library; it is written for the SDK v1 but the latest version is v2.</p>

<p>You have a few options:</p>

<ol>
<li>Use v1 of the library and follow that tutorial2. </li>
<li>Follow these steps for migrating the various classes used in the tutorial (<a href=""https://github.com/alexa/alexa-skills-kit-sdk-for-java/blob/2.0.x/docs/en/Migrating-To-ASK-SDK-v2-For-Java.rst#request-handlers"" rel=""nofollow noreferrer"">https://github.com/alexa/alexa-skills-kit-sdk-for-java/blob/2.0.x/docs/en/Migrating-To-ASK-SDK-v2-For-Java.rst#request-handlers</a>)</li>
<li>Use the v2 version of the hello-world example (<a href=""https://github.com/alexa/alexa-skills-kit-sdk-for-java/tree/2.0.x/samples/helloworld"" rel=""nofollow noreferrer"">https://github.com/alexa/alexa-skills-kit-sdk-for-java/tree/2.0.x/samples/helloworld</a>)</li>
</ol>
",2531132,2018-10-31T09:27:13.497,1,CC BY-SA 4.0,,1,,"

",0,"It looks like that documentation is outdated with the library; it is written for the SDK v1 but the latest version is v2.</p>

",False,False
420,53334157,2,37249475,2018-11-16T08:39:05.367,6,"<p><strong>Important: AMAZON.LITERAL is deprecated as of October 22, 2018. Older skills built with AMAZON.LITERAL do continue to work, but you must migrate away from AMAZON.LITERAL when you update those older skills, and for all new skills.</strong></p>

<p>Instead of using AMAZON.LITERAL, you can use a custom slot to trick alexa into passing the free flow text into the backend.</p>

<p>You can use this configuration to do it:</p>

<pre><code>{
    ""interactionModel"": {
        ""languageModel"": {
            ""invocationName"": ""siri"",
            ""intents"": [
                {
                    ""name"": ""SaveIntent"",
                    ""slots"": [
                        {
                            ""name"": ""text"",
                            ""type"": ""catchAll""
                        }
                    ],
                    ""samples"": [
                        ""{text}""
                    ]
                }
            ],
            ""types"": [
                {
                    ""name"": ""catchAll"",
                    ""values"": [
                        {
                            ""name"": {
                                ""value"": ""allonymous isoelectrically salubrity apositia phantomize Sangraal externomedian phylloidal""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""imbreviate Bertie arithmetical undramatically braccianite eightling imagerially leadoff""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""mistakenness preinspire tourbillion caraguata chloremia unsupportedness squatarole licitation""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""Cimbric sigillarid deconsecrate acceptableness balsamine anostosis disjunctively chafflike""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""earsplitting mesoblastema outglow predeclare theriomorphism prereligious unarousing""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""ravinement pentameter proboscidate unexigent ringbone unnormal Entomophila perfectibilism""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""defyingly amoralist toadship psoatic boyology unpartizan merlin nonskid""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""broadax lifeboat progenitive betel ashkoko cleronomy unpresaging pneumonectomy""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""overharshness filtrability visual predonate colisepsis unoccurring turbanlike flyboy""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""kilp Callicarpa unforsaken undergarment maxim cosenator archmugwump fitted""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""ungutted pontificially Oudenodon fossiled chess Unitarian bicone justice""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""compartmentalize prenotice achromat suitability molt stethograph Ricciaceae ultrafidianism""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""slotter archae contrastimulant sopper Serranus remarry pterygial atactic""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""superstrata shucking Umbrian hepatophlebotomy undreaded introspect doxographer tractility""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""obstructionist undethroned unlockable Lincolniana haggaday vindicatively tithebook""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""unsole relatively Atrebates Paramecium vestryish stockfish subpreceptor""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""babied vagueness elabrate graphophonic kalidium oligocholia floccus strang""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""undersight monotriglyphic uneffete trachycarpous albeit pardonableness Wade""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""minacious peroratory filibeg Kabirpanthi cyphella cattalo chaffy savanilla""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""Polyborinae Shakerlike checkerwork pentadecylic shopgirl herbary disanagrammatize shoad""
                            }
                        }
                    ]
                }
            ]
        }
    }
}
</code></pre>
",6741167,2018-11-16T08:39:05.367,4,CC BY-SA 4.0,,1,AMAZON.LITERAL,.,0,"AMAZON.LITERAL is deprecated as of October 22, 2018.",False,False
421,53427302,2,49553573,2018-11-22T09:08:58.530,0,"<p>The problem is in the amazon's outdated SNS documentation.</p>

<p>I have changed the payload format and it works for me.</p>

<pre><code>{
""GCM"": ""{ \""notification\"": { \""body\"": \""hello....\"", \""title\"": \""title123\"" } }""
}
</code></pre>
",97651,2018-11-22T09:08:58.530,0,CC BY-SA 4.0,,1,,documentation.</p,0,The problem is in the amazon's outdated SNS documentation.</p,False,False
422,53463672,2,53463247,2018-11-25T00:34:14.337,1,"<p>The question seems premised on obsolete information.</p>

<p><a href=""https://web.archive.org/web/20140105032747/https://aws.amazon.com/dynamodb/pricing/"" rel=""nofollow noreferrer"">Previously</a>, DynamoDB capacity was sold in blocks of 10 WCU and 50 RCU, each of which was $0.0065.  (It may have been $0.01 several years ago, since AWS does sometimes decrease pricing when new technologies or efficiency gains permit it or this might be the result of rounding.)  The 4 is 200 ÷ 50.</p>
",1695906,2018-11-25T00:34:14.337,0,CC BY-SA 4.0,,1,,information.</p,0,The question seems premised on obsolete information.</p,False,False
423,53491672,2,53491598,2018-11-27T01:55:37.023,1,"<p>The only way I can see the same query returning two different result sets is if, somehow, the underlying data is being changed.  The obvious cause might be if data is being added/removed in between when you run the same query twice.</p>

<p>However, in the case of AWS, there is another explanation.  If your database instance is logically replicated across RDS, then it might be possible that the first query hits the database before newly added/removed data has been replicated across RDS.  That is, your query could actually be executing against a slightly out of date database.</p>
",1863229,2018-11-27T01:55:37.023,4,CC BY-SA 4.0,,1,,database.</p,0,"That is, your query could actually be executing against a slightly outdated database.</p",False,True
424,53589551,2,53586562,2018-12-03T07:53:11.870,0,"<p>Depends what image you use. Lightsail is just a type of instance, which is kind of isolated from aws ecosystem(I would go for a ec2 t2 instance). </p>

<p>When you start an instance you select an image with an operating system. It doesn't mean it comes with the nodejs. You have to install it after you start the image(or to look for an image which has nodejs preinstalled).</p>

<p>If it has node installed, usually, in linux it's an obsolete version. Better to install nvm and then to select the desired node version. </p>
",268856,2018-12-03T07:53:11.870,1,CC BY-SA 4.0,,1,,.,0,"If it has node installed, usually, in linux it's an obsolete version.",False,False
425,53737797,2,43000359,2018-12-12T07:12:31.353,0,"<p>@DonMag solution seem to be outdated but was of big help !!</p>

<p>Here is the updated one : </p>

<pre><code>com.amazon.mobile.shopping://www.amazon.com/products/B00KWFCV32/
</code></pre>
",4046561,2018-12-12T07:12:31.353,0,CC BY-SA 4.0,,1,,!,0,<p>@DonMag solution seem to be outdated but was of big help !!,False,False
426,53772908,2,48340877,2018-12-14T02:56:14.347,0,"<p>After reading your final comment on the boot sequence and answering that question instead, I solved this (even in non-AWS) using the docker-compose <code>depends</code>.</p>

<p>Simple e.g.</p>

<pre><code>services:
  web:
    depends_on:
      - ""web_db""
  web_db:
    image: mongo:3.6
    container_name: my_mongodb
</code></pre>

<p>You should be able to remove the deprecated <code>links</code> and just use the hostnames  that docker creates from the service container names.  e.g. above the website would connect to the hostname: ""my_mongodb"".</p>
",209288,2018-12-14T02:56:14.347,0,CC BY-SA 4.0,,1,,code,0,You should be able to remove the deprecated <code,False,False
427,53789287,2,34065157,2018-12-15T03:30:29.617,11,"<p>The previous answers are now out of date. The newer AWS RDS Aurora does support autoscaling. Aurora Auto Scaling is available for both Aurora MySQL and Aurora PostgreSQL.</p>

<blockquote>
  <p><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html"" rel=""noreferrer"">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html</a></p>
</blockquote>
",102675,2018-12-15T03:30:29.617,2,CC BY-SA 4.0,,1,answers,.,0,>The previous answers are now outdated.,False,False
428,53893955,2,53893575,2018-12-22T07:50:58.410,0,"<p>Which AWS services are supported by <code>boto3</code> isn't directly defined in <a href=""https://github.com/boto/boto3/"" rel=""nofollow noreferrer""><code>boto3</code></a>, but in <a href=""https://github.com/boto/botocore"" rel=""nofollow noreferrer""><code>botocore</code></a>. For Quicksight support, you need at least version <code>1.12.49</code> of <code>botocore</code>, which is the version where Quicksight support got added (although the changelog erroneously talks about <a href=""https://github.com/boto/botocore/blob/develop/CHANGELOG.rst#11249"" rel=""nofollow noreferrer"">updated Quicksight support</a>).</p>

<p>When using AWS Lambda you can either use an AWS-provided version of <code>boto3</code> or bundle your own. As you're using the AWS-provided versions, your code currently runs with outdated versions of <a href=""https://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html"" rel=""nofollow noreferrer""><code>botocore</code> (1.10.74) and <code>boto3</code> (1.7.74)</a>, as AWS hasn't updated them in a while. These old versions don't support Quicksight yet.</p>

<p>We can only speculate why AWS stopped updating <code>botocore</code> and <code>boto3</code> for the AWS Lambda environment, but it might have to do with some backwards-incompatible changes introduced with <a href=""https://botocore.amazonaws.com/v1/documentation/api/latest/index.html#upgrading-to-1-12-0"" rel=""nofollow noreferrer""><code>botocore</code> 1.12.0</a> and <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/upgrading.html#id1"" rel=""nofollow noreferrer""><code>boto3</code> 1.9.0</a>.</p>

<p>To solve your problem you can add recent versions of <code>botocore</code> and <code>boto3</code>, which do support Quicksight, to your <a href=""https://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html"" rel=""nofollow noreferrer"">deployment package</a>, to use them, instead of the AWS-provided ones.</p>
",4779904,2018-12-22T07:50:58.410,2,CC BY-SA 4.0,,1,,code,0,"As you're using the AWS-provided versions, your code currently runs with outdated versions of <a href=""https://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html"" rel=""nofollow noreferrer""><code>botocore</code> (1.10.74) and <code",False,False
429,53966864,2,53959664,2018-12-29T05:07:02.047,0,"<p>Michael's description is good. Amazon has also stated (link below) ""Signature Version 2 is being deprecated, and the final support for Signature Version 2 will end on June 24, 2019.""</p>

<p><a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/auth-request-sig-v2.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonS3/latest/dev/auth-request-sig-v2.html</a></p>
",3743602,2018-12-29T05:07:02.047,0,CC BY-SA 4.0,,1,,"2019.""</p",0,"Amazon has also stated (link below) ""Signature Version 2 is being deprecated, and the final support for Signature Version 2 will end on June 24, 2019.""</p",False,False
430,54012207,2,18683206,2019-01-02T19:39:22.933,0,"<p>""S3Client::factory is deprecated in SDK 3.x, otherwise the solution is valid"" said by RADU</p>

<p>Here is the updated solution to help others who come across this answer:</p>

<pre><code># composer dependencies
require '/vendor/aws-autoloader.php';
//AWS access info  DEFINE command makes your Key and Secret more secure
if (!defined('awsAccessKey')) define('awsAccessKey', 'ACCESS_KEY_HERE');///  &lt;- put in your key instead of ACCESS_KEY_HERE
if (!defined('awsSecretKey')) define('awsSecretKey', 'SECRET_KEY_HERE');///  &lt;- put in your secret instead of SECRET_KEY_HERE


use Aws\S3\S3Client;

$config = [
    's3-access' =&gt; [
        'key' =&gt; awsAccessKey,
        'secret' =&gt; awsSecretKey,
        'bucket' =&gt; 'bucket',
        'region' =&gt; 'us-east-1', // 'US East (N. Virginia)' is 'us-east-1', research this because if you use the wrong one it won't work!
        'version' =&gt; 'latest',
        'acl' =&gt; 'public-read',
        'private-acl' =&gt; 'private'
    ]
];

# initializing s3
$s3 = Aws\S3\S3Client::factory([
    'credentials' =&gt; [
        'key' =&gt; $config['s3-access']['key'],
        'secret' =&gt; $config['s3-access']['secret']
    ],
    'version' =&gt; $config['s3-access']['version'],
    'region' =&gt; $config['s3-access']['region']
]);
$bucket = 'bucket';

$objects = $s3-&gt;getIterator('ListObjects', array(
    ""Bucket"" =&gt; $bucket,
    ""Prefix"" =&gt; 'filename' //must have the trailing forward slash for folders ""folder/"" or just type the beginning of a filename ""pict"" to list all of them like pict1, pict2, etc.
));

foreach ($objects as $object) {
    echo $object['Key'] . ""&lt;br&gt;"";
}
</code></pre>
",7480839,2019-01-02T19:39:22.933,0,CC BY-SA 4.0,,1,,RADU</p,0,"<p>""S3Client::factory is deprecated in SDK 3.x, otherwise the solution is valid"" said by RADU</p",False,True
431,54013726,2,53278110,2019-01-02T21:59:58.820,1,"<p>This is one of the top google results but there is some outdated information here.</p>

<p>You can enable sql native backup / restore for sql server in RDS, backup a single database to S3 and then restore that database to the same RDS instance with a new database name.</p>

<p>Steps for <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/native-backup-rds-sql-server/"" rel=""nofollow noreferrer"">environment setup and performing the backup/restore</a>:</p>

<ol>
<li>Create an S3 bucket to store your native backups.</li>
<li>Enable Sql native backup by configuring a new option group for your RDS instance and adding the ""SQLSERVER_BACKUP_RESTORE"" option.</li>
<li>Executing a series of procedures in SSMS to run the backup and restore tasks:</li>
</ol>

<hr>

<pre><code>exec msdb.dbo.rds_backup_database 
@source_db_name='database_name', @s3_arn_to_backup_to='arn:aws:s3:::bucket_name/file_name_and_extension', 
@overwrite_S3_backup_file=1;

exec msdb.dbo.rds_restore_database 
@restore_db_name='database_name', 
@s3_arn_to_restore_from='arn:aws:s3:::bucket_name/file_name_and_extension';

exec msdb.dbo.rds_task_status @db_name='database_name'
</code></pre>

<hr>

<p>Note: I cannot find any announcement about removing this prior limitation:</p>

<blockquote>
  <p>You can't restore a backup file to the same DB instance that was used to create the backup file. Instead, restore the backup file to a new DB instance.</p>
</blockquote>

<p>However, as of today, I can confirm restoring a native backup to the same instance works as you would expect.</p>
",185200,2019-01-02T21:59:58.820,1,CC BY-SA 4.0,,1,,here.</p,0,but there is some outdated information here.</p,False,False
432,54045954,2,52758171,2019-01-04T20:57:11.637,5,"<p>Make sure Eclipse is running with Java 8.  AWS Toolkit requires JAXB for the upload to AWS S3, but JAXB was deprecated in Java 9 &amp; 10, and removed from Java 11.  If you're running Eclipse under Java 9, 10, or 11, Eclipse/AWS Toolkit won't find JAXB and you'll get this error.</p>

<p>You can resolve the problem by specifying the JVM Eclipse should use on startup.  Instructions for various platforms found here: <a href=""https://wiki.eclipse.org/Eclipse.ini"" rel=""noreferrer"">https://wiki.eclipse.org/Eclipse.ini</a></p>
",1836871,2019-01-04T20:57:11.637,1,CC BY-SA 4.0,,1,, ,0,"AWS Toolkit requires JAXB for the upload to AWS S3, but JAXB was deprecated in Java 9 &amp; 10, and removed from Java 11.  ",False,False
433,54078758,2,53992133,2019-01-07T17:07:30.727,0,"<p>The documentation was outdated. To be able to use your own cloudfront resource, you need to deploy the stack without the demo ui.</p>
",2344682,2019-01-07T17:07:30.727,0,CC BY-SA 4.0,,1,documentation,.,0,>The documentation was outdated.,False,False
434,54194074,2,54191023,2019-01-15T07:03:42.907,0,"<p>With Lambda's Node 6/8 runtimes, it's easier if you use Promises (or even the <code>async/await</code> syntax).</p>

<p>The old <code>context.succeed()</code>/<code>context.fail()</code> syntax was for the old Node versions and those have been deprecated.</p>

<pre><code>// Leave this outside your handler so you don't instantiate in every invocation.
const codepipeline = new aws.CodePipeline();

exports.handler = event =&gt; {

    if (!('CodePipeline.job' in event)) {
        return Promise.resolve();
    }

    const jobId = event['CodePipeline.job'].id;

    const environment = event['CodePipeline.job'].data.actionConfiguration.configuration.UserParameters;

    return doStuff
        .then(() =&gt; putJobSuccess(message, jobId));
}


function putJobSuccess(message, jobId) {
    console.log('Post Job Success For JobID: '.concat(jobId));
    const params = {
        jobId: jobId
    };

    return codepipeline.putJobSuccessResult(params).promise()
        .then(() =&gt; {
            console.log(`Post Job Success Succeeded Message: ${message}`);

            return message;
        })
        .catch((err) =&gt; {
            console.log(`Post Job Failure Message: ${message}`);

            throw err;
        });
}
</code></pre>
",1252647,2019-01-15T07:03:42.907,1,CC BY-SA 4.0,,1,,deprecated.</p,0,syntax was for the old Node versions and those have been deprecated.</p,False,False
435,54204722,2,54204287,2019-01-15T18:18:06.297,1,"<p>If you use express.js for the Webapplication, you first could use <code>pm2</code> (<a href=""https://www.npmjs.com/package/pm2"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/pm2</a>) to create a ""container"" for your application. If you want to Host your application, I recommend using Nginx with a reverse proxy.
Here are some Links: </p>

<ol>
<li><a href=""https://serverfault.com/questions/601332/how-to-configure-nginx-so-it-works-with-express"">https://serverfault.com/questions/601332/how-to-configure-nginx-so-it-works-with-express</a></li>
<li><a href=""https://stackoverflow.com/questions/53807393/nginx-reverse-proxy-expressjs-angular-ssl-configuration-issues"">Nginx Reverse Proxy + ExpressJS + Angular + SSL configuration issues</a></li>
</ol>

<p>I hope that helps you. And if you want to install your application, just run <code>npm install</code> in the directory where the <code>package.json</code> is. Maybe you should update node.js because version 4.x is absolutely outdated.</p>
",10058437,2019-01-15T18:18:06.297,1,CC BY-SA 4.0,,1,,outdated.</p,0,Maybe you should update node.js because version 4.x is absolutely outdated.</p,False,False
436,54209272,2,54203838,2019-01-16T01:35:30.483,1,"<p>It looks like you missed a step.  [cloud-configs have been deprecated for quite some time now.  You correctly converted that cloud-config into a <a href=""https://coreos.com/os/docs/latest/migrating-to-clcs.html"" rel=""nofollow noreferrer"">container linux config</a> (CLC) file, but missed using <a href=""https://coreos.com/os/docs/latest/overview-of-ct.html"" rel=""nofollow noreferrer"">config transpiler</a> (CT) to then render an ignition sequence.  You can check this by running your config through the <a href=""https://coreos.com/validate/"" rel=""nofollow noreferrer"">online validator</a>.  After running that CLC config through the config transpiler I get the following, which validates correctly:</p>

<pre><code>{
  ""ignition"": {
    ""config"": {},
    ""timeouts"": {},
    ""version"": ""2.1.0""
  },
  ""networkd"": {},
  ""passwd"": {},
  ""storage"": {
    ""filesystems"": [
      {
        ""mount"": {
          ""device"": ""/dev/xvdb"",
          ""format"": ""ext4"",
          ""wipeFilesystem"": true
        },
        ""name"": ""ephemeral1""
      }
    ]
  },
  ""systemd"": {
    ""units"": [
      {
        ""contents"": ""[Unit]\nBefore=local-fs.target\n[Mount]\nWhat=/dev/xvdb\nWhere=/media/ephemeral\nType=ext4\n[Install]\nWantedBy=local-fs.target\n"",
        ""enable"": true,
        ""name"": ""media-ephemeral.mount""
      },
      {
        ""contents"": ""[Unit]\nDescription=Mount ephemeral to /var/lib/docker\nBefore=local-fs.target\n[Mount]\nWhat=/dev/xvdb\nWhere=/var/lib/docker\nType=ext4\n[Install]\nWantedBy=local-fs.target\n"",
        ""enable"": true,
        ""name"": ""var-lib-docker.mount""
      },
      {
        ""dropins"": [
          {
            ""contents"": ""[Unit]\nAfter=var-lib-docker.mount\nRequires=var-lib-docker.mount\n"",
            ""name"": ""10-wait-docker.conf""
          }
        ],
        ""name"": ""docker.service""
      }
    ]
  }
}
</code></pre>

<p>Additionally, it's important to note that <a href=""https://coreos.com/ignition/docs/latest/what-is-ignition.html#ignition-vs-coreos-cloudinit"" rel=""nofollow noreferrer"">there are other differences as well</a> between <code>ignition</code> and <code>coreos-cloud-init</code>.  The most important of which is that <em>ignition only runs once</em>.  Thus, for things like wiping the contents of that ephemeral disk, you should not expect <code>wipe_filesystem: true</code> to be run every single boot.</p>

<p>Try booting the machine with this config instead.  You should get the expected results.</p>
",2780658,2019-01-16T01:35:30.483,1,CC BY-SA 4.0,,1,configs, ,0,[cloud-configs have been deprecated for quite some time now.  ,False,False
437,54216158,2,54186070,2019-01-16T11:30:40.943,0,"<p>As pointed out by @John Rotenstein , all bucket name must be DNS-compliant, so a direct slash is not allowed for bucket name. </p>

<blockquote>
  <p>In fact the 2nd piece of code above does work, when I added this: for
  key in bucket.get_all_keys(prefix='s-DI-S/', delimiter='/') and took
  what was really the prefix off the Bucketname.</p>
</blockquote>

<p>From the above OP comment,  it seems OP maybe misunderstood S3 object store with a file directory system.  Each s3 object are never store in a hierarchy format. The so call ""prefix""  is simply an text filter that retrieve object name that contains the the similar ""prefix"" in the object name. </p>

<p>In OP, there is a bucket name call  <code>b-datasci</code>, while there are many object with the prefix <code>x-DI-S</code> store under the bucket.</p>

<p>In addition, the usage of <code>delimiter</code> under <code>get_all_keys</code> is not what OP think. If you check out AWS documentation on <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/ListingKeysHierarchy.html"" rel=""nofollow noreferrer"">Listing Keys Hierarchically Using a Prefix and Delimiter</a>, you will learn that it is just a secondary filter.  i.e. </p>

<blockquote>
  <p>The prefix and delimiter parameters limit the kind of results returned by a list operation. Prefix limits results to only those keys that begin with the specified prefix, and delimiter causes list to roll up all keys that share a common prefix into a single summary list result.</p>
</blockquote>

<p>p/s: OP should use boto3 because boto is deprecated by AWS for more than 2 years. </p>
",6017840,2019-01-16T11:30:40.943,0,CC BY-SA 4.0,,1,,.,0,>p/s: OP should use boto3 because boto is deprecated by AWS for more than 2 years.,False,False
438,54264937,2,52108481,2019-01-19T07:19:15.633,2,"<p>When I first asked this question, AWS had the ability to <strong>record</strong> metrics with the Swift SDK, but <strong>not</strong> view them in the Pinpoint API, which is absurd, because then you can only record metrics. What's the point? I asked in the AWS forums, and a couple months later, they responded something along the lines of ""Please wait - coming soon.""</p>

<p>This feature is now available, whereas before it simply wasn't.</p>

<p>Go to Pinpoint, your project, then click the Analytics drop-down menu, then click events. You can see that you can sort by metric. If you look at my outdated screenshot above, you'll see that this was <strong>not</strong> an option.</p>

<p><a href=""https://i.stack.imgur.com/DBTLt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DBTLt.png"" alt=""Image of Pinpoint console""></a></p>
",8808193,2019-01-19T07:19:15.633,0,CC BY-SA 4.0,,1,,>,0,"If you look at my outdated screenshot above, you'll see that this was <strong>",False,False
439,54295705,2,9648255,2019-01-21T18:17:09.660,0,"<p>Previous answers are obsolete now
<a href=""https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/</a>
""This S3 request rate performance increase removes any previous guidance to randomize object prefixes to achieve faster performance. That means you can now use logical or sequential naming patterns in S3 object naming without any performance implications. ""</p>
",6700830,2019-01-21T18:17:09.660,0,CC BY-SA 4.0,,1,answers,new/2018/07,0,"Previous answers are obsolete now
<a href=""https://aws.amazon.com/about-aws/whats-new/2018/07",False,False
440,54329221,2,54184611,2019-01-23T14:15:25.297,0,"<p>The TransferManager component in the AWS Android SDK has been deprecated in favor of the TransferUtility component. The TransferUtility component allows you to pause and resume transfers. It also has support for network monitoring and will automatically pause and resume transfers when the network goes down and comes back up. Here is the link to the TransferUtility documentation - <a href=""https://aws-amplify.github.io/docs/android/storage"" rel=""nofollow noreferrer"">https://aws-amplify.github.io/docs/android/storage</a></p>
",9282303,2019-01-23T14:15:25.297,4,CC BY-SA 4.0,,1,component,.,0,The TransferManager component in the AWS Android SDK has been deprecated in favor of the TransferUtility component.,False,False
441,54459928,2,54439440,2019-01-31T11:47:50.640,16,"<p>My experience has been it does not obsolete the need for a users table, for a couple of reasons.</p>

<ol>
<li>I would routinely run into AWS throttling errors when invoking the Cognito getUser/listUser methods while using Cognito as the primary user data store. AWS support would increase the API limits on our account but I was always worried they would reappear. </li>
<li>You are essentially limited to querying users by username/email/phone. The listUser method is very limited for searching</li>
<li>If you want to store other user data then you have to put them in custom Cognito attributes and managing those got tiresome quickly</li>
</ol>

<p>The design I ended up on was to set a post-confirmation trigger on the Cognito user pool and have it copy all the user data into a relational database or DynamoDB when a user signed up. The primary key of the users table would be the Cognito username so if necessary I could lookup the user in both the database and Cognito. Basically I just use Cognito for authentication and not as a primary user database</p>
",6234168,2019-01-31T11:47:50.640,3,CC BY-SA 4.0,,1,,reasons.</p,1,"My experience has been it does not obsolete the need for a users table, for a couple of reasons.</p",False,False
442,54526914,2,54452410,2019-02-05T02:09:15.880,0,"<p>Jenkins is hopelessly out of date and unmaintained. I added the Post Build Task plugin, <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install.html"" rel=""nofollow noreferrer"">installed <code>eb</code> tool</a> as <code>jenkins</code> user, ran <code>eb init</code> in the job directory, edited <code>.elasticbeanstalk/config.yml</code> to add the lines</p>

<pre><code>deploy:
  artifact: target/AppName-Sprint5-SNAPSHOT-bin.zip
</code></pre>

<p>Then entered in the shell command to deploy the build.</p>

<pre><code>/var/lib/jenkins/.local/bin/eb deploy -l sprint5-${BUILD_NUMBER}
</code></pre>

<p><a href=""https://i.stack.imgur.com/S2wcG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S2wcG.png"" alt=""enter image description here""></a></p>
",148844,2019-02-05T02:09:15.880,0,CC BY-SA 4.0,,1,,.,0,Jenkins is hopelessly outdated and unmaintained.,False,False
443,54540168,2,54539708,2019-02-05T17:41:52.630,4,"<p>GraphQL designers have a strong opinion against versioning of schemas. </p>

<p>Versioning was used in the past, or is used with REST API, to avoid introducing breaking changes. </p>

<p>GraphQL only returns the data that's explicitly requested, so new capabilities can be added via new types and new fields on those types without creating a breaking change. Deprecated fields can just be marked as deprecated (see <a href=""https://facebook.github.io/graphql/draft/#sec--deprecated"" rel=""nofollow noreferrer"">https://facebook.github.io/graphql/draft/#sec--deprecated</a>)</p>

<p>This has led to a common practice of always avoiding breaking changes and serving a versionless API.</p>
",663360,2019-02-05T17:41:52.630,0,CC BY-SA 4.0,,1,,deprecated</a>)</p,0,"Deprecated fields can just be marked as deprecated (see <a href=""https://facebook.github.io/graphql/draft/#sec--deprecated"" rel=""nofollow noreferrer"">https://facebook.github.io/graphql/draft/#sec--deprecated</a>)</p",False,False
444,54877580,2,54877345,2019-02-26T02:30:15.387,1,"<p>You forwarded port 49001 to the Docker container's port 8080. That's how you access Jenkins. It looks like you got the right response for <code>curl localhost:49001</code>. It's asking you to login.</p>

<p>Also note the <code>jenkins</code> image is deprecated. You should use <code>jenkins/jenkins:lts</code> instead.</p>
",492773,2019-02-26T02:30:15.387,0,CC BY-SA 4.0,,1,image,.,0,image is deprecated.,False,False
445,54891138,2,54887416,2019-02-26T17:33:48.310,3,"<p>Either option would work but since your using AutoScaling I would recommend creating a custom AMI.</p>

<p><strong>Pros:</strong></p>

<ol>
<li>You will have the same image every time you scale up. Installing from a script could provide new instances that have different software versions.  That can turn into a troubleshooting nightmare.</li>
<li>It's faster to have your software pre-loaded.</li>
<li>Allows for a structured Blue/Green Deployment</li>
</ol>

<p><strong>Cons:</strong></p>

<ol>
<li>Static images are outdated as soon as they are created.</li>
</ol>
",1723857,2019-02-26T17:33:48.310,2,CC BY-SA 4.0,,1,images,created.</li,0,<li>Static images are outdated as soon as they are created.</li,False,False
446,54978632,2,54961986,2019-03-04T07:41:28.333,2,"<p>The failure to install is due to an outdated version of <code>setuptools</code>. Upgrade <code>pip</code> to version 19+ and setuptools to version 40+.</p>
",1998200,2019-03-04T07:41:28.333,2,CC BY-SA 4.0,,1,,setuptools</code,0,>The failure to install is due to an outdated version of <code>setuptools</code,False,False
447,55228196,2,44907295,2019-03-18T18:50:37.117,5,"<p>Unsigned's answer is good but is a tad outdated. As of February 2019, CodeBuild allows both caching in an S3 bucket and allows the user to cache locally. You can now specify cache at 3 different layers of a build:</p>

<ul>
<li>Docker Layer Caching</li>
<li>Git Layer Cahing (cache the last build and then only build from <code>git diff</code>)</li>
<li>Custom caching - specified within the <code>cache:</code> portion of your buildspec.yml file. Personally, I cache my node_modules/ here and then cache at the Git Layer.</li>
</ul>

<p>Source: <a href=""https://aws.amazon.com/blogs/devops/improve-build-performance-and-save-time-using-local-caching-in-aws-codebuild/"" rel=""noreferrer"">https://aws.amazon.com/blogs/devops/improve-build-performance-and-save-time-using-local-caching-in-aws-codebuild/</a></p>
",6891245,2019-03-18T18:50:37.117,0,CC BY-SA 4.0,,1,answer,.,0,Unsigned's answer is good but is a tad outdated.,False,False
448,55232970,2,46932607,2019-03-19T02:54:59.977,0,"<p>You can try this answer: <a href=""https://stackoverflow.com/a/42300647/2727462"">https://stackoverflow.com/a/42300647/2727462</a></p>

<blockquote>
  <p>Solution If you're DNS is configured to hit directly on the ELB -> you
  should reduce the TTL of the association (IP,DNS). The IP can change
  at any time with the ELB so you can have serious damage on your
  traffic.</p>
  
  <p>The client keep Some IP from the ELB in cache so you can have those
  can of trouble.</p>
  
  <p>Scaling Elastic Load Balancers Once you create an elastic load
  balancer, you must configure it to accept incoming traffic and route
  requests to your EC2 instances. These configuration parameters are
  stored by the controller, and the controller ensures that all of the
  load balancers are operating with the correct configuration. The
  controller will also monitor the load balancers and manage the
  capacity that is used to handle the client requests. It increases
  capacity by utilizing either larger resources (resources with higher
  performance characteristics) or more individual resources. The Elastic
  Load Balancing service will update the Domain Name System (DNS) record
  of the load balancer when it scales so that the new resources have
  their respective IP addresses registered in DNS. The DNS record that
  is created includes a Time-to-Live (TTL) setting of 60 seconds, with
  the expectation that clients will re-lookup the DNS at least every 60
  seconds. By default, Elastic Load Balancing will return multiple IP
  addresses when clients perform a DNS resolution, with the records
  being randomly ordered on each DNS resolution request. As the traffic
  profile changes, the controller service will scale the load balancers
  to handle more requests, scaling equally in all Availability Zones.</p>
</blockquote>

<p>In my case the problem was in TTL. Issue can be tracked by a command like <code>wget https://your-url</code>. The comand output will show you the IP address to which it tries to connect. And when connection hangs you can figure out a wrong outdated IP address. If it happen - check your DNS settings and update TTL.</p>
",2727462,2019-03-19T02:54:59.977,0,CC BY-SA 4.0,,1,,.,0,And when connection hangs you can figure out a wrong outdated IP address.,False,False
449,55249677,2,52860945,2019-03-19T20:43:58.103,0,"<p>tyron's comment on your question is spot on. Check permissions of the user executing the CloudFormation. If you're running commands directly, this is usually pretty easy to check. In some cases, you may be working with a more complicated environment with automation.</p>

<p>I find the best way to troubleshoot permissions in an automated world is via CloudTrail. After any API call has failed, whether from the CLI, CloudFormation, or another source, you can look up the call in CloudTrail.</p>

<p>In this case, searching for ""Event Name"" = ""CreateQueue"" in the time range of the failure will turn up a result with details like the following:</p>

<ul>
<li><strong>Source IP Address</strong>; this field may say something like cloudformation.amazonaws.com, or the IP of your machine/office. Helpful when you need to filter events based on the source.</li>
<li><strong>User name</strong>; In my case, this was the EC2 instance ID of the agent running the CFN template.</li>
<li><strong>Access Key ID</strong>; For EC2 instances, this is likely a set of temporary access credentials, but for a real user, it will show you what key was used.</li>
<li><strong>Actual event data</strong>; Especially helpful for non-permissions errors, the actual event may show you errors in the request itself.</li>
</ul>

<p>In my case, the specific EC2 instance that ran automation was out of date and needed to be updated to use the correct IAM Role/Instance Profile. CloudTrail helped me track that down.</p>
",3570747,2019-03-19T20:43:58.103,0,CC BY-SA 4.0,,1,instance,.,0,"In my case, the specific EC2 instance that ran automation was outdated and needed to be updated to use the correct IAM Role/Instance Profile.",False,False
450,55403239,2,55317415,2019-03-28T17:04:09.093,0,"<p>So this is the OOM issue in existing Alpakka framework and get resolved in 1.0-RC2 - <a href=""https://github.com/akka/alpakka/milestone/27"" rel=""nofollow noreferrer"">https://github.com/akka/alpakka/milestone/27</a></p>

<p>However as an alternate <a href=""https://github.com/s12v/akka-stream-sqs"" rel=""nofollow noreferrer"">https://github.com/s12v/akka-stream-sqs</a> work as charm ( though it is deprecated in favor of Alpakka Sqs)</p>
",412957,2019-03-28T17:04:09.093,0,CC BY-SA 4.0,,1,,"
",0,"> work as charm ( though it is deprecated in favor of Alpakka Sqs)</p>
",False,False
451,55436250,2,55435954,2019-03-30T22:32:59.570,3,"<p><code>saveData()</code> is <code>async</code>, therefore it returns a promise. You need to <code>await</code> on it. Also, I don't understand why you gave up on the .promise() call. Using callbacks is outdated and much more convoluted.</p>

<p>Your code should look like this:</p>

<pre><code>module.exports.saveData = async () =&gt; {
     let params = {
        TableName: ""SynData"",
        Item:{
            ""synId"": 66,
            ""synValue"": 1579.21,
            ""synTime"": ""2019-01-01""
        }
    };
    return await documentClient.put(params).promise();
};
</code></pre>

<p>On your client, just await on saveData():</p>

<pre><code>for(let i = 0; i &lt; allValues.length; i++){
   await db.saveData(i, allValues[i], allStart);
}
</code></pre>
",10950867,2019-03-30T22:32:59.570,0,CC BY-SA 4.0,,1,,convoluted.</p,0,Using callbacks is outdated and much more convoluted.</p,False,False
452,55477120,2,55476533,2019-04-02T14:20:36.243,2,"<p>CloudTrail logs can sometimes be slow, as mentioned <a href=""https://aws.amazon.com/cloudtrail/faqs/"" rel=""nofollow noreferrer"">in the CloudTrail FAQ under ""How long does it take CloudTrail to deliver an event for an API call?""</a>:</p>

<blockquote>
  <p>Typically, CloudTrail delivers an event within 15 minutes of the API call.</p>
</blockquote>

<p>Concerning the event rule for a specific sub-folder, it is currently (and someone update me if I'm wrong, might be outdated) possible only through an S3 Lambda trigger. The only thing you can do though through CloudWatch event rules is add a specific key to the event rule by following <a href=""https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source-console.html"" rel=""nofollow noreferrer"">this guide</a>, it just won't act as a prefix but would be a specific key that triggers the event, which might be helpful.</p>

<p>If a specific key doesn't suffice and you still need a prefix/suffix in your S3 trigger definition, then consider adding a Lambda that executes your step function, which is often used anyways because it allows you to customize the event sent to the step function. You can use the <a href=""https://docs.aws.amazon.com/step-functions/latest/apireference/API_StartExecution.html"" rel=""nofollow noreferrer"">StartExecution</a> API call from inside your Lambda, and setup an S3 Lambda trigger with a prefix, <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html#notification-how-to-event-types-and-destinations"" rel=""nofollow noreferrer"">which you can find information on here.</a></p>
",7207514,2019-04-02T14:20:36.243,1,CC BY-SA 4.0,,1,,.,0,"Concerning the event rule for a specific sub-folder, it is currently (and someone update me if I'm wrong, might be outdated) possible only through an S3 Lambda trigger.",False,False
453,55485222,2,55477419,2019-04-03T00:09:21.797,0,"<p>Figured I'd take a look at the actual code (go figure, right?) and found that <code>apigateway.x.API</code> is now under <code>awsx.apigateway.API</code>. Was wondering if <code>awsx</code> was related. Feel better now.</p>

<p><a href=""https://github.com/pulumi/pulumi-aws/pull/508"" rel=""nofollow noreferrer"">https://github.com/pulumi/pulumi-aws/pull/508</a></p>

<blockquote>
  <p>The deprecated api aws.apigateway.x.API has been removed. Its replacement is in the @pulumi/awsx package with the name awsx.apigateway.API.</p>
</blockquote>

<p><code>pulumi-awsx</code> is described as ""AWS infrastructure best practices in component form.</p>

<p><a href=""https://github.com/pulumi/pulumi-awsx"" rel=""nofollow noreferrer"">https://github.com/pulumi/pulumi-awsx</a></p>
",124563,2019-04-03T00:09:21.797,0,CC BY-SA 4.0,,1,,api,0,The deprecated api,False,False
454,55524598,2,55514647,2019-04-04T20:42:35.990,1,"<p>get rid of that line about fs.s3a.impl. All it does is change the default mapping of ""s3a"" to ""the modern, supported, maintained S3A connector"" to the ""old, obsolete, unsupported S3N connector""</p>

<p>you do not need that line. The fact that people writing spark apps always do this is just superstition. Hadoop-common knows which filesystem class handles s3a URLs the same way it knows who handles ""file"" and ""hdfs""</p>
",2261274,2019-04-04T20:42:35.990,2,CC BY-SA 4.0,,1,,"connector""</p",0,"All it does is change the default mapping of ""s3a"" to ""the modern, supported, maintained S3A connector"" to the ""old, obsolete, unsupported S3N connector""</p",False,False
455,55653384,2,55633029,2019-04-12T14:00:22.330,2,"<p>After @a_horse_with_no_name pointed me in the right direction and chatting with AWS I am able to answer my own question, at least if you are using AWS Database Migration Service (DMS).</p>

<p>The problem is, DMS only focuses on the data itself and not really the schema (which to me seems like a major oversight, especially if your using the same database technology but that is another issue).
So the schema itself is not migrated. The documentation does not really make this clear.</p>

<p>To fix this issue:</p>

<ol>
<li>Stop (if it still exists) the existing AWS DMS migration</li>
<li>Drop the existing migrated database, and create a new empty schema to use</li>
<li>Follow the steps here <a href=""https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Installing.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Installing.html</a> to install and setup the Amazon Schema Conversation Tool (SCT)</li>
<li>Once you are connected to both databases, follow the steps here <a href=""https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Converting.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Converting.html</a> to ""convert"" your schema (I did the entire ""public"" schema for this database to ensure everything is covered</li>
<li>Create or modify your AWS DMS Migration, ensuring Target Table Preparation Mode = ""TRUNCATE"" and disable foreign keys on the target database. If modifying, make sure when asked you ""RESTART"" not resume</li>
</ol>

<p>What I have not yet tested is how to handle the fact that I am migrating a live database. So the sequences may be out of date on the target database when the migration is done. I believe I can just later go into SCT and only migrate the sequences but I have not tested this yet.</p>
",1955996,2019-04-12T14:00:22.330,0,CC BY-SA 4.0,,1,sequences,.,0,So the sequences may be outdated on the target database when the migration is done.,False,True
456,55657852,2,55657701,2019-04-12T18:47:46.707,1,"<p>Security group rules can reference CIDR blocks, prefix list IDs (for VPC endpoints) or other security groups.</p>

<p>This allows you to say that everything that has a security group of <code>foo</code> is allowed to communicate with everything with a security group of <code>bar</code>.</p>

<p>The bit specifically about whether it can be IDs (of the form <code>sg-123456</code>) or the names is down to a quirk in the AWS APIs and support for the long deprecated EC2 classic accounts. In general you want to use the IDs to reference security groups.</p>
",2291321,2019-04-12T18:47:46.707,0,CC BY-SA 4.0,,1,,.,0,or the names is down to a quirk in the AWS APIs and support for the long deprecated EC2 classic accounts.,False,False
457,55761393,2,55295305,2019-04-19T11:50:23.273,0,"<p>This what The python 3.7 AWS lambda environment looks like at time of writing:</p>

<pre><code>python: 3.7.2 (default, Mar 1 2019, 11:28:42)
[GCC 4.8.3 20140911 (Red Hat 4.8.3-9)], boto3: 1.9.42, botocore: 1.12.42
</code></pre>

<p>By comparing botocore 1.12.42 (error) with 1.12.133 (working ok) I found that an outdated botocore in AWS Lambda is the culprit. One solution could be to include the latest botocore in your lambda package. For example using the the python requirements plugin:</p>

<pre><code>serverless plugin install -n serverless-python-requirements
</code></pre>

<p>And creating a <code>requirements.txt</code> file containing <code>botocore==1.12.133</code></p>

<p>(instead of 1.12.133 you might want to use the latest version at the time you read this)</p>
",923557,2019-04-19T11:50:23.273,0,CC BY-SA 4.0,,1,,.,0,I found that an outdated botocore in AWS Lambda is the culprit.,False,False
458,55891586,2,55890992,2019-04-28T15:06:30.677,0,"<p>You are making a mistake trying to move the database <em>and</em> change the engine from MySQL to Aurora at the same time.</p>

<p>Migrate the MySQL 5.7 system now, and convert to Aurora later.  You do not need to ask for trouble, and doing both at the same time is exactly that.</p>

<p>It is not possible to ""quickly"" migrate a primary database over distance, but it is possible to make the amount of setup time irrelevant, and activation time near zero.</p>

<p>Instead of trying to do a copy, create an RDS cross-region replica of your data, and at the last moment, promote that replica to master.</p>

<blockquote>
  <p><strong>Creating a Read Replica in a Different AWS Region</strong></p>
  
  <p>With Amazon RDS, you can create a MariaDB, MySQL, or PostgreSQL Read Replica in a different AWS Region than the source DB instance. You create a Read Replica to do the following:</p>
  
  <ul>
  <li><p>Improve your disaster recovery capabilities.</p></li>
  <li><p>Scale read operations into an AWS Region closer to your users.</p></li>
  <li><p>Make it easier to migrate from a data center in one AWS Region to a data center in another AWS Region.</p></li>
  </ul>
  
  <p><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.XRgn"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.XRgn</a></p>
</blockquote>

<p>It doesn't matter how long it takes for RDS to copy the data and set up the replica, because as soon as it is copied, it starts replicating everything that changed on the master server since the process began.</p>

<p>Once you have verified that everything is correct and consistent, then you <a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.Promote"" rel=""nofollow noreferrer"">promote a replica</a>.  It is permanently and irrevocably detached from its original upstream instance, and becomes writable.  This is the last thing you do and after the application starts writing to this new database, your original system in São Paulo is obsolete because changes to it will no longer replicate to the new system -- they're permanently isolated.</p>

<p>This arrangement does not require you to establish any networking or make the databases publicly accessible.</p>

<p>And, you can create and destroy multiple replicas to test this process, without disturbing production.</p>
",1695906,2019-04-28T15:06:30.677,0,CC BY-SA 4.0,,1,This,isolated.</p,1,"This is the last thing you do and after the application starts writing to this new database, your original system in São Paulo is obsolete because changes to it will no longer replicate to the new system -- they're permanently isolated.</p",False,True
459,56003144,2,55918088,2019-05-06T10:16:55.057,3,"<p>I am part of the team building the new Tag Editor. Yes, you are correct: Classic Tag Editor is deprecated, and will be shut down soon entirely. We are working on full feature parity between the two Editors, so you will very soon find everything you can do in the old one as well in the new one.</p>

<p>To add some more context on your different items below:</p>

<p>1) Both old and new Tag Editor use the same underlying tagging infrastructure, so this should never happen. Maybe there is some browser issue involved here? Feel free to open a support issue so we can look deeper into it, if this continues the case.</p>

<p>2) Yes, the new one also includes Lambda, and will very soon add more resource types. The same by the way for regions: The old Tag Editor supports not all regions, for example <code>eu-north-1</code> or <code>eu-west-3</code>.</p>

<p>3) No, Route53 Hosted Zones are supported in both Editors. Route53 resources only exists in the <code>us-east-1</code> region, so maybe you used the Tag Editor in another region?</p>

<p>4) Both Editors show the same data. The old editor merged what you used as Name Tag and the ID in the same field - in the new one, you see only the ID in the column ID, and the Name Tag is displayed in the column <code>Tag: Name</code>.</p>

<p>Searching across regions is something the new Editor soon will support, too, and the same applies for the filter you mention. For showing resources without a specific tag, there is a workaround you already can do: Click on the settings icon in the top right of the table, and enable the tag you are interested in as a column. You then can sort this column so that all untagged ones show up on top.</p>

<p>If you have any other ideas or requests for the Tag Editor, please let us know. The fastest and most reliable way is to just use the 'Feedback' Button in the console in the bottom left.</p>

<p>Cheers,
Florian </p>
",972592,2019-05-06T10:16:55.057,2,CC BY-SA 4.0,,1,Editor,.,0,"Yes, you are correct: Classic Tag Editor is deprecated, and will be shut down soon entirely.",False,False
460,56038511,2,56034053,2019-05-08T10:23:02.063,6,"<p>Put <code>async</code> back to its place as using callbacks is outdated and much more error prone. Use the built-in <code>promise()</code> methods available on the node.js aws-sdk and just <code>await</code> on these promises. If you want to deal with errors, just surround your code with a <code>try/catch</code> block.</p>

<pre><code>const AWS = require('aws-sdk')

const dynamodb = new AWS.DynamoDB.DocumentClient({region: 'ap-southeast-2'});

exports.handler = async (event) =&gt; {

    const params = {
        TableName: 'people-dev',
        Item: {
            id: '1',
            name: 'person',
            email: 'person@example.com'
        }
    };

    await dynamodb.put(params).promise()

    return {
        statusCode: 200,
        body: JSON.stringify({message: 'Success'})
    }

};
</code></pre>

<p>More on <a href=""https://javascript.info/async-await"" rel=""noreferrer"">async/await</a></p>
",10950867,2019-05-08T10:23:02.063,0,CC BY-SA 4.0,,1,back,.,0,back to its place as using callbacks is outdated and much more error prone.,False,False
461,56053053,2,55982706,2019-05-09T06:00:31.453,0,"<p>From S3 perspective you can't check free space.</p>

<p>Ceph has implemented expiration mechanism, which will delete outdated objects from your object storage.
Check the doc: <a href=""http://docs.ceph.com/docs/master/radosgw/s3/#features-support"" rel=""nofollow noreferrer"">http://docs.ceph.com/docs/master/radosgw/s3/#features-support</a></p>
",1881757,2019-05-09T06:00:31.453,1,CC BY-SA 4.0,,1,,"
",0,"Ceph has implemented expiration mechanism, which will delete outdated objects from your object storage.
",False,False
462,56087567,2,56085647,2019-05-11T05:45:00.143,0,"<p>You should not need to have to create each and every type, what you
would need to do is make a generic tag handling routine, that looks at
the type of node the tag is on (mapping, sequence, scalar), then
creates such a node as a Ruby type on which the tag can be attached. </p>

<p>I don't know how to do that with <code>Psych</code> and <code>Ruby</code>, but you indicated
neither is a strict requirement, and most of the hard work
for this kind of round-tripping in <code>ruamel.yaml</code> for <code>Python</code>
(disclaimer: I am the author of that package). </p>

<p>If this is your input
file <code>input.yaml</code>:</p>

<pre><code>Foo: !Bar baz
N1:
   - !mytaggedmaptype
     parm1: 3
     parm3: 4
   - !mytaggedseqtype
     - 8
     - 9
N2: &amp;someanchor1
   a: ""some stuff""
   b: 0.2e+1
   f: |
     within a literal scalar newlines
     are preserved
N3: &amp;someanchor2
   c: 0x3
   b: 4    # this value is not taken, as the first entry found is taken 
   ['the', 'answer']: still unknown
   {version: 28}: tested!
N4:
   d: 5.000
   &lt;&lt;: [*someanchor1, *someanchor2]
</code></pre>

<p>Then this Python (3) program:</p>

<pre><code>import sys
from pathlib import Path
import ruamel.yaml

yaml_in = Path('input.yaml')
yaml_out = Path('output.yaml')


yaml = ruamel.yaml.YAML()
yaml.preserve_quotes = True
# uncomment next line if your YAML is the outdated version 1.1 YAML but has no tag
# yaml.version = (1, 1) 
data = yaml.load(yaml_in)

# do your updating here
data['Foo'].value = 'hello world!'  # see the first of the notes
data['N1'][0]['parm3'] = 4444
data['N1'][0].insert(1, 'parm2', 222)
data['N1'][1][1] = 9999
data['N3'][('the', 'answer')] = 42

# and dump to file
yaml.dump(data, yaml_out)
</code></pre>

<p>creates <code>output.yaml</code>:</p>

<pre><code>Foo: !Bar hello world!
N1:
- !mytaggedmaptype
  parm1: 3
  parm2: 222
  parm3: 4444
- !mytaggedseqtype
  - 8
  - 9999
N2: &amp;someanchor1
  a: ""some stuff""
  b: 0.2e+1
  f: |
    within a literal scalar newlines
    are preserved
N3: &amp;someanchor2
  c: 0x3
  b: 4     # this value is not taken, as the first entry found is taken 
  ['the', 'answer']: 42
  {version: 28}: tested!
N4:
  d: 5.000
  &lt;&lt;: [*someanchor1, *someanchor2]
</code></pre>

<p>Please note:</p>

<ul>
<li><p>you can update tagged scalars while preserving the tag on the
scalar, but since you replace such a scalar with its assignment
(instead of updating a value as with lists (sequences/arrays) or
dicts (mappings/hashes), you cannot just assign the new value or
you'll lose the tagging information, you have to update the <code>.value</code> attribute.</p></li>
<li><p>things like anchors, merges, comments, quotes are preserved, as are
special forms of integers (hex, octal, etc) and floats.</p></li>
<li><p>For YAML sequences that are mapping keys, you need to use a tuple
(<code>('the', 'answer')</code>) instead of a sequence (<code>['the', 'answer']</code>),
as Python doesn't allow mutable keys in mappings. And for YAM
mappings that are mapping keys you would need to use the immutable
<code>Mapping</code> from
<a href=""https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping"" rel=""nofollow noreferrer""><code>collections.abc</code></a>.
(I am not sure if Psych supports these kind of valid YAML keys)</p></li>
<li><p>See <a href=""https://stackoverflow.com/a/55717146/1307905"">this</a> if you need to update anchored/aliased scalars</p></li>
</ul>
",1307905,2019-05-11T05:45:00.143,1,CC BY-SA 4.0,,1,,"
",0,"if your YAML is the outdated version 1.1 YAML but has no tag
",False,False
463,56212216,2,56212071,2019-05-19T21:38:01.500,1,"<p>PHP 7.0 is obsolete -- <a href=""https://php.net/eol.php"" rel=""nofollow noreferrer"">it reached end-of-life in January 2019</a> -- and it is not available in Ubuntu 18.04. The appropriate version for this release is PHP 7.2 (e.g. <a href=""https://packages.ubuntu.com/bionic/php7.2"" rel=""nofollow noreferrer""><code>php7.2</code></a>, <a href=""https://packages.ubuntu.com/bionic/php7.2-mysql"" rel=""nofollow noreferrer""><code>php7.2-mysql</code></a>, etc); you will need to change your install commands accordingly.</p>

<p>The mcrypt extension <a href=""https://www.php.net/manual/en/migration71.deprecated.php"" rel=""nofollow noreferrer"">was deprecated in PHP 7.1</a> and subsequently <a href=""https://www.php.net/manual/en/migration72.other-changes.php"" rel=""nofollow noreferrer"">removed in PHP 7.2</a>. If you are using this extension in your application, <a href=""https://stackoverflow.com/questions/41272257/mcrypt-is-deprecated-what-is-the-alternative"">you will need to rework those parts of your code</a>.</p>
",149341,2019-05-19T21:38:01.500,0,CC BY-SA 4.0,,4,PHP,--,0,PHP 7.0 is obsolete --,False,False
464,56234110,2,37385411,2019-05-21T08:24:36.103,0,"<p>As the error message rather clearly tells you, you somehow ended up passing in <code>169.254.169.254http</code> as the host name, and <code>169.254.169.254http:80</code> as the host.  Just to spell this out completely, you probably wanted the host to be <code>169.254.169.254</code>. You need to figure out why your request was botched like this, and correct the code or your configuration files so you send what you wanted to send.</p>

<p><code>ENOTFOUND</code> in response to <code>getaddrinfo</code> simply means that you wanted to obtain the address of something which doesn't exist or is unknown. Very often this means that you have a typo, or that the information you used to configure your service is obsolete or otherwise out of whack (attempting to reach a private corporate server when you are outside the  corporate firewall, for example).</p>
",874188,2019-05-21T08:24:36.103,0,CC BY-SA 4.0,,1,this,"
",0,"Very often this means that you have a typo, or that the information you used to configure your service is obsolete or otherwise out of whack (attempting to reach a private corporate server when you are outside the  corporate firewall, for example).</p>
",False,False
465,56251044,2,56248883,2019-05-22T07:02:09.373,0,"<p><a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBMarshalling.html"" rel=""nofollow noreferrer"">DynamoDBMarshalling</a> is deprecated, so I suggest using the newer <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBTypeConverted.html"" rel=""nofollow noreferrer"">DynamoDBTypeConverted</a> annotation. </p>

<p>There are some useful notes on <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.ArbitraryDataMapping.html"" rel=""nofollow noreferrer"">Mapping Arbitrary Data</a>.</p>

<p>You can also see an example of mine in <a href=""https://stackoverflow.com/questions/30793481/dynamodb-jsonmarshaller-cannot-deserialize-list-of-object/47153880#47153880"">this answer</a></p>

<p>In summary, you create an <code>AllCities</code> plain java object. You then write a simple converter class which tells DynamoDB how to turn your <code>AllCities</code> object into a string to get into DynamoDB. Similarly, the converter class tells your application how to turn the string back into a Java object.</p>
",4985580,2019-05-22T07:02:09.373,1,CC BY-SA 4.0,,1,,.,0,"> is deprecated, so I suggest using the newer <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBTypeConverted.html"" rel=""nofollow noreferrer"">DynamoDBTypeConverted</a> annotation.",False,True
466,56376017,2,56368882,2019-05-30T10:04:58.430,0,"<p>Bitnami Engineer here.</p>

<p>Let's try these changes to fix the errors you are running into:</p>

<ul>
<li>Remove the following block</li>
</ul>

<pre><code>        &lt;Directory /&gt;
                Options -Indexes
                AllowOverride All
        &lt;/Directory&gt;
</code></pre>

<p>You only need the one that sets the configuration for the folder where you have your app's files.</p>

<ul>
<li>You will need to substitute these lines because they are not supported in the new Apache's version </li>
</ul>

<pre><code>Order allow,deny
allow from all
</code></pre>

<p>is deprecated. You need to use <code>Require all granted</code> when using Apache 2.4</p>

<p><a href=""https://httpd.apache.org/docs/2.4/mod/mod_authz_core.html#require"" rel=""nofollow noreferrer"">https://httpd.apache.org/docs/2.4/mod/mod_authz_core.html#require</a></p>

<ul>
<li>Set the permissions of your app's folder properly</li>
</ul>

<pre><code>sudo chown -R bitnami:daemon /opt/bitnami/apache2/site1.com/htdocs
sudo chmod -R g+w /opt/bitnami/apache2/site1.com/htdocs
</code></pre>
",4709625,2019-05-30T10:04:58.430,1,CC BY-SA 4.0,,1,>,.,0,<p>is deprecated.,False,False
467,56391585,2,56224913,2019-05-31T08:43:21.403,1,"<p>You should use DynamoDB because it has a lot of useful features such as <a href=""https://aws.amazon.com/blogs/aws/new-amazon-dynamodb-continuous-backups-and-point-in-time-recovery-pitr/"" rel=""nofollow noreferrer"">Point in Time Recovery</a>, <a href=""https://aws.amazon.com/about-aws/whats-new/2018/11/announcing-amazon-dynamodb-support-for-transactions/"" rel=""nofollow noreferrer"">transactions</a>, <a href=""https://aws.amazon.com/blogs/security/now-available-encryption-at-rest-for-amazon-dynamodb/"" rel=""nofollow noreferrer"">encryption-at-rest</a>, and <a href=""https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/"" rel=""nofollow noreferrer"">activity streams</a> that SimpleDB does not have.</p>

<p>If you're operating on a small scale, DynamoDB has the advantage that it allows you to set a maximum capacity for your table, which means you can make sure you stay in the free tier.</p>

<p>If you're operating at a larger scale, DynamoDB automatically handles all of the partitioning of your data (and has, for all practical purposes, limitless capacity), whereas <a href=""https://docs.aws.amazon.com/AmazonSimpleDB/latest/DeveloperGuide/SDBLimits.html"" rel=""nofollow noreferrer"">SimpleDB has a limit of 10 GB</a> per domain (aka ""table"") and you are required to manage any horizontal partitioning across domains that you might need.</p>

<p>Finally, there are signs that SimpleDB is already on a deprecation path. For example, if you look at the <a href=""https://aws.amazon.com/releasenotes/?tag=releasenotes%23keywords%23amazon-simpledb"" rel=""nofollow noreferrer"">SimpleDB release notes</a>, you will see that the last update was in 2011, whereas DynamoDB had several new features announced at the last re:Invent conference. Also, there are a number of reddit posts (such as <a href=""https://www.reddit.com/r/aws/comments/a3280t/no_amazon_simpledb/"" rel=""nofollow noreferrer"">here</a>, <a href=""https://www.reddit.com/r/aws/comments/8rwov0/whats_wrong_with_simpledb_and_should_i_use_it/"" rel=""nofollow noreferrer"">here</a>, and <a href=""https://www.reddit.com/r/aws/comments/2iuw11/cant_find_simpledb/"" rel=""nofollow noreferrer"">here</a>) where the general consensus is that SimpleDB is already deprecated, and in some of the threads, <a href=""https://aws.amazon.com/blogs/aws/author/jbarr/"" rel=""nofollow noreferrer"">Jeff Barr</a> even commented and did not contradict any of the assertions that SimpleDB is deprecated.</p>

<hr>

<p>That being said, in DynamoDB, you can support your desired queries. 
You will need two <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html"" rel=""nofollow noreferrer"">Global Secondary Indexes</a>, which use a <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-sort-keys.html"" rel=""nofollow noreferrer"">composite sort key</a>. Your queries can be supported with the following schema:</p>

<ul>
<li><code>ID</code> — hash key of your table</li>
<li><code>Author</code> — hash key of the <code>Author-Status-CreatedDateTime-index</code></li>
<li><code>Category</code> — hash key of the <code>Category-Status-CreatedDateTime-index</code></li>
<li><code>Status</code></li>
<li><code>CreatedDateTime</code></li>
<li><code>UpdatedDateTime</code></li>
<li><code>Status-CreatedDateTime</code> — sort key of <code>Author-Status-CreatedDateTime-index</code> and <code>Category-Status-CreatedDateTime-index</code>. This is a composite attribute that exists to enable some of your queries. It is simply the value of <code>Status</code> with a separator character (I'll assume it's <code>#</code> for the rest of this answer), and <code>CreatedDateTime</code> appended to the end. (Personal opinion here: use <a href=""https://en.wikipedia.org/wiki/ISO_8601"" rel=""nofollow noreferrer"">ISO-8601</a> timestamps instead of unix timestamps. It will make troubleshooting a lot easier.)</li>
</ul>

<p>Using this schema, you can satisfy all of your queries.</p>

<p><strong>query by ID:</strong>
Simply perform a <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_GetItem.html"" rel=""nofollow noreferrer""><code>GetItem</code></a> request on the main table using the blog post Id.</p>

<p><strong>query by Author, paginated:</strong>
Perform a <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html"" rel=""nofollow noreferrer""><code>Query</code></a> on the <code>Author-Status-CreatedDateTime-index</code> with a key condition expression of <code>Author = :author</code>.</p>

<p><strong>query by (Author, Status), sorted by CreatedDateTime, paginated:</strong> 
Perform a <code>Query</code> on the <code>Author-Status-CreatedDateTime-index</code> with a key condition expression of <code>Author = :author and begins_with(Status-CreatedDateTime, :status)</code>. The results will be returned in order of ascending <code>CreatedDateTime</code>.</p>

<p><strong>query by (Category, Status), sorted by CreatedDateTime, paginated:</strong> 
Perform a <code>Query</code> on the <code>Category-Status-CreatedDateTime-index</code> with a key condition expression of <code>Author = :author and begins_with(Status-CreatedDateTime, :status)</code>. The results will be returned in order of ascending <code>CreatedDateTime</code>. (Additionally, if you wanted to get all the blog posts in the ""technology"" category that have the status <code>published</code> and were created in 2019, you could use a key condition expression of <code>Category = ""technology"" and begins_with(Status-CreatedDateTime, ""published#2019"")</code>.</p>

<p>The sort order of the results can be controlled using the <code>ScanIndexForward</code> field of the Query request. The default is <code>true</code> (sort ascending); but by setting it to <code>false</code> DynamoDB will return results in descending order.</p>

<p>DynamoDB has built in support for paginating the results of a Query operation. Basically, any time that there are more results that were not returned, the query response will contain a <code>lastEvaluatedKey</code> which you can pass into your next query request to pick up where you left off. (See <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html#Query.Pagination"" rel=""nofollow noreferrer"">Query Pagination</a> for more details about how it works.)</p>

<hr>

<p>On the other hand, if you're already familiar with SQL, and you want to make this as easy for yourself as possible, consider just using the <a href=""https://aws.amazon.com/blogs/aws/new-data-api-for-amazon-aurora-serverless/"" rel=""nofollow noreferrer"">Aurora Serverless Data API</a>.</p>
",5563569,2019-05-31T08:43:21.403,3,CC BY-SA 4.0,,1,,deprecated.</p,1,"Also, there are a number of reddit posts (such as <a href=""https://www.reddit.com/r/aws/comments/a3280t/no_amazon_simpledb/"" rel=""nofollow noreferrer"">here</a>, <a href=""https://www.reddit.com/r/aws/comments/8rwov0/whats_wrong_with_simpledb_and_should_i_use_it/"" rel=""nofollow noreferrer"">here</a>, and <a href=""https://www.reddit.com/r/aws/comments/2iuw11/cant_find_simpledb/"" rel=""nofollow noreferrer"">here</a>) where the general consensus is that SimpleDB is already deprecated, and in some of the threads, <a href=""https://aws.amazon.com/blogs/aws/author/jbarr/"" rel=""nofollow noreferrer"">Jeff Barr</a> even commented and did not contradict any of the assertions that SimpleDB is deprecated.</p",False,False
468,56401067,2,56392645,2019-05-31T19:58:44.883,1,"<p>""Good choices"" depend on context, and here the contexts differ. A Device Shadow is meant to track and accurately reflect the state of the device. The page you linked does not discuss having multiple shadows for one device, and may not have been written with multiple receivers in mind. Still, its simple topic architecture would work in a system that had multiple shadows per device because a <em>shadow</em> should receive all responses from its device. Any of those responses could reveal a change in device state, and any shadow whose client does not receive the response will be out of date, and out of sync with the other shadows.</p>

<p><em>Your masters do not sound like shadows.</em> Perhaps they independently report their results to a data store that functions as the shadow. Perhaps nothing in the responses represents a state change worth tracking beyond the request. Either way, the documentation does not sound relevant to your goal. </p>

<p>I support your preference for the first choice, especially as the number of nodes grows. The main drawback is the extra work keeping track of the requesting master ID. It's easy for the masters (each can just subscribe to <code>mySystem/+/res/masterId</code>, and in a system with topic access control you could even isolate them). If the request body is otherwise simple (not already multiple attributes to parse), you might consider having the masters post to <code>mysystem/slaveId/req/masterId</code> (slave could subscribe to <code>mysystem/slaveId/req/+</code>).</p>

<p>The biggest example to take from AWS might be the clear sense of hierarchy in the topics. If the parsing convenience of having the masterId at the end of the topic is not your top concern, it might make more sense to order as: <code>mysystem/masterId/slaveId/req</code> (or <code>res</code>). Very dependent on your system and goals.</p>
",3960558,2019-05-31T19:58:44.883,0,CC BY-SA 4.0,,1,,>,1,"Any of those responses could reveal a change in device state, and any shadow whose client does not receive the response will be outdated, and out of sync with the other shadows.</p>

<p><em>",False,False
469,56411234,2,56405090,2019-06-01T23:52:37.257,3,"<p>You can point to an Amazon S3 bucket using the <code>s3.amazonaws.com/bucketname</code> method, however:</p>

<ul>
<li>This does not enable the <strong>Static Website Hosting</strong> features (such as using default <code>index</code> and <code>error</code> pages)</li>
<li><strong>It is being deprecated</strong>. See: <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/"" rel=""nofollow noreferrer"">Amazon S3 Path Deprecation Plan – The Rest of the Story | AWS News Blog</a></li>
</ul>
",174777,2019-06-01T23:52:37.257,3,CC BY-SA 4.0,,1,,deprecated</strong,0,It is being deprecated</strong,False,False
470,56420710,2,49918991,2019-06-03T03:11:13.870,0,"<p>Note: so far as I can tell, adding 'replica' servers does nothing unless you are using the db_* functional calls (which are deprecated), or if you manually instantiate the database.replica connection in any of your custom queries
e.g. 
<code>/** @var \Drupal\Core\Database\Connection $database_replica */
$database_replica = \Drupal::service('database.replica');
$query = $database_replica-&gt;select('node', 'n');....</code></p>
",11591707,2019-06-03T03:11:13.870,0,CC BY-SA 4.0,,1,,"
",0,"so far as I can tell, adding 'replica' servers does nothing unless you are using the db_* functional calls (which are deprecated), or if you manually instantiate the database.replica connection in any of your custom queries
",False,True
471,56511181,2,56509693,2019-06-09T00:50:27.133,3,"<p><a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html"" rel=""nofollow noreferrer"">CloudFormer</a> has <a href=""https://forums.aws.amazon.com/ann.jspa?annID=1048"" rel=""nofollow noreferrer"">been in beta since 2011</a>. It does not appear to have been maintained much lately, so it might be deprecated in future.</p>

<p>So, it looks like you'll need to add the User Data section manually.</p>
",174777,2019-06-09T00:50:27.133,3,CC BY-SA 4.0,,1,,future.</p,1,"It does not appear to have been maintained much lately, so it might be deprecated in future.</p",False,False
472,56511427,2,56510763,2019-06-09T01:59:02.603,3,"<p>As far as I know, this is legacy functionality, no longer serving any useful purpose.  Many years ago, X.509 certificates were used to sign requests for AWS SOAP APIs.  </p>

<p>All of these are likely deprecated if not gone entirely.</p>

<p>In some cases -- like EC2 -- the SOAP functionality was deprecated and later <a href=""https://docs.aws.amazon.com/AWSEC2/latest/APIReference/using-soap-api.html"" rel=""nofollow noreferrer"">completely removed from the API and SDKs in late 2015</a>.</p>

<p>So this IAM feature would only be used for legacy systems using other AWS services that still support the old SOAP APIs that expect X.509-based authentication, assuming there are any.  If there are, the documentation has long since been deleted or is buried, as is the case for <a href=""https://docs.aws.amazon.com/AmazonS3/latest/API/APISoap.html"" rel=""nofollow noreferrer"">S3</a>, which only supports SOAP over HTTPS these days, and appears to expect Access Keys rather than certificates.  </p>

<p>Perhaps SimpleDB still supports the old SOAP API.  SimpleDB?  One of the original Amazon Web Services like SQS and S3, SimpleDB was -- sort of, kind of, in a manner of speaking, loosely -- a predecessor to DynamoDB... it was quite an innovation back in its day, and it is still alive though perhaps not well, and you will be hard-pressed to find anyone talk about it, or find much evidence of it in the AWS documentation, though it's there <a href=""https://aws.amazon.com/simpledb/"" rel=""nofollow noreferrer"">if you know where to look</a>.  SimpleDB hasn't been deployed in any AWS region that launched after about 2013.</p>
",1695906,2019-06-09T01:59:02.603,1,CC BY-SA 4.0,,2,All,entirely.</p,1,All of these are likely deprecated if not gone entirely.</p,False,False
473,56703700,2,56698407,2019-06-21T12:42:51.123,0,"<p><code>AWSIotDataClient</code> is not deprecated, just the constructors are deprecated, as are the constructors of all <code>AWSClient</code> implementations in favor of builders. You should use <code>AwsClientBuilder.build()</code> to obtain an instance of <code>AWSIotDataClient</code>. Then you can call the <code>publish()</code> method on the <code>AWSIotDataClient</code> instance to publish to your IoT topic.</p>
",13070,2019-06-21T12:42:51.123,3,CC BY-SA 4.0,,1,constructors,code,1,"AWSIotDataClient</code> is not deprecated, just the constructors are deprecated, as are the constructors of all <code",False,False
474,56744352,2,56742001,2019-06-24T21:48:27.977,1,"<p>If you drop the tables you will lose assigned permissions to these tables. If you have views for these tables they will be obsolete.</p>

<p>Truncate is a better option, truncate does not require vacuum or analyze, it is built for use cases like this. </p>

<p>For further info <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwi8kpTVi4PjAhVRG80KHYgnBWIQFjABegQIChAE&amp;url=https%3A%2F%2Fdocs.aws.amazon.com%2Fredshift%2Flatest%2Fdg%2Fr_TRUNCATE.html&amp;usg=AOvVaw0y8mi4pI66UU9tNhAw1xr1"" rel=""nofollow noreferrer"">Redshift Truncate documentation</a></p>
",1628812,2019-06-24T21:48:27.977,0,CC BY-SA 4.0,,1,,"

",0,"obsolete.</p>

",False,False
475,56745301,2,54011315,2019-06-25T00:10:29.290,0,"<h1>Update Your Runtime</h1>

<p>The <code>transactWrite</code> function was added in a later version of the AWS SDK. To access the updated AWS SDK, switch your Lambda function to a newer runtime.</p>

<p>AWS does not update the AWS SDK version for old runtimes. For example, if you are using the outdated <code>nodejs8.10</code> runtime, you only have access to version <code>2.290.0</code> of the AWS SDK which does not support <code>transactWrite</code>. If you switch your runtime to <code>nodejs10.x</code>, you get version <code>2.437.0</code> (at the time of this post) which does support DynamoDB transactions.</p>

<p>View the complete list of SDK versions available in each runtime here:
<a href=""https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html"" rel=""nofollow noreferrer"">AWS Lambda Runtimes</a></p>

<h2>How To Update Your Runtime</h2>

<p>To update your runtime, click the ""Runtime"" dropdown in the ""Function Code"" section of your Lambda function and select an updated runtime.</p>

<p><a href=""https://i.stack.imgur.com/ZS8sA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZS8sA.png"" alt=""Update Lambda runtime""></a></p>
",3305145,2019-06-25T00:10:29.290,0,CC BY-SA 4.0,,1,,nodejs8.10</code,0,"For example, if you are using the outdated <code>nodejs8.10</code",False,False
476,56837581,2,54410850,2019-07-01T14:34:09.787,1,"<p>You DON'T NEED Username and Password in your Python Script to authenticate!
What you need is <strong>CLIENT_ID, SCOPE</strong> and <strong>REDIRECT_URI</strong> and three requests:</p>

<ol>
<li><p>Get authorization code:</p>

<p><code>GET https://www.amazon.com/ap/oa?client_id={{CLIENT_ID}}&amp;scope={{SCOPE}}&amp;response_type=code&amp;redirect_uri={{REDIRECT_URI}}</code></p></li>
</ol>

<p>This will open the 'Login with Amazon' Consent Page, where you (or your customer) log into your Amazon Seller Central account and grant access to the Console APP with API access rights.</p>

<ol start=""2"">
<li><p>Request tokens</p>

<p><code>POST https://api.amazon.com/auth/o2/token</code></p>

<p>with headers:</p>

<p><code>Content-Type:application/x-www-form-urlencoded</code></p>

<p>with body data:</p>

<pre><code>grant_type:authorization_code
code:{{AUTH_CODE}}    &lt;----- returned from step 1
client_id:{{CLIENT_ID}}
client_secret:{{CLIENT_SECRET}}
redirect_uri:{{REDIRECT_URI}}
</code></pre></li>
<li><p>Get/Refresh <strong>access token</strong> (every time it is outdated):</p>

<p><code>POST https://api.amazon.com/auth/o2/token</code></p>

<p>with headers:</p>

<pre><code>Content-Type:application/x-www-form-urlencoded
charset:UTF-8
</code></pre>

<p>with body data:</p>

<pre><code>grant_type:refresh_token
refresh_token:{{REFRESH_TOKEN}}   &lt;------ returned from step 2
client_id:{{CLIENT_ID}}
client_secret:{{CLIENT_SECRET}}
</code></pre></li>
</ol>

<hr>

<ol start=""4"">
<li><p>With the CLIENT_ID and (fresh) access token you can now request every service from the API. For excample listCampaigns:</p>

<p><code>GET https://advertising-api.amazon.com/v2/sp/campaigns</code></p>

<p>Headers:</p>

<pre><code>Content-Type:application/json
Amazon-Advertising-API-ClientId:{{CLIENT_ID}}
Amazon-Advertising-API-Scope:{{PROFILE_ID}}
Authorization:Bearer {{ACCESS_TOKEN}}   &lt;----- returned from step 3
</code></pre></li>
</ol>
",850547,2019-07-01T14:34:09.787,5,CC BY-SA 4.0,,1,,"

",0,"(every time it is outdated):</p>

",False,False
477,56913209,2,37365009,2019-07-06T10:25:53.253,0,"<p>Approach given in the accepted answer is now deprecated. Answer given by the user @dassum is the approach to be followed, but the answer lacks a bit of explanation.</p>

<p>When creating the InvokeRequest, set the InvocationType as ""Event"" for asynchronous invocation and ""RequestResponse"" for synchronous invocation. </p>

<pre><code>AWSLambda lambda = //create your lambda client here
lambda.invoke(new InvokeRequest()
                            .withFunctionName(LAMBDA_FUNCTION_NAME)
                            .withInvocationType(InvocationType.Event) //asynchronous
                            .withPayload(payload))
</code></pre>

<p>Reference to docs:</p>

<p><a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/lambda/AWSLambda.html#invoke-com.amazonaws.services.lambda.model.InvokeRequest-"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/lambda/AWSLambda.html#invoke-com.amazonaws.services.lambda.model.InvokeRequest-</a></p>
",1800900,2019-07-06T10:25:53.253,0,CC BY-SA 4.0,,1,Approach,.,0,>Approach given in the accepted answer is now deprecated.,False,False
478,57001791,2,57000602,2019-07-12T07:06:48.000,0,"<p>Use <code>S3TransferUtility</code> instead of the deprecated TransferManager. 
Sample code is available here <a href=""https://github.com/awslabs/aws-sdk-ios-samples/tree/master/S3TransferUtility-Sample/Swift"" rel=""nofollow noreferrer"">https://github.com/awslabs/aws-sdk-ios-samples/tree/master/S3TransferUtility-Sample/Swift</a> </p>
",663360,2019-07-12T07:06:48.000,0,CC BY-SA 4.0,,1,,"
",0,"> instead of the deprecated TransferManager. 
",False,False
479,57100724,2,57100171,2019-07-18T18:33:34.780,0,"<p>From PHP documentation <code>mysql</code> class is deprecated since version 5.5 and removed in version 7. that's why you're receiving the class not found error.</p>

<p>Try using <code>mysqli</code>. The PHP documentation has good examples on this. Look at example 2. I've pasted part of the example in this post for you.
<a href=""https://www.php.net/manual/en/function.mysql-connect.php"" rel=""nofollow noreferrer"">https://www.php.net/manual/en/function.mysql-connect.php</a></p>

<pre><code>&lt;?php
$mysqli = new mysqli(""example.com"", ""user"", ""password"", ""database"");
if ($mysqli-&gt;connect_errno) {
    echo ""Failed to connect to MySQL: "" . $mysqli-&gt;connect_error;
}

$res = $mysqli-&gt;query(""SELECT 'choices to please everybody.' AS _msg FROM DUAL"");
$row = $res-&gt;fetch_assoc();
echo $row['_msg'];
?&gt;
</code></pre>
",864538,2019-07-18T18:33:34.780,1,CC BY-SA 4.0,,1,class,.,0,>mysql</code> class is deprecated since version 5.5 and removed in version 7.,False,False
480,57136371,2,42892451,2019-07-21T19:37:46.667,1,"<p>I had the same issue. And the problem was that my ssh client was outdated.</p>

<p>You can solve by downloading the latest binaries: <a href=""https://github.com/PowerShell/Win32-OpenSSH/releases"" rel=""nofollow noreferrer"">OpenSSH Binaries Windows</a></p>

<p>Just extract the zip file and add the folder into your environment variables and you won't see that problem anymore :)</p>
",4051258,2019-07-21T19:37:46.667,0,CC BY-SA 4.0,,1,,outdated.</p,0,And the problem was that my ssh client was outdated.</p,False,False
481,57271489,2,56966682,2019-07-30T12:17:20.970,0,"<p>I worked around this issue by changing logical ids of lambdas that I was updating. This way, CloudFormation deletes old lambdas and creates brand new lambdas with new runtime. Unfortunately, this seems to be the only way for now as updates to any lambda pointing to 2.0 runtime is deprecated (event if the update is upgrading the runtime).</p>

<p>Not an ideal way to do it, but does the job.  </p>
",2717391,2019-07-30T12:17:20.970,0,CC BY-SA 4.0,,1,,runtime).</p,0,"Unfortunately, this seems to be the only way for now as updates to any lambda pointing to 2.0 runtime is deprecated (event if the update is upgrading the runtime).</p",False,False
482,57341974,2,57281604,2019-08-03T20:46:10.050,0,"<p>Okay, here's what I've learned.</p>

<p>As of version 1.18.0, the call to <code>getCredentials</code> does <em>NOT</em> consider id token expiration.  It <em>only</em> checks if the access token is expired, and if it is, it will then refresh the <code>id_token</code> and <code>access token</code>.  Unfortunately the <code>access token</code> expiry is locked in at 24 hours unless you do additional work.</p>

<p>Make sure you have <code>setOIDCCompliant</code> to <code>true</code> when you create your Auth0Account instance, or else the call to renew will hit the <code>/delegation</code> endpoint which is now deprecated and only works if your client id is setup to support non oidc compliant calls.</p>

<p>One other thing to be aware of that's somewhat off topic.  The <code>SecureCredentialsManager</code> clears out credentials if anything appears to go wrong.  This isn't acceptable for me, as simply being offline and being unable to make the call causes the credentials to be cleared.</p>
",2080104,2019-08-03T20:46:10.050,0,CC BY-SA 4.0,,1,,support,0,"> when you create your Auth0Account instance, or else the call to renew will hit the <code>/delegation</code> endpoint which is now deprecated and only works if your client id is setup to support",False,False
483,57385215,2,57380939,2019-08-06T23:32:54.420,2,"<p>Bucket names are in the format:</p>

<pre><code>http://BUCKETNAME.s3-REGION.amazonaws.com
</code></pre>

<p>Examples:</p>

<pre><code>http://mybucket.s3-ap-southeast-2.amazonaws.com
http://my.bucket.name.has.dots.s3-us-west-2.amazonaws.com
http://invoices.s3.amazonaws.com   (Defaults to us-east-1, can involve redirects to other regions)
</code></pre>

<p>Note the period after the bucket name and the dash in the <code>s3-REGION</code> portion.</p>

<p>There is also an older format:</p>

<pre><code>http://s3-ap-southeast-2.amazonaws.com/mybucket
</code></pre>

<p>However, that format is being deprecated. See: <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/"" rel=""nofollow noreferrer"">Amazon S3 Path Deprecation Plan – The Rest of the Story | AWS News Blog</a></p>
",174777,2019-08-06T23:32:54.420,0,CC BY-SA 4.0,,1,format,.,0,"However, that format is being deprecated.",False,False
484,57457807,2,54566268,2019-08-12T08:25:07.270,0,"<p>It started happening for me, when my AWS password has expired and i had reset it. Locally git started giving me a 403 error because of outdated credentials. I was trying to connect to codecommit with my normal aws account login credentials , which again gave me the same 403 error. Finally i realized that i need to generate new git credentials through AWS account settings and use them . That solved my problem.</p>
",4406866,2019-08-12T08:25:07.270,0,CC BY-SA 4.0,,1,,.,0,Locally git started giving me a 403 error because of outdated credentials.,False,False
485,57466036,2,57456318,2019-08-12T18:00:25.373,0,"<blockquote>
  <p>I am calling the following REST API for fetching email id of the user
  by passing Bearer token (consent token in permissions object received
  in JSON input)</p>
</blockquote>

<p>According to this <a href=""https://developer.amazon.com/docs/custom-skills/device-address-api.html"" rel=""nofollow noreferrer"">doc</a>, <code>consentToken</code> has been deprecated, you should use <code>apiAccessToken</code> instead.</p>

<blockquote>
  <p>Important: Requests from Alexa may also include a <code>consentToken</code> within
  <code>session.user.permissions</code> and <code>context.System.user.permissions</code>. This
  property is deprecated. Existing skills that use <code>consentToken</code> continue
  to work, but the <code>context.System.apiAccessToken</code> property should be used
  instead.</p>
</blockquote>

<p>Thus, <a href=""https://developer.amazon.com/docs/custom-skills/request-customer-contact-information-for-use-in-your-skill.html#get-the-api-access-token"" rel=""nofollow noreferrer""><code>accessToken = this.event.context.System.apiAccessToken</code></a>.</p>

<p>Also, double-check your header: <code>{""Authorization"": ""Bearer "" + apiAccessToken}</code>, make sure you have a space between <code>Bearer</code> and <code>apiAccessToken</code>.</p>
",2072811,2019-08-12T18:00:25.373,2,CC BY-SA 4.0,,2,,>,0,"> has been deprecated, you should use <code>",False,False
486,57523535,2,57517794,2019-08-16T11:06:18.997,0,"<p>This link </p>

<p><a href=""https://github.com/tinkerpop/gremlin/wiki/Traversal-Optimization"" rel=""nofollow noreferrer"">https://github.com/tinkerpop/gremlin/wiki/Traversal-Optimization</a></p>

<p>explicitly states that it refers to ""an outdated version of the TinkerPop framework and Gremlin language documentation"" - please ignore that...it is for TinkerPop 2.x.</p>

<p>That said, the <code>profile()</code> step is the best that Gremlin directly offers and it can tel you a lot about query execution as you can identify which steps are running slowest and if you are seeing the expected number of traversers at specific parts of your query.</p>

<p>If you need memory consumption information you will either need to use tools specific to the graph database that you are using to get that information (if they offer such things) or you will need to use standard profiling tools like Java Flight Recorder, VisualVM, etc.</p>
",1831717,2019-08-16T11:06:18.997,4,CC BY-SA 4.0,,1,,"""",0,">explicitly states that it refers to ""an outdated version of the TinkerPop framework and Gremlin language documentation""",False,False
487,57545554,2,57541516,2019-08-18T14:37:28.740,1,"<p>In general, please heed three best practices:</p>

<ol>
<li><p>Avoid the <a href=""https://stackoverflow.com/a/36489724/1422451"">quadratic copy</a> of using <code>DataFrame.append</code> in a loop. Instead, build a list of data frames to be concatenated once outside the loop.</p></li>
<li><p>Use parameterization and not string concatenation which is supported with pandas <a href=""https://stackoverflow.com/a/24418294/1422451""><code>read_sql</code></a>. This avoids the need to string format and punctuate with quotes.</p></li>
<li><p>Discontinue using the modulo operator, <code>%</code>, for string concatenation as it is <a href=""https://stackoverflow.com/a/13452357/1422451"">de-emphasised</a> (not officially deprecated). Instead, use the superior <a href=""https://docs.python.org/3/library/stdtypes.html#str.format"" rel=""nofollow noreferrer""><code>str.format</code></a>.</p></li>
</ol>

<p>Specifically, for your needs iterate elementwise between two lists using <code>zip</code> without layering it in a dictionary:</p>

<pre><code>query= """"""SELECT count(distinct ids), first_date, last_date 
          FROM table1 
          WHERE first_date = %s and last_date = %s 
          GROUP BY 2, 3"""""" 

df_list = []
for f, l in zip(fd, ld): 
   df = pd.read_sql(query, conn, params=[f, l]) 
   df_list.append(df)

final_df = pd.concat(df_list)
</code></pre>

<hr>

<p>Alternatively, avoid the loop and parameters by aggregating on first and last of days of every month in table:</p>

<pre><code>query= """"""SELECT count(distinct ids), first_date, last_date 
          FROM table1 
          WHERE DATE_PART(d, first_date) = 1
            AND last_date = LAST_DAY(first_date)
          GROUP BY 2, 3
          ORDER BY 2, 3"""""" 

final_df = pd.read_sql(query, conn) 
</code></pre>
",1422451,2019-08-18T14:37:28.740,0,CC BY-SA 4.0,,1,,.,1,-emphasised</a> (not officially deprecated).,False,False
488,57561951,2,57561465,2019-08-19T18:10:45.687,1,"<p>This is all documented at <a href=""https://docs.aws.amazon.com/lambda/latest/dg/runtime-support-policy.html"" rel=""nofollow noreferrer"">Runtime Support Policy</a>.</p>

<p>Specifically, <a href=""https://devguide.python.org/#status-of-python-branches"" rel=""nofollow noreferrer"">Python 2.7 will be EOL on 2020-01-01</a>. AWS will typically notify you 60 days in advance of a runtime deprecation. Upon deprecation, you can update an existing Lambda function for 30 days but you can't create new Lambda functions using that runtime. The Lambda function and its deprecated runtime environment will still be available for execution.</p>

<p>I would plan to migrate functions before the Python 2.7 EOL date.</p>
",271415,2019-08-19T18:10:45.687,1,CC BY-SA 4.0,,1,,execution.</p,0,The Lambda function and its deprecated runtime environment will still be available for execution.</p,False,False
489,57562991,2,57555082,2019-08-19T19:41:37.237,1,"<p>I don't see anything obviously wrong. </p>

<ol>
<li>Does it do hdfs-hdfs or s3a-s3a?</li>
<li>Upgrade your hadoop version; 2.7.x is woefully out of date, especially with the S3A code. It's unlikely to make whatever this problem go away, but it will avoid other issues. Once you've upgraded, switch to the <a href=""http://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/#Stabilizing:_S3A_Fast_Upload"" rel=""nofollow noreferrer"">fast upload</a> and it will do incremental updates of large files; currently your code will be saving each file to /tmp somewhere and then uploading it in the close() call.</li>
<li>turn on the logging for the org.apache.hadoop.fs.s3a module and see what it says</li>
</ol>
",2261274,2019-08-19T19:41:37.237,2,CC BY-SA 4.0,,1,,.,0,">Upgrade your hadoop version; 2.7.x is woefully outdated, especially with the S3A code.",False,False
490,57571923,2,56261083,2019-08-20T10:47:16.660,0,"<p>We hit a very similar issue to that. In our case it turned out in the classpath of the Hive deployment there is an obsolete version of maxmind-db. This might even be by default on newer releases of Hive, but I haven't confirmed yet.</p>

<p>To fix, just shade any references to com.maxmind. Also, just to be safe, shade any any references to fasterxml.jackson as well - that's one of the core dependencies of GeoIP2 and conflicts of that in the classpath tend to cause weird runtime issues like this one. You can see an example of that in a <a href=""https://github.com/Spuul/hive-udfs/pull/4/files"" rel=""nofollow noreferrer"">PR addressing such issue</a> in one of the Hive UDF GeoIP libs.</p>
",9362116,2019-08-20T10:47:16.660,0,CC BY-SA 4.0,,1,,.,0,In our case it turned out in the classpath of the Hive deployment there is an obsolete version of maxmind-db.,False,False
491,57616000,2,57512577,2019-08-22T19:32:11.617,1,"<p>If you want to use Python3 with EMR notebooks, the recommended way is to use pyspark kernel and configure Spark to use Python3 within the notebook as,</p>

<pre><code>%%configure -f {""conf"":{ ""spark.pyspark.python"": ""python3"" }}
</code></pre>

<p>Note that,</p>

<ul>
<li><p>Any on cluster configuration related to PYSPARK_PYTHON or PYSPARK_PYTHON_DRIVER is overridden by EMR notebook configuration. The only way to configure for Python3 is from within the notebook as mentioned above.</p></li>
<li><p>pyspark3 kernel is deprecated for Livy 4.0+, and henceforth pyspark kernel is recommended to be used for both Python2 and Python3 by configuring spark.pyspark.python accordingly.</p></li>
<li><p>If you want to install additional Python dependencies which are not already present on the cluster, you can use <a href=""https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks-scoped-libraries.html"" rel=""nofollow noreferrer"">notebook-scoped libraries</a>. It works for both Python2 as well as Python3.</p></li>
</ul>
",5729017,2019-08-22T19:32:11.617,2,CC BY-SA 4.0,,1,kernel,accordingly.</p></li,0,"pyspark3 kernel is deprecated for Livy 4.0+, and henceforth pyspark kernel is recommended to be used for both Python2 and Python3 by configuring spark.pyspark.python accordingly.</p></li",False,False
492,57635038,2,57634353,2019-08-24T04:15:20.623,3,"<p>Please refer to the official documentation, the method you are using is deprecated and while it is maintained for backwards compatibility, it may not function as expected.  See <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.put_bucket_notification"" rel=""nofollow noreferrer"">here</a> for more information; you should be using <code>PutBucketNotificationConfiguration</code> which is <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.BucketNotification.put"" rel=""nofollow noreferrer"">bucket_notification.put()</a>.</p>

<p>Nonetheless, the error that you are seeing is:  </p>

<p><code>Not authorized to invoke function [arn:aws:lambda:...]</code></p>

<p>This is because when your Lambda executes, the service invoking it needs permission to invoke it; you simply need to give S3 permission to invoke your Lambda. You do this by creating a Resource Policy for your Lambda.  </p>

<p>In Python you're going to do this something like this:</p>

<pre><code> client = boto3.client('lambda')
 response = client.add_permission(
     FunctionName='arn:aws:lambda:...',
     StatementId='LambdaInvoke',
     Action='lambda:InvokeFunction',
     Principal='s3.amazonaws.com',
     SourceArn='arn:aws:s3:::my_test_bucket',
     SourceAccount='123456789012'
 )
</code></pre>

<p>Don't forget to replace the <code>function-name</code> with your functions ARN, as well as the <code>source-arn</code> to be your S3 buckets ARN, and <code>source-account</code> to be the source account number.</p>

<p>An alternative way, if you wanted to do it from the CLI, you can do it like this:</p>

<pre><code>aws lambda add-permission --function-name arn:aws:lambda:... --principal s3.amazonaws.com --statement-id S3EventTrigger --action ""lambda:InvokeFunction"" --source-arn arn:aws:s3:::my_test_bucket --source-account 123456789012
</code></pre>

<p>Both of these methods will attach a resource policy, that looks like the policy below, to your Lambda, allowing your S3 bucket to invoke it.</p>

<pre><code>{""Policy"" : ""{""Version"":""2012-10-17"",""Id"":""default"",""Statement"":[{""Sid"":""S3EventTrigger"",""Effect"":""Allow"",""Principal"":{""Service"":""s3.amazonaws.com""},""Action"":""lambda:InvokeFunction"",""Resource"":""arn:aws:lambda:..."",""Condition"":{""StringEquals"":{""AWS:SourceAccount"":""123456789012""},""ArnLike"":{""AWS:SourceArn"":""arn:aws:s3:::my_test_bucket""}}}]}"" ,
""RevisionId"" : ""999a9999-99ab-9999-a9a9-9999999a999a""}
</code></pre>
",3167238,2019-08-24T04:15:20.623,1,CC BY-SA 4.0,,1,method, ,1,"<p>Please refer to the official documentation, the method you are using is deprecated and while it is maintained for backwards compatibility, it may not function as expected.  ",False,False
493,57644479,2,57642713,2019-08-25T08:36:58.550,0,"<p>In the new release of your application add User-Agent header to your requests.
This header will contain type of OS (Android/iOS) and version of your application (for example 2.0). On backend check this header and if app version is 2.0 and above use v2 version of database for request else 1.0 version.</p>

<p>Also I advice to add ""You use out of date version of application. Please update"" screen so you can force all users to update after some period of time.</p>
",5212417,2019-08-25T08:36:58.550,0,CC BY-SA 4.0,,1,,.,0,You use outdated version of application.,False,False
494,57687605,2,57686698,2019-08-28T08:01:29.710,0,"<p>The main difference between Classic Load Balancers (v1 - old generation 2009) and Application Load Balancers (v2 - new generation 2016) is that ALBs have a port mapping feature to redirect to a dynamic port. In Comparison we would need to create one CLB per application.</p>

<p>Overall CLBs are deprecated and you use ALBs for HTTP/HTTPS and Websockets and Network Load Balancers for TCP.</p>

<p>Coming to your question. On ALBs you map certain paths (like an API endpoint) to a target group (e.g. EC2 instances). Within those instances you could trigger a lambda or whatever to execute your logic. This logic can stay the same as it is when you used it with a CLB.</p>
",10413468,2019-08-28T08:01:29.710,2,CC BY-SA 4.0,,1,CLBs,for,0,>Overall CLBs are deprecated and you use ALBs for HTTP/HTTPS and Websockets and Network Load Balancers for,False,False
495,57733596,2,57729647,2019-08-30T21:54:29.750,2,"<p>From the Elasticsearch <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-body.html"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>The results that are returned from a scroll request reflect the state of the index at the time that the initial search request was made, like a snapshot in time. Subsequent changes to documents (index, update or delete) will only affect later search requests.</p>
</blockquote>

<p>Therefore, you don't need to delete the scroll context.  In fact, you never <strong>need</strong> to delete the context as it will eventually delete itself.  However, it is best practice to delete the scroll context when you are finished to free up resources.</p>

<p>One use-case for the situation you described would be to see if the program is still using the outdated documents.  Depending on the code, you may not want it to be using deleted documents and instead want to retrieve a fresh scroll context.</p>
",10271247,2019-08-30T21:54:29.750,0,CC BY-SA 4.0,,1,, ,0,One use-case for the situation you described would be to see if the program is still using the outdated documents.  ,False,False
496,57834290,2,57831708,2019-09-07T13:49:02.507,1,"<p>S3n is obsolete. S3A will work with it -look at the hadoop docs on fs.s3a.endpoint and signing algorithm. These are supported in Hadoop 2.8+</p>
",2261274,2019-09-07T13:49:02.507,4,CC BY-SA 4.0,,1,S3n,.,0,<p>S3n is obsolete.,False,False
497,57834323,2,57828760,2019-09-07T13:52:48.460,0,"<p>S3A is <em>not</em> deprecated in ASF hadoop; I will argue that it is now ahead of what EMR's own connector will do. If you are using EMR you may be able to use it, otherwise you get to work with what they implement.</p>

<p>FWIW in S3A we're looking at what it'd take to actually dynamically change the header for a specific query, so you go beyond specific users to specific hive/spark queries in shared clusters. Be fairly complex to do this though as you need to do it on a per request setting.</p>
",2261274,2019-09-07T13:52:48.460,1,CC BY-SA 4.0,,1,,.,0,> deprecated in ASF hadoop; I will argue that it is now ahead of what EMR's own connector will do.,False,False
498,57871746,2,57871002,2019-09-10T13:20:37.000,0,"<p>You added obsolete curly braces around your <code>Fn::ImportValue</code>.</p>

<p>Try the following:</p>

<pre><code>Environment:
  - Name: DB_USERNAME
    Value: !Join ['', ['{{resolve:secretsmanager:', Fn::ImportValue: !Sub NameOfRDSSecret, ':SecretString:username}}' ]]
</code></pre>
",3115607,2019-09-10T13:20:37.000,0,CC BY-SA 4.0,,1,,code,0,You added obsolete curly braces around your <code,False,False
499,57935816,2,14077414,2019-09-14T13:10:42.377,2,"<p>As others have stated, you can increment with one operation, however ""action: ADD"" is now deprecated. The current way to achieve this is with UpdateExpression. I am also assuming the OP meant that there was a numeric [attribute] whose value needs incrementing. I am calling the attribute loginCount:</p>

<pre><code>dynamoDB.updateItem({
  TableName: ""Users"",
  Key: { ""UserId"": { S: ""c6af9ac6-7b61"" } },
  ExpressionAttributeValues: { "":inc"": {N: ""1""} },
  UpdateExpression: ""ADD loginCount :inc""
})
</code></pre>
",2483011,2019-09-14T13:10:42.377,0,CC BY-SA 4.0,,1,,.,0,"<p>As others have stated, you can increment with one operation, however ""action: ADD"" is now deprecated.",False,False
500,57993950,2,57993884,2019-09-18T13:30:25.767,0,"<blockquote>
  <p>storedBytes -> (long)</p>
  
  <p>The number of bytes stored.</p>
  
  <p>IMPORTANT: <strong>Starting on June 17, 2019, this parameter will be
  deprecated for log streams, and will be reported as zero</strong>. This change
  applies only to log streams. The storedBytes parameter for log groups
  is not affected.</p>
</blockquote>

<p><a href=""https://docs.aws.amazon.com/cli/latest/reference/logs/describe-log-streams.html"" rel=""nofollow noreferrer"">Source</a></p>
",3367818,2019-09-18T13:30:25.767,1,CC BY-SA 4.0,,1,parameter,zero</strong,0,"Starting on June 17, 2019, this parameter will be
  deprecated for log streams, and will be reported as zero</strong",False,False
501,58169149,2,50231691,2019-09-30T13:40:48.753,5,"<p>Old question but the answer is outdated - you can now get this inside the step function - with <code>$$.Execution.id</code>. Example I'm using:</p>

<pre><code>""run_task"": {
  ""Type"": ""Task"",
  ""Parameters"": {
    ""task.$"": ""$.task"",
    ""executionId.$"": ""$$.Execution.Id""
  },
  ""Resource"": ""${runTaskLambdaArn}"",
  ""End"": true
}
</code></pre>
",1375972,2019-09-30T13:40:48.753,0,CC BY-SA 4.0,,1,,code>$$.Execution.id</code,0,the answer is outdated - you can now get this inside the step function - with <code>$$.Execution.id</code,False,False
502,58251037,2,57140077,2019-10-05T18:11:29.407,2,"<p>There is nothing wrong with using low-level resources, as it's a useful building block. Yes, there are high level constructs, but like everything else in software, occasionally you have to dig deeper. It's a shame that the attitude like this has continued, because for some of us, that have spent years building out a nice collection of CloudFormation templates, are discouraged from using CDK. </p>

<p>Yes, there are some issues with this, mostly because of some poor architecture decisions and a desire to focus on the new instead of building up layers of functionality, but it's possible and you shouldn't be dissuaded in converting your templates to low-level constructs, and then refactoring into higher-level ones as your use case requires.</p>

<p>That said, I would NOT use any kind of automatic tools to do this conversion. You won't actually understand what's going on, and you'll more than likely have issues that you won't know how to handle. Dig in, convert line-by-line, and then enjoy the results.</p>
",1044947,2019-10-05T18:11:29.407,0,CC BY-SA 4.0,,1,,.,0,"It's a shame that the attitude like this has continued, because for some of us, that have spent years building out a nice collection of CloudFormation templates, are discouraged from using CDK.",False,False
503,58282926,2,58280969,2019-10-08T08:44:18.780,0,"<p>The notices that you get happen because these 2 variables <code>skip_requesting_account_id</code> and <code>skip_get_ec2_platforms</code> are deprecated, see <a href=""https://www.terraform.io/docs/backends/types/s3.html"" rel=""nofollow noreferrer"">docs</a>.</p>

<p>For the credentials error, move the credentials into ~/.aws/credentials and leave in the file just:</p>

<pre><code>provider ""aws"" {}
</code></pre>

<p>This is a better practice in terms of security as well, as you do not store the credentials with the code. In your original code you also had a typo, is <code>access_key</code> not <code>acces_key</code>.</p>
",95353,2019-10-08T08:44:18.780,0,CC BY-SA 4.0,,1,>,types,0,"> are deprecated, see <a href=""https://www.terraform.io/docs/backends/types",False,False
504,58289743,2,58289502,2019-10-08T15:41:55.847,2,"<p>Probably neither. Of course it all depends on the key schema and the data in the table, but you probably want to create an Global Secondary Index for your most frequently used queries.</p>

<p>Having said that; performing scans is highly discouraged, especially when working with large volumes of data. So if you know the primary key of the items you're interested in, go for <code>BatchGetItems</code> over doing a scan.</p>
",5409,2019-10-08T15:41:55.847,0,CC BY-SA 4.0,,1,,.,0,">Having said that; performing scans is highly discouraged, especially when working with large volumes of data.",False,True
505,58323558,2,58306283,2019-10-10T13:02:56.840,0,"<p>I was simply using an out of date container agent. From the <a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html"" rel=""nofollow noreferrer"">docs</a>:</p>

<blockquote>
  <p>For tasks that use the EC2 launch type, this feature requires that your container instance have version 1.22.0 or later of the container agent. </p>
</blockquote>

<p>After updating my <code>LaunchConfiguration</code>, everything worked:</p>

<pre><code>Parameters:

  ECSAMI:
    Description: AMI ID
    Type: AWS::SSM::Parameter::Value&lt;AWS::EC2::Image::Id&gt;
    Default: /aws/service/ecs/optimized-ami/amazon-linux/recommended/image_id
    Description: The Amazon Machine Image ID used for the cluster, leave it as the default value to get the latest AMI

  ContainerInstances:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      ImageId: !Ref ECSAMI
      ...
</code></pre>
",6084948,2019-10-10T13:02:56.840,0,CC BY-SA 4.0,,1,,.,0,I was simply using an outdated container agent.,False,False
506,58366247,2,42769975,2019-10-13T17:51:28.840,5,"<p>I got similar kind of error as fallows
<a href=""https://i.stack.imgur.com/xzLAN.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xzLAN.png"" alt=""enter image description here""></a></p>

<p>When I looked at more details section I found this ,where it  was an error in the <code>bcrypt</code> library 
<a href=""https://i.stack.imgur.com/FjE59.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FjE59.png"" alt=""enter image description here""></a></p>

<p>So I removed that library and installed <a href=""https://www.npmjs.com/package/bcryptjs"" rel=""noreferrer"">bcryptjs</a>   library and deployed it again ,then it was successful deployed. So this kind of errors may appear when there are deprecated libraries and not supportive libraries .</p>
",7865621,2019-10-13T17:51:28.840,1,CC BY-SA 4.0,,1,,.</p,1,So this kind of errors may appear when there are deprecated libraries and not supportive libraries .</p,False,False
507,58484134,2,50984454,2019-10-21T10:14:04.920,0,"<p>db2prereqcheck checks and understands only /etc/SuSE-release with the following content:</p>

<blockquote>
  <p>SUSE   Linux Enterprise Server 15 (x86_64)</p>
  
  <p>VERSION = 15</p>
  
  <p>PATCHLEVEL= 1</p>
</blockquote>

<p>This works also with OpenSuse Leap 15.1. This file is deprecated since SLES 12. So you must create it your self. Then run db2prereqcheck and install all missing libraries and kernel sources mentioned. Having all requirements fulfilled you may finally see the segmentation fault message:</p>

<blockquote>
<pre><code>  Validating ""Intel TCO WatchDog Timer Driver modules"" ...
  DBT3546E The db2prereqcheck utility failed to determine whether the
  following file or package exists: """".
  Segmentation fault (core dumped)
</code></pre>
</blockquote>

<p>Don't worry!
Simply retest with db2prereqcheck -i. The -i parameter checks for prerequisites that are not Db2 pureScale related. If we don't install pureScale and all requirement are fulfilled, we can ignore this ugly segmentation fault.
Otherwise you must blacklist by adding:</p>

<blockquote>
<pre><code>  blacklist iTCO_wdt
  blacklist iTCO_vendor_support
  Into the file /etc/modprobe.d/blacklist.conf
</code></pre>
</blockquote>

<p>A further issue is related to:</p>

<blockquote>
<pre><code> export DISPLAY=your.machine.ip:0.0
</code></pre>
</blockquote>

<p>running ./db2setup as root doesn't work. 
./db2_install is deprecated, but it works.
First create the db2 users and groups as described by the IBM Knowledge Center.
Then run ./db2_install as root, followed by creating an instance using db2icrt.
Login as db2inst1 and test as described by the IBM Knowledge Center eventually creating the SAMPLE Database, etc. Normally ""first steps"" would do the job, but it crashes with javascript error. Hence you must do it manually!
Additional manual configuration may be required as opening the firewall for port 50001 and setting this port within /etc/services and within dbm cfg with: </p>

<blockquote>
  <p>db2 update dbm cfg using SVCENAME 50001</p>
</blockquote>

<p>or</p>

<blockquote>
  <p>db2 update dbm cfg using SVCENAME db2c_db2inst1</p>
</blockquote>

<p>If you use the latter you must update /etc/services with the line: </p>

<blockquote>
  <p>db2c_db2inst1         50001/tcp         #and a comment like db2 tcp/ip
  connection port.</p>
</blockquote>
",2496001,2019-10-21T10:14:04.920,0,CC BY-SA 4.0,,2,file,.,0,This file is deprecated since SLES 12.,False,False
508,58513758,2,58509844,2019-10-23T00:04:06.670,4,"<p>I was having the same issue and it turned it out to be an outdated AWS session manager plugin for the aws cli. After updating the plugin it worked. </p>

<p>Instruction to install/update the plugin is <a href=""https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html#install-plugin-macos"" rel=""nofollow noreferrer"">here</a>.</p>
",3513857,2019-10-23T00:04:06.670,0,CC BY-SA 4.0,,1,,.,0,and it turned it out to be an outdated AWS session manager plugin for the aws cli.,False,False
509,58542412,2,58540432,2019-10-24T13:25:16.247,1,"<p>For anyone with serious troubles with PhantomJS on Elastic Beanstalk, what finally solved my issue was installing AWS CLI, going through the steps of setting up EB:</p>

<pre><code>eb ssh [ENV] [REGION] - Don't worry, the CLI will guide you
</code></pre>

<p>and after configuring IAM online with a user having ElasticBeanstalkFullAccess (and console password) and then the configuration files for aws cli at:</p>

<pre><code>~/.aws/config and credentials with the keys and secrets from IAM
</code></pre>

<p>and finally connecting to the environment with eb ssh and then running:</p>

<pre><code>sudo npm install -g phantomjs-prebuilt --unsafe-perm
sudo npm install html-pdf -g (will give deprecated warning)
sudo npm link html-pdf
sudo npm link phantomjs-prebuilt
</code></pre>

<p>Finally, restart the app server via the dashboard and check logs (last 100 lines) - Should just be:</p>

<pre><code>&gt; node server.js

Server is running.
</code></pre>

<p>or similar. Good luck.</p>

<p>Thanks to <a href=""https://github.com/marcbachmann/node-html-pdf/issues/437#issuecomment-467463285"" rel=""nofollow noreferrer"">Paul-JO</a> for a solution that worked for me.</p>
",533901,2019-10-24T13:25:16.247,0,CC BY-SA 4.0,,1,,"
",0,"(will give deprecated warning)
",False,False
510,58560790,2,58550567,2019-10-25T14:47:06.467,1,"<p>A few things...</p>

<p>hot partitions, only come into play <em>if</em> you have multiple partitions...</p>

<p>Just because you've got multiple partition (hash) keys, doesn't automatically mean DDB will need multiple partitions.  You'll also need more than 10GB of data and/or more than 3000 RCU or 1000 WCU being used.</p>

<p>Next, DDB now supports ""Adaptive Capacity"", so hot partitions aren't as big a deal as they used to be.  <a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/"" rel=""nofollow noreferrer"">why what you know about DynamoDB might be outdated</a> </p>

<p>In connection with the even newer ""Instantaneous Adaptive Capacity"", you've got DDB on demand.  </p>

<p>One final note, you may be under the impression that a given partition (hash) key can only have a maximum of 10GB of data under it.  This is true if your table utilizes Local Secondary Indexes (LSI) but is not true otherwise.  Thus, consider using global secondary indexes (GSI).  There's extra cost associated with GSIs, so it's a trade off to consider.</p>
",2933177,2019-10-25T14:47:06.467,0,CC BY-SA 4.0,,2,,outdated/,0,"<a href=""https://aws.amazon.com/blogs/database/how-amazon-dynamodb-adaptive-capacity-accommodates-uneven-data-access-patterns-or-why-what-you-know-about-dynamodb-might-be-outdated/",False,True
511,58572379,2,35429632,2019-10-26T15:56:58.450,0,"<p>I had this same problem and all the solutions I found on StackOverflow were outdated or way more complicated than necessary. After trying a couple propositions, I ended up writing a NodeJS script to move the data over. Maybe that could work for you? If you want to try, I documented my approach on my blog <a href=""https://tvernon.tech/blog/move-dynamodb-data-between-regions"" rel=""nofollow noreferrer"">here</a>.</p>
",1385243,2019-10-26T15:56:58.450,0,CC BY-SA 4.0,,1,,.,0,I had this same problem and all the solutions I found on StackOverflow were outdated or way more complicated than necessary.,False,False
512,58601265,2,58601054,2019-10-29T03:58:51.107,-1,"<p>You haven't mentioned what the data type of the <code>time_stamp</code> column is, but assuming it's something that can be cast to <code>date</code> then I think the issue could be due to your use of the <code>NOW()</code> function which is deprecated on Redshift. Try replacing this with either <code>GETDATE()</code> or <code>SYSDATE</code> as per <a href=""https://docs.aws.amazon.com/redshift/latest/dg/Date_functions_header.html"" rel=""nofollow noreferrer"">the documentation</a>.</p>
",46239,2019-10-29T03:58:51.107,0,CC BY-SA 4.0,,1,,.,0,> function which is deprecated on Redshift.,False,False
513,58618775,2,58617894,2019-10-30T04:29:08.483,0,"<p>Normally such question is strongly discouraged, you should show what you have done and what the problem you are facing. but here is a step.</p>

<ul>
<li>Better to push your image to <a href=""https://aws.amazon.com/ecr/"" rel=""nofollow noreferrer"">ECR</a></li>
<li>Specify an image ARN in the <a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html"" rel=""nofollow noreferrer"">task definition</a></li>
<li>Create <a href=""https://docs.aws.amazon.com/AmazonECS/latest/userguide/ecs_services.html"" rel=""nofollow noreferrer"">service</a> from the task definition</li>
</ul>

<pre><code>{
   ""containerDefinitions"": [ 
      { 

         ""entryPoint"": [
            ""sh"",
            ""-c""
         ],
         ""essential"": true,
         ""image"": ""PUT_RESGISTRY_ECR_IMAGE_ARN_HERE"",
         ""logConfiguration"": { 
            ""logDriver"": ""awslogs"",
            ""options"": { 
               ""awslogs-group"" : ""/ecs/fargate-task-definition"",
               ""awslogs-region"": ""us-east-1"",
               ""awslogs-stream-prefix"": ""ecs""
            }
         },
         ""name"": ""sample-fargate-app"",
         ""portMappings"": [ 
            { 
               ""containerPort"": 80,
               ""hostPort"": 80,
               ""protocol"": ""tcp""
            }
         ]
      }
   ],
   ""cpu"": ""256"",
   ""executionRoleArn"": ""arn:aws:iam::012345678910:role/ecsTaskExecutionRole"",
   ""family"": ""fargate-task-definition"",
   ""memory"": ""512"",
   ""networkMode"": ""awsvpc"",
   ""requiresCompatibilities"": [ 
       ""FARGATE"" 
    ]
}

</code></pre>

<p><a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/example_task_definitions.html"" rel=""nofollow noreferrer"">example_task_definitions</a></p>

<p><a href=""https://aws.amazon.com/getting-started/tutorials/deploy-docker-containers/"" rel=""nofollow noreferrer"">getting-started/tutorials/deploy-docker-containers</a></p>

<p><a href=""https://medium.com/@mohitshrestha02/deploying-a-simple-hello-world-httpd-container-on-an-ecs-cluster-64c880d265f0"" rel=""nofollow noreferrer"">deploying-a-simple-hello-world-httpd-container-on-an-ecs-cluster</a></p>
",3288890,2019-10-30T04:29:08.483,0,CC BY-SA 4.0,,1,,.,0,"Normally such question is strongly discouraged, you should show what you have done and what the problem you are facing.",False,False
514,58924337,2,58924204,2019-11-18T23:17:03.967,0,"<p>Looks like <code>platform.linux_distribution</code> is deprecated starting Python 3.7 so it would not be available in Python 3.8. </p>

<p>Please see <a href=""https://stackoverflow.com/questions/49554443/platform-linux-distribution-deprecated-what-are-the-alternatives"">this post</a></p>

<p>The alternative is to use:</p>

<pre><code>import distro
distro.linux_distribution()
</code></pre>

<p>Hope it helps!</p>
",5287434,2019-11-18T23:17:03.967,0,CC BY-SA 4.0,,2,,3.7,0,is deprecated starting Python 3.7,False,False
515,58926312,2,58908823,2019-11-19T03:44:36.240,0,"<p>I've figured it out, it should be </p>

<pre><code>use Aws\DynamoDb\SessionHandler;
</code></pre>

<p>AWS guide is outdated...</p>
",1530644,2019-11-19T03:44:36.240,0,CC BY-SA 4.0,,1,guide,"
",0,"<p>AWS guide is outdated...</p>
",False,False
516,59042329,2,28287464,2019-11-26T01:13:00.647,0,"<p>This question is old, but in case anyone else comes across this question, the halite project has been deprecated and is no longer developed.</p>

<p><a href=""https://github.com/saltstack/halite"" rel=""nofollow noreferrer"">https://github.com/saltstack/halite</a></p>
",114866,2019-11-26T01:13:00.647,0,CC BY-SA 4.0,,1,,longer,1,">This question is old, but in case anyone else comes across this question, the halite project has been deprecated and is no longer",False,False
517,59045972,2,59044712,2019-11-26T07:42:10.103,1,"<p>Since your function already is <code>async</code> you don't need to use the old, outdated <code>callback</code> approach.</p>

<p>The AWS SDK methods provide a <code>.promise()</code> method which you can append to every AWS asynchronous call and, with <code>async</code>, you can simply <code>await</code> on a Promise.</p>

<pre class=""lang-js prettyprint-override""><code>    var AWS = require('aws-sdk');
    var iam = new AWS.IAM();
    AWS.config.loadFromPath('./config.json');

    let getUsers = async event =&gt; {
        var params = {
            UserName: ""5dc6f49d50498e2907f8ee69""
        };

        const user = await iam.getUser(params).promise()
        console.log(user)
    };
</code></pre>

<p>Hopefully you have a handler that invokes this code, otherwise this is not going to work.
If you want to export <code>getUsers</code> as your handler function, make sure export it through <code>module.exports</code> first. I.e: <code>module.exports.getUsers = async event...</code>. Double check that your Lambda handler is properly configured on the function itself, where <code>index.getUsers</code> would stand for <code>index</code> being the filename (<code>index.js</code>) and <code>getUsers</code> your exported function.</p>
",10950867,2019-11-26T07:42:10.103,0,CC BY-SA 4.0,,1,,code,1,"you don't need to use the old, outdated <code",False,False
518,59107642,2,58787789,2019-11-29T15:37:33.917,0,"<p>Solved it: switch processing to RMagick</p>

<p>Within the carriewave image_uploader.rb file</p>

<p>replace:</p>

<p>include CarrierWave::MiniMagick</p>

<p>include CarrierWave::Processing::MiniMagick</p>

<p>with:</p>

<p>include CarrierWave::RMagick</p>

<p>include CarrierWave::Processing::RMagick</p>

<p>MiniMagick is considered to have better memory management, but is rather outdated. Plus is corrupted the images. Fingers crossed RMagick has better memory management by now.</p>

<p>RMagick for the win!</p>
",1368601,2019-11-29T15:37:33.917,0,CC BY-SA 4.0,,1,,.,0,"MiniMagick is considered to have better memory management, but is rather outdated.",False,False
519,59154278,2,59153685,2019-12-03T09:51:57.710,1,"<p>Taken from here: <a href=""https://askubuntu.com/questions/1031921/php-mcrypt-package-missing-in-ubuntu-server-18-04-lts"">https://askubuntu.com/questions/1031921/php-mcrypt-package-missing-in-ubuntu-server-18-04-lts</a></p>

<p>Mcrypt has been deprecated in PHP 7.2, so it's not available by default.</p>

<p>You can still install the mcrypt extension using pecl. These instructions are for the apache web server.</p>

<pre><code># Install prerequisites
sudo apt-get install php-dev libmcrypt-dev gcc make autoconf libc-dev pkg-config

# Compile mcrypt extension
sudo pecl install mcrypt-1.0.1
# Just press enter when it asks about libmcrypt prefix

# Enable extension for apache
echo ""extension=mcrypt.so"" | sudo tee -a /etc/php/7.2/apache2/conf.d/mcrypt.ini

# Restart apache
sudo service apache2 restart
That should get you going.
</code></pre>

<p>In the long term you might want to replace mcrypt, it's deprecated for a reason.</p>
",7500779,2019-12-03T09:51:57.710,1,CC BY-SA 4.0,,2,Mcrypt,default.</p,1,"Mcrypt has been deprecated in PHP 7.2, so it's not available by default.</p",False,False
520,59276158,2,59268806,2019-12-10T22:15:50.560,0,"<p>""Bundle"" commands are outdated.</p>

<p>You should simply use <strong>Create Image</strong> to create an Amazon Machine Image (AMI) of the existing instance. Then, you can <strong>Launch an instance from the AMI</strong> and it will contain an exact copy of the AMI.</p>

<p>See: <a href=""https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/Creating_EBSbacked_WinAMI.html"" rel=""nofollow noreferrer"">Creating a Custom Windows AMI - Amazon Elastic Compute Cloud</a></p>

<p><strong>Recommendation:</strong> Run <code>sysprep</code> before creating the image. This will avoid conflicting machine IDs.</p>

<p>See: <a href=""https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ami-create-standard.html"" rel=""nofollow noreferrer"">Create a Standard Amazon Machine Image Using Sysprep - Amazon Elastic Compute Cloud</a></p>
",174777,2019-12-10T22:15:50.560,0,CC BY-SA 4.0,,1,,outdated.</p,0,"<p>""Bundle"" commands are outdated.</p",False,False
521,59316600,2,59316340,2019-12-13T05:26:56.113,3,"<p>You can set the <code>MaxPods</code> field in the <a href=""https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/"" rel=""nofollow noreferrer"">kubelet config file</a>:</p>

<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
MaxPods: 250
</code></pre>

<p>You can then supply the config file to the kubelet binary with the <code>--config</code> flag, for example:</p>

<pre class=""lang-sh prettyprint-override""><code>kubelet --config my-kubelet-config.yaml
</code></pre>

<p>Alternatively, the kubelet binary also has a <code>--max-pods</code> flag that allows you to set the value directly. However, this flag is deprecated and the use of a config file, as shown above, is recommended. See the <a href=""https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/"" rel=""nofollow noreferrer"">kubelet reference</a>.</p>
",4747193,2019-12-13T05:26:56.113,0,CC BY-SA 4.0,,1,flag,.,0,"However, this flag is deprecated and the use of a config file, as shown above, is recommended.",False,False
522,59331553,2,59316647,2019-12-14T01:16:17.493,0,"<p>I think the problem here is that your boto3 version might be outdated. Could you try updating?</p>

<pre><code>pip install boto3 --upgrade
</code></pre>
",1247948,2019-12-14T01:16:17.493,2,CC BY-SA 4.0,,1,,.,0,I think the problem here is that your boto3 version might be outdated.,False,False
523,59508546,2,59508159,2019-12-28T06:04:17.777,1,"<p>That's a really poor question. I would recommend spending your time elsewhere rather than trying to diagnose this question.</p>

<p>I suspect the question is also pretty old because Auto Scaling groups can now also accept Launch Templates, and the link you provided doesn't work. (The updated link is: <a href=""https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-lc-with-instanceID.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-lc-with-instanceID.html</a>)</p>

<p>Questions that use ""should"" are not well-written. Exam questions are trying to test your knowledge of how to do something successfully, not how you ""should"" do something. For example, questions would normally ask for the lowest-cost solution or a solution with the highest availability, leaving no questions about the correct answer.</p>

<p>To answer your question:</p>

<ul>
<li>A Launch Configuration can be created in the management console, via the AWS CLI or via AWS SDK API calls (so C and D are valid and A is incorrect)</li>
<li>It is <em>not</em> possible to create a Launch Configuration from an existing Amazon EC2 instance (so B is incorrect)</li>
<li>It is now possible to create a <strong>Launch Template</strong> from an existing Amazon EC2 instance, and this can be used by an Auto Scaling group (but this isn't what they asked)</li>
</ul>

<p><strong>Bottom line:</strong> Let the author know that the question is outdated/incorrect.</p>
",174777,2019-12-28T06:04:17.777,0,CC BY-SA 4.0,,1,,outdated,0,Let the author know that the question is outdated,False,False
524,59522054,2,54285471,2019-12-29T18:15:35.650,0,"<p>The error indicates that you have not specified the S3 bucket to upload the data to. You will have to pass that in either in the <code>uploadRequest1</code> variable you have created there or instantiate AWSS3TransferManager with the bucket information. AWSS3TransferManager is already deprecated and so if you are still looking for a client library to interact with your S3 resource, I would suggest starting off with AWS Amplify framework's Storage plugin <a href=""https://aws-amplify.github.io/docs/ios/storage"" rel=""nofollow noreferrer"">https://aws-amplify.github.io/docs/ios/storage</a></p>
",2464632,2019-12-29T18:15:35.650,0,CC BY-SA 4.0,,1,AWSS3TransferManager,"noreferrer"">https://aws",0,"AWSS3TransferManager is already deprecated and so if you are still looking for a client library to interact with your S3 resource, I would suggest starting off with AWS Amplify framework's Storage plugin <a href=""https://aws-amplify.github.io/docs/ios/storage"" rel=""nofollow noreferrer"">https://aws",False,False
525,59572123,2,59559826,2020-01-03T01:23:31.583,1,"<p>The problem is being caused by <strong>deprecated images</strong>.</p>

<p>For example, Windows images are deprecated when a newer version is made available. While the image object can be retrieved, it does not actually contain any information (name, tags, etc). Thus, it is not possible to display any information about it.</p>

<p>You will need to add a <code>try</code> statement to catch such situations and skip-over the deprecated image.</p>

<pre class=""lang-py prettyprint-override""><code>import boto3

ec2_resource = boto3.resource('ec2')

# Display AMI information for all instances
for instance in ec2_resource.instances.all():
    image = instance.image

    # Handle situation where image has been deprecated
    try:
        tags = image.description
    except:
        continue

    if image.tags is None:
        description_tag = ''
    else:
        description_tag = [tag['Value'] for tag in image.tags if tag['Name'] == 'Description'][0]

    print(image.description, image.image_type, image.name, image.platform, description_tag) 
</code></pre>
",174777,2020-01-03T01:23:31.583,1,CC BY-SA 4.0,,4,,"

",0,"The problem is being caused by <strong>deprecated images</strong>.</p>

",False,False
526,59594292,2,59585572,2020-01-04T19:53:20.270,0,"<p>Signature Version 2 (which is what you are using, here) always expects the resource to be <code>/${bucket}/${file}</code> regardless of whether the  URL is path-based (bucket as the first element of the path) or virtual (bucket as part of the hostname).</p>

<p>Additionally, for legacy reasons, S3 expects <code>+</code> in the signature to be url-escaped as <code>%2B</code> and (iirc) will accept but does not require any <code>/</code> to be transformed to <code>%2F</code> and any <code>=</code> to be transformed to <code>%3D</code> so you need to verify (<code>curl -v ...</code>) whether curl is transforming it, and if not, use something like <code>sed</code> or maybe <code>tr</code> in the pipeline to do that encoding of the signature, or perhaps find a curl option that can enable the query string parameter encoding.</p>

<p>Be aware that <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/"" rel=""nofollow noreferrer"">Signature V2 is deprecated</a> and will be deactivated at some point after June, 2020, so even though it still works on existing buckets in regions where S3 originally launched prior to 2014, its usable lifespan is most definitely coming to an end... so your interests would be served by not creating new systems that use it.  The successor algorithm, Signature V4, is more complicated but there are solid security-related justifications for its increased complexity.</p>
",1695906,2020-01-04T19:53:20.270,0,CC BY-SA 4.0,,1,,...,0,">Be aware that <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/"" rel=""nofollow noreferrer"">Signature V2 is deprecated</a> and will be deactivated at some point after June, 2020, so even though it still works on existing buckets in regions where S3 originally launched prior to 2014, its usable lifespan is most definitely coming to an end...",False,False
527,59777560,2,59777111,2020-01-16T20:45:03.097,0,"<p>Now() has been deprecated.
Getdate() works.</p>
",11059442,2020-01-16T20:45:03.097,0,CC BY-SA 4.0,,1,,"
",0,"Now() has been deprecated.
",False,False
528,59803102,2,59798049,2020-01-18T17:29:30.820,1,"<blockquote>
  <p><em>Why is AWS S3 refusing to open a document because of an expiry time value, which it itself allowed us to use to generate the pre-signed URL?</em></p>
</blockquote>

<p>S3 didn't allow that.  Signed URLs are generated locally, and S3 doesn't see them or know about them (or validate their authenticity or authorization to fetch the specified object) until you actually try to use them.</p>

<p>This is probably best characterized as a bug in boto3... Signature Version 2 expirations are tied to the Unix epoch, which ends 2038-01-19T03:14:08Z (the ""Y2.038K bug"").  It's unlikely to be fixed at this point since <a href=""https://aws.amazon.com/blogs/aws/amazon-s3-update-sigv2-deprecation-period-extended-modified/"" rel=""nofollow noreferrer"">Signature V2 is deprecated</a>.</p>

<p>Theoretically, you could V2-sign a URL that does't expire until mid-January, 2038 but this isn't viable, either, because signed URLs are (re)validated each time they are used.  Best practice is to periodically rotate your keys, so the AWS Access Key ID you are using today should not still be valid in 100 years, or even in the 18 years between now and 2038.  Once you deactivate those particular credentials, any URLs they signed will no longer be usable.</p>
",1695906,2020-01-18T17:29:30.820,0,CC BY-SA 4.0,,1,,"

",0,"deprecated</a>.</p>

",False,False
529,59899118,2,59874265,2020-01-24T15:10:28.777,0,"<p><code>AttachmenTargetType</code> has been wrongly deprecated.</p>

<p>Only some of its constants are deprecated: <code>secretsmanager.AttachmentTargetType.CLUSTER</code> is now <code>secretsmanager.AttachmentTargetType.RDS_DB_CLUSTER</code> and <code>secretsmanager.AttachmentTargetType.INSTANCE</code> is now <code>secretsmanager.AttachmentTargetType.RDS_DB_INSTANCE</code>.</p>

<p>Note that if in your example <code>this.database</code> is a <code>Cluster</code> instance, you can simply do:</p>

<pre class=""lang-ts prettyprint-override""><code>this.dbSecrets.attach(this.database);
</code></pre>
",11339287,2020-01-24T15:10:28.777,2,CC BY-SA 4.0,,2,,deprecated.</p,0,<p><code>AttachmenTargetType</code> has been wrongly deprecated.</p,False,False
530,59953367,2,59953053,2020-01-28T16:45:31.850,3,"<p>All AWS customers should understand the <a href=""https://aws.amazon.com/compliance/shared-responsibility-model/"" rel=""nofollow noreferrer"">Shared Responsibility Model</a>.</p>

<p>Specifically related to your question:</p>

<blockquote>
  <p>Customers that deploy an Amazon EC2 instance are responsible for
  management of the guest operating system (including updates and
  security patches), any application software or utilities installed by
  the customer on the instances, and the configuration of the
  AWS-provided firewall (called a security group) on each instance.</p>
</blockquote>

<p>No, AWS will not prevent you from running deprecated versions of Ruby or old, buggy web servers. It might notify you, however, in certain situations such as your RDS database having an expiring SSL certificate.</p>

<p>If your application behaves in a way that threatens the platform itself or other computer systems, for example being part of a DDoS botnet, then it's conceivable that AWS will implement measures to prevent it.</p>
",271415,2020-01-28T16:45:31.850,0,CC BY-SA 4.0,,1,,.,1,"No, AWS will not prevent you from running deprecated versions of Ruby or old, buggy web servers.",False,False
531,59954650,2,59889088,2020-01-28T18:06:30.623,1,"<p>If anyone bumps in to this issue, the way I solved it was to create another project with exactly same settings and code (diff compare if you want) but just change the names of the stack, the app, the functions and the policies. </p>

<p>The initial version of this app was deployed using Node 8.10 which was recently deprecated. Maybe that was part of the problem.</p>

<p>If this helps you, like this post , I probably saved you a day of productivity :p</p>
",11239755,2020-01-28T18:06:30.623,1,CC BY-SA 4.0,,1,,.,0,The initial version of this app was deployed using Node 8.10 which was recently deprecated.,False,False
532,60015844,2,28245224,2020-02-01T10:03:53.543,2,"<p><code>/var/app/current</code> may be outdated. It doesn't exist on my instance.<br>
As said there <a href=""https://stackoverflow.com/questions/27973619/i-cant-find-my-web-app-when-i-ssh-to-my-aws-elastic-beanstalk-instance"">I can&#39;t find my Web app when I SSH to my AWS Elastic Beanstalk instance</a>, for python the app is in <code>/opt/python/bundle/2/app/</code><br>
Otherwise use <code>find</code> to search for the location (look at the link).</p>
",7479997,2020-02-01T10:03:53.543,1,CC BY-SA 4.0,,1,>,.,0,<p><code>/var/app/current</code> may be outdated.,False,False
533,60066826,2,30154461,2020-02-04T23:05:05.350,4,"<p>The main advantage of using CloudFront is to get your files from a source (S3 in your case) and store it on edge servers to respond to GET requests faster. CloudFront will not go back to S3 source for each http request. </p>

<p>To have CloudFront serve latest fiels/objects, you have multiple options:</p>

<h3>Use CloudFront to Invalidate modified Objects</h3>

<p>You can use CloudFront to invalidate one or more files or directories manually or using a trigger. This option have been described in other responses here. More information at <a href=""https://aws.amazon.com/about-aws/whats-new/2015/05/amazon-cloudfront-makes-it-easier-to-invalidate-multiple-objects/"" rel=""nofollow noreferrer"">Invalidate Multiple Objects in CloudFront</a>. This approach comes handy if you are updating your files infrequently and do not want to impact the performance benefits of cached objects.</p>

<h3>Setting object expiration dates on S3 objects</h3>

<p>This is now the recommended solution. It is straight forward:</p>

<ul>
<li>Log in to AWS Management Console</li>
<li>Go into S3 bucket</li>
<li>Select all files</li>
<li>Choose ""Actions"" drop down from the menu</li>
<li>Select ""Change metadata""</li>
<li>In the ""Key"" field, select ""Cache-Control"" from the drop down menu.</li>
<li>In the ""Value"" field, enter ""max-age=300"" (number of seconds)</li>
<li>Press ""Save"" button
The default cache value for CloudFront objects is 24 hours. By changing it to a lower value, CloudFront checks with the S3 source to see if a newer version of the object is available in S3.</li>
</ul>

<p>I use a combination of these two methods to make sure updates are propagated to an edge locations quickly and avoid serving outdated files managed by CloudFront. </p>

<p>AWS however recommends changing the object names by using a version identifier in each file name. If you are using a build command and compiling your files, that option is usually available (as in react npm build command).</p>
",3984664,2020-02-04T23:05:05.350,1,CC BY-SA 4.0,,1,,.,0,I use a combination of these two methods to make sure updates are propagated to an edge locations quickly and avoid serving outdated files managed by CloudFront.,False,False
534,60114105,2,56825003,2020-02-07T13:03:41.703,0,"<p>Running the above code in a workflow gives the error:</p>

<pre><code>usage: workflow-test.py [-h] --JOB_NAME JOB_NAME --WORKFLOW_NAME WORKFLOW_NAME
                    --WORKFLOW_RUN_ID WORKFLOW_RUN_ID
workflow-test.py: error: the following arguments are required: --JOB_NAME
</code></pre>

<p>followed by what you have pasted above.</p>

<p>Seems the AWS documentation is outdated and the <code>JOB_NAME</code> parameter is not used in jobs that are part of a workflow.</p>

<p>You'll get the workflow parameters with:</p>

<pre><code>args = getResolvedOptions(sys.argv, ['WORKFLOW_NAME', 'WORKFLOW_RUN_ID'])
</code></pre>
",7808647,2020-02-07T13:03:41.703,0,CC BY-SA 4.0,,1,,JOB_NAME</code,0,>Seems the AWS documentation is outdated and the <code>JOB_NAME</code,False,False
535,60256320,2,60255983,2020-02-17T05:19:19.203,1,"<p>Storing such credentials in the environment file is discouraged because generally, environment files are committed into the git.<br>
1. If it repository is public, then all of your credentials are available for public (mis)use.<br>
2. If the repository is private, then also there may come such a situation where multiple people are working on same project but only very few people will need these credentials and not everyone.<br>
3. In your case, if you get the AWS credentials in the front end, then anyone can get those credentials from the network tab (easiest way) and will be able to (mis)use it.  </p>

<p>How / Where you should store such credentials depends upon your use case. There cannot be an answer which would fit everyone's need. Though there are few good approaches for it:<br>
1. Keep the environment file out of the repository, and distributed it to developers as they need it.<br>
2. Like you mentioned, storing it in a separate place and fetching them during runtime.<br>
3. If you have a backend, then instead of fetching the credentials on the front end and then accessing the cloud service, store the credentials on the backend, and access the cloud service through the backend. (recommended)  </p>

<p>There can be many other ways also to tackle this problem but they would also depend upon those projects.</p>
",5043190,2020-02-17T05:19:19.203,0,CC BY-SA 4.0,,1,,git.<br,0,"Storing such credentials in the environment file is discouraged because generally, environment files are committed into the git.<br",False,False
536,60327445,2,44612835,2020-02-20T19:44:10.567,-1,"<p>Updating this wonderful answer by John, for those that find this and see warning messages coming from the cryptography module in Python..  <code>signer</code> has been deprecated in favor of the <code>sign</code> function on the serialization object according to the documentation.</p>

<pre><code>import datetime

from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import padding
from botocore.signers import CloudFrontSigner
def rsa_signer(message):
   with open('path/to/key.pem', 'rb') as key_file:
       private_key = serialization.load_pem_private_key(
           key_file.read(),
           password=None,
           backend=default_backend()
       )
   return private_key.sign(message, padding.PKCS1v15(), hashes.SHA1())

key_id = 'AKIAIOSFODNN7EXAMPLE'
url = 'http://d2949o5mkkp72v.cloudfront.net/hello.txt'
expire_date = datetime.datetime(2017, 1, 1)

cloudfront_signer = CloudFrontSigner(key_id, rsa_signer)

# Create a signed url that will be valid until the specfic expiry date
# provided using a canned policy.
signed_url = cloudfront_signer.generate_presigned_url(
     url, date_less_than=expire_date)
print(signed_url)
</code></pre>

<p>This just updates the answer so it doesn't throw the deprecation warning, and doesn't look at security aspects of pieces in use.</p>
",5910171,2020-02-20T19:44:10.567,2,CC BY-SA 4.0,,1,>,sign</code,0,> has been deprecated in favor of the <code>sign</code,False,False
537,60359380,2,60225185,2020-02-23T05:11:12.083,3,"<p>DurandA - I believe you are absolutely correct: the simplified Lambda Proxy Integration approach relies on you catching your exceptions and returning the standardized format:</p>

<pre class=""lang-py prettyprint-override""><code>def lambda_handler(event, context):
    return {
        'statusCode': 400,
        'body': json.dumps('This is a bad request!')
    }
</code></pre>

<p>The simplified Lambda Proxy Integration feature was announced in a <a href=""https://aws.amazon.com/blogs/aws/api-gateway-update-new-features-simplify-api-development/"" rel=""nofollow noreferrer"">September 2016 blog post</a>, but one of the examples you cited was posted earlier, in a <a href=""https://aws.amazon.com/blogs/compute/error-handling-patterns-in-amazon-api-gateway-and-aws-lambda/"" rel=""nofollow noreferrer"">June 2016 blog post</a>, back when the more complicated Integration Response method was the only way.   Maybe you have stumbled on an out of date example.</p>

<p>You also posted a link to the <a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/handle-errors-in-lambda-integration.html"" rel=""nofollow noreferrer"">product documentation for error handling</a>, at the top in the section covering the Lambda proxy integration feature, it says:</p>

<blockquote>
  <p>With the Lambda proxy integration, Lambda is required to return an
  output of the following format:</p>
  
  <pre><code>{
  ""isBase64Encoded"" : ""boolean"",
  ""statusCode"": ""number"",
  ""headers"": { ... },
  ""body"": ""JSON string""
}
</code></pre>
</blockquote>

<p>Here is a working example that returns a HTTP 400 with message ""This is an exception!"" using Lambda Proxy Integration.</p>

<pre class=""lang-py prettyprint-override""><code>import json

def exception_handler(e):
    # exception to status code mapping goes here...
    status_code = 400
    return {
        'statusCode': status_code,
        'body': json.dumps(str(e))
    }

def lambda_handler(event, context):
    try:
        raise Exception('This is an exception!')
        return {
            'statusCode': 200,
            'body': json.dumps('This is a good request!')
        }
    except Exception as e:
        return exception_handler(e)
</code></pre>

<p>Output from the above:</p>

<pre class=""lang-sh prettyprint-override""><code>$ http https://**********.execute-api.us-east-2.amazonaws.com/test
HTTP/1.1 400 Bad Request
Connection: keep-alive
Content-Length: 23
Content-Type: application/json
Date: Sun, 23 Feb 2020 05:06:59 GMT
X-Amzn-Trace-Id: Root=1-********-************************;Sampled=0
x-amz-apigw-id: ****************
x-amzn-RequestId: ********-****-****-****-************

""This is an exception!""
</code></pre>

<p>I understand your frustration that you do not want to build a custom exception handler.   Fortunately, you only have to build a single handler wrapping your lambda_handler function.  Wishing you all the best!</p>
",3933995,2020-02-23T05:11:12.083,1,CC BY-SA 4.0,,1,,example.</p,0,Maybe you have stumbled on an outdated example.</p,False,False
538,60494793,2,60493487,2020-03-02T18:41:13.003,0,"<p>Solution:
Import this
<code>var unmarshalItem = require('dynamodb-marshaler').unmarshalItem;</code>
I have the response here in ""results.Items"" and now you can do this to get the normal json format perfectly.</p>

<pre><code>var items = results.Items.map(unmarshalItem);
        console.log(items);
        callback(null, items);
</code></pre>

<p>But i see this npm package was deprecated, So i will try doing this with latest <code>DynamoDB/Converter</code> and will update here.</p>
",1930155,2020-03-02T18:41:13.003,0,CC BY-SA 4.0,,1,,",",0,"But i see this npm package was deprecated,",False,False
539,60668860,2,60666957,2020-03-13T10:34:03.570,-1,"<p>Compose files only dictate communication between containers running on the same node. Service discovery is usually considered a desirable feature of distributed systems, so I'm going to assume that you're running your stack on several nodes and load-balance between them in some way to give you resilience and high-availability (HA). Otherwise, you'd just use EC2 and install docker.</p>

<p>You need to run your containers/services on a node, so you can continue to use Compose files for this. You shouldn't be using this to define the communication between services though, as you want this to pass through your service-mesh (services + discovery + proxy == service-mesh). Alternatively, it would be relatively easy to replace Compose with a bunch of <code>docker container run</code> statements in a bash script.</p>

<p>Your choice of service-mesh is (arguably) outdated. I'd take a look at consul and consul-connect these days rather than eureka, with the Envoy proxy -- this tech is a couple of generations on from Eureka and a lot simpler. </p>
",2051454,2020-03-13T10:34:03.570,0,CC BY-SA 4.0,,1,,.,0,Your choice of service-mesh is (arguably) outdated.,False,False
540,60764721,2,60761162,2020-03-19T20:39:56.200,1,"<p>t1.micro is an obsolete instance class, available only in some older AWS regions.  The us-east-2 region was never equipped with any t1.micro hardware. </p>

<p>If for some reason you still need to try to launch a t1.micro,  you may find capacity in some availability zones of us-east-1 or us-west-2, but the AMI would be different since those are regional.</p>

<p>Alternately, you might be able to launch this as t2.micro or t3.micro if the AMI is compatible with those classes.</p>
",1695906,2020-03-19T20:39:56.200,1,CC BY-SA 4.0,,1,, ,0,">t1.micro is an obsolete instance class, available only in some older AWS regions.  ",False,False
541,60770478,2,8931175,2020-03-20T08:03:24.887,0,"<p>It seems that this <strong>XSD files are outdated</strong>.</p>

<p>Just checked the official sellercentral help page for the XSD files <a href=""https://sellercentral-europe.amazon.com/gp/help/G1611"" rel=""nofollow noreferrer"">https://sellercentral-europe.amazon.com/gp/help/G1611</a>
For the OrderReport there is still <strong>release_4_1</strong> referenced.</p>

<p>Some time ago amazon has added a new field to <a href=""https://images-na.ssl-images-amazon.com/images/G/01/rainier/help/xsd/release_4_1/OrderReport.xsd"" rel=""nofollow noreferrer"">OrderReport</a> for EU markets. The new field is <code>IsSoldByAB</code>.
I am using the xsd files since many years for automatic code generation. And this fails from time to time because of new fields like this. This field is not descriped in one of this:</p>

<ul>
<li><strong>release_1_9</strong> ($Revision: #7 $, $Date: 2006/05/23 $)</li>
<li><strong>release_4_1</strong> ($Revision: #10 $, $Date: 2007/09/06 $)</li>
</ul>

<p>XSD files and I am not able to find a version that include this field.</p>

<p>Since some years I extend the XSD file on my own to generate my code. <code>IsSoldByAB</code> is just a boolean field as <code>IsPrime</code> or <code>IsBusinessOrder</code>. So this was an easy task but not ""official""...</p>
",2306226,2020-03-20T08:03:24.887,0,CC BY-SA 4.0,,1,,"

",0,"It seems that this <strong>XSD files are outdated</strong>.</p>

",False,False
542,60797039,2,42757905,2020-03-22T07:51:54.447,0,"<p>3 years later, python 2.7 is deprecated on AWS Lambda.</p>

<p>With Python 3.x you may want some steps to add this dependency.</p>

<p>You can check [1] for more information about performing a non-standard build of psycopg2.</p>

<p>These are the bash commands that I used to build a layer zip on an EC2 instance using the Amazon Linux 1 AMI from [2]:</p>

<pre><code>$ sudo yum update
$ sudo yum install python36
$ curl -O https://bootstrap.pypa.io/get-pip.py
$ python3 get-pip.py --user
$ sudo yum groupinstall ""Development Tools""
$ sudo yum install python36-devel
$ sudo yum install postgresql-devel
$ mkdir -p package/python/lib/python3.6/site-packages/
$ pip3 install psycopg2 -t package/python/lib/python3.6/site-packages/
$ mkdir package/lib
$ sudo find / -name libpq.so*
$ cp /usr/lib64/libpq.so package/lib
$ cp /usr/lib64/libpq.so.5 package/lib
$ cp /usr/lib64/libpq.so.5.5 package/lib
$ cd package
$ zip -r psycopg2.zip *
</code></pre>

<p>You can then use the zip file created when creating a layer on the Lambda console. </p>

<p>[1] <a href=""http://initd.org/psycopg/docs/install.html"" rel=""nofollow noreferrer"">http://initd.org/psycopg/docs/install.html</a></p>

<p>[2] <a href=""https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html</a></p>
",3478533,2020-03-22T07:51:54.447,0,CC BY-SA 4.0,,1,python,Lambda.</p,0,"<p>3 years later, python 2.7 is deprecated on AWS Lambda.</p",False,False
543,60970700,2,60966945,2020-04-01T12:17:08.933,3,"<p>Ozone s3-gateway uses by default the <a href=""https://hadoop.apache.org/ozone/docs/0.4.0-alpha/s3.html"" rel=""nofollow noreferrer"">path-style addressing</a> while updated sdk libraries use the virtual-hosted addressing. The quickest solution would be to switch to path-style:</p>

<pre><code>// AmazonS3Config config = new AmazonS3Config();
config.ForcePathStyle = true;
</code></pre>

<p>Alternatively, as mentioned on the docs, you could enable the virtual-hosted schema in ozone.</p>

<p>Please notice that the path-style <a href=""https://aws.amazon.com/es/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/"" rel=""nofollow noreferrer"">is going to be deprecated</a> in aws s3. </p>
",2706709,2020-04-01T12:17:08.933,0,CC BY-SA 4.0,,1,,deprecated</a,0,"Please notice that the path-style <a href=""https://aws.amazon.com/es/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/"" rel=""nofollow noreferrer"">is going to be deprecated</a",False,False
544,60986488,2,34689445,2020-04-02T07:40:44.073,19,"<h2><strong>Update</strong></h2>

<p>Since AWS CLI version 2 - <code>aws ecr get-login</code> is deprecated and the correct method is <a href=""https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html"" rel=""noreferrer""><code>aws ecr get-login-password</code></a>.</p>

<p>Therefore the correct and updated answer is the following:
<code>docker login -u AWS -p $(aws ecr get-login-password --region us-east-1) xxxxxxxx.dkr.ecr.us-east-1.amazonaws.com</code></p>
",1326702,2020-04-02T07:40:44.073,2,CC BY-SA 4.0,,1,ecr,password.html,0,"ecr get-login</code> is deprecated and the correct method is <a href=""https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html",False,False
545,61058484,2,36154484,2020-04-06T11:13:19.293,0,"<p>as of </p>

<p>aws-java-sdk-core-1.11.163</p>

<blockquote>
  <p><strong>SDKGlobalConfiguration.ENABLE_S3_SIGV4_SYSTEM_PROPERTY is deprecated.</strong></p>
</blockquote>

<p>I used the below code to call using signature_version='s3'</p>

<pre><code>private static ClientConfiguration config = new ClientConfiguration();

static {
    config.setProtocol(Protocol.HTTP);
    config.setSignerOverride(""S3SignerType"");
}
private static AmazonS3 s3client = AmazonS3ClientBuilder
    .standard()
    .withClientConfiguration(config)
    .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(""endpoint"", null))
    .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(""access_key_id"", ""secret_Key"")))
    .build();
</code></pre>
",3892213,2020-04-06T11:13:19.293,0,CC BY-SA 4.0,,1,,deprecated.</strong></p,0,ENABLE_S3_SIGV4_SYSTEM_PROPERTY is deprecated.</strong></p,False,False
546,61059883,2,11031608,2020-04-06T12:29:05.903,0,"<h2>java.time</h2>

<p>I am providing the modern answer: use java.time, the modern Java date and time API, for your date and time work. First of all because it is so much nicer to work with than the old date and time classes like <code>Date</code> and (oh, horrors) <code>SimpleDateFormat</code>, which are poorly designed. We’re fortunate that they are long outdated. An added advantage is: Your date-time string is in ISO 8601 format, and the classes of java.time parse this format as their default, that is, without any explicit formatter.</p>

<pre><code>    String stringFromCloud = ""2014-06-14T08:55:56.789Z"";
    Instant timestamp = Instant.parse(stringFromCloud);
    System.out.println(""Parsed timestamp: "" + timestamp);
</code></pre>

<p>Output:</p>

<blockquote>
  <p>Parsed timestamp: 2014-06-14T08:55:56.789Z</p>
</blockquote>

<p>Now it’s clear to see that the string has been parsed with full millisecond precision (<code>Instant</code> can parse with nanosecond precision, up to 9 decimals on the seconds). <code>Instant</code> objects will work fine as keys for your <code>SortedMap</code>.</p>

<p>Corner case: if the fraction of seconds i 0, it is not printed.</p>

<pre><code>    String stringFromCloud = ""2014-06-14T08:56:59.000Z"";
</code></pre>

<blockquote>
  <p>Parsed timestamp: 2014-06-14T08:56:59Z</p>
</blockquote>

<p>You will need to trust that when no fraction is printed, it is because it is 0. The <code>Instant</code> will still work nicely for your purpose, being sorted before instants with fraction .001, .002, etc.</p>

<h2>What went wrong in your parsing?</h2>

<p>First, you’ve got a problem that is much worse than missing milliseconds: You are parsing into the wrong time zone. The trailing <code>Z</code> in your incoming string is a UTC offset of 0 and needs to be parsed as such. What happened in your code was that <code>SimpleDateFormat</code> used the time zone setting of your JVM instead of UTC, giving rise to an error of up to 14 hours. In most cases your sorting would still be correct. Around transition from summer time (DST) in your local time zone the time would be ambiguous and parsing may therefore be incorrect leading to wrong sort order.</p>

<p>As the Mattias Isegran Bergander says in his answer, parsing of milliseconds should work in your code. The reason why you didn’t think so is probably because just a minor one of the many design problems with the old <code>Date</code> class: even though internally it has millisecond precision, its <code>toString</code> method only prints seconds, it leaves out the milliseconds.</p>

<h2>Links</h2>

<ul>
<li><a href=""https://docs.oracle.com/javase/tutorial/datetime/"" rel=""nofollow noreferrer"">Oracle tutorial: Date Time</a> explaining how to use java.time.</li>
<li><a href=""https://en.wikipedia.org/wiki/ISO_8601"" rel=""nofollow noreferrer"">Wikipedia article: ISO 8601</a></li>
</ul>
",5772882,2020-04-06T12:29:05.903,0,CC BY-SA 4.0,,1,We,.,0,We’re fortunate that they are long outdated.,False,False
547,61068801,2,61068708,2020-04-06T20:54:39.617,0,"<p>Seems like your node version is out of date.</p>

<p>You can:</p>

<ul>
<li>update node version (for example using <a href=""https://www.npmjs.com/package/n"" rel=""nofollow noreferrer"">n package</a>)</li>
<li>transpile your code to es standard that can be handled with your version of node</li>
</ul>
",11911285,2020-04-06T20:54:39.617,0,CC BY-SA 4.0,,1,,outdated.</p,0,>Seems like your node version is outdated.</p,False,False
548,61099600,2,61085087,2020-04-08T11:28:32.490,0,"<p>It turned out that I was using a pre-made lambda deployment package that was a little bit outdated. I created a venv then used a build script to create the zip file. It worked perfectly fine. </p>
",3719201,2020-04-08T11:28:32.490,0,CC BY-SA 4.0,,1,It,.,0,It turned out that I was using a pre-made lambda deployment package that was a little bit outdated.,False,False
549,61183724,2,61182599,2020-04-13T08:02:12.893,2,"<p>I would recommend that you <em>not</em> use the Classic Load Balancer. These days, you should use the <strong>Application Load Balancer</strong> or <strong>Network Load Balancer</strong>. (Anything with the name 'classic' basically means it is outdated, but still available for legacy use.)</p>

<p>There are <strong>many ways to create scaling triggers</strong>. The easiest method is to use <a href=""https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html"" rel=""nofollow noreferrer"">Target Tracking Scaling Policies for Amazon EC2 Auto Scaling</a>. This allows you to provide a <strong>target</strong> (eg <em>""CPU Utilization of 75%""</em>) and Auto Scaling will handle the details.</p>

<p>However, I note that you tagged this question as using Elastic Beanstalk. I don't think it supports Target Tracking, so instead you can specify a ""Scale-out"" and ""Scale-In"" threshold.</p>

<p>As to what number you should put in... this <strong>depends totally on your application and its typical usage patterns</strong>. You can only determine the 'correct' setting by observing your normal traffic, or by creating a test system and simulating typical usage.</p>

<p><strong>CPU Utilization</strong> <em>might</em> be a good metric to use for scaling, but this depends on what the application is doing. For example, if it is doing heavy calculations (eg video encoding), it is a good metric. However, there might be other indications of heavy usage, such as the amount of free memory or the number of users. You can only figure out which is the 'right' metric by observing what your system does when it is under load.</p>
",174777,2020-04-13T08:02:12.893,0,CC BY-SA 4.0,,1,,use.)</p,0,"(Anything with the name 'classic' basically means it is outdated, but still available for legacy use.)</p",False,False
550,61193214,2,34009873,2020-04-13T17:25:20.550,0,"<blockquote>
  <p><strong>It is possible to use Spring Data Elasticsearch with Amazon Elasticsearch</strong></p>
</blockquote>

<p>As Excerpt from Spring-data elastic search <a href=""https://docs.spring.io/spring-data/elasticsearch/docs/current/reference/html/#elasticsearch.clients.transport"" rel=""nofollow noreferrer"">doc</a></p>

<blockquote>
  <p>TransportClient is deprecated as of Elasticsearch 7 and will be
  removed in Elasticsearch 8. This is in favor of Java High Level REST
  Client. Spring Data Elasticsearch will support the TransportClient as
  long as it is available in the Elasticsearch.</p>
  
  <p>The Java High Level REST Client now is the default client of
  Elasticsearch, it provides a straight forward replacement for the
  TransportClient as it accepts and returns the very same
  request/response objects and therefore depends on the Elasticsearch
  core project</p>
</blockquote>

<p><strong>Spring Data ElasticSearch</strong> has streamlined with the latest standards of <strong>ElasticSearch</strong>, hence from <code>spring-data-elasticsearch:3.2.X</code> it provides a flexible way to achieve a custom <code>RestHighLevelClient</code>.(<a href=""https://docs.spring.io/spring-data/elasticsearch/docs/3.2.x/api/org/springframework/data/elasticsearch/config/AbstractElasticsearchConfiguration.html#elasticsearchClient"" rel=""nofollow noreferrer"">link</a>)
Even though it is possible to use HTTP based elastic search API calls with or without authentication, it will not solve the problem associated with the AWS elastic-search API calls. </p>

<p>Because any <strong>HTTP requests</strong> to <strong>AWS services or APIGW backed services</strong> have to follow <strong><code>""Signature Version 4 Signing Process(SigV4)""</code></strong> which eventually adds authentication information to AWS requests sent by HTTP. For security, most requests to AWS must be signed with an access key, which consists of an <strong><code>accesskey ID</code></strong> and <strong><code>secret access key</code></strong>. Hence, we have to follow the standards when calling the AWS ElasticSearch service.</p>

<p><strong>Let's get our hands dirty with code and delve deep into the implementation</strong></p>

<p>Please follow the steps:</p>

<p><strong>Step 1: Adding required dependencies</strong></p>

<pre><code>       &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;2.2.2.RELEASE&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;
            &lt;artifactId&gt;aws-java-sdk-elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;1.11.346&lt;/version&gt;
        &lt;/dependency&gt;
</code></pre>

<p><strong>Step 2: Adding AWS CredentialsProvider</strong></p>

<pre><code>import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicAWSCredentials;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class AWSCredentialsConfiguration {

    @Value(""${aws.es.accessKey}"")
    private String esAccessKey = null;

    @Value(""${aws.es.secretKey}"")
    private String esSecretKey = null;

    @Bean
    public AWSStaticCredentialsProvider awsDynamoCredentialsProviderDevelopment() {
        return new AWSStaticCredentialsProvider(new BasicAWSCredentials(
                esAccessKey, esSecretKey));
    }
}
</code></pre>

<p>Or if your Application running on AWS instance and you don't want to use the property-driven/hardcoded AccessKey and SecretKey then you have to assign the <strong>IAM role</strong> to your <strong>Amazon ECS task</strong> <a href=""https://docs.aws.amazon.com/AmazonECS/latest/userguide/task-iam-roles.html"" rel=""nofollow noreferrer"">for more</a>.</p>

<pre><code>import com.amazonaws.auth.AWSCredentialsProvider;
import com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class AWSCredentialsConfiguration {

    @Bean
    public AWSCredentialsProvider amazonAWSCredentialsProvider() {
        return new EC2ContainerCredentialsProviderWrapper();
    }

}
</code></pre>

<p><strong>Step 3: Adding ElasticSearchRestClientConfiguration</strong></p>

<ul>
<li>If you observe the below code we are providing a custom implementation of <code>**RestHighLevelClient**</code> to abstract method <code>**AbstractElasticsearchConfiguration#elasticsearchClient()**</code>. In this way, we are injecting the incorporated customRestHighLevelClient of <strong><code>""elasticsearchOperations""</code></strong>, <strong><code>""elasticsearchTemplate""</code></strong> beans into the spring container.</li>
<li><strong><code>HttpRequestInterceptor</code></strong> is another important thing to notice. A Special thanks to <a href=""https://github.com/awslabs/aws-request-signing-apache-interceptor/blob/master/src/main/java/com/amazonaws/http/AWSRequestSigningApacheInterceptor.java"" rel=""nofollow noreferrer"">AWSRequestSigningApacheInterceptor.java</a>, a sample implementation provided by <strong><code>AWSLabs</code></strong> helps us to add the interceptor to the RestClient with the <strong><code>AWS4Signer</code></strong> mechanism.</li>
<li><strong><code>@EnableElasticsearchRepositories</code></strong> annotation help to enable the elasticsearch data repositories.</li>
</ul>

<pre><code>import com.amazonaws.auth.AWS4Signer;
import com.amazonaws.auth.AWSCredentialsProvider;
import org.apache.http.HttpHost;
import org.apache.http.HttpRequestInterceptor;
import org.elasticsearch.client.RestClient;
import org.elasticsearch.client.RestHighLevelClient;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.elasticsearch.config.AbstractElasticsearchConfiguration;
import org.springframework.data.elasticsearch.repository.config.EnableElasticsearchRepositories;


@Configuration
@EnableElasticsearchRepositories(basePackages = ""com.demo.aws.elasticsearch.data.repository"")
public class ElasticSearchRestClientConfiguration extends AbstractElasticsearchConfiguration {

    @Value(""${aws.es.endpoint}"")
    private String endpoint = null;

    @Value(""${aws.es.region}"")
    private String region = null;

    @Autowired
    private AWSCredentialsProvider credentialsProvider = null;

    @Override
    @Bean
    public RestHighLevelClient elasticsearchClient() {
        AWS4Signer signer = new AWS4Signer();
        String serviceName = ""es"";
        signer.setServiceName(serviceName);
        signer.setRegionName(region);
        HttpRequestInterceptor interceptor = new AWSRequestSigningApacheInterceptor(serviceName, signer, credentialsProvider);
        return new RestHighLevelClient(RestClient.builder(HttpHost.create(endpoint)).setHttpClientConfigCallback(e -&gt; e.addInterceptorLast(interceptor)));
    }
}

</code></pre>

<p>Bravo Zulu! that's it. We have completed the configuration part. Now with this solution, you can leverage the spring-data elastic benefits along with Amazon elastic search service. The complete solution has been documented in <a href=""https://medium.com/@prasanth_rajendran/how-to-integrate-spring-boot-elasticsearch-data-with-aws-445e6fc72998"" rel=""nofollow noreferrer"">Medium Post</a></p>
",3303074,2020-04-13T17:25:20.550,8,CC BY-SA 4.0,,1,TransportClient,.,0,"TransportClient is deprecated as of Elasticsearch 7 and will be
  removed in Elasticsearch 8.",False,False
551,61199645,2,61199563,2020-04-14T02:08:29.793,2,"<p>You'd use the built-in <a href=""https://nodejs.org/api/crypto.html"" rel=""nofollow noreferrer"">crypto API</a> to compute hashes in node.js.</p>

<pre class=""lang-js prettyprint-override""><code>const crypto = require('crypto');
//...
const etag = crypto.createHash('md5');
// .update means to add to the buffer, you can call .update multiple times
etag.update(Buffer.from(fileBase64, 'base64'));
// .digest(encoding) gives you the computed value of buffer
const localHash = etag.digest('hex');
console.log(`localHash: ${localHash}`);
</code></pre>

<p>And as a tip, using <code>new</code> with <code>Buffer</code> is deprecated, see <a href=""https://nodejs.org/api/buffer.html#buffer_new_buffer_array"" rel=""nofollow noreferrer"">the documentation</a>.</p>
",12989672,2020-04-14T02:08:29.793,0,CC BY-SA 4.0,,1,>,"rel=""nofollow",0,"> is deprecated, see <a href=""https://nodejs.org/api/buffer.html#buffer_new_buffer_array"" rel=""nofollow",False,False
552,61236224,2,54094750,2020-04-15T18:42:17.703,1,"<p>From the Secret Manager <a href=""https://docs.aws.amazon.com/secretsmanager/latest/userguide/terms-concepts.html"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>Secrets Manager can automatically rotate your secret for you on a specified schedule. You can rotate credentials without interrupting the service if you choose to store a complete set of credentials for a user or account, instead of only the password. If you change or rotate only the password, then the old password immediately becomes obsolete, and clients must immediately start using the new password or fail. If you can instead create a new user with a new password, or at least alternate between two users, then the old user and password can continue to operate side by side with the new one, until you choose to deprecate the old one. This gives you a window of time when all of your clients can continue to work while you test and validate the new credentials. After your new credentials pass testing, you commit all of your clients to using the new credentials and remove the old credentials.</p>
</blockquote>
",2387977,2020-04-15T18:42:17.703,1,CC BY-SA 4.0,,1,you,.,0,"If you change or rotate only the password, then the old password immediately becomes obsolete, and clients must immediately start using the new password or fail.",False,False
553,61275194,2,52087307,2020-04-17T15:27:01.637,1,"<p>The <a href=""https://github.com/alliefitter/boto3_type_annotations"" rel=""nofollow noreferrer"">boto3_type_annotations</a> mentioned by Allie Fitter is deprecated, but she links to an alternative: <a href=""https://pypi.org/project/boto3-stubs/"" rel=""nofollow noreferrer"">https://pypi.org/project/boto3-stubs/</a></p>
",3927228,2020-04-17T15:27:01.637,0,CC BY-SA 4.0,,1,"noreferrer"">boto3_type_annotations</a",boto3-stubs/,0,">The <a href=""https://github.com/alliefitter/boto3_type_annotations"" rel=""nofollow noreferrer"">boto3_type_annotations</a> mentioned by Allie Fitter is deprecated, but she links to an alternative: <a href=""https://pypi.org/project/boto3-stubs/",False,False
554,61351243,2,61349955,2020-04-21T19:06:25.493,0,"<p>This works fine for me with both the awscli and boto3. For example:</p>

<pre><code>import boto3

client = boto3.client('ec2')

subnets = client.describe_subnets()

for subnet in subnets['Subnets']:
    print(subnet['AvailabilityZone'], subnet['AvailabilityZoneId'])
</code></pre>

<p>Output is:</p>

<pre><code>us-east-1b use1-az2
us-east-1e use1-az3
us-east-1d use1-az6
...
</code></pre>

<p>I think your installation of awscli and boto3 may be out of date.</p>
",271415,2020-04-21T19:06:25.493,1,CC BY-SA 4.0,,1,,outdated.</p,0,I think your installation of awscli and boto3 may be outdated.</p,False,False
555,61360061,2,61335975,2020-04-22T08:01:39.453,0,"<p>Got the same error, and this is how I solved it.</p>

<p><strong>1. Cognito policy as</strong></p>

<pre><code>{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Sid"": ""VisualEditor0"",
            ""Effect"": ""Allow"",
            ""Action"": [
                ""iot:Receive"",
                ""cognito-identity:*"",
                ""iot:Subscribe"",
                ""iot:AttachPolicy"",
                ""iot:AttachPrincipalPolicy"",
                ""iot:Connect"",
                ""mobileanalytics:PutEvents"",
                ""iot:GetThingShadow"",
                ""iot:DeleteThingShadow"",
                ""iot:UpdateThingShadow"",
                ""iot:Publish"",
                ""cognito-sync:*""
            ],
            ""Resource"": ""*""
        }
    ]
}
</code></pre>

<p>Also note that AttachPrincipalPolicy is deprecated, but for safer side I included it</p>

<p><strong>2. IoT Policy as</strong></p>

<pre><code>{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": ""iot:*"",
      ""Resource"": ""*""
    }
  ]
}
</code></pre>

<p><strong>3. Attach IoT policy to individual cognito Identity through lambda or AWS CLI.</strong> 
Using CLI this command looks like</p>

<pre><code>aws iot attach-policy --policy-name ""iot-policy"" --target ""ap-south-1:XXXX-USER-COGNITO-IDENTITY”
</code></pre>

<p>Again note AttachPrincipalPolicy is deprecated, use AttachPolicy</p>

<p>Using lambda:</p>

<pre><code>export const main = async (event, context, callback) =&gt; {
    const principal = event.requestContext.identity.cognitoIdentityId;
    const policyName = 'iot-policy';

    const iot = new AWS.Iot();
    await iot.attachPrincipalPolicy({ principal, policyName }).promise();
    callback(null, ""success"");
};
</code></pre>

<p><strong>4. Test</strong> 
If your frontend is configured properly, you should be able to solve 
<em>errorCode: 8, errorMessage: AMQJS0008I Socket closed</em> error.</p>

<p><strong>5. Fine Tune</strong>
Now Fine tune iot-policy according to your requirements and check immediately if the changes are working</p>
",3739896,2020-04-22T08:01:39.453,0,CC BY-SA 4.0,,2,,it</p,0,">Also note that AttachPrincipalPolicy is deprecated, but for safer side I included it</p",False,False
556,61383401,2,59282601,2020-04-23T09:08:41.630,0,"<p>So, as it turned out - these annotations will be supported only since Kubernetes <code>1.16</code>, which is ""coming soon"" on AWS.
Currently supported version is <code>1.15</code>, which just ignores those annotations...</p>

<p>Considering that you are using AWS-specific annotations here (<code>service.beta.kubernetes.io/aws-load-balancer-eip-allocations</code>) - I assume that this is exactly the reason why it does not work on your case.</p>

<p>As a workaround, I would advice:</p>

<ol>
<li>Create custom post-deployment script that re-configures newly-created LoadBalancer, after each Kubernetes Service Update.</li>
<li>Switch to use something more conventional, like ELB with your Container, and AutoScaling groups (that's what we did.)</li>
<li>Setup your own Kubernetes Controller (super-hard thingie, which will become completely obsolete and will just be basically a lost of time, as soon as 1.16 is officially out). See <a href=""https://www.linuxschoolonline.com/how-to-set-up-kubernetes-1-16-on-aws-from-scratch/"" rel=""nofollow noreferrer"">this how-to</a></li>
<li>Wait...</li>
</ol>

<p>Official statement:
<a href=""https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html#1-16-prequisites"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html#1-16-prequisites</a></p>

<p>Full list of annotations (when they will be ""supported"" ofc):
<a href=""https://github.com/kubernetes/kubernetes/blob/v1.16.0/staging/src/k8s.io/legacy-cloud-providers/aws/aws.go#L208-L211"" rel=""nofollow noreferrer"">https://github.com/kubernetes/kubernetes/blob/v1.16.0/staging/src/k8s.io/legacy-cloud-providers/aws/aws.go#L208-L211</a></p>

<p>Stay tuned! :(</p>
",1266071,2020-04-23T09:08:41.630,0,CC BY-SA 4.0,,1,which,.,0,"Setup your own Kubernetes Controller (super-hard thingie, which will become completely obsolete and will just be basically a lost of time, as soon as 1.16 is officially out).",False,False
557,61498340,2,61464170,2020-04-29T09:20:06.753,1,"<p>No, i think not.</p>

<p>Currently you have to aggregate the data anyways because it is only possible to request data for one day per request. Performance data is only returned for datasets that have at least one impression. So on campaign level it is very hard to reach row limitations if there are any (i did not yet reached any limit with the reporting service).</p>

<p>For the keyword report i get definitly a lot more than just 100 rows (just tested).</p>

<p>And btw.: 
API endpoints beginning with /v2/hsa are deprecated. Use /v2/sb instead.
<a href=""https://advertising.amazon.com/API/docs/en-us/info/release-notes"" rel=""nofollow noreferrer"">https://advertising.amazon.com/API/docs/en-us/info/release-notes</a></p>
",850547,2020-04-29T09:20:06.753,0,CC BY-SA 4.0,,1,endpoints,.,0,API endpoints beginning with /v2/hsa are deprecated.,False,False
558,61520167,2,53233499,2020-04-30T09:27:21.150,0,"<p>As follow up on @jarmod's answer:</p>

<p>If you want to call <em>update_item</em> with a String Set, then you'll insert a set via <em>ExpressionAttributeValues</em> property like shown below:</p>

<pre><code>entry = table.put_item(
    ExpressionAttributeNames={
        ""#begintime"": ""begintime"",
        ""#description"": ""description"",
        ""#endtime"": ""endtime"",
        ""#name"": ""name"",
        ""#type"": ""type"",
        ""#weekdays"": ""weekdays""
    },
    ExpressionAttributeValues={
        "":begintime"": '09:00',
        "":description"": 'Hello there',
        "":endtime"": '14:00',
        "":name"": 'james',
        "":type"": ""period"",
        "":weekdays"": set(['mon', 'wed', 'fri'])
    },
    UpdateExpression=""""""
        SET #begintime= :begintime,
        #description = :description,
        #endtime = :endtime,
        #name = :name,
        #type = :type,
        #weekdays = :weekdays
        """"""
)
</code></pre>

<p>(Hint: Usage of <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LegacyConditionalParameters.html"" rel=""nofollow noreferrer"">AttributeUpdates</a> (related <em>Item</em>s equivalent for <em>put_item</em> calls) is deprecated, therefore I recommend using <em>ExpressionAttributeNames</em>, <em>ExpressionAttributeValues</em> and <em>UpdateExpression</em>).</p>
",3680249,2020-04-30T09:27:21.150,0,CC BY-SA 4.0,,1,,em,0,">put_item</em> calls) is deprecated, therefore I recommend using <em",False,False
559,61576684,2,21575428,2020-05-03T15:05:11.457,1,"<p>I think the <strong>aws-cloudfront-sign</strong> package is deprecated now. You can use this package
<a href=""https://www.npmjs.com/package/aws-sdk"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/aws-sdk</a></p>

<p>Here is the link to know how you can use this:-</p>

<p><a href=""https://medium.com/roam-and-wander/using-cloudfront-signed-urls-to-serve-private-s3-content-e7c63ee271db"" rel=""nofollow noreferrer"">https://medium.com/roam-and-wander/using-cloudfront-signed-urls-to-serve-private-s3-content-e7c63ee271db</a></p>
",9358652,2020-05-03T15:05:11.457,0,CC BY-SA 4.0,,1,,.,0,I think the <strong>aws-cloudfront-sign</strong> package is deprecated now.,False,False
560,61632539,2,61619575,2020-05-06T10:10:09.033,0,"<p>I finally managed to find a solution to the issue, albeit it is not explaining the strange behavior with the charts that I explained in the question.</p>

<p>My problem was similar to what Abhinaya suggested in her response. The Lambda function was not sending the signal properly because of a programming error. Essentially, I took the code from <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html"" rel=""nofollow noreferrer"">the documentation</a> (the one for Python 3, second fragment starting by the end) and apparently I mistakenly removed the line for retrieving the <code>ResponseURL</code>.  Of course, that was failing.</p>

<p>A side-comment about this: be careful when using Python's <code>cfnresponse</code> library or even the code snippet I linked in the documentation. It relies on <code>botocore.vendored</code> which was deprecated and no longer exist in latest <code>botocore</code> releases. Therefore, it will fail if your code relies on new versions of this library (as in my case). A simple solution is to replace <code>botocore.vendored.requests</code> with the <code>requests</code> library.</p>

<p>Still, there is some strange behavior that I cannot understand. On creation, the Lambda function is not recording anything to CloudWatch and there is this strange behavior in the charts that I explained in my question. However, this only happens on creation. If the function is manually invoked, or is invoked as part of the delete process (when removing the CFN stack), then it does write to CloudWatch. Therefore, the problem only occurs in the first invokation, apparently.</p>

<p>Best.</p>
",2377885,2020-05-06T10:10:09.033,0,CC BY-SA 4.0,,1,,botocore</code,1,It relies on <code>botocore.vendored</code> which was deprecated and no longer exist in latest <code>botocore</code,False,False
561,61679858,2,61679775,2020-05-08T13:05:32.290,1,"<p>Try updating your version of the CLI, it is likely out of date if the verb does not display yet.</p>
",13460933,2020-05-08T13:05:32.290,4,CC BY-SA 4.0,,1,,yet.</p,1,"<p>Try updating your version of the CLI, it is likely outdated if the verb does not display yet.</p",False,False
562,61705390,2,61667335,2020-05-09T23:55:47.710,1,"<p>Just adding to the existing comment. SMTPClient is actually obsolete/deprecated but is not being marked as so. See comments here (<a href=""https://github.com/dotnet/dotnet-api-docs/issues/2986#issuecomment-430805681"" rel=""nofollow noreferrer"">https://github.com/dotnet/dotnet-api-docs/issues/2986#issuecomment-430805681</a>)</p>

<p>Essentially it boils down to SmtpClient hasn't been updated in years and is missing many features. The .NET Core team wanted to mark it as obsolete, but some developers have existing projects with ""Warnings as Errors"" turned on. It would instantly make any project that is using SmtpClient with Warnings as Errors turned on suddenly stop building. So... It's kinda deprecated, but not being marked so in some official docs. </p>

<p>MailKit is actually being pushed by Microsoft full stop for people to use when it comes to Email. Much to some developers chargrin who don't want to use a third party library for such a ""simple"" and common feature. Just in my personal experience, I think Mailkit is great and super easy to use. A quick guide to getting up and running is here : <a href=""https://dotnetcoretutorials.com/2017/11/02/using-mailkit-send-receive-email-asp-net-core/"" rel=""nofollow noreferrer"">https://dotnetcoretutorials.com/2017/11/02/using-mailkit-send-receive-email-asp-net-core/</a></p>
",177516,2020-05-09T23:55:47.710,0,CC BY-SA 4.0,,3,SMTPClient,.,2,SMTPClient is actually obsolete/deprecated but is not being marked as so.,False,False
563,61740781,2,61719005,2020-05-11T22:52:18.423,2,"<p>There are several SDK's that can be used with Cognito.</p>

<p>You can use the AWS js SDK to make low level API calls but you will lose the benefits that the Cognito Client Side SDKS provide like session management, caching of tokens and SRP calculations.
<a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityServiceProvider.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityServiceProvider.html</a></p>

<p>You can also use the Cognito Identity SDK which Amplify uses under the hood:
<a href=""https://www.npmjs.com/package/amazon-cognito-identity-js"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/amazon-cognito-identity-js</a></p>

<p>The Auth SDK if you are going to be integrating with the OAuth endpoints, which is now deprecated but can still be used or referenced, Amplify also uses or has similar functionality.
<a href=""https://github.com/amazon-archives/amazon-cognito-auth-js"" rel=""nofollow noreferrer"">https://github.com/amazon-archives/amazon-cognito-auth-js</a></p>

<p>And then finally Amplify which is the go to, feature rich client side SDK which for the majority of usecases should be the SDK of choice in my opinion.</p>
",12750871,2020-05-11T22:52:18.423,1,CC BY-SA 4.0,,1,,"
",0,"if you are going to be integrating with the OAuth endpoints, which is now deprecated but can still be used or referenced, Amplify also uses or has similar functionality.
",False,False
564,61760340,2,61746052,2020-05-12T19:29:29.257,2,"<p>It's possible that certificate was signed by a CA which is also not trusted by your browser. You'll need <em>one</em> of the following public certificates in your browser's truststore:
 * Edge/Node/NiFi certificate - the certificate you're viewing above
 * CA certificate - the certificate of the entity which signed this certificate (""Enterprise"" according to the dialog)</p>

<p>Using a tool like <code>openssl s_client</code> may help with diagnosing and obtaining the certificates in the chain. </p>

<p>For the record, <a href=""https://nifi.apache.org/docs/nifi-docs/html/toolkit-guide.html#wildcard_certificates"" rel=""nofollow noreferrer"">wildcard certificates are expressly discouraged</a> and not supported for Apache NiFi. </p>
",70465,2020-05-12T19:29:29.257,0,CC BY-SA 4.0,,1,,discouraged</a,0,"noreferrer"">wildcard certificates are expressly discouraged</a",False,False
565,61848227,2,61845389,2020-05-17T07:13:48.327,0,"<p>I'd say it really does not matter as both share 95% of the same content. There are many good resources out there for it but do be aware the 001 exam is soon due to be deprecated so will not be available to sit (you'll still keep the cert).</p>
",13460933,2020-05-17T07:13:48.327,0,CC BY-SA 4.0,,1,,deprecated,0,There are many good resources out there for it but do be aware the 001 exam is soon due to be deprecated,False,False
566,61854149,2,61842861,2020-05-17T15:18:52.527,1,"<p>The usual way to get the DDL from a Glue table is to run <a href=""https://docs.aws.amazon.com/athena/latest/ug/show-create-table.html"" rel=""nofollow noreferrer""><code>SHOW CREATE TABLE foo</code></a>, but since Glue has created a table that does not work in Athena, I assume this fails.</p>

<p>In your specific case you should just us the schema suggested by the <a href=""https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html"" rel=""nofollow noreferrer"">Athena documentation on querying CloudTrail</a>. Apart from the partitioning it's as good as it gets. As you know, there are free-form properties that are service-dependent in CloudTrail events, and there is no schema that will capture everything (even if there was, as soon as a new service was launched it would be outdated). Stick with <code>string</code> for the columns corresponding to the free-form properties and use <a href=""https://prestodb.github.io/docs/0.172/functions/json.html"" rel=""nofollow noreferrer"">Athena/Presto's JSON functions</a> to query these columns.</p>

<p>I would make one small modification to the schema and have a different set of partition keys. The docs use region, year, month, day. You may want to add account ID and organization ID (if your trail is an organization trail) – but more importantly you should not have year, month, and date as separate partition keys, it makes querying date ranges unnecessarily complicated. There is no reason not to simply use ""date"" (or ""dt"" if you want to avoid having to quote it), typed either as a <code>string</code> or a <code>date</code> as the partition key, and add partitions like this:</p>

<pre><code>ALTER TABLE cloud_trail ADD 
PARTITION (account_id = '1234567890', region = 'us-east-1', dt = '2020-05-17')
LOCATION 's3://trails/AWSLogs/Account_ID/CloudTrail/us-east-1/2020/05/17/'
</code></pre>

<p>Just because something is separated by slashes in the S3 key doesn't mean it has to be separate partition keys. Using a single key makes it easy to do range queries like <code>WHERE ""date"" BETWEEN '2019-12-01' AND '2020-06-01'</code>.</p>

<p>Glue Crawlers are pretty terrible when you don't hit the use case they were intended for spot on, and I'm not surprised at all that it creates an unusable schema. In a way it's pretty amazing that there are so many cases of AWS services that predate Glue and where Glue just creates unusable results when used on their outputs.</p>

<p>For CloudTrail the schema-discovery aspect of Glue Crawlers is not necessary, and will probably mostly cause problems due to the free-form properties. The other aspect, adding new partitions, can instead be solved with a Lambda function running once per day adding the next day's partitions (since tomorrow's partitions are deterministic you don't have to wait until there is data to add partitions).</p>
",1109,2020-05-17T15:18:52.527,0,CC BY-SA 4.0,,1,,.,0,"As you know, there are free-form properties that are service-dependent in CloudTrail events, and there is no schema that will capture everything (even if there was, as soon as a new service was launched it would be outdated).",False,False
567,61861373,2,61857002,2020-05-18T02:12:00.830,0,"<p>The cognito sdk is specific to aws and is not a general purpose oauth sdk. Also note that this sdk (<a href=""https://github.com/amazon-archives/amazon-cognito-identity-js"" rel=""nofollow noreferrer"">https://github.com/amazon-archives/amazon-cognito-identity-js</a>) is now deprecated in favour of aws amplify js (<a href=""https://github.com/aws-amplify/amplify-js"" rel=""nofollow noreferrer"">https://github.com/aws-amplify/amplify-js</a>).</p>

<p>If you want to use OAuth and openId connect approach, I would recommend to use oidc-client-js (<a href=""https://github.com/IdentityModel/oidc-client-js"" rel=""nofollow noreferrer"">https://github.com/IdentityModel/oidc-client-js</a>).  </p>
",139799,2020-05-18T02:12:00.830,0,CC BY-SA 4.0,,1,,"rel=""nofollow",0,"Also note that this sdk (<a href=""https://github.com/amazon-archives/amazon-cognito-identity-js"" rel=""nofollow noreferrer"">https://github.com/amazon-archives/amazon-cognito-identity-js</a>) is now deprecated in favour of aws amplify js (<a href=""https://github.com/aws-amplify/amplify-js"" rel=""nofollow",False,False
568,61868761,2,61863957,2020-05-18T11:41:15.863,1,"<p>Sorted elements are sorted within a partition. You would need to have all results on the same partition.</p>

<p>But obviously you don't want just one partition, you are back to an SQL database. The uncomfortable and uneasy way this is done in DynamoDB is using Streams. When you have new elements or updates, you can check if those elements are in the top <code>N</code> positions. If they are then replace that value, for example, say you have people with money:</p>

<pre><code>PK            Attributes:  
#Entity#21    name=Fred.      money=5,000     
#Entity#22    name=Bob.       money=10,000     
#Entity#23    name=Smith.     money=1,000     
...
</code></pre>

<p>Then we can keep track of the top 10 richest people:</p>

<pre><code>PK              SORT               Attributes:      
#Money#Highest    1               id=#Entity#22    value=10,000
#Money#Highest    2               id=#Entity#102   value=9,000
...
</code></pre>

<p>Then when you want the richest people, you do a query with <code>PK=#Money#Highest</code>. You might also copy more attributes in depending on your query. This is pretty much the go, if you want to calculate the top <code>something</code> across partitions you setup streams and do it yourself. Note that though these totals will be out of date by some seconds depending on your stream settings. You stream Lambda would be something like:</p>

<pre><code>const handler = (event, context, callback) =&gt; {
    event.Records.forEach((ev, i) =&gt; {
        if (ev.eventName === ""INSERT"" || ev.eventName === ""UPDATE"" || ) {
          // TODO
        }
      }
    }
</code></pre>

<p>Pretty annoying I know! But this is the weird way this stuff is implemented. But it is extremely quick as you are only ever retrieving pre-calculated values. And that is the whole way which you work with Dynamo, Storage is cheap, compute expensive, optimize compute, and duplicate data as you need because hey it's cheap anyway.</p>
",4033292,2020-05-18T11:41:15.863,0,CC BY-SA 4.0,,1,,.,0,Note that though these totals will be outdated by some seconds depending on your stream settings.,False,False
569,61957641,2,61938052,2020-05-22T14:47:09.187,0,"<p>I got help in the lightbend forum <a href=""https://discuss.lightbend.com/t/alpakka-s3-connection-issue/6551"" rel=""nofollow noreferrer"">here</a>. </p>

<p>The issue was solved by setting the following parameter: </p>

<pre><code>alpakka.s3.path-style-access = true
</code></pre>

<p>Since the documentation says, this value is going to be deprecated, I did not consider to specify it.</p>

<p>In my original post, I outlined two approaches setting the params, one via <code>application.conf</code> and one programmatically via <code>S3Ext</code>. First does work by setting the value as shown above, the second approach looks like this:</p>

<pre><code>val settings: S3Settings = S3Ext(system).settings
      .withEndpointUrl(s3Host)
      .withBufferType(MemoryBufferType)
      .withCredentialsProvider(credentialsProvider)
      .withListBucketApiVersion(ApiVersion.ListBucketVersion2)
      .withS3RegionProvider(regionProvider)
      .withPathStyleAccess(true)
</code></pre>

<p>Here the last line is crucial, even though I'm getting a deprecation warning.</p>

<p>But in the end, this was solving the issue.</p>
",979457,2020-05-22T14:47:09.187,0,CC BY-SA 4.0,,1,,it.</p,1,">Since the documentation says, this value is going to be deprecated, I did not consider to specify it.</p",False,False
570,62019491,2,62013028,2020-05-26T09:59:58.160,1,"<blockquote>
  <p>Does anyone happen to know which it is? Why doesn't the AWS walkthrough need me to install Kube2iam?</p>
</blockquote>

<p>Yes, I can authoritatively answer this. In 09/2019 <a href=""https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/"" rel=""nofollow noreferrer"">we launched</a> a feature in EKS called <a href=""https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html"" rel=""nofollow noreferrer"">IAM Roles for Service Accounts</a>. This makes <code>kube2iam</code> and other solutions obsolete since we support least-privileges access control on the pod level now natively.</p>

<p>Also, yes, the ALB IC walkthrough should be updated.</p>
",396567,2020-05-26T09:59:58.160,1,CC BY-SA 4.0,,1,,natively.</p,0,This makes <code>kube2iam</code> and other solutions obsolete since we support least-privileges access control on the pod level now natively.</p,False,False
571,62087601,2,62086741,2020-05-29T13:55:04.757,-1,"<p>Could you provide the handler code using this function callServerEndpoint?</p>

<p>The issue can also be a timeout on the lambda function. It occurs when the post is longer than the timeout set on the lambda function. Cloud watch should trace it by the way.</p>

<p>PS: The request package is deprecated see <a href=""https://github.com/request/request/issues/3142"" rel=""nofollow noreferrer"">link</a>. It might be interesting to look at axios, superagent or other http client packages</p>
",1910637,2020-05-29T13:55:04.757,0,CC BY-SA 4.0,,1,package,issues/3142,0,"PS: The request package is deprecated see <a href=""https://github.com/request/request/issues/3142",False,False
572,62098397,2,40383470,2020-05-30T05:50:13.120,0,"<p>botocore.vendored is deprecated and will be removed from Lambda after 2021/01/30.</p>

<p>Here is an update version</p>

<pre><code>Type: AWS::Lambda::Function
Properties:
  Code: 
    ZipFile: 
      !Sub |
        import json, boto3, logging
        import cfnresponse
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)

        def lambda_handler(event, context):
            logger.info(""event: {}"".format(event))
            try:
                bucket = event['ResourceProperties']['BucketName']
                logger.info(""bucket: {}, event['RequestType']: {}"".format(bucket,event['RequestType']))
                if event['RequestType'] == 'Delete':
                    s3 = boto3.resource('s3')
                    bucket = s3.Bucket(bucket)
                    for obj in bucket.objects.filter():
                        logger.info(""delete obj: {}"".format(obj))
                        s3.Object(bucket.name, obj.key).delete()

                sendResponseCfn(event, context, cfnresponse.SUCCESS)
            except Exception as e:
                logger.info(""Exception: {}"".format(e))
                sendResponseCfn(event, context, cfnresponse.FAILED)

        def sendResponseCfn(event, context, responseStatus):
            responseData = {}
            responseData['Data'] = {}
            cfnresponse.send(event, context, responseStatus, responseData, ""CustomResourcePhysicalID"")            

  Handler: ""index.lambda_handler""
  Runtime: python3.7
  MemorySize: 128
  Timeout: 60
  Role: !GetAtt TSIemptyBucketOnDeleteFunctionRole.Arn    
</code></pre>
",3445514,2020-05-30T05:50:13.120,0,CC BY-SA 4.0,,1,botocore.vendored,2021/01/30.</p,0,botocore.vendored is deprecated and will be removed from Lambda after 2021/01/30.</p,False,False
573,62124324,2,56531567,2020-06-01T01:10:01.477,1,"<p>Windows <em>Network Discovery</em> actually opens a bunch of different inbound/outbound ports on the Windows Server instance. A (somewhat) outdated list is available in some scientific publications and books, e.g. [1]:</p>

<p><strong>Inbound</strong></p>

<blockquote>
  <p>■ Network Discovery (LLMNR-UDP-In) Creates an inbound rule to allow Link Local Multicast Name Resolution on UDP port 5355.</p>
  
  <p>■
  Network Discovery (NB-Datagram-ln) Creates an inbound rule to allow NetBIOS Datagram transmission and reception on UDP port 138.</p>
  
  <p>■
  Network Discovery (NB-Name-In) Creates an inbound rule to allow NetBIOS Name Resolution on UDP port 137.</p>
  
  <p>■
  Network Discovery (Pub-WSD-In) Creates an inbound rule to discover devices via Function Discovery on UDP port 3702.</p>
  
  <p>■
  Network Discovery (SSDP-In) Creates an inbound rule to allow use of the Simple Service Discovery Protocol on UDP port 1900.</p>
  
  <p>■
  Network Discovery (UPnP-In) Creates an inbound rule to allow use of Universal Plug and Play on TCP port 2869.</p>
  
  <p>■
  Network Discovery (WSD Events-In) Creates an inbound rule to allow WSDAPI Events via Function Discovery on TCP port 5357.</p>
  
  <p>■
  Network Discovery (WSD EventsSecure-In) Creates an inbound rule to allow Secure WSDAPI Events via Function Discovery on TCP port 5358.</p>
  
  <p>■
  Network Discovery (WSD-In) Creates an inbound rule to discover devices via Function Discovery on UDP port 3702.</p>
</blockquote>

<p><strong>Outbound</strong></p>

<blockquote>
  <p>■ Network Discovery (LLMNR-TCP-Out) Creates an outbound rule to allow LLMNIL on TCP port 5355.</p>
  
  <p>■
  Network Discovery (LLMNR-UDP-Out) Creates an outbound rule to allow LLMNR on UDP port 5355.</p>
  
  <p>■
  Network Discovery (NB-Datagram-Out) Creates an outbound rule to allow NetBIOS Datagram transmission and reception on UDP port 138.</p>
  
  <p>■
  Network Discovery (NB-Name-Out) Creates an outbound rule to allow NetBIOS Name Resolution on UDP port 137.</p>
  
  <p>■
  Network Discovery (Pub WSD-Out) Creates an outbound rule to discover devices via Function Discovery on UDP port 3702.</p>
  
  <p>■
  Network Discovery (SSDP-Out) Creates an outbound rule to allow use of the Simple Service Discovery Protocol on UDP port 1900.</p>
  
  <p>■
  Network Discovery (UPnPHost-Out) Creates an outbound rule to allow the use of Universal Plug and Play over TCP (all ports).</p>
  
  <p>■
  Network Discovery (UPnP-Out) Creates a second outbound rule to allow the use of Universal Plug and Play over TCP (all ports).</p>
  
  <p>■
  Network Discovery (WSD Events-Out) Creates an outbound rule to allow WSDAPI Events via Function Discovery on TCP port 5357.</p>
  
  <p>■
  Network Discovery (WSD EventsSecure-Out) Creates an outbound rule to allow for Secure WSDAPI Events via Function Discovery on TCP port 5358.</p>
  
  <p>■
  Network Discovery (WSD-Out) Creates an outbound rule to discover devices via Function Discovery on UDP port 3702.</p>
</blockquote>

<p>If your security groups for the EC2 instance are set up correctly, there should probably be no real security implications. Inbound ports used by several network discovery features should be blocked by AWS Security Groups by default. </p>

<p>What I honestly do not know: Whether the Windows services create outbound traffic which makes the EC2 instance visible to other instances inside the same VPC subnet. That is definitely possible... Maybe there is someone reading this thread and knows this for sure??</p>

<p>Btw.: I found two discussions on the offical AWS forums. Maybe they are useful to someone reading this thead: [2][3].</p>

<h3>References</h3>

<p>[1] <a href=""https://books.google.de/books?id=O3XsuCoYj6kC&amp;pg=PA200&amp;lpg=PA200&amp;dq=%22Enabling+Network+Discovery+opens+the+following+inbound+ports+in+the+Windows+Firewall:%22&amp;source=bl&amp;ots=jEHw02AMfR&amp;sig=ACfU3U20QrJMbcOgOcRp38uDnh7ZSqwtdg&amp;hl=de&amp;sa=X&amp;ved=2ahUKEwiu2Pm0sd_pAhUiw8QBHS2IBQYQ6AEwAXoECAoQAQ#v=onepage&amp;q=%22Enabling%20Network%20Discovery%20opens%20the%20following%20inbound%20ports%20in%20the%20Windows%20Firewall%3A%22&amp;f=false"" rel=""nofollow noreferrer"">Google Books: How to Cheat at Microsoft Vista Administration</a><br>
[2] <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=62760"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=62760</a><br>
[3] <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=110844"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=110844</a></p>
",10473469,2020-06-01T01:10:01.477,0,CC BY-SA 4.0,,1,,1]:</p,0,"A (somewhat) outdated list is available in some scientific publications and books, e.g. [1]:</p",False,False
574,62340675,2,62339253,2020-06-12T08:41:50.330,0,"<p>Based on the comments, the issue was not the command used. <strong>The command was correct</strong>. The problem was with the outdated AWS CLI used. </p>

<p>The solution was to <strong>updated the AWS CLI</strong>.</p>
",248823,2020-06-12T08:41:50.330,0,CC BY-SA 4.0,,1,,.,0,The problem was with the outdated AWS CLI used.,False,False
575,62407506,2,62269305,2020-06-16T11:34:53.313,0,"<p>if you want to monitor Application Load Balancer, not a deprecated Classic one, use this configuration</p>

<pre><code>---
region: us-east-1
metrics:
- aws_namespace: AWS/ApplicationELB
  aws_metric_name: HTTPCode_Target_5XX_Count
  aws_dimensions: [AvailabilityZone, LoadBalancer, TargetGroup]
  aws_statistics: [Sum]
</code></pre>
",13751405,2020-06-16T11:34:53.313,0,CC BY-SA 4.0,,1,,configuration</p,1,"<p>if you want to monitor Application Load Balancer, not a deprecated Classic one, use this configuration</p",False,False
576,62553122,2,62552039,2020-06-24T10:38:31.507,3,"<p>It's not currently possible.</p>
<p>The <a href=""https://www.terraform.io/docs/providers/aws/r/vpc.html"" rel=""nofollow noreferrer""><code>aws_vpc</code> resource</a> has support for <a href=""https://www.terraform.io/docs/providers/aws/r/vpc.html#enable_classiclink"" rel=""nofollow noreferrer"">enabling ClassicLink</a> but there's no way to attach an EC2 instance to a VPC via ClassicLink. There's an <a href=""https://github.com/terraform-providers/terraform-provider-aws/issues/293"" rel=""nofollow noreferrer"">existing, stale feature request for it</a> but not much interest in it.</p>
<p>Unfortunately EC2 Classic things are deprecated in favour of VPC based EC2 resources and it's not even possible for a person without an EC2 Classic enabled account to be able to test the functionality either so feature requests like this must be driven entirely by the very few AWS customers still using EC2 Classic but are also using Terraform for automation. If you feel up to the task you could raise a pull request to expose the functionality requested in that issue via the <a href=""https://docs.aws.amazon.com/sdk-for-go/api/service/ec2/#EC2.AttachClassicLinkVpc"" rel=""nofollow noreferrer""><code>AttachClassicLinkVPC API call</code></a>.</p>
<p>I'd recommend migrating your remaining EC2 Classic instances to VPC based ones to avoid issues around the use of EC2 Classic.</p>
",2291321,2020-06-24T10:38:31.507,0,CC BY-SA 4.0,,1,things,.,1,>Unfortunately EC2 Classic things are deprecated in favour of VPC based EC2 resources and it's not even possible for a person without an EC2 Classic enabled account to be able to test the functionality either so feature requests like this must be driven entirely by the very few AWS customers still using EC2 Classic but are also using Terraform for automation.,False,False
577,62663804,2,62462790,2020-06-30T18:19:11.550,1,"<p>For older containers using the deprecated <code>sagemaker_containers</code>, the approach you described is correct.</p>
<p>For newer containers that use <a href=""https://github.com/aws/sagemaker-training-toolkit"" rel=""nofollow noreferrer""><code>sagemaker-training-toolkit</code></a>, this is how you retrieve information about the environment: <a href=""https://github.com/aws/sagemaker-training-toolkit#get-information-about-the-container-environment"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-training-toolkit#get-information-about-the-container-environment</a></p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker_training import environment

env = environment.Environment()

job_name = env[&quot;job_name&quot;]
</code></pre>
<p>You can check the <a href=""https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/dlc-release-notes.html"" rel=""nofollow noreferrer"">DLC Release Notes</a> to see what's installed in each version.</p>
",1344518,2020-06-30T18:19:11.550,0,CC BY-SA 4.0,,1,,code,0,>For older containers using the deprecated <code,False,False
578,62692054,2,25900224,2020-07-02T08:22:22.887,1,"<p>Today (2020) The answer is well outdated. Bitbucket pipelines exists today and there are fairly good documentations on this:</p>
<p><a href=""https://support.atlassian.com/bitbucket-cloud/docs/deploy-to-aws-with-elastic-beanstalk/"" rel=""nofollow noreferrer"">https://support.atlassian.com/bitbucket-cloud/docs/deploy-to-aws-with-elastic-beanstalk/</a></p>
",3101643,2020-07-02T08:22:22.887,0,CC BY-SA 4.0,,1,answer,.,0,The answer is well outdated.,False,False
579,62719148,2,62716606,2020-07-03T16:12:04.853,3,"<p>That is the Redshift error for referencing leader node information in a compute node based query - yes, it is a little cryptic.  This often happens when you references leader tables and try to join them with compute node tables.  But this isn't what you are doing.  In your case I believe that &quot;now()&quot; is a leader function and deprecated - you need to use &quot;getdate()&quot; instead.</p>
",13350652,2020-07-03T16:12:04.853,0,CC BY-SA 4.0,,1,,;,0,In your case I believe that &quot;now()&quot; is a leader function and deprecated - you need to use &quot;getdate()&quot;,False,False
580,62827015,2,61893719,2020-07-10T03:44:47.160,2,"<p>Maybe your code is outdated. For anyone who aims to use <code>fetch_mldata</code> in digit handwritten project, you should <code>fetch_openml</code> instead. (<a href=""https://stackoverflow.com/questions/47324921/cant-load-mnist-original-dataset-using-sklearn/52297457"">link</a>)</p>
<p>In old version of sklearn:</p>
<pre><code>from sklearn.externals import joblib
mnist = fetch_mldata('MNIST original')
</code></pre>
<p>In <strong>sklearn 0.23</strong> (stable release):</p>
<pre><code>import sklearn.externals
import joblib
    
dataset = datasets.fetch_openml(&quot;mnist_784&quot;)

features = np.array(dataset.data, 'int16')
labels = np.array(dataset.target, 'int')
</code></pre>
<p>For more info about deprecating <code>fetch_mldata</code> see scikit-learn <a href=""https://scikit-learn.org/0.20/modules/generated/sklearn.datasets.fetch_mldata.html"" rel=""nofollow noreferrer"">doc</a></p>
",7165473,2020-07-10T03:44:47.160,0,CC BY-SA 4.0,,1,code,.,0,Maybe your code is outdated.,False,False
581,62944334,2,62943396,2020-07-16T22:17:59.073,3,"<p>Sadly <code>CloudFormer</code> is deprecated and <strong>no longer maintained</strong> by AWS. Thus Its not recommended for use as its not reliable.</p>
<p>You can have a look at a third party tool called <a href=""https://former2.com/"" rel=""nofollow noreferrer"">Former2</a> which seems to be much more useful and reliable than the <code>CloudFormer</code>:</p>
<blockquote>
<p>Former2 allows you to <strong>generate Infrastructure-as-Code outputs</strong> from your existing resources within your AWS account. By making the relevant calls using the AWS JavaScript SDK, Former2 will scan across your infrastructure and present you with the list of resources for you to choose which to generate outputs for.</p>
</blockquote>
",248823,2020-07-16T22:17:59.073,1,CC BY-SA 4.0,,1,>,.,1,> is deprecated and <strong>no longer maintained</strong> by AWS.,False,False
582,62944571,2,62943437,2020-07-16T22:44:17.210,2,"<p>Yeah from <a href=""https://docs.aws.amazon.com/lambda/latest/dg/runtime-support-policy.html"" rel=""nofollow noreferrer"">this</a> page you can see that Node.js 8.10 was deprecated March 6, 2020.</p>
<p>Deprecation occurs in two phases. During the first phase, you can no longer create functions that use the deprecated runtime. For at least 30 days, you can continue to update existing functions that use the deprecated runtime. After this period, both function creation and updates are disabled permanently. However, the function continues to be available to process invocation events.</p>
<p>You will however be able to change your runtime to a newer version of node and then you'll be able to update your function with new code.</p>
",9115027,2020-07-16T22:44:17.210,0,CC BY-SA 4.0,,3,,2020.</p,1,"> page you can see that Node.js 8.10 was deprecated March 6, 2020.</p",False,False
583,62979598,2,62978643,2020-07-19T11:22:58.010,0,"<p><code>create_default_context</code> will use ssl certificates stored on your instance. Seems one for google/gmail is outdated or can be found.</p>
<p>You can try to manually add them from <a href=""https://support.google.com/a/answer/6180220?hl=en"" rel=""nofollow noreferrer"">google page</a>. Also try updating your instance. Its not clear which OS or its version you use. Maybe it needs update or upgrade.</p>
<p>A quick and rather not-recommended workaround would be to disable ssl cert verrification:</p>
<pre><code>cont = ssl.create_default_context()
cont.check_hostname = False
cont.verify_mode = ssl.CERT_NONE
</code></pre>
",248823,2020-07-19T11:22:58.010,2,CC BY-SA 4.0,,1,,be,0,Seems one for google/gmail is outdated or can be,False,False
584,62983232,2,62968120,2020-07-19T16:59:36.297,1,"<p>As it is stated in the <a href=""https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/accessing-elasticache.html"" rel=""nofollow noreferrer"">documentation</a> it is not recommended to access your Elasticache cluster like you are trying to connect.</p>
<blockquote>
<p>Elasticache is a service designed to be used internally to your VPC. External access is discouraged due to the latency of Internet traffic and security concerns. However, if external access to Elasticache is required for test or development purposes, it can be done through a VPN.</p>
</blockquote>
<p>What you may do is creating a new api which communicates with AWS Elasticache Redis cluster and your mobile back-end will communicate with Redis by using this API.</p>
<ul>
<li>You may deploy your new api to a EC2 instance, arrange VPC and security groups.</li>
<li>You may deploy your new api to a lambda, put into the VPC. Your mobile back-end may communicate with Redis via using this lambda(integrated with Api gateway).</li>
</ul>
<p>By this way it will be more secure, easily manageable(no need to put credentials on your mobile app's), easily maintainable (you may make changes easier).</p>
",2188922,2020-07-19T16:59:36.297,3,CC BY-SA 4.0,,1,access,.,0,External access is discouraged due to the latency of Internet traffic and security concerns.,False,False
585,63010234,2,56982947,2020-07-21T08:13:54.443,0,"<p><code>fluent.**</code> has been deprecated, we could use this match annotation in order to exclude fluentd logs:</p>
<pre class=""lang-xml prettyprint-override""><code>    &lt;match @FLUENT_LOG&gt;
      @type null
    &lt;/match&gt;
</code></pre>
",7690356,2020-07-21T08:13:54.443,0,CC BY-SA 4.0,,1,,logs:</p,0,">fluent.**</code> has been deprecated, we could use this match annotation in order to exclude fluentd logs:</p",False,False
586,63209965,2,10222843,2020-08-01T20:53:22.577,0,"<p>There has been some development since this question was asked. The question was discussed in more detail in jira <a href=""https://issues.apache.org/jira/browse/MAPREDUCE-2389"" rel=""nofollow noreferrer"">MAPREDUCE-2389</a>.</p>
<p>A key part in this jira, and any other jira that was linked here or there, is that the issue was linked to the jetty version. At the time it was not possible to go to a newer jetty version.</p>
<p>By now the referred version is long deprecated, and as such the situation should be fully resolved for everyone.</p>
",983722,2020-08-01T20:53:22.577,0,CC BY-SA 4.0,,1,version,"
",0,"By now the referred version is long deprecated, and as such the situation should be fully resolved for everyone.</p>
",False,False
587,63216719,2,51457441,2020-08-02T13:43:57.970,0,"<p>Before, there were &quot;hooks&quot; but now they're almost deprecated <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/custom-platform-hooks.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/custom-platform-hooks.html</a></p>
<p>Now you can use Buildfile and Procfile as described here <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/platforms-linux-extend.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/platforms-linux-extend.html</a></p>
",1768553,2020-08-02T13:43:57.970,1,CC BY-SA 4.0,,1,they,hooks.html,0,"now they're almost deprecated <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/custom-platform-hooks.html",False,False
588,63252409,2,63236398,2020-08-04T17:46:21.153,0,"<p>I was using and outdated browser. Updating the browser solved the issues for me.</p>
",13299635,2020-08-04T17:46:21.153,0,CC BY-SA 4.0,,1,,.,0,I was using and outdated browser.,False,True
589,63292045,2,63281605,2020-08-06T21:31:34.183,1,"<p>Up to version 9.6, hash indexes were not crash safe and so were discouraged for that reason.  WAL was added to them in v10, so there is no longer anything wrong with using them.</p>
<p>Although they also shouldn't be necessary, and I see no reason the regular (btree) indexes were not being used.  Are you sure they were actually present and marked as valid?  Or maybe they were very bloated and could have been fixed with a <code>REINDEX INDEX ...</code> command, but it hard to see how they could be so bloated that they wouldn't have still been preferred over the seq scans.</p>
",1721239,2020-08-06T21:31:34.183,2,CC BY-SA 4.0,,1,, ,1,"<p>Up to version 9.6, hash indexes were not crash safe and so were discouraged for that reason.  ",False,False
590,63293266,2,13455168,2020-08-06T23:47:47.047,0,"<p>Here is another whay to upload a null (or o Byte) file to S3. I verified this works You can also use the S3 API to upload a file with no body, like so:</p>
<pre><code>aws s3api put-object --bucket &quot;myBucketName&quot; --key &quot;dir-1/my_null_file&quot;
</code></pre>
<p>Normally you would specify a <code>--body</code> blob, but its option and will just add the key as expected. See more on <a href=""https://docs.aws.amazon.com/cli/latest/reference/s3api/put-object.html#examples"" rel=""nofollow noreferrer"">S3 API put-object</a></p>
<p>The version of AWS CLI tested with is: <code>aws-cli/2.0.4 Python/3.7.5 Windows/10 botocore/2.0.0dev8</code></p>
<p>Here's how I did it in PHP (even works in outdated 5.4, had to go way back):</p>
<pre class=""lang-php prettyprint-override""><code>// Init an S3Client
$awsConfig = $app-&gt;config('aws');
$aws       = Aws::factory($awsConfig);
$s3Bucket  = $app-&gt;config('S3_Bucket');
$s3Client  = $aws-&gt;get('s3');

// Set null/empty file.
$result = $s3Client-&gt;putObject([
    'Bucket' =&gt; $s3Bucket,
    'Key' =&gt; &quot;dir-1/my_null_file&quot;,
    'Body' =&gt; '',
    'ServerSideEncryption' =&gt; 'AES256',
]);
</code></pre>
",419097,2020-08-06T23:47:47.047,0,CC BY-SA 4.0,,1,,way,0,"Here's how I did it in PHP (even works in outdated 5.4, had to go way",False,False
591,63395575,2,63386524,2020-08-13T12:51:09.413,0,"<p>The one-sentence answer is that Amazon chose to do it that way.</p>
<p>From an operational perspective, it simplifies life considerably if you associate an access key with an account. The alternative is to specify an account separately with every request.</p>
<p>From an organization management perspective, trees are easier to understand than forests. What I mean by that is that everything branches off the account's IAM root. If you  allow arbitrary users across the AWS universe to have access to arbitrary resources within arbitrary accounts, you end up with a management nightmare.</p>
<p>As a sidebar to that last comment, the early AWS services had resource-based policies. These are now discouraged in favor of identity-based policies, and newer services don't have resource policies.</p>
<p>Lastly, from a security perspective, a multi-tenant environment is a lot easier to control if you establish a hard wall around each tenant.</p>
",13400729,2020-08-13T12:51:09.413,3,CC BY-SA 4.0,,1,These,policies.</p,1,"These are now discouraged in favor of identity-based policies, and newer services don't have resource policies.</p",False,False
592,63452269,2,63438293,2020-08-17T13:51:47.257,1,"<p>It is a compatibility issue. Current version of <code>aws-appsync</code> doesn't support <code>apollo-client</code> v3, see this thread for progress:
<a href=""https://github.com/awslabs/aws-mobile-appsync-sdk-js/issues/448"" rel=""nofollow noreferrer"">https://github.com/awslabs/aws-mobile-appsync-sdk-js/issues/448</a></p>
<p>Best workaround is this: <a href=""https://stackoverflow.com/questions/60804372/proper-way-to-setup-awsappsyncclient-apollo-react"">Proper way to setup AWSAppSyncClient, Apollo &amp; React</a></p>
<p>Note the workaround does use two deprecated libraries but can be slightly improved as:</p>
<pre><code>import { ApolloClient, ApolloLink, InMemoryCache } from &quot;@apollo/client&quot;;
import { createAuthLink } from &quot;aws-appsync-auth-link&quot;;
import { createHttpLink } from &quot;apollo-link-http&quot;;
import AppSyncConfig from &quot;./aws-exports&quot;;

const url = AppSyncConfig.aws_appsync_graphqlEndpoint;
const region = AppSyncConfig.aws_project_region;
const auth = {
  type: AppSyncConfig.aws_appsync_authenticationType,
  apiKey: AppSyncConfig.aws_appsync_apiKey,
};
const link = ApolloLink.from([
  // @ts-ignore
  createAuthLink({ url, region, auth }),
  // @ts-ignore
  createHttpLink({ uri: url }),
]);
const client = new ApolloClient({
  link,
  cache: new InMemoryCache(),
});

export default client;

</code></pre>
",5080370,2020-08-17T13:51:47.257,0,CC BY-SA 4.0,,1,,as:</p,0,Note the workaround does use two deprecated libraries but can be slightly improved as:</p,False,False
593,63459678,2,63455876,2020-08-17T22:58:56.913,1,"<p>The reason why <code>WebServerSecurityGroup</code> (SG) is not found is because you are creating the SG in a  <strong>different VPC</strong> then your EB environment. Specifically, you are lunching EB in a default VPC, while you seem to be creating your  SG in different VPC as specified in the following line:</p>
<pre><code>      VpcId: !Ref Vpc # &lt;--- your EB will be in different VPC than your SG
</code></pre>
<p>Since its not clear what you are doing with the VPC (are you launching EB in custom VPC, creating new VPC or using default VPC?), the easiest fix to your template is simply removing the <code>VpcId: !Ref Vpc</code>.</p>
<p>Also your <strong>platform version is outdated</strong> and needs to be changed. The list of available PHP platform versions is <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/platforms/platforms-supported.html#platforms-supported.PHP"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I <strong>fixed</strong> the template and I can <strong>verify that it works</strong> in <code>us-east-1</code>. It launches EB and its SG in a <code>default VPC</code>. For custom VPC many more changes are required to your template, such as definitions of subnets, route tables and VPC specific changes to EB environment itself.</p>
<pre><code>AWSTemplateFormatVersion: '2010-09-09'
Description: &quot;Pathein Directory web application deployment template.&quot;
Parameters:
  KeyName:
    Default: 'PatheinDirectory'
    Type: String
  InstanceType:
    Default: 't2.micro'
    Type: String
  SSHLocation:
    Description: The IP address range that can be used to SSH to the EC2 instances
    Type: String
    MinLength: '9'
    MaxLength: '18'
    Default: 0.0.0.0/0
    AllowedPattern: &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;
    ConstraintDescription: Must be a valid IP CIDR range of the form x.x.x.x/x
  # Vpc:
  #   Default: &quot;vpc-dd53ada4&quot;
  #   Type: String
  # VpcCidr:
  #   Default: &quot;172.31.0.0/16&quot;
  #   Type: String

Mappings:
  Region2Principal:
    us-east-1:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    us-west-2:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    us-west-1:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    eu-west-1:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    eu-west-2:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    eu-west-3:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    ap-southeast-1:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    ap-northeast-1:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    ap-northeast-2:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    ap-northeast-3:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    ap-southeast-2:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    ap-south-1:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    us-east-2:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    ca-central-1:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    sa-east-1:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    cn-north-1:
      EC2Principal: ec2.amazonaws.com.cn
      OpsWorksPrincipal: opsworks.amazonaws.com.cn
    cn-northwest-1:
      EC2Principal: ec2.amazonaws.com.cn
      OpsWorksPrincipal: opsworks.amazonaws.com.cn
    eu-central-1:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
    eu-north-1:
      EC2Principal: ec2.amazonaws.com
      OpsWorksPrincipal: opsworks.amazonaws.com
Resources:

  WebServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security Group for EC2 instances
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: '80'
          ToPort: '80'
          CidrIp: 0.0.0.0/0
        - IpProtocol: tcp
          FromPort: '22'
          ToPort: '22'
          CidrIp:
            Ref: SSHLocation
      #VpcId: !Ref Vpc

  WebServerRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - Fn::FindInMap:
                    - Region2Principal
                    - Ref: AWS::Region
                    - EC2Principal
            Action:
              - sts:AssumeRole
      Path: /
  WebServerRolePolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: WebServerRole
      PolicyDocument:
        Statement:
          - Effect: Allow
            NotAction: iam:*
            Resource: '*'
      Roles:
        - Ref: WebServerRole

  WebServerInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: /
      Roles:
        - Ref: WebServerRole
  Application:
    Type: AWS::ElasticBeanstalk::Application
    Properties:
      Description: AWS Elastic Beanstalk Pathein Directory Laravel application
  ApplicationVersion:
    Type: AWS::ElasticBeanstalk::ApplicationVersion
    Properties:
      Description: Version 1.0
      ApplicationName:
        Ref: Application
      SourceBundle:
        S3Bucket:
          Fn::Join:
            - '-'
            - - elasticbeanstalk-samples
              - Ref: AWS::Region
        S3Key: php-sample.zip

  ApplicationConfigurationTemplate:
    Type: AWS::ElasticBeanstalk::ConfigurationTemplate
    Properties:
      ApplicationName:
        Ref: Application
      Description: SSH access to Pathein Directory Laravel application
      SolutionStackName: 64bit Amazon Linux 2018.03 v2.9.9 running PHP 7.2 
      OptionSettings:
        - Namespace: aws:autoscaling:launchconfiguration
          OptionName: EC2KeyName
          Value:
            Ref: KeyName
        - Namespace: aws:autoscaling:launchconfiguration
          OptionName: IamInstanceProfile
          Value:
            Ref: WebServerInstanceProfile
        - Namespace: aws:autoscaling:launchconfiguration
          OptionName: SecurityGroups
          Value:
            Ref: WebServerSecurityGroup

  Environment:
    Type: AWS::ElasticBeanstalk::Environment
    Properties:
      Description: AWS Elastic Beanstalk Environment running Pathein Directory Laravel application
      ApplicationName:
        Ref: Application
      EnvironmentName: PatheinDirectoryTesting
      TemplateName:
        Ref: ApplicationConfigurationTemplate
      VersionLabel:
        Ref: ApplicationVersion
      OptionSettings:
        - Namespace: aws:elasticbeanstalk:environment
          OptionName: EnvironmentType
          Value: SingleInstance
</code></pre>
",248823,2020-08-17T22:58:56.913,1,CC BY-SA 4.0,,1,,.,0,>Also your <strong>platform version is outdated</strong> and needs to be changed.,False,False
594,63532405,2,63526333,2020-08-22T03:52:15.740,1,"<p><code>external.metrics.k8s.io</code> is not part of upstream kube API as mentioned in <a href=""https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis"" rel=""nofollow noreferrer"">docs</a></p>
<blockquote>
<p>For external metrics, this is the external.metrics.k8s.io API. It may be provided by the custom metrics adapters provided above.</p>
</blockquote>
<p>So you need to install custom metric provider such as <a href=""https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter"" rel=""nofollow noreferrer"">GCP stackdriver</a>. I assume you are using EKS with prometheus, here is one good option that I used before <a href=""https://github.com/DirectXMan12/k8s-prometheus-adapter"" rel=""nofollow noreferrer"">prometheus-adapter</a>. Kindly note that the API is <code>v1beta1.custom.metrics.k8s.io</code> though.</p>
<p>PS: I thought EKS 1.13 is already deprecated, you might need to update it first.</p>
",8763847,2020-08-22T03:52:15.740,1,CC BY-SA 4.0,,1,,it,0,"PS: I thought EKS 1.13 is already deprecated, you might need to update it",False,False
595,63572667,2,61893719,2020-08-25T05:41:56.313,-1,"<p>When getting error:</p>
<p><strong>from sklearn.externals import joblib</strong> it deprecated older version.</p>
<p>For new version follow:</p>
<ol>
<li>conda install -c anaconda scikit-learn  (install using &quot;Anaconda Promt&quot;)</li>
<li>import joblib (Jupyter Notebook)</li>
</ol>
",13030784,2020-08-25T05:41:56.313,0,CC BY-SA 4.0,,1,,older,0,it deprecated older,False,False
596,63618135,2,63617992,2020-08-27T14:22:53.153,2,"<p>The SDK returns at most 1000 results. If results are paginated, then you need to re-issue the list call with a continuation token. See <code>IsTruncated</code> and <code>NextContinuationToken</code> in the response, and <code>ContinuationToken</code> in the request.</p>
<p>Also, you should use <code>listObjectsV2</code> rather than the deprecated <code>listObjects</code>.</p>
",271415,2020-08-27T14:22:53.153,0,CC BY-SA 4.0,,1,,code,0,rather than the deprecated <code,False,False
597,63631475,2,63626179,2020-08-28T09:52:58.280,0,"<p>There are some options you should consider:</p>
<ul>
<li><p>don't update anything and just stick to Kubernetes 1.15 (not recommended as it is 4 main versions behind the latest one)</p>
</li>
<li><p><code>git clone</code> your repo and change <code>apiVersion</code> to <code>apps/v1</code> in all your resources</p>
</li>
<li><p>use <a href=""https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#convert"" rel=""nofollow noreferrer"">kubectl convert</a> in order to change the <code>apiVersion</code>, for example: <code>kubectl convert -f deployment.yaml --output-version apps/v1</code></p>
</li>
</ul>
<p>It is worth to mention that stuff gets deprecated for a reason and it is strongly not recommended to stick to old ways if they are not supported anymore.</p>
",11560878,2020-08-28T09:52:58.280,0,CC BY-SA 4.0,,1,,supported,1,It is worth to mention that stuff gets deprecated for a reason and it is strongly not recommended to stick to old ways if they are not supported,False,False
