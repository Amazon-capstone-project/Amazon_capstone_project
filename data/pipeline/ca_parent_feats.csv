Id,PostId,Score,Text,CreationDate,UserId,ContentLicense,of_answer,cnt_keywords,subject,punctuation,negative_statement,sentence,subj_irrel,include_irrel,Id_ans,ParentId,CreationDate_ans,Score_ans,Body,LastActivityDate_ans,CommentCount_ans,Id_que,CreationDate_que,Score_que,ViewCount,LastEditDate,LastActivityDate_que,Title,Tags,AnswerCount,CommentCount_que,ClosedDate,FavoriteCount,CommunityOwnedDate
679802,864745,0,"Delphi does not support SOAP 1.2 (SOAP 1.1 seems to be deprecated for Amazon Web Services), Delphi has no built-in support for WS-Security, and the SimpleDB WSDL is incomplete and thus usable for a Delphi SOAP client. Actually I am looking for success stories :)",2009-05-16T13:01:06.570,80901,CC BY-SA 2.5,True,1,,.,1,"Delphi does not support SOAP 1.2 (SOAP 1.1 seems to be deprecated for Amazon Web Services), Delphi has no built-in support for WS-Security, and the SimpleDB WSDL is incomplete and thus usable for a Delphi SOAP client.",False,False,864745.0,842909.0,2009-05-14T17:45:46.030,0.0,"<p>It depends on what you mean by ""out of the box"". With AWS, you can make either SOAP or REST calls which means you should be able to use almost any language you like. If you wanted to use a helper library, I have not heard of any for Delphi.</p>

<p>For the reference, you can check out this for SimpleDB:
<a href=""http://docs.amazonwebservices.com/AmazonSimpleDB/latest/DeveloperGuide/APIRequests.html"" rel=""nofollow noreferrer"">http://docs.amazonwebservices.com/AmazonSimpleDB/latest/DeveloperGuide/APIRequests.html</a></p>
",1,CC BY-SA 2.5,842909.0,2009-05-09T08:03:45.303,5.0,1549.0,2017-05-23T11:55:42.787,2009-06-27T07:35:00.287,Which Amazon web services can be used with Delphi 'out of the box'?,<delphi><soap><delphi-2009><amazon-web-services>,2.0,0.0,,1.0,
679802,864745,0,"Delphi does not support SOAP 1.2 (SOAP 1.1 seems to be deprecated for Amazon Web Services), Delphi has no built-in support for WS-Security, and the SimpleDB WSDL is incomplete and thus usable for a Delphi SOAP client. Actually I am looking for success stories :)",2009-05-16T13:01:06.570,80901,CC BY-SA 2.5,True,1,,.,1,"Delphi does not support SOAP 1.2 (SOAP 1.1 seems to be deprecated for Amazon Web Services), Delphi has no built-in support for WS-Security, and the SimpleDB WSDL is incomplete and thus usable for a Delphi SOAP client.",False,False,864745.0,842909.0,2009-05-14T17:45:46.030,0.0,"<p>It depends on what you mean by ""out of the box"". With AWS, you can make either SOAP or REST calls which means you should be able to use almost any language you like. If you wanted to use a helper library, I have not heard of any for Delphi.</p>

<p>For the reference, you can check out this for SimpleDB:
<a href=""http://docs.amazonwebservices.com/AmazonSimpleDB/latest/DeveloperGuide/APIRequests.html"" rel=""nofollow noreferrer"">http://docs.amazonwebservices.com/AmazonSimpleDB/latest/DeveloperGuide/APIRequests.html</a></p>
",1,CC BY-SA 2.5,842909.0,2009-05-09T08:03:45.303,5.0,1549.0,2017-05-23T11:55:42.787,2009-06-27T07:35:00.287,Which Amazon web services can be used with Delphi 'out of the box'?,<delphi><soap><delphi-2009><amazon-web-services>,2.0,0.0,,1.0,
1489794,1620935,0,"1) The article is out of date - as you can see from the link I provided, App Engine now has dump/restore support.

If you want to back up an EBS volume on Amazon, you can take a snapshot of it, or you can download it - back it up in the same manner you would any other filesystem on any other system.",2009-10-25T22:25:14.517,12030,CC BY-SA 2.5,True,1,article,"

",0,"The article is outdated - as you can see from the link I provided, App Engine now has dump/restore support.

",False,False,1620935.0,1620680.0,2009-10-25T13:34:09.113,2.0,"<p>For App Engine, the <a href=""http://code.google.com/appengine/docs/python/tools/uploadingdata.html#Downloading_and_Uploading_All_Data"" rel=""nofollow noreferrer"">bulkloader</a> permits dumping and loading the entire datastore. Loading to a different instance that you dumped from is also possible, but currently requires manually  setting the ID counters using allocate_ids so that they don't allocate already used IDs. Future versions of the SDK should support doing this automatically.</p>

<p>Asking about backing up EC2 doesn't make much sense - EC2 is for computation, not storage. If you mean one of the other services - S3, EBS, or SimpleDB, you need to specify which you mean.</p>
",7,CC BY-SA 2.5,1620680.0,2009-10-25T11:37:08.177,0.0,548.0,2009-11-08T01:41:48.477,2009-11-08T11:03:30.347,Db Backup Options (for Cloud DBs like Amazon ESB and GAE),<google-app-engine><amazon-s3><amazon-ec2><amazon>,3.0,0.0,2012-10-11T03:13:57.593,1.0,
1560592,1680318,0,"Alex, thanks for answer. I'm rolling my own since two years and keeping those installation as AMI but normally when it comes time again using AMI many things have changed. And now I feel that bit outdated and again rolled my own. Just wanted to get out of that cycle with some pre built up-to-date AMIs, exactly something like EY Cloud. Which I think over priced.",2009-11-06T07:42:27.730,33065,CC BY-SA 2.5,True,1,,.,0,And now I feel that bit outdated and again rolled my own.,False,False,1680318.0,1639646.0,2009-11-05T12:41:25.180,1.0,"<p>If you are able to afford them, I would recommend <a href=""http://www.engineyard.com/products/cloud"" rel=""nofollow noreferrer"">EngineYard's Cloud</a>, which is built with EC2 AMIs.  They are around 30% more expensive than EC2 on-demand rates last time I checked.</p>

<p><a href=""http://ec2onrails.rubyforge.org/"" rel=""nofollow noreferrer"">ec2onrails</a> is a good start, but the project has stagnated, and I don't believe there is a release version that uses EBS for the database files, although the development branches do have that feature.  I have used it and it worked well.  You will definitely want your database on EBS for snapshot and mobility reasons.</p>

<p>There are a few others floating around but I have not looked at them.</p>

<p>I ended up rolling my own using Passenger, which was not very difficult and left me far more comfortable.  Your requirements are pretty basic.</p>
",1,CC BY-SA 2.5,1639646.0,2009-10-28T19:48:34.097,0.0,1057.0,,2009-11-29T16:39:57.913,"Any Full ""Ruby Stack"" AMI on EC2?",<ruby><web-services><amazon-ec2><nginx><cloud>,2.0,0.0,2017-01-13T13:49:03.940,2.0,
1673411,1786502,0,"Yes, I have looked @ RDS, but even there the backups (snapshots) are up to 5 minutes out of date. I'd need another method to preserve critical tables which can't be out of date.",2009-11-23T22:30:19.900,104181,CC BY-SA 2.5,True,2,,.,1,"Yes, I have looked @ RDS, but even there the backups (snapshots) are up to 5 minutes outdated.",False,False,1786502.0,1786342.0,2009-11-23T22:28:18.450,1.0,"<p>Have you looked into moving your MySQL into Amazon Web Services as well?  You can use <a href=""http://aws.amazon.com/rds/"" rel=""nofollow noreferrer"">Amazon Relational Database Service (RDS)</a>.  Also see <a href=""http://aws.amazon.com/solutions/global-solution-providers/mysql/"" rel=""nofollow noreferrer"">MySQL Enterprise Support</a>.</p>
",2,CC BY-SA 2.5,1786342.0,2009-11-23T21:59:39.817,1.0,334.0,,2012-07-16T02:45:43.710,Homemade cheap and cheerful clustering with MySQL+EC2?,<mysql><jdbc><amazon-ec2><amazon-ebs>,2.0,0.0,,,
1940614,2012755,0,"@lostInTransit: clicking on your link gives me a page that says that the ""Amazon Ecommerce Web Service 3.0"" was deprecated in March 2008, and that you should read the Migration Guide for information on how to switch to version 4.0 - I've updated my answer with this text, and links.",2010-01-07T07:24:33.363,128625,CC BY-SA 2.5,True,1,,.,0,"clicking on your link gives me a page that says that the ""Amazon Ecommerce Web Service 3.0"" was deprecated in March 2008, and that you should read the Migration Guide for information on how to switch to version 4.0 - I've updated my answer with this text, and links.",False,False,,,,,,,,,,,,,,,,,,,,
5270353,4757065,0,"I've dug through those, they are old, and it seriously seems like there a million different wsdl, and half of them do similar things, and half of them out of date.",2011-01-21T20:40:07.260,184466,CC BY-SA 2.5,True,1,,.,0,"I've dug through those, they are old, and it seriously seems like there a million different wsdl, and half of them do similar things, and half of them outdated.",False,False,4757065.0,4755090.0,2011-01-21T09:09:56.427,0.0,"<p>There's a whole set of examples on amazon's website:
<a href=""http://aws.amazon.com/code/Product%20Advertising%20API?_encoding=UTF8&amp;jiveRedirect=1"" rel=""nofollow"">http://aws.amazon.com/code/Product%20Advertising%20API?_encoding=UTF8&amp;jiveRedirect=1</a></p>
",1,CC BY-SA 2.5,4755090.0,2011-01-21T03:27:19.920,5.0,4594.0,2011-01-21T15:48:13.987,2017-12-19T21:44:07.083,ASP.NET Amazon ItemSearch,<c#><asp.net><amazon>,4.0,0.0,,2.0,
5902554,5241796,0,"Lovely, thanks Geoff - I had noticed the success_action_redirect in the docs, but read `The redirect field name is deprecated and support for the redirect field name will be removed in the future.` Now that you've made me read it again, I realise that it's not the whole feature that is deprecated, but just the field called `redirect` for doing it (must read slower!). This should do the trick. Thank you.",2011-03-09T09:15:45.863,123217,CC BY-SA 2.5,True,2,,.,1,"Lovely, thanks Geoff - I had noticed the success_action_redirect in the docs, but read `The redirect field name is deprecated and support for the redirect field name will be removed in the future.",False,False,,,,,,,,,,,,,,,,,,,,
6001758,5286691,0,"I think the link you sent is pointing to the deprecated API. http://docs.amazonwebservices.com/AWSMechTurk/latest/AWSMturkAPI/ is the new one which doesn't have the ""Accept mode"".",2011-03-15T22:51:28.053,196007,CC BY-SA 2.5,True,1,,.,0,I think the link you sent is pointing to the deprecated API.,False,False,,,,,,,,,,,,,,,,,,,,
6402036,757117,0,This information is a bit outdated now anyway.,2011-04-11T09:41:06.577,5409,CC BY-SA 3.0,True,1,information,.,0,This information is a bit outdated now anyway.,False,False,757117.0,757063.0,2009-04-16T17:08:09.983,9.0,"<p>Highly sensitive data might be better to control yourself. And there's legislation; some privacy sensitive information, for example, might not leave the the country. </p>

<p>Also, except for Microsoft Azure in combination with SDS, the data stores tend to be not relational, which is a nuisance in certain cases.</p>
",2,CC BY-SA 2.5,757063.0,2009-04-16T16:55:03.477,42.0,8649.0,,2013-07-11T08:41:38.613,Why would you not want to use Cloud Computing,<amazon-ec2><cloud>,18.0,0.0,2012-09-23T15:17:46.183,21.0,
6402036,757117,0,This information is a bit outdated now anyway.,2011-04-11T09:41:06.577,5409,CC BY-SA 3.0,True,1,information,.,0,This information is a bit outdated now anyway.,False,False,757117.0,757063.0,2009-04-16T17:08:09.983,9.0,"<p>Highly sensitive data might be better to control yourself. And there's legislation; some privacy sensitive information, for example, might not leave the the country. </p>

<p>Also, except for Microsoft Azure in combination with SDS, the data stores tend to be not relational, which is a nuisance in certain cases.</p>
",2,CC BY-SA 2.5,757063.0,2009-04-16T16:55:03.477,42.0,8649.0,,2013-07-11T08:41:38.613,Why would you not want to use Cloud Computing,<amazon-ec2><cloud>,18.0,0.0,2012-09-23T15:17:46.183,21.0,
6619801,5767989,0,Looks interesting but non of the Sample Requests actually work. It seems the doc is outdated or something...,2011-04-25T10:18:46.767,352959,CC BY-SA 3.0,True,1,,...,0,It seems the doc is outdated or something...,False,False,5767989.0,5765562.0,2011-04-24T01:01:55.743,0.0,"<p><a href=""http://docs.amazonwebservices.com/AWSECommerceService/2007-08-27/DG/ItemLookup.html"" rel=""nofollow"">ItemLookup</a> may be interesting to you.</p>
",2,CC BY-SA 3.0,5765562.0,2011-04-23T16:52:57.660,0.0,2604.0,,2011-06-15T17:55:34.420,Get product information from amazon.com using ASIN code,<php><amazon>,1.0,0.0,,,
6619801,5767989,0,Looks interesting but non of the Sample Requests actually work. It seems the doc is outdated or something...,2011-04-25T10:18:46.767,352959,CC BY-SA 3.0,True,1,,...,0,It seems the doc is outdated or something...,False,False,5767989.0,5765562.0,2011-04-24T01:01:55.743,0.0,"<p><a href=""http://docs.amazonwebservices.com/AWSECommerceService/2007-08-27/DG/ItemLookup.html"" rel=""nofollow"">ItemLookup</a> may be interesting to you.</p>
",2,CC BY-SA 3.0,5765562.0,2011-04-23T16:52:57.660,0.0,2604.0,,2011-06-15T17:55:34.420,Get product information from amazon.com using ASIN code,<php><amazon>,1.0,0.0,,,
6752023,5872687,0,"Yeah, I think its out of date. I tried some of the code and it looks like it doesn't include the signature with time stamp features.",2011-05-03T16:33:48.367,201140,CC BY-SA 3.0,True,1,,.,0,"Yeah, I think its outdated.",False,False,5872687.0,5872635.0,2011-05-03T16:31:59.750,3.0,"<p>Have you checked out <a href=""http://www.kokogiak.com/gedankengang/2006/05/consuming-amazons-web-api-directly.html"" rel=""nofollow"">this article</a> on grabbing JSON asynchronously?</p>
",2,CC BY-SA 3.0,5872635.0,2011-05-03T16:28:12.067,1.0,4733.0,,2018-09-17T19:34:33.673,Retrieve Amazon Product Information in Javascript,<javascript><amazon-web-services><amazon><amazon-product-api>,5.0,1.0,,2.0,
6752023,5872687,0,"Yeah, I think its out of date. I tried some of the code and it looks like it doesn't include the signature with time stamp features.",2011-05-03T16:33:48.367,201140,CC BY-SA 3.0,True,1,,.,0,"Yeah, I think its outdated.",False,False,5872687.0,5872635.0,2011-05-03T16:31:59.750,3.0,"<p>Have you checked out <a href=""http://www.kokogiak.com/gedankengang/2006/05/consuming-amazons-web-api-directly.html"" rel=""nofollow"">this article</a> on grabbing JSON asynchronously?</p>
",2,CC BY-SA 3.0,5872635.0,2011-05-03T16:28:12.067,1.0,4733.0,,2018-09-17T19:34:33.673,Retrieve Amazon Product Information in Javascript,<javascript><amazon-web-services><amazon><amazon-product-api>,5.0,1.0,,2.0,
8109582,4715613,2,"This response is really outdated. First, nobody uses EBS with cassandra. Seriously don't do it. Instead, create an LVM RAID-0 volume of all of the ephemeral disks with ext4. Use m1.xlarge since it has four ephemerals and I/O bandwidth is your biggest bottleneck on EC2.

Make sure to stripe your nodes across availability zones so that you can survive a net split / full AZ outage.

Write a cron job to do regular nodetool snapshots and upload to S3 for backup.

Again, since I/O bandwidth is your worst enemy, you're more likely to grow your cluster than increase instance size once in production.",2011-07-26T05:12:33.673,106958,CC BY-SA 3.0,True,1,response,.,0,This response is really outdated.,False,False,,,,,,,,,,,,,,,,,,,,
8585917,7154807,0,"Support for much of HTML5 is still in beta stages – That is, they include functionality on a basic level, but the level of support for specific tags is still poorly supported. http://www.wired.com/epicenter/2010/11/ie9-leads-pack-in-html5-support-not-exactly/ While the information posted here is outdated, my PC at work with current IE9 still ranks in the mid 100s on html5test. HTML5 itself is still not widely supported, and is commonly not recommended for development.",2011-08-23T04:31:26.810,345585,CC BY-SA 3.0,True,1,,.,0,"http://www.wired.com/epicenter/2010/11/ie9-leads-pack-in-html5-support-not-exactly/ While the information posted here is outdated, my PC at work with current IE9 still ranks in the mid 100s on html5test.",False,False,7154807.0,7145579.0,2011-08-22T23:34:54.390,0.0,"<p>I know for a fact that the <code>&lt;video&gt;</code> tag does not uniformly support all codecs in all browsers, and I suspect the same is true of audio. IE9's support for the <code>&lt;audio&gt;</code> tag is still in beta, and right now the only formats discussed for the web are ogg vorbis, mp3, and wav.</p>

<p>In all likelihood, the reason you can play an m4a in Safari is because of the inclusion of quicktime – with quicktime installed, Safari will play anything in the <code>&lt;audio&gt;</code> tag that quicktime can play. This is not true, however, for any other browser combined with quicktime.</p>
",3,CC BY-SA 3.0,7145579.0,2011-08-22T09:43:16.557,0.0,475.0,2011-08-22T23:14:20.297,2011-08-22T23:34:54.390,With IE9 i can not see audio player(HTML5 audio tag) in Amazon S3 to upload with iOS,<ios><html><audio><amazon-s3><internet-explorer-9>,1.0,0.0,,,
8585917,7154807,0,"Support for much of HTML5 is still in beta stages – That is, they include functionality on a basic level, but the level of support for specific tags is still poorly supported. http://www.wired.com/epicenter/2010/11/ie9-leads-pack-in-html5-support-not-exactly/ While the information posted here is outdated, my PC at work with current IE9 still ranks in the mid 100s on html5test. HTML5 itself is still not widely supported, and is commonly not recommended for development.",2011-08-23T04:31:26.810,345585,CC BY-SA 3.0,True,1,,.,0,"http://www.wired.com/epicenter/2010/11/ie9-leads-pack-in-html5-support-not-exactly/ While the information posted here is outdated, my PC at work with current IE9 still ranks in the mid 100s on html5test.",False,False,7154807.0,7145579.0,2011-08-22T23:34:54.390,0.0,"<p>I know for a fact that the <code>&lt;video&gt;</code> tag does not uniformly support all codecs in all browsers, and I suspect the same is true of audio. IE9's support for the <code>&lt;audio&gt;</code> tag is still in beta, and right now the only formats discussed for the web are ogg vorbis, mp3, and wav.</p>

<p>In all likelihood, the reason you can play an m4a in Safari is because of the inclusion of quicktime – with quicktime installed, Safari will play anything in the <code>&lt;audio&gt;</code> tag that quicktime can play. This is not true, however, for any other browser combined with quicktime.</p>
",3,CC BY-SA 3.0,7145579.0,2011-08-22T09:43:16.557,0.0,475.0,2011-08-22T23:14:20.297,2011-08-22T23:34:54.390,With IE9 i can not see audio player(HTML5 audio tag) in Amazon S3 to upload with iOS,<ios><html><audio><amazon-s3><internet-explorer-9>,1.0,0.0,,,
10024059,8164075,1,"If it's free/open source, ask the devs and offer to help (as I said, might not be much work - and we can probably help if you ask specific question). If it's commercial, put some pressure on the company telling them that their software forces you to downgrade to a possibly insecure and certainly outdated version of PHP.",2011-11-17T08:37:00.453,298479,CC BY-SA 3.0,True,1,,.,0,"If it's commercial, put some pressure on the company telling them that their software forces you to downgrade to a possibly insecure and certainly outdated version of PHP.",False,False,8164075.0,8164018.0,2011-11-17T08:26:32.530,1.0,"<p>You need to fix the software or get it fixed. The changes between 5.2 and 5.3 should not cause <em>that</em> many problems anyway.</p>

<p>To quote PHP.net:</p>

<blockquote>
  <p>All PHP users should note that the PHP 5.2 series is NOT supported anymore. All users are strongly encouraged to upgrade to PHP 5.3.8.</p>
</blockquote>

<p>This includes security fixes, so if you switch to PHP 5.2 you risk opening security holes.</p>
",2,CC BY-SA 3.0,8164018.0,2011-11-17T08:20:25.767,0.0,633.0,2011-11-17T08:57:23.610,2011-11-17T08:57:23.610,Installed php 5.3 on amazon linux server.. i need to downgrade to 5.2 how can i do this?,<php><amazon>,1.0,0.0,,,
10024059,8164075,1,"If it's free/open source, ask the devs and offer to help (as I said, might not be much work - and we can probably help if you ask specific question). If it's commercial, put some pressure on the company telling them that their software forces you to downgrade to a possibly insecure and certainly outdated version of PHP.",2011-11-17T08:37:00.453,298479,CC BY-SA 3.0,True,1,,.,0,"If it's commercial, put some pressure on the company telling them that their software forces you to downgrade to a possibly insecure and certainly outdated version of PHP.",False,False,8164075.0,8164018.0,2011-11-17T08:26:32.530,1.0,"<p>You need to fix the software or get it fixed. The changes between 5.2 and 5.3 should not cause <em>that</em> many problems anyway.</p>

<p>To quote PHP.net:</p>

<blockquote>
  <p>All PHP users should note that the PHP 5.2 series is NOT supported anymore. All users are strongly encouraged to upgrade to PHP 5.3.8.</p>
</blockquote>

<p>This includes security fixes, so if you switch to PHP 5.2 you risk opening security holes.</p>
",2,CC BY-SA 3.0,8164018.0,2011-11-17T08:20:25.767,0.0,633.0,2011-11-17T08:57:23.610,2011-11-17T08:57:23.610,Installed php 5.3 on amazon linux server.. i need to downgrade to 5.2 how can i do this?,<php><amazon>,1.0,0.0,,,
10837363,8677786,0,Hmmm createSignedGetUrl is deprecated any idea what is the equivalent of this?,2012-01-03T05:29:34.430,659906,CC BY-SA 3.0,True,1,createSignedGetUrl,idea,0,Hmmm createSignedGetUrl is deprecated any idea,False,False,,,,,,,,,,,,,,,,,,,,
10888277,8737041,1,"Yup, that definitly helps. Since i -->also<-- had troubles getting the IFrame. Still, my main goal (and what the bounty is set for) is to get the (isolated) reviewrank information described as ""Average customer review""[..](for a specific product) in older API documentations (yet removed since outdated). I just need a way to get that number of avarage product stars (1-5) somehow.",2012-01-05T11:21:12.973,393969,CC BY-SA 3.0,True,1,,.,0,"Still, my main goal (and what the bounty is set for) is to get the (isolated) reviewrank information described as ""Average customer review""[..](for a specific product) in older API documentations (yet removed since outdated).",False,False,8737041.0,8700875.0,2012-01-05T02:55:27.423,14.0,"<p><strong>Preamble:</strong> I'm not sure that I understand exactly what you are looking for here but I'll share my findings anyways.</p>

<p>I was able to retrieve the iframe URL for the reviews and was able to see the reviews iframe after embedding it into an .html page. I used the following attributes to retrieve the iframe URL:</p>

<pre><code>Operation=ItemLookup&amp;
ItemId=1451648537&amp;
ResponseGroup=Reviews&amp;
TruncateReviewsAt=""256""&amp;
IncludeReviewsSummary=""False""&amp;
Version=2011-08-01                 &lt;= important: can't be less than this version
AssociateTag=&lt;YourAssociateTag&gt;    &lt;= required when using this version, can be anything (not verified by Amazon)
</code></pre>

<p>The relevant part of the response:</p>

<pre><code>&lt;Item&gt;
    &lt;ASIN&gt;1451648537&lt;/ASIN&gt;
    &lt;CustomerReviews&gt;
        &lt;IFrameURL&gt;http://www.amazon.com/reviews/iframe?akid=&lt;YourAmazonKey&gt;&amp;amp;alinkCode=xm2&amp;amp;asin=1451648537&amp;amp;atag=&lt;YourAssociateTag&gt;&amp;amp;exp=2012-01-06T02%3A10%3A38Z&amp;amp;summary=0&amp;amp;truncate=256&amp;amp;v=2&amp;amp;sig=kjWPue1N75%2FiI1hW67XYApWxnKeT2tlT%2FJ1rw4WLlUo%3D&lt;/IFrameURL&gt;
        &lt;HasReviews&gt;true&lt;/HasReviews&gt;
    &lt;/CustomerReviews&gt;
&lt;/Item&gt;
</code></pre>

<p><em>Note that you will not be able to use this iframe URL due to the fact that (1) I've removed my Amazon Key and the Associate Tag that was used to create the signature and (2) it expires 24 hours after the call.</em></p>

<p>If you use the same attributes that I've used, paying close attention to the <code>Version</code> and <code>AssociateTag</code> fields, you will get a result with an iframe URL included.</p>

<p>I then embedded the url in an iframe:</p>

<pre><code>&lt;html&gt;
    &lt;body&gt;
        &lt;iframe src=""http://www.amazon.com/reviews/iframe?akid=&lt;YourAmazonKey&gt;&amp;amp;alinkCode=xm2&amp;amp;asin=1451648537&amp;amp;atag=&lt;YourAssociateTag&gt;&amp;amp;exp=2012-01-06T02%3A10%3A38Z&amp;amp;summary=0&amp;amp;truncate=256&amp;amp;v=2&amp;amp;sig=kjWPue1N75%2FiI1hW67XYApWxnKeT2tlT%2FJ1rw4WLlUo%3D""/&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Which looked like the following:</p>

<p><img src=""https://i.stack.imgur.com/ousQI.png"" alt=""Amazon Review IFrame""></p>

<p>As I said in the preamble, I'm unsure if this is what you are looking for since the requirements for the bounty was:</p>

<blockquote>
  <p>...to get access to amazons reviewrank informations</p>
</blockquote>

<p>This isn't exactly the review rank information but the actual reviews and I take the meaning of review rank info to be more along the lines of the data itself (such as 100 reviews @ 4 stars etc.). However, in your question you stated that the iframe did not work:</p>

<blockquote>
  <p>However, testing with the params they suggest to get the Iframe, but it seems that now even the Iframe doesn't work anymore.</p>
</blockquote>

<p>So I thought that I would at least provide you with the proper method of getting and using the iframe.</p>
",4,CC BY-SA 3.0,8700875.0,2012-01-02T12:32:14.927,17.0,10886.0,2012-01-04T16:53:14.707,2014-03-26T17:29:35.950,Amazon Product Advertising API - get review-rank,<amazon-web-services><amazon><amazon-product-api>,3.0,2.0,,5.0,
10888277,8737041,1,"Yup, that definitly helps. Since i -->also<-- had troubles getting the IFrame. Still, my main goal (and what the bounty is set for) is to get the (isolated) reviewrank information described as ""Average customer review""[..](for a specific product) in older API documentations (yet removed since outdated). I just need a way to get that number of avarage product stars (1-5) somehow.",2012-01-05T11:21:12.973,393969,CC BY-SA 3.0,True,1,,.,0,"Still, my main goal (and what the bounty is set for) is to get the (isolated) reviewrank information described as ""Average customer review""[..](for a specific product) in older API documentations (yet removed since outdated).",False,False,8737041.0,8700875.0,2012-01-05T02:55:27.423,14.0,"<p><strong>Preamble:</strong> I'm not sure that I understand exactly what you are looking for here but I'll share my findings anyways.</p>

<p>I was able to retrieve the iframe URL for the reviews and was able to see the reviews iframe after embedding it into an .html page. I used the following attributes to retrieve the iframe URL:</p>

<pre><code>Operation=ItemLookup&amp;
ItemId=1451648537&amp;
ResponseGroup=Reviews&amp;
TruncateReviewsAt=""256""&amp;
IncludeReviewsSummary=""False""&amp;
Version=2011-08-01                 &lt;= important: can't be less than this version
AssociateTag=&lt;YourAssociateTag&gt;    &lt;= required when using this version, can be anything (not verified by Amazon)
</code></pre>

<p>The relevant part of the response:</p>

<pre><code>&lt;Item&gt;
    &lt;ASIN&gt;1451648537&lt;/ASIN&gt;
    &lt;CustomerReviews&gt;
        &lt;IFrameURL&gt;http://www.amazon.com/reviews/iframe?akid=&lt;YourAmazonKey&gt;&amp;amp;alinkCode=xm2&amp;amp;asin=1451648537&amp;amp;atag=&lt;YourAssociateTag&gt;&amp;amp;exp=2012-01-06T02%3A10%3A38Z&amp;amp;summary=0&amp;amp;truncate=256&amp;amp;v=2&amp;amp;sig=kjWPue1N75%2FiI1hW67XYApWxnKeT2tlT%2FJ1rw4WLlUo%3D&lt;/IFrameURL&gt;
        &lt;HasReviews&gt;true&lt;/HasReviews&gt;
    &lt;/CustomerReviews&gt;
&lt;/Item&gt;
</code></pre>

<p><em>Note that you will not be able to use this iframe URL due to the fact that (1) I've removed my Amazon Key and the Associate Tag that was used to create the signature and (2) it expires 24 hours after the call.</em></p>

<p>If you use the same attributes that I've used, paying close attention to the <code>Version</code> and <code>AssociateTag</code> fields, you will get a result with an iframe URL included.</p>

<p>I then embedded the url in an iframe:</p>

<pre><code>&lt;html&gt;
    &lt;body&gt;
        &lt;iframe src=""http://www.amazon.com/reviews/iframe?akid=&lt;YourAmazonKey&gt;&amp;amp;alinkCode=xm2&amp;amp;asin=1451648537&amp;amp;atag=&lt;YourAssociateTag&gt;&amp;amp;exp=2012-01-06T02%3A10%3A38Z&amp;amp;summary=0&amp;amp;truncate=256&amp;amp;v=2&amp;amp;sig=kjWPue1N75%2FiI1hW67XYApWxnKeT2tlT%2FJ1rw4WLlUo%3D""/&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Which looked like the following:</p>

<p><img src=""https://i.stack.imgur.com/ousQI.png"" alt=""Amazon Review IFrame""></p>

<p>As I said in the preamble, I'm unsure if this is what you are looking for since the requirements for the bounty was:</p>

<blockquote>
  <p>...to get access to amazons reviewrank informations</p>
</blockquote>

<p>This isn't exactly the review rank information but the actual reviews and I take the meaning of review rank info to be more along the lines of the data itself (such as 100 reviews @ 4 stars etc.). However, in your question you stated that the iframe did not work:</p>

<blockquote>
  <p>However, testing with the params they suggest to get the Iframe, but it seems that now even the Iframe doesn't work anymore.</p>
</blockquote>

<p>So I thought that I would at least provide you with the proper method of getting and using the iframe.</p>
",4,CC BY-SA 3.0,8700875.0,2012-01-02T12:32:14.927,17.0,10886.0,2012-01-04T16:53:14.707,2014-03-26T17:29:35.950,Amazon Product Advertising API - get review-rank,<amazon-web-services><amazon><amazon-product-api>,3.0,2.0,,5.0,
11697972,9278535,1,"Yep, wasn't aware that I was this out of date. Updated to the latest version and can already see a lot of new features. I bumbed into another issue ""Anonymous users cannot initiate multipart upload"" but this is a separae issu and I guess I can always work around it by initiating on the server side. Thanks for your help. Will accept this answer.",2012-02-14T14:45:56.377,526457,CC BY-SA 3.0,True,1,I,.,1,"Yep, wasn't aware that I was this outdated.",False,False,,,,,,,,,,,,,,,,,,,,
12060525,9520377,0,"@Mark_54: This clearly indicates that you are still using your original XSLT transform by some means, insofar this is the only source where the now outdated `2005-10-05` namespace can stem from - you must have done a mistake somewhere, either by not updating the correct XSLT file in the first place (e.g. a local copy only) or not using the correct XSLT file in turn (e.g. not replacing the one in use in your server with the modified one). The latter might actually be caused by caching eventually, i.e. depending on how you do the XSLT processing, you might need to restart your (web) server.",2012-03-01T18:19:08.180,45773,CC BY-SA 3.0,True,1,,05,0,"This clearly indicates that you are still using your original XSLT transform by some means, insofar this is the only source where the now outdated `2005-10-05",False,False,,,,,,,,,,,,,,,,,,,,
12241799,9526131,0,"That Azure thing looks like a good approach, but JSONP is in my opinion absolutely deprecated technique.",2012-03-09T23:48:55.603,437523,CC BY-SA 3.0,True,1,,.,0,"That Azure thing looks like a good approach, but JSONP is in my opinion absolutely deprecated technique.",False,False,,,,,,,,,,,,,,,,,,,,
12390749,9742168,1,"OK, that's because you're using the VERY VERY old S3 gem, this gem is clearly outdated. You should use the official AWS SDK gem from Amazon.",2012-03-16T18:00:57.883,323384,CC BY-SA 3.0,True,1,,.,0,"OK, that's because you're using the VERY VERY old S3 gem, this gem is clearly outdated.",False,False,9742168.0,9740919.0,2012-03-16T17:54:08.007,7.0,"<p>See :
<a href=""http://docs.amazonwebservices.com/AWSRubySDK/latest/AWS/S3/S3Object.html"" rel=""noreferrer"">http://docs.amazonwebservices.com/AWSRubySDK/latest/AWS/S3/S3Object.html</a></p>

<p>Basically you have to use the <code>read</code> and <code>write</code> methods on S3 objects.</p>

<p>So :</p>

<pre><code>File.open(""/Users/matt/local_copy.mp3"", ""w"") do |f|
  f.write(bucket.objects[1].read)
end
</code></pre>
",5,CC BY-SA 3.0,9740919.0,2012-03-16T16:23:42.577,3.0,9730.0,2012-03-16T17:59:52.110,2012-03-16T17:59:52.110,Is there a way to download a file from s3 using the ruby gem aws-s3?,<ruby-on-rails><ruby><rubygems><amazon-s3>,2.0,1.0,,1.0,
12390749,9742168,1,"OK, that's because you're using the VERY VERY old S3 gem, this gem is clearly outdated. You should use the official AWS SDK gem from Amazon.",2012-03-16T18:00:57.883,323384,CC BY-SA 3.0,True,1,,.,0,"OK, that's because you're using the VERY VERY old S3 gem, this gem is clearly outdated.",False,False,9742168.0,9740919.0,2012-03-16T17:54:08.007,7.0,"<p>See :
<a href=""http://docs.amazonwebservices.com/AWSRubySDK/latest/AWS/S3/S3Object.html"" rel=""noreferrer"">http://docs.amazonwebservices.com/AWSRubySDK/latest/AWS/S3/S3Object.html</a></p>

<p>Basically you have to use the <code>read</code> and <code>write</code> methods on S3 objects.</p>

<p>So :</p>

<pre><code>File.open(""/Users/matt/local_copy.mp3"", ""w"") do |f|
  f.write(bucket.objects[1].read)
end
</code></pre>
",5,CC BY-SA 3.0,9740919.0,2012-03-16T16:23:42.577,3.0,9730.0,2012-03-16T17:59:52.110,2012-03-16T17:59:52.110,Is there a way to download a file from s3 using the ruby gem aws-s3?,<ruby-on-rails><ruby><rubygems><amazon-s3>,2.0,1.0,,1.0,
13228574,10252770,0,Hmm this content seems to be outdated or maybe I didn't realize how to do that. Is there any pay support from Amazon?,2012-04-23T15:04:45.643,719427,CC BY-SA 3.0,True,1,,.,1,Hmm this content seems to be outdated or maybe I didn't realize how to do that.,False,False,,,,,,,,,,,,,,,,,,,,
13332210,1693376,27,"It's worth mentioning that the PA-API is now deprecated, and the Amazon Marketplace Web Service (MWS) should now be used: https://developer.amazonservices.com/index.html",2012-04-27T11:06:59.217,28190,CC BY-SA 3.0,True,1,,https://developer.amazonservices.com/index.html,0,"It's worth mentioning that the PA-API is now deprecated, and the Amazon Marketplace Web Service (MWS) should now be used: https://developer.amazonservices.com/index.html",False,False,,,,,,,,,,,,,,,,,,,,
13340537,10353123,0,"1. It seems that that [pyramid_socketio](https://github.com/abourget/pyramid_socketio) is deprecated. Instead they support [gevent-socketio](https://github.com/abourget/gevent-socketio) . However, if I use the same server wouldn't that overkill the pyramid application?",2012-04-27T16:51:27.600,312509,CC BY-SA 3.0,True,1,,.,0,It seems that that [pyramid_socketio](https://github.com/abourget/pyramid_socketio) is deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
13499087,10453972,0,"Hey Nont, thqnks for your comment. Thing is, the opencv package available in ubuntu is waaay outdated, and I really need some newer version. I can still compile it on host though, and try to send it one way or another.",2012-05-04T18:23:00.800,282677,CC BY-SA 3.0,True,1,Thing,.,0,"Thing is, the opencv package available in ubuntu is waaay outdated, and I really need some newer version.",False,False,,,,,,,,,,,,,,,,,,,,
13643679,10521081,0,"Yes, composite key layout would work there. I can't say for sure that it would result in less I/O than supercolumns—that likely depends on a lot of hard-to-measure variables—but I can tell you for sure that composites will make your life easier down the road. Supercolumns are deprecated, already unsupported with certain new features, and are considerably more problematic in some areas of Cassandra operation.",2012-05-11T01:50:15.060,193468,CC BY-SA 3.0,True,1,Supercolumns,.,0,"Supercolumns are deprecated, already unsupported with certain new features, and are considerably more problematic in some areas of Cassandra operation.",False,False,10521081.0,10513516.0,2012-05-09T17:25:49.690,4.0,"<p>I'm not positive whether you can tweak your config easily to get more disk performance, but using Snappy compression could help a good deal in making your app need to read less overall. It <em>may</em> also help to use the new composite key layout instead of supercolumns.</p>

<p>One thing I can say for sure: EBS will <em>NOT</em> work better. Stay away from that at all costs if you care about latency.</p>
",8,CC BY-SA 3.0,10513516.0,2012-05-09T09:39:54.623,3.0,1101.0,,2012-05-09T17:25:49.690,"Cassandra Amazon EC2 , lots of IOWait",<amazon-ec2><cassandra><iowait>,1.0,0.0,,1.0,
14095955,8093549,0,This presentation was given to client (little outdated) so thats why presentation is showing suggested architecture.,2012-05-31T05:26:48.390,1173629,CC BY-SA 3.0,True,1,,),0,This presentation was given to client (little outdated),False,False,8093549.0,8073293.0,2011-11-11T11:47:40.187,4.0,"<p>Recently we have done a very good comparison between AWS and Heroku and we decided to move to Heroku, here is the detail of this <a href=""http://www.confiz.com/blog/tech-session/selecting-the-right-cloud-platform/"" rel=""nofollow"">http://www.confiz.com/blog/tech-session/selecting-the-right-cloud-platform/</a></p>
",3,CC BY-SA 3.0,8073293.0,2011-11-09T23:42:56.510,38.0,14890.0,,2019-01-08T04:01:10.363,AWS vs Heroku vs something else for scalable platform?,<heroku><amazon-ec2><amazon-web-services><scalability>,8.0,0.0,,12.0,
14095955,8093549,0,This presentation was given to client (little outdated) so thats why presentation is showing suggested architecture.,2012-05-31T05:26:48.390,1173629,CC BY-SA 3.0,True,1,,),0,This presentation was given to client (little outdated),False,False,8093549.0,8073293.0,2011-11-11T11:47:40.187,4.0,"<p>Recently we have done a very good comparison between AWS and Heroku and we decided to move to Heroku, here is the detail of this <a href=""http://www.confiz.com/blog/tech-session/selecting-the-right-cloud-platform/"" rel=""nofollow"">http://www.confiz.com/blog/tech-session/selecting-the-right-cloud-platform/</a></p>
",3,CC BY-SA 3.0,8073293.0,2011-11-09T23:42:56.510,38.0,14890.0,,2019-01-08T04:01:10.363,AWS vs Heroku vs something else for scalable platform?,<heroku><amazon-ec2><amazon-web-services><scalability>,8.0,0.0,,12.0,
14985638,8460831,1,"If you happen to bump Spree to newer version, for example 1.0.x, I'd recommend you to change the gem from `aws-s3` to `aws-sdk` because the former one is deprecated. Another quick way is to use spree-heroku extension https://github.com/joneslee85/spree-heroku.

If you go for Spree 1.1.x, you don't need to install any extension or override the model, you can configure that in the admin setting. Hope this help.",2012-07-07T05:54:40.783,421326,CC BY-SA 3.0,True,1,,.,0,"If you happen to bump Spree to newer version, for example 1.0.x, I'd recommend you to change the gem from `aws-s3` to `aws-sdk` because the former one is deprecated.",False,False,8460831.0,8294333.0,2011-12-10T23:45:25.713,9.0,"<p>I am running rails 3.0.10 and spree 0.60 and was able to get spree to use s3 storage over writing to the public folder of the app by doing the following The process should be alike.</p>

<p>add aws-s3 gem to your Gemfile</p>

<pre><code>gem 'aws-s3'
</code></pre>

<p>bundle installed and after doing that I created a yaml file in the config directory called s3.yml and it should look something like this.</p>

<pre><code>development: &amp;DEFAULTS
  bucket: ""YOUR_BUCKET""
  access_key_id: ""YOUR_ACCESS_KEY""
  secret_access_key: ""YOUR_ACCESS_SECRET""

test:
  &lt;&lt;: *DEFAULTS
  bucket: ""YOUR_BUCKET""

production:
  &lt;&lt;: *DEFAULTS
  bucket: ""YOUR_BUCKET""
</code></pre>

<p>You can specify individual credentials per environment if you like but since mine are all using the same S3 accont I opted to set defaults.</p>

<p>after that you are going to have to override the image model or make a decorator for your  which tells paperclip to use S3 and to have it parse the yaml file created for credentials.</p>

<p>the area you want want to override would be this</p>

<pre><code>has_attached_file :attachment,
                :styles =&gt; {:mini =&gt; '48x48&gt;', :small =&gt; '200x100&gt;', :product =&gt; '240x240&gt;', :large =&gt; '600x600&gt;'},
                :default_style =&gt; :small,
                :storage =&gt; :s3,
                :s3_credentials =&gt; ""#{RAILS_ROOT}/config/s3.yml"",
                :url =&gt; ""/assets/products/:id/:style/:basename.:extension"",
                :path =&gt; "":rails_root/public/assets/products/:id/:style/:basename.:extension""
</code></pre>

<p>you can change these properties as needed but whats important is that you specify :storage and :s3_credentials.</p>
",1,CC BY-SA 3.0,8294333.0,2011-11-28T10:16:09.120,1.0,2786.0,,2013-02-05T09:57:36.663,Amazon S3 and Spree setup,<ruby-on-rails><ruby-on-rails-3><heroku><amazon-s3><spree>,3.0,2.0,,3.0,
14985638,8460831,1,"If you happen to bump Spree to newer version, for example 1.0.x, I'd recommend you to change the gem from `aws-s3` to `aws-sdk` because the former one is deprecated. Another quick way is to use spree-heroku extension https://github.com/joneslee85/spree-heroku.

If you go for Spree 1.1.x, you don't need to install any extension or override the model, you can configure that in the admin setting. Hope this help.",2012-07-07T05:54:40.783,421326,CC BY-SA 3.0,True,1,,.,0,"If you happen to bump Spree to newer version, for example 1.0.x, I'd recommend you to change the gem from `aws-s3` to `aws-sdk` because the former one is deprecated.",False,False,8460831.0,8294333.0,2011-12-10T23:45:25.713,9.0,"<p>I am running rails 3.0.10 and spree 0.60 and was able to get spree to use s3 storage over writing to the public folder of the app by doing the following The process should be alike.</p>

<p>add aws-s3 gem to your Gemfile</p>

<pre><code>gem 'aws-s3'
</code></pre>

<p>bundle installed and after doing that I created a yaml file in the config directory called s3.yml and it should look something like this.</p>

<pre><code>development: &amp;DEFAULTS
  bucket: ""YOUR_BUCKET""
  access_key_id: ""YOUR_ACCESS_KEY""
  secret_access_key: ""YOUR_ACCESS_SECRET""

test:
  &lt;&lt;: *DEFAULTS
  bucket: ""YOUR_BUCKET""

production:
  &lt;&lt;: *DEFAULTS
  bucket: ""YOUR_BUCKET""
</code></pre>

<p>You can specify individual credentials per environment if you like but since mine are all using the same S3 accont I opted to set defaults.</p>

<p>after that you are going to have to override the image model or make a decorator for your  which tells paperclip to use S3 and to have it parse the yaml file created for credentials.</p>

<p>the area you want want to override would be this</p>

<pre><code>has_attached_file :attachment,
                :styles =&gt; {:mini =&gt; '48x48&gt;', :small =&gt; '200x100&gt;', :product =&gt; '240x240&gt;', :large =&gt; '600x600&gt;'},
                :default_style =&gt; :small,
                :storage =&gt; :s3,
                :s3_credentials =&gt; ""#{RAILS_ROOT}/config/s3.yml"",
                :url =&gt; ""/assets/products/:id/:style/:basename.:extension"",
                :path =&gt; "":rails_root/public/assets/products/:id/:style/:basename.:extension""
</code></pre>

<p>you can change these properties as needed but whats important is that you specify :storage and :s3_credentials.</p>
",1,CC BY-SA 3.0,8294333.0,2011-11-28T10:16:09.120,1.0,2786.0,,2013-02-05T09:57:36.663,Amazon S3 and Spree setup,<ruby-on-rails><ruby-on-rails-3><heroku><amazon-s3><spree>,3.0,2.0,,3.0,
15868606,1703132,0,"This is a bit out of date, as you can now get a [static IP](http://aws.amazon.com/articles/1346).  I'm not sure what you mean by ""if the shard goes away you lose it unless you pay"" - any service, if you don't pay eventually they will delete your instance.  You do also get root on EC2.",2012-08-11T21:11:33.027,243712,CC BY-SA 3.0,True,1,This, ,0,"This is a bit outdated, as you can now get a [static IP](http://aws.amazon.com/articles/1346).  ",False,False,,,,,,,,,,,,,,,,,,,,
16459630,12268941,0,The problem with doing that is some packages may have optimized compiled code that may not work on your target server; also components like database drivers have other build requirements that need to be satisfied - they may install but won't work; which is why it is discouraged. You can execute `pip freeze > requirements.txt` on your development machine. This will list out all the packages. Then copy the `requirements.txt` file to your server and run `sudo pip install -r requirements.txt`. This will install everything that was on your development machine on the server properly.,2012-09-05T02:34:52.830,790387,CC BY-SA 3.0,True,1,,.,1,The problem with doing that is some packages may have optimized compiled code that may not work on your target server; also components like database drivers have other build requirements that need to be satisfied - they may install but won't work; which is why it is discouraged.,False,True,,,,,,,,,,,,,,,,,,,,
17798285,9406577,0,This is effectively a link-only answer; such answers are discouraged.,2012-10-27T01:09:05.560,2509,CC BY-SA 3.0,True,1,answers,.,0,This is effectively a link-only answer; such answers are discouraged.,False,False,9406577.0,7816334.0,2012-02-23T03:06:37.567,0.0,"<p>I've been looking for solutions to deal with large scale remote sensing image in Hadoop for a long time. And I got nothing till now!</p>

<p>Here is a open source project about spliting the large scale image into samller ones in Hadoop.  I had read the code carefully and tested them. But I found that the performances are not as good as expectation. Anyway,  it may be helpful and shed some light on the problem.</p>

<p>Project Matsu:
<a href=""http://www.cloudbook.net/directories/research-clouds/research-project.php?id=100057"" rel=""nofollow"">http://www.cloudbook.net/directories/research-clouds/research-project.php?id=100057</a></p>

<p>Good luck!</p>
",1,CC BY-SA 3.0,7816334.0,2011-10-19T03:47:30.843,4.0,4450.0,,2013-01-30T10:35:52.050,Using Amazon MapReduce/Hadoop for Image Processing,<bash><hadoop><amazon-web-services><imagemagick><elastic-map-reduce>,4.0,0.0,,2.0,
17798285,9406577,0,This is effectively a link-only answer; such answers are discouraged.,2012-10-27T01:09:05.560,2509,CC BY-SA 3.0,True,1,answers,.,0,This is effectively a link-only answer; such answers are discouraged.,False,False,9406577.0,7816334.0,2012-02-23T03:06:37.567,0.0,"<p>I've been looking for solutions to deal with large scale remote sensing image in Hadoop for a long time. And I got nothing till now!</p>

<p>Here is a open source project about spliting the large scale image into samller ones in Hadoop.  I had read the code carefully and tested them. But I found that the performances are not as good as expectation. Anyway,  it may be helpful and shed some light on the problem.</p>

<p>Project Matsu:
<a href=""http://www.cloudbook.net/directories/research-clouds/research-project.php?id=100057"" rel=""nofollow"">http://www.cloudbook.net/directories/research-clouds/research-project.php?id=100057</a></p>

<p>Good luck!</p>
",1,CC BY-SA 3.0,7816334.0,2011-10-19T03:47:30.843,4.0,4450.0,,2013-01-30T10:35:52.050,Using Amazon MapReduce/Hadoop for Image Processing,<bash><hadoop><amazon-web-services><imagemagick><elastic-map-reduce>,4.0,0.0,,2.0,
18783230,13612396,0,"Hi Michel again, thank you! I now solved it with a (kind of bad) workaround: Even with external cronjobs by cronjob.de it's beeing aborted after about 200 refreshed objects (-> A bit more than 3 minutes running). I now have the job started every 5 minutes and query 175 elements which are out of date from my the DB and sync them with amazon. If no object is out of date, the job will be finished immediately. So this is not ideal, but it works and I don't have to spend more hours on setting up the cronjob serverside. Thank you",2012-12-03T11:35:06.867,1845463,CC BY-SA 3.0,True,2,,.,0,I now have the job started every 5 minutes and query 175 elements which are outdated from my the DB and sync them with amazon.,False,False,13612396.0,13591117.0,2012-11-28T18:56:09.860,1.0,"<p>OP stated he was still getting a timeout after a specific time, although overwriting the configured <code>max_execution_time</code> to unlimited, which was working according to <code>phpinfo()</code></p>

<p>Consider it to be bad practise to call a <code>worker script</code> via a browser.
This can cause uncontrollable states and causes other troubles just like those you're experiencing right now - asides from possible troubles this could cause when somebody else gets the URL into his hands.</p>

<p>Also - a hourly import is quite uncomfortable to do manually in a browser isn't it?</p>

<p>Your problem is, that not php stops execution because of a timeout, but the webserver.
You said you dont pass any GET or POST parameters. Just run it on php-cli on your command line</p>

<pre><code>php yourscript.php
</code></pre>

<p>All output will be logged into console. if you want you can write it into a logfile like this</p>

<pre><code>php yourscript.php &gt; log.txt
</code></pre>

<p>This still doesn't do anything automatically.
On any unix you can easily set up a cronjob doing this job for you at a desired time and interval</p>

<p>e.g. </p>

<pre><code>crontab -e
</code></pre>

<p>to edit your configured cronjobs</p>

<pre><code>* */2 * * * /usr/bin/php /yourpath/yourscript.php &gt; ~/yourlog.txt 2&gt;/dev/null
</code></pre>

<p>This would run every two hours and log your script output to <code>~/yourlog.txt</code>, redirect errors into <code>/dev/null</code></p>

<p>Check this to find out how the schedule table works
<a href=""http://www.adminschoice.com/crontab-quick-reference"" rel=""nofollow"">http://www.adminschoice.com/crontab-quick-reference</a></p>

<p>I also advice to move your script out of your webserver's <code>document root</code> so it is no longer accessible from the web.</p>
",2,CC BY-SA 3.0,13591117.0,2012-11-27T18:57:13.930,2.0,676.0,,2012-11-28T18:56:09.860,Running a PHP cronjob for more than 15 minutes,<php><.htaccess><amazon-web-services><crontab>,3.0,5.0,,,
18944014,13762560,0,@Civilian - Regarding the former: Using the AWS command line tools with the legacy certificate/key authentication scheme is deprecated already and will be removed at some point; please consider switching to the meanwhile ubiquitous `access key ID`/`secret access key` approach - your life will be much easier (and more secure if used in combination with [AWS Identity and Access Management (IAM)](http://aws.amazon.com/iam/)).,2012-12-08T13:32:44.960,45773,CC BY-SA 3.0,True,1,,-,0,- Regarding the former: Using the AWS command line tools with the legacy certificate/key authentication scheme is deprecated already and will be removed at some point; please consider switching to the meanwhile ubiquitous `access key ID`/`secret access key` approach -,False,False,,,,,,,,,,,,,,,,,,,,
19089647,13863702,0,"You could instead use the riak-admin reip command (http://docs.basho.com/riak/latest/references/Command-Line-Tools---riak-admin/#reip), although this may be about to be deprecated.",2012-12-13T16:12:49.460,1670037,CC BY-SA 3.0,True,1,,.,0,"You could instead use the riak-admin reip command (http://docs.basho.com/riak/latest/references/Command-Line-Tools---riak-admin/#reip), although this may be about to be deprecated.",False,False,,,,,,,,,,,,,,,,,,,,
19091777,13863702,0,"@ChristianDahlqvist Very true, shame it's about to be deprecated though. The alternative they've given to `riak-admin reip` is to use `riak-admin cluster replace`, but that requires you to have more than node, where in my case I only have one. :(",2012-12-13T17:18:52.250,1149354,CC BY-SA 3.0,True,1,,.,0,it's about to be deprecated though.,False,False,,,,,,,,,,,,,,,,,,,,
19161688,13871204,2,Using x509 certs for authentication is deprecated and will likely be going away.  You're better off using AWS_ACCESS_KEY and AWS_SECRET_KEY environment variables.  But then you have to embed the keys in the AMI which is strongly discouraged.  That's why using an AMI role is your best option.,2012-12-16T19:47:16.953,201225,CC BY-SA 3.0,True,2,, ,0,Using x509 certs for authentication is deprecated and will likely be going away.  ,False,False,13871204.0,13860361.0,2012-12-14T01:03:04.150,9.0,"<p>You should consider taking advantage of AWS's new <a href=""http://docs.amazonwebservices.com/IAM/latest/UserGuide/WorkingWithRoles.html"">IAM Roles</a> functionality.  Essentially what you do is create a new role in the IAM control panel and then assign it rights to create snapshots.  The policy you would need to attach to the role would look something like this:</p>

<pre><code>{
  ""Statement"": [
    {
      ""Sid"": ""Stmt1355446824880"",
      ""Action"": [
        ""ec2:CreateSnapshot"",
        ""ec2:DescribeSnapshots"",
        ""ec2:DescribeVolumes""
      ],
      ""Effect"": ""Allow"",
      ""Resource"": ""*""
    }
  ]
}
</code></pre>

<p>Then when you create your instance, you define the IAM role in the launch configuration (it's under ""advanced settings"").  The effect is that your instance now automatically has permission to create snapshots and you don't have to worry about storing the access keys or other credentials anywhere on the instance.  It will work via cron without issue.</p>
",4,CC BY-SA 3.0,13860361.0,2012-12-13T12:58:04.573,9.0,4114.0,,2017-07-25T08:37:57.023,Amazon EC2 EBS automatic backup one-liner works manually but not from cron,<amazon-web-services><amazon-ec2><cron><backup>,6.0,2.0,,,
19912912,14328213,1,"No, `ToBase64` works fine (though deprecated). You were just using it incorrectly ;) The API probably said return the hashed *bytes* as base64 ie `ToBase64( binaryDecode( hexValue, ""hex"" ))` That is actually very different than the original code which was doing this: `ToBase64( hexValue.getBytes(""UTF-8"") )`",2013-01-14T23:14:29.743,104223,CC BY-SA 3.0,True,1,,.,0,"No, `ToBase64` works fine (though deprecated).",False,False,,,,,,,,,,,,,,,,,,,,
19937077,14328213,1,"Either should be fine (though `toBase64` is deprecated). The reason the original did not work was because it was base64 encoding the wrong thing. When you decode the hashed string *as hex*, you get 16 bytes (one byte for each pair of characters). The original did not treat the string as hex. Decoding it as a plain string produced 32 bytes (one byte for each character). That is why your final string was always so much longer than Amazon's and did not match.",2013-01-15T17:05:55.463,104223,CC BY-SA 3.0,True,1,,.,0,Either should be fine (though `toBase64` is deprecated).,False,False,,,,,,,,,,,,,,,,,,,,
20018130,14387737,0,How would I retain a strong reference to the **delegate** AKA **self**? **request:didFailWithServiceException:** is deprecated BTW.,2013-01-17T23:30:55.777,1260708,CC BY-SA 3.0,True,1,,.,0,** is deprecated BTW.,False,False,14387737.0,14381063.0,2013-01-17T20:41:14.553,6.0,"<p>When using <code>AmazonServiceRequestDelegate</code>, <a href=""http://mobile.awsblog.com/post/Tx3RYJD3PLLN161/Using-the-AWS-SDK-for-iOS-Asynchronously-Part-II-AmazonRequestDelegate-Best-Prac"">this blog post</a> can be helpful. Basically, you need to:</p>

<ol>
<li>Avoid using AmazonServiceRequestDelegate in a background thread.</li>
<li>Retain a strong reference to the client.</li>
<li>Retain a strong reference to the delegate.</li>
</ol>

<p>Also, you should make sure you called <code>[AmazonErrorHandler shouldNotThrowExceptions]</code> since you didn't implement <code>request:didFailWithServiceException:</code>. Please take a look at <a href=""http://mobile.awsblog.com/post/Tx2PZV371MJJHUG/How-Not-to-Throw-Exceptions-with-the-AWS-SDK-for-iOS"">this blog post</a> for the detail.</p>
",7,CC BY-SA 3.0,14381063.0,2013-01-17T14:19:29.703,1.0,807.0,2013-01-19T02:26:07.517,2013-01-19T02:26:07.517,AWS iOS SDK Delegate Method Not Getting Called,<ios><amazon-web-services><amazon-simpledb>,1.0,0.0,,1.0,
20018214,14387737,0,"The object that has a reference to your delegate (`self`, in this case) has to make sure the delegate is not released until the AWS operation completes. We intentionally deprecated `request:didFailWithServiceException:`, however, you need to call `[AmazonErrorHandler shouldNotThrowExceptions]` as instructed in the blog post. Otherwise, `request:didFailWithError:` won't be called.",2013-01-17T23:35:44.633,1721492,CC BY-SA 3.0,True,1,,.,0,"We intentionally deprecated `request:didFailWithServiceException:`, however, you need to call `[AmazonErrorHandler shouldNotThrowExceptions]` as instructed in the blog post.",False,False,14387737.0,14381063.0,2013-01-17T20:41:14.553,6.0,"<p>When using <code>AmazonServiceRequestDelegate</code>, <a href=""http://mobile.awsblog.com/post/Tx3RYJD3PLLN161/Using-the-AWS-SDK-for-iOS-Asynchronously-Part-II-AmazonRequestDelegate-Best-Prac"">this blog post</a> can be helpful. Basically, you need to:</p>

<ol>
<li>Avoid using AmazonServiceRequestDelegate in a background thread.</li>
<li>Retain a strong reference to the client.</li>
<li>Retain a strong reference to the delegate.</li>
</ol>

<p>Also, you should make sure you called <code>[AmazonErrorHandler shouldNotThrowExceptions]</code> since you didn't implement <code>request:didFailWithServiceException:</code>. Please take a look at <a href=""http://mobile.awsblog.com/post/Tx2PZV371MJJHUG/How-Not-to-Throw-Exceptions-with-the-AWS-SDK-for-iOS"">this blog post</a> for the detail.</p>
",7,CC BY-SA 3.0,14381063.0,2013-01-17T14:19:29.703,1.0,807.0,2013-01-19T02:26:07.517,2013-01-19T02:26:07.517,AWS iOS SDK Delegate Method Not Getting Called,<ios><amazon-web-services><amazon-simpledb>,1.0,0.0,,1.0,
20065265,14288563,1,"Thanks, that definitely did it. All of the forms I read said that wouldn't work, but they were out of date. For anyone who is interested, you can still make this work using my above method, and adding the token in the query string as ""x-amz-security-token"".",2013-01-19T18:01:26.163,1832615,CC BY-SA 3.0,True,1,,.,1,"All of the forms I read said that wouldn't work, but they were outdated.",False,False,,,,,,,,,,,,,,,,,,,,
20194014,14476243,0,"ok, I was thinking this page was outdated (due to all items except one being finished, look at the end of the page): http://www.mono-project.com/Article:ThreadPool_Deadlocks , but maybe not. You could then try the workarounds that are there, like export MONO_THREADS_PER_CPU=2000 or <httpRuntime minFreeThreads=",2013-01-23T22:38:09.990,544947,CC BY-SA 3.0,True,1,,:,0,"ok, I was thinking this page was outdated (due to all items except one being finished, look at the end of the page):",False,False,,,,,,,,,,,,,,,,,,,,
20671191,14761913,2,I think this information is a bit outdated. I would not recommend deployment with fastcgi and flup because it is discontinued. I deploy my apps with Apache2 + mod_wsgi (simple to setup) or nginx + uwsgi (better performance). Check this question http://stackoverflow.com/questions/12305146/python-2-7-with-webpy-flup-or-modwsgi,2013-02-08T06:54:59.050,329226,CC BY-SA 3.0,True,1,I,.,0,I think this information is a bit outdated.,False,False,14761913.0,14761813.0,2013-02-07T22:05:57.500,2.0,"<p>FastCGI with lighttpd is the recommended way of using web.py in production. I recommend the web.py documentation on deployment: <a href=""http://webpy.org/deployment"" rel=""nofollow"">http://webpy.org/deployment</a></p>
",1,CC BY-SA 3.0,14761813.0,2013-02-07T21:59:34.987,0.0,618.0,2013-02-08T22:39:19.283,2013-06-25T02:31:25.407,Trying to host a persistent web app using EC2,<web-applications><amazon-ec2><web.py>,2.0,0.0,,,
22242459,5286133,0,"Zach, I double checked the link and it works fine. Nonetheless, the information there is probably now out of date as Amazon AMIs do change often.",2013-03-27T21:33:50.483,656551,CC BY-SA 3.0,True,1,information,.,0,"Nonetheless, the information there is probably now outdated as Amazon AMIs do change often.",False,False,,,,,,,,,,,,,,,,,,,,
22313428,15710553,0,"Are you using the 2.0 line?  send_email() is from the old 1.0 library, which has been deprecated.  See http://docs.aws.amazon.com/aws-sdk-php-2/latest/class-Aws.Ses.SesClient.html",2013-03-29T20:21:02.150,389012,CC BY-SA 3.0,True,1,, ,0,"send_email() is from the old 1.0 library, which has been deprecated.  ",False,False,15710553.0,15710428.0,2013-03-29T19:58:05.363,0.0,"<p>First of all, it seems that you should include this code for instantiating the client and sending the email within a try-catch block, that will certainly resolve the Catchable fatal error part and allow your code to continue executing.</p>

<p>As far as the getCommand parameter problem, my guess is that there is some issue with your arguments to <code>send_email()</code> that are passed down the call stack.  Without digging through the SDK I don;t know off the top of my head what arguments are specifically passed to <code>getCommand</code>, but you have all the information you need there to debug the issue, as you should be able to map how your arguments are passed through each of the calls shown in the stack trace, debugging along the way to verify what is passed to each function is what is expected.</p>
",3,CC BY-SA 3.0,15710428.0,2013-03-29T19:48:09.307,-2.0,1684.0,2013-03-29T21:23:15.547,2013-10-13T03:22:50.130,AWS SDK Guzzle error when trying to send a email with SES,<php><amazon-ses>,3.0,1.0,,,
22769028,15970218,0,"@BoomShaka - I highly recommend switching to using IAM access keys only, which are much easier to use and significantly more versatile, and thus implicitly more secure; X.509 certificates are [meanwhile deprecated](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/setting_up_ec2_command_linux.html#set_aws_credentials_linux) for the EC2 Command Line Tools and hardly ever used elsewhere for quite some time already. See my answer to [How to download an EC2 X.509 certificate with an IAM User account?](http://stackoverflow.com/a/8995926/45773) for a bit more on this.",2013-04-12T14:17:25.137,45773,CC BY-SA 3.0,True,1,,latest,0,"@BoomShaka - I highly recommend switching to using IAM access keys only, which are much easier to use and significantly more versatile, and thus implicitly more secure; X.509 certificates are [meanwhile deprecated](http://docs.aws.amazon.com/AWSEC2/latest",False,False,15970218.0,15969649.0,2013-04-12T11:30:24.130,3.0,"<p>Weird issue - as usual when encountering something weird in software development, one should first question the assumptions:</p>

<blockquote>
  <p>I KNOW the instance ID exists, I copied it out of the AWS web console,
  and it is in the eu-west-1 region, and my env vars are set to this
  region.</p>
</blockquote>

<p>So the instance ID stems from a different environment than the one you want to use it in - I would try to derive the instance ID via the same environment instead, i.e.:</p>

<pre><code>ec2-describe-instances
</code></pre>

<p>I venture the guess that the list won't return the instances you are expecting. This would indicate that you are either using AWS credentials that belong to another account or that these credentials do not have the required <a href=""http://aws.amazon.com/ec2/"" rel=""nofollow"">Amazon EC2</a> read permissions assigned via <a href=""http://docs.aws.amazon.com/IAM/latest/UserGuide/PoliciesOverview.html"" rel=""nofollow"">IAM policies</a> for example.</p>
",3,CC BY-SA 3.0,15969649.0,2013-04-12T11:01:15.473,22.0,14939.0,2014-11-03T02:35:56.860,2019-07-21T01:44:20.170,ec2-describe-instance-status Client.InvalidInstanceID.NotFound but I KNOW instance exists,<amazon-web-services><amazon-ec2><amazon-elb><aws-cli>,6.0,0.0,,3.0,
22984645,16086195,0,"Its an alternate method which uses the git deployment feature of AWS S3.

I haven't written too much about it, so thats the reason I haven't released 1.0. Btw, can you pastebin a full trace? The version it seems to be running seems quite outdated, as recent versions print out its status",2013-04-19T05:26:45.280,39261,CC BY-SA 3.0,True,1,,status,0,"The version it seems to be running seems quite outdated, as recent versions print out its status",False,False,16086195.0,16085236.0,2013-04-18T14:50:11.970,1.0,"<p>We don't support too much the S3 Uploading (too slow, error prone), and suggest using fast-deploy instead.</p>

<p>$ mvn archetype:generate -Dfilter=elasticbeanstalk]</p>

<p>to get a sample pom to apply</p>
",3,CC BY-SA 3.0,16085236.0,2013-04-18T14:06:39.590,0.0,850.0,,2013-04-18T17:19:16.903,Uploading WAR file to S3 using ingenieux beanstalk maven plugin hangs,<maven><amazon-s3><beanstalk-maven-plugin>,2.0,0.0,,,
23125151,16172251,1,"Actually question is,since  `S3UploadInputStream *stream ` is deprecated how do I upload file sizes bigger than 5mb",2013-04-23T19:49:57.403,977466,CC BY-SA 3.0,True,1,,upload,0,"Actually question is,since  `S3UploadInputStream *stream ` is deprecated how do I upload",False,False,16172251.0,15790651.0,2013-04-23T14:40:25.903,0.0,"<p>I hope you need to check the size of a local file exceeds 5MB right ?</p>

<pre><code>NSDictionary *fileAttributes = [[NSFileManager defaultManager] attributesOfItemAtPath:_client.filename error:nil];
if(fileAttributes)
{
    NSNumber *fileSizeNumber = [fileAttributes objectForKey:NSFileSize];
    long totalFileSize = [fileSizeNumber longLongValue];
    if(totalFileSize &gt; 5*1024*1024)
    // Go for multipart upload
    else 
    // Go for Direct Put request
}
</code></pre>

<p>Or from the NSData you can take the Length and perform the same calculation.</p>
",2,CC BY-SA 3.0,15790651.0,2013-04-03T14:49:35.333,1.0,665.0,,2013-06-20T17:45:57.913,Multipart Upload to Amazon S3 with IOS 6,<ios><ios6><amazon-s3>,2.0,0.0,,,
24371162,16886701,1,The usage of s3bfs (block storage file system) is discouraged by AWS: http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-file-systems.html,2013-06-03T05:33:58.080,179529,CC BY-SA 3.0,True,1,usage,http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-file-systems.html,0,The usage of s3bfs (block storage file system) is discouraged by AWS: http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-file-systems.html,False,False,16886701.0,16877532.0,2013-06-02T19:53:47.490,0.0,"<p>You have both the option. You could stream data directly from Amazon S3 or first copy it to HDFS and then process it locally. The first way is good if you only intend to read the data once. And if your plan is to query the same input data multiple times, then you'd probably want to copy it to HDFS first.</p>

<p>And yes, by using S3 as an input to MapReduce you lose the data locality optimization. Also, if your plan is to use S3 as a replacement for HDFS, I would recommend you to go with <code>S3 Block FileSystem</code> instead of <code>S3 Native FileSystem</code> as it imposes a limit of 5GB on file size.</p>

<p>HTH</p>
",1,CC BY-SA 3.0,16877532.0,2013-06-01T21:41:09.737,1.0,1063.0,2013-06-03T05:31:51.573,2013-06-03T05:31:51.573,AWS EMR loading from S3,<amazon-web-services><mapreduce><bigdata><amazon-emr><emr>,1.0,0.0,,,
25348935,7653699,0,"Thank you very much for posting this! I was about to tear out what little hair I have. I was wondering why the setSecurityContextRepository method of SecurityContextPersistenceFilter was deprecated (the docs saying to use constructor injection, which is not right either).",2013-07-03T12:58:22.093,269361,CC BY-SA 3.0,True,1,,.,1,"I was wondering why the setSecurityContextRepository method of SecurityContextPersistenceFilter was deprecated (the docs saying to use constructor injection, which is not right either).",False,False,7653699.0,2504590.0,2011-10-04T20:33:10.390,27.0,"<p>We worked on the same issue (injecting a custom SecurityContextRepository to SecurityContextPersistenceFilter) for 4-5 hours today. Finally, we figured it out.
First of all, in the section 8.3 of Spring Security ref. doc, there is a SecurityContextPersistenceFilter bean definition</p>

<pre><code>&lt;bean id=""securityContextPersistenceFilter"" class=""org.springframework.security.web.context.SecurityContextPersistenceFilter""&gt;
    &lt;property name='securityContextRepository'&gt;
        &lt;bean class='org.springframework.security.web.context.HttpSessionSecurityContextRepository'&gt;
            &lt;property name='allowSessionCreation' value='false' /&gt;
        &lt;/bean&gt;
    &lt;/property&gt;
&lt;/bean&gt;
</code></pre>

<p>And after this definition, there is this explanation:
""Alternatively you could provide a null implementation of the SecurityContextRepository interface, which will prevent the security context from being stored, even if a session has already been created during the request.""</p>

<p>We needed to inject our custom SecurityContextRepository into the SecurityContextPersistenceFilter. So we simply changed the bean definition above with our custom impl and put it into the security context. </p>

<p>When we run the application, we traced the logs and saw that SecurityContextPersistenceFilter was not using our custom impl, it was using the HttpSessionSecurityContextRepository. </p>

<p>After a few other things we tried, we figured out that we had to give our custom SecurityContextRepository impl with the ""security-context-repository-ref"" attribute of ""http"" namespace. If you use ""http"" namespace and want to inject your own SecurityContextRepository impl, try ""security-context-repository-ref"" attribute.</p>

<p>When ""http"" namespace is used, a seperate SecurityContextPersistenceFilter definition is ignored. As I copied above, the reference doc. does not state that. </p>

<p>Please correct me if I misunderstood the things.</p>
",4,CC BY-SA 3.0,2504590.0,2010-03-24T00:21:23.997,99.0,79631.0,,2014-06-02T22:40:16.287,How can I use Spring Security without sessions?,<spring><spring-security><load-balancing><amazon-ec2>,7.0,0.0,,47.0,
25348935,7653699,0,"Thank you very much for posting this! I was about to tear out what little hair I have. I was wondering why the setSecurityContextRepository method of SecurityContextPersistenceFilter was deprecated (the docs saying to use constructor injection, which is not right either).",2013-07-03T12:58:22.093,269361,CC BY-SA 3.0,True,1,,.,1,"I was wondering why the setSecurityContextRepository method of SecurityContextPersistenceFilter was deprecated (the docs saying to use constructor injection, which is not right either).",False,False,7653699.0,2504590.0,2011-10-04T20:33:10.390,27.0,"<p>We worked on the same issue (injecting a custom SecurityContextRepository to SecurityContextPersistenceFilter) for 4-5 hours today. Finally, we figured it out.
First of all, in the section 8.3 of Spring Security ref. doc, there is a SecurityContextPersistenceFilter bean definition</p>

<pre><code>&lt;bean id=""securityContextPersistenceFilter"" class=""org.springframework.security.web.context.SecurityContextPersistenceFilter""&gt;
    &lt;property name='securityContextRepository'&gt;
        &lt;bean class='org.springframework.security.web.context.HttpSessionSecurityContextRepository'&gt;
            &lt;property name='allowSessionCreation' value='false' /&gt;
        &lt;/bean&gt;
    &lt;/property&gt;
&lt;/bean&gt;
</code></pre>

<p>And after this definition, there is this explanation:
""Alternatively you could provide a null implementation of the SecurityContextRepository interface, which will prevent the security context from being stored, even if a session has already been created during the request.""</p>

<p>We needed to inject our custom SecurityContextRepository into the SecurityContextPersistenceFilter. So we simply changed the bean definition above with our custom impl and put it into the security context. </p>

<p>When we run the application, we traced the logs and saw that SecurityContextPersistenceFilter was not using our custom impl, it was using the HttpSessionSecurityContextRepository. </p>

<p>After a few other things we tried, we figured out that we had to give our custom SecurityContextRepository impl with the ""security-context-repository-ref"" attribute of ""http"" namespace. If you use ""http"" namespace and want to inject your own SecurityContextRepository impl, try ""security-context-repository-ref"" attribute.</p>

<p>When ""http"" namespace is used, a seperate SecurityContextPersistenceFilter definition is ignored. As I copied above, the reference doc. does not state that. </p>

<p>Please correct me if I misunderstood the things.</p>
",4,CC BY-SA 3.0,2504590.0,2010-03-24T00:21:23.997,99.0,79631.0,,2014-06-02T22:40:16.287,How can I use Spring Security without sessions?,<spring><spring-security><load-balancing><amazon-ec2>,7.0,0.0,,47.0,
25378810,10899935,0,`@moosgummi` your answer is outdated as of 2012.,2013-07-04T08:35:25.393,227755,CC BY-SA 3.0,True,1,answer,.,0,`@moosgummi` your answer is outdated as of 2012.,False,False,,,,,,,,,,,,,,,,,,,,
25389200,17471072,0,I am working with AWS S3 and my S3.php file is not having the function `listObjects`.I tried finding it from [GIT](https://github.com/tpyo/amazon-s3-php-class/blob/master/S3.php) but its missing their too!Can you tell me if its removed or deprecated?If yes whats the alternate.Thanks,2013-07-04T13:42:11.397,1094850,CC BY-SA 3.0,True,1,,.,0,you tell me if its removed or deprecated?If yes whats the alternate.,False,False,,,,,,,,,,,,,,,,,,,,
25597966,12147709,0,"This behaviour seems to be problematic in Ruby 2.0.0, and deprecated (though I can't find a deprecation notice in the code). See https://github.com/aws/aws-sdk-ruby/issues/192, where Trevor says ""The block form is deprecated. That said, we do support Ruby 2 and I'll take a look at why this is failing.""",2013-07-11T09:58:48.237,187264,CC BY-SA 3.0,True,2,,.,1,"This behaviour seems to be problematic in Ruby 2.0.0, and deprecated (though I can't find a deprecation notice in the code).",False,False,12147709.0,12085751.0,2012-08-27T18:45:42.150,10.0,"<p>The ""buffer"" object yielded when passing a block to #write is an instance of StringIO.  You can write to the buffer using #write or #&lt;&lt;.  Here is an example that uses the block form to upload a file.</p>

<pre><code>file = File.open('/path/to/file', 'r')

obj = s3.buckets['my-bucket'].objects['object-key']
obj.write(:content_length =&gt; file.size) do |buffer, bytes|
  buffer.write(file.read(bytes))
  # you could do some interesting things here to track progress
end

file.close
</code></pre>
",5,CC BY-SA 3.0,12085751.0,2012-08-23T06:14:30.057,8.0,4453.0,,2013-07-25T23:00:34.030,Tracking Upload Progress of File to S3 Using Ruby aws-sdk,<ruby-on-rails><file-upload><amazon-s3><progress-bar>,2.0,0.0,,2.0,
27696186,18031407,5,`AWS::DynamoDB::ClientV2` is deprecated. You should use: `AWS::DynamoDB::Client.new(api_version: '2012-08-10')`,2013-09-13T10:45:59.830,514786,CC BY-SA 3.0,True,1,AWS::DynamoDB::ClientV2,.,0,`AWS::DynamoDB::ClientV2` is deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
28096482,18347952,1,The AWS SDK marks this approach as deprecated and will be removed in v2.0,2013-09-25T22:07:52.797,481927,CC BY-SA 3.0,True,1,,v2.0,0,The AWS SDK marks this approach as deprecated and will be removed in v2.0,False,False,18347952.0,7616810.0,2013-08-21T02:33:57.900,2.0,"<p>Consider CannedACL approach, shown in the follwing snippet:</p>

<pre><code>        string filePath = @""Path\Desert.jpg"";
        Stream input = new FileStream(filePath, FileMode.Open);
        PutObjectRequest request2 = new PutObjectRequest();
        request2.WithMetaData(""title"", ""the title"")
            .WithBucketName(bucketName)
            .WithCannedACL(S3CannedACL.PublicRead)
            .WithKey(keyName)
            .WithInputStream(input);
</code></pre>
",1,CC BY-SA 3.0,7616810.0,2011-09-30T22:47:05.543,30.0,18955.0,,2018-06-09T14:04:51.907,Amazon S3 upload with public permissions,<c#><amazon-s3>,5.0,0.0,,9.0,
28774994,19411383,0,link only answers are discouraged on SO,2013-10-16T19:08:21.417,2112418,CC BY-SA 3.0,True,1,answers,SO,0,only answers are discouraged on SO,False,False,19411383.0,22556.0,2013-10-16T18:49:26.587,0.0,"<p>I would highly recommend listing to this keynote presentation: How Enterprises like The Weather Company use the AWS Cloud <a href=""http://aws.typepad.com/aws/2013/10/how-the-weather-company-is-predicting-the-cloud-cover-using-the-aws-cloud.html"" rel=""nofollow"">http://aws.typepad.com/aws/2013/10/how-the-weather-company-is-predicting-the-cloud-cover-using-the-aws-cloud.html</a></p>
",1,CC BY-SA 3.0,22556.0,2008-08-22T14:50:18.640,2.0,2427.0,2012-05-03T01:10:06.220,2015-11-13T13:43:47.643,Amazon Web Services,<web-services><amazon-web-services><cloud>,8.0,2.0,2017-01-29T08:55:25.627,,
28936881,14930504,1,"_""WARNING: DOS-compatible mode is deprecated. It's strongly recommended to switch off the mode (command 'c') and change display units to sectors (command 'u')""_ This was not necessary for me (Ubuntu 13.04). It had already switched off DOS compatibility and used Sectors by default. Pressing `c` and `u` actually switched TO the deprecated modes.",2013-10-22T00:04:05.240,1081043,CC BY-SA 3.0,True,2,mode,.,0,DOS-compatible mode is deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
29746988,5502390,2,"The test page http://blog.kosny.com/testpages/safari-gz/ indicates that the warning  ""Be careful naming and test in Safari. Because safari won't handle css.gz or js.gz"" is out of date. In Safari 7 on Mavericks, and in Safari on iOS 7, both css.gz and js.gz work. I don't know when this change occurred, I'm only testing with the devices I have.",2013-11-14T15:46:55.083,595094,CC BY-SA 3.0,True,1,js.gz,.,0,"or js.gz"" is outdated.",False,False,,,,,,,,,,,,,,,,,,,,
30195149,20231087,0,Do you mean the `Content-type` header in the HTML? Wasn't that deprecated with HTML5?,2013-11-27T14:54:54.383,2497586,CC BY-SA 3.0,True,1,that,?,1,Wasn't that deprecated with HTML5?,False,False,,,,,,,,,,,,,,,,,,,,
30884730,9306973,0,"It's an old post, with outdated information.  I was updating it, so that it is still true.  Is that bad?  If you prefer that your answer stand for historical reasons without being marred by the presence of the truth I don't mind deleting my comment.",2013-12-17T12:34:19.683,1253362,CC BY-SA 3.0,True,1,, ,0,"It's an old post, with outdated information.  ",False,False,,,,,,,,,,,,,,,,,,,,
31108099,20755732,0,"He's actually not using the SDK 1.x. He's using an [unofficial, third-party S3 class](http://undesigned.org.za/2007/10/22/amazon-s3-php-class/)... which is also old and outdated. Using the _official_ [AWS SDK for PHP](https://aws.amazon.com/sdkforphp/) will give you the best performance and results.",2013-12-24T08:42:33.823,228514,CC BY-SA 3.0,True,1,,.,0,which is also old and outdated.,False,False,20755732.0,20755527.0,2013-12-24T06:07:12.740,0.0,"<p>You can list objects in a bucket by using the <a href=""http://docs.aws.amazon.com/AWSSDKforPHP/latest/#m=AmazonS3/list_objects"" rel=""nofollow""><code>list_objects()</code></a> function.</p>

<p>Note that the version of the Amazon S3 SDK you're using is old. You may have better results using <a href=""http://docs.aws.amazon.com/aws-sdk-php/guide/latest/index.html"" rel=""nofollow"">version 2 of the AWS SDK</a>.</p>
",2,CC BY-SA 3.0,20755527.0,2013-12-24T05:47:26.560,2.0,4660.0,2014-02-11T12:02:05.743,2014-02-11T12:02:05.743,How can I get a file list for a folder in my AWS bucket?,<php><amazon-s3>,2.0,0.0,,,
31215314,20805590,0,"That error is thown by the code that is inspecting the model, not the data, so it would be confusing if it were being triggered by some property of the data. Is it possible that the model class is out of date soemhow? Is it in a different DLL perhaps? Project needs a clean/rebuild in VS?",2013-12-28T17:41:43.990,203866,CC BY-SA 3.0,True,1,,?,0,Is it possible that the model class is outdated soemhow?,False,False,20805590.0,20798227.0,2013-12-27T18:06:23.857,2.0,"<p>The <code>DynamoDBContext.Scan&lt;T&gt;()</code> method only accepts names of public properties of the class <code>T</code> that is mapped to that table using the <code>[DynamoDBTable()]</code> attribute. <code>Scan&lt;T&gt;</code> will not recognize unmapped attributes in the table.</p>

<p>That exception will be thrown if the class <code>T</code> either doesn't have a <strong>public</strong> property of that name, or if the property is explicitly marked <code>[DynamoDBIgnore]</code>. </p>

<p>If you need to scan the table for table attributes that are not modeled, you can use the low-level API provided by <code>AmazonDynamoDBClient.Scan()</code>, but it won't return objects of type T.</p>
",4,CC BY-SA 3.0,20798227.0,2013-12-27T09:36:49.167,6.0,1524.0,2013-12-27T09:54:40.110,2015-09-09T06:46:25.093,"What does ""Unable to find storage information for property"" mean when using DynamoDB?",<.net><amazon-web-services><amazon-dynamodb>,2.0,0.0,,,
31566308,20956011,0,"What version of ec2-attach-volume are you using? The `-K` option is deprecated. can you run `ec2-attach-volume -h` and see if says something like: `-K, --private-key KEY
          [DEPRECATED] Specify KEY as the private key to use`",2014-01-08T18:59:42.587,2989261,CC BY-SA 3.0,True,1,option,.,0,The `-K` option is deprecated.,False,False,20956011.0,20947208.0,2014-01-06T17:46:55.880,1.0,"<p>Sound like your are missing the environment variables for your AWS keys. Assuming you are using Unix/Linux try:</p>

<pre><code>export AWS_ACCESS_KEY_ID=&lt;your user AWS access key&gt;
export AWS_SECRET_ACCESS_KEY=&lt;your user AWS secret access key&gt;
</code></pre>

<p>Hope it helps.</p>
",1,CC BY-SA 3.0,20947208.0,2014-01-06T09:51:44.433,0.0,227.0,,2014-01-06T17:46:55.880,"ec2-attach-volume Required option '-K, --private-key KEY' missing",<amazon-web-services><amazon-ec2>,1.0,1.0,,,
31571414,21002826,0,I was reading an outdated source. Ty for clarifications (+1).,2014-01-08T21:20:32.330,2619107,CC BY-SA 3.0,True,1,,.,0,I was reading an outdated source.,False,False,,,,,,,,,,,,,,,,,,,,
32027268,21237864,0,"I would have thought storing third party data in a relational way, is the perfect example of when your don't need ACID (If a user gets a slightly out of date piece of data it is not critical). But if you need to store your data in a structured format that is ACID, then you've answered your own question. A relational db is what you need.",2014-01-21T12:54:27.453,3208274,CC BY-SA 3.0,True,1,,.,1,(If a user gets a slightly outdated piece of data it is not critical).,False,True,21237864.0,21141496.0,2014-01-20T15:30:23.417,1.0,"<ol>
<li>Yes</li>
<li>Depends on how structured you wish to represent your data at the storage level. If you build that structure up in memory, or using Lucene for search look at NoSQL options (Dynamo for AWS). </li>
<li>Look at using a Hadoop cluster for normalising your data in a timely fashion.</li>
</ol>
",4,CC BY-SA 3.0,21141496.0,2014-01-15T15:28:49.280,5.0,467.0,2014-01-15T15:44:27.727,2016-04-26T21:16:55.827,Setting architecture for a mobile application with external APIs and smart content suggestions,<architecture><amazon-web-services><nosql><rdbms>,3.0,1.0,,2.0,
32075239,21273753,0,"My Ec2 Group seems to be a VPC Group, over the webinterface i can choose that group for my rds security group. :/ , I will try it in java, to check if its working

Thanks, but i think it is a bug, maybe it is the same as here https://github.com/boto/boto/issues/561 ? 
Even with the ID i get the error, data the api is outdated db = connRds.create_dbinstance(self.dbName,5,""db.t1.micro"",self.dbUser,self.dbUserPassword, security_groups=[sgEc2.id]) 

then i get the error, that i have an outdated api call. i raised an issue in github. https://github.com/boto/boto/issues/2010",2014-01-22T14:40:59.683,1584115,CC BY-SA 3.0,True,2,api,"

",0,"i get the error, data the api is outdated db = connRds.create_dbinstance(self.dbName,5,""db.t1.micro"",self.dbUser,self.dbUserPassword, security_groups=[sgEc2.id]) 

",False,True,21273753.0,21265463.0,2014-01-22T03:43:09.333,1.0,"<blockquote>
  <p><strong>Cannot apply DB Security Group membership to DB Instance in VPC.Please use VPC Security Groups instead</strong></p>
</blockquote>

<p>Always remember a rule of thumb, <strong><em>when you have your stuff running inside a VPC, then ONLY the VPC security groups prevail.</em></strong></p>

<p>Other individual security groups e.g. RDS Security group, RedShift Security Groups...etc are valid only for stuff that is running in <code>EC2-Classic</code>, meaning, outside of VPC.</p>

<p>So, you are essentially trying to apply an EC2-Classic (in other words, RDS Security Group) to an RDS instance which is running inside a VPC. This is wrong and hence you are receiving aforementioned error.</p>
",1,CC BY-SA 3.0,21265463.0,2014-01-21T17:47:10.070,0.0,739.0,2014-02-04T10:40:38.977,2014-02-04T10:40:38.977,Open Port for MySQL Amazon RDS with python or CLI,<python><amazon-web-services><boto><amazon-rds>,2.0,1.0,,,
32395288,15535141,0,"'rvm pkg' is deprecated, by the way. At least that's the message I get when I get to the step to install openssl. As of the time I'm replying here, the current stable version of Ruby is also no longer 2.0.0 but 2.1.0.",2014-01-30T19:25:57.817,1465153,CC BY-SA 3.0,True,1,pkg,.,0,"'rvm pkg' is deprecated, by the way.",False,False,15535141.0,15535140.0,2013-03-20T21:32:14.087,64.0,"<p>Log into your brand new instance:</p>

<pre><code>[19:59:22] paul:~ $ ssh -i ~/.ssh/server.pem ec2-user@your.ip

   __|  __|_  )
   _|  (     /   Amazon Linux AMI
  ___|\___|___|

  https://aws.amazon.com/amazon-linux-ami/2012.09-release-notes/
</code></pre>

<p>and impersonate the root:</p>

<pre><code>$ sudo su -
</code></pre>

<p>You are logged in as root from here. Apply the server updates:</p>

<pre><code>[root@ip-xx ~]# yum update
...
Complete!
[root@ip-xx ~]# yum groupinstall ""Development Tools""
...
Install      72 Package(s)
...
Complete!
</code></pre>

<p>Now, this is where it differs from the standard solution. Install RVM but without a distribution of ruby:</p>

<pre><code>[root@ip-xx ~]# \curl -L https://get.rvm.io | bash -s stable

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   184  100   184    0     0    135      0  0:00:01  0:00:01 --:--:--   183
100 11861  100 11861    0     0   7180      0  0:00:01  0:00:01 --:--:-- 64113
Downloading RVM from wayneeseguin branch stable
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   124  100   124    0     0    161      0 --:--:-- --:--:-- --:--:--   185
100 1615k  100 1615k    0     0   258k      0  0:00:06  0:00:06 --:--:--  401k

Installing RVM to /usr/local/rvm/
    Creating group 'rvm'

# RVM:  Shell scripts enabling management of multiple ruby environments.
# RTFM: https://rvm.io/
# HELP: http://webchat.freenode.net/?channels=rvm (#rvm on irc.freenode.net)
# Cheatsheet: http://cheat.errtheblog.com/s/rvm/
# Screencast: http://screencasts.org/episodes/how-to-use-rvm

# In case of any issues read output of 'rvm requirements' and/or 'rvm notes'

Installation of RVM in /usr/local/rvm/ is almost complete:

  * First you need to add all users that will be using rvm to 'rvm' group,
    and logout - login again, anyone using rvm will be operating with `umask u=rwx,g=rwx,o=rx`.

  * To start using RVM you need to run `source /etc/profile.d/rvm.sh`
    in all your open shell windows, in rare cases you need to reopen all shell windows.

# root,
#
#   Thank you for using RVM!
#   I sincerely hope that RVM helps to make your life easier and
#   more enjoyable!!!
#
# ~Wayne
</code></pre>

<p>Let's check that no ruby version is installed:</p>

<pre><code>[root@ip-xx ~]# rvm list

rvm rubies


# No rvm rubies installed yet. Try 'rvm help install'.
</code></pre>

<p>Now, openssl is already installed on the system but is incompatible with setup:</p>

<pre><code>[root@ip-xx ~]# openssl version
OpenSSL 1.0.0k-fips 5 Feb 2013

[root@ip-xx ~]# openssl version -d
OPENSSLDIR: ""/etc/pki/tls""
</code></pre>

<p>Attempting to install ruby2 with this version will result in the following error:</p>

<pre><code>[root@ip-xx ~]# rvm install 2.0.0 -- --with-openssl-dir=/etc/pki/tls
Fetching yaml-0.1.4.tar.gz to /usr/local/rvm/archives
######################################################################## 100.0%
Extracting yaml to /usr/local/rvm/src/yaml-0.1.4
Prepare yaml in /usr/local/rvm/src/yaml-0.1.4.
Configuring yaml in /usr/local/rvm/src/yaml-0.1.4.
Compiling yaml in /usr/local/rvm/src/yaml-0.1.4.
Installing yaml to /usr/local/rvm/usr
Installing Ruby from source to: /usr/local/rvm/rubies/ruby-2.0.0-p0, this may take a while depending on your cpu(s)...
ruby-2.0.0-p0 - #downloading ruby-2.0.0-p0, this may take a while depending on your connection...
######################################################################## 100.0%
ruby-2.0.0-p0 - #extracting ruby-2.0.0-p0 to /usr/local/rvm/src/ruby-2.0.0-p0
ruby-2.0.0-p0 - #extracted to /usr/local/rvm/src/ruby-2.0.0-p0
ruby-2.0.0-p0 - #configuring
ruby-2.0.0-p0 - #compiling
ruby-2.0.0-p0 - #installing 
Retrieving rubygems-2.0.3
######################################################################## 100.0%
Extracting rubygems-2.0.3 ...
Removing old Rubygems files...
Installing rubygems-2.0.3 for ruby-2.0.0-p0 ...
Error running 'env GEM_PATH=/usr/local/rvm/gems/ruby-2.0.0-p0:/usr/local/rvm/gems/ruby-2.0.0-p0@global:/usr/local/rvm/gems/ruby-2.0.0-p0:/usr/local/rvm/gems/ruby-2.0.0-p0@global GEM_HOME=/usr/local/rvm/gems/ruby-2.0.0-p0 /usr/local/rvm/rubies/ruby-2.0.0-p0/bin/ruby /usr/local/rvm/src/rubygems-2.0.3/setup.rb', please read /usr/local/rvm/log/ruby-2.0.0-p0/rubygems.install.log
Installation of rubygems did not complete successfully.
Saving wrappers to '/usr/local/rvm/bin'.
ruby-2.0.0-p0 - #adjusting #shebangs for (gem irb erb ri rdoc testrb rake).
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/gem'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/irb'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/erb'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/ri'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/rdoc'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/testrb'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/rake'? y
ruby-2.0.0-p0 - #importing default gemsets, this may take time ...
Install of ruby-2.0.0-p0 - #complete
</code></pre>

<p>Although you'll get ruby2, the ""Installation of rubygems did not complete successfully"" - notice the warning: ""Error running 'env GEM_PATH=...""</p>

<p>Instead, we'll get RVM install a copy of openssl for us (see <a href=""https://rvm.io/packages/openssl/"" rel=""noreferrer"">https://rvm.io/packages/openssl/</a>). The zlib-devel package is required:</p>

<pre><code>[root@ip-xx ~]# yum install zlib-devel
...
Installed:
  zlib-devel.x86_64 0:1.2.5-7.11.amzn1                                                                                  

Complete!
[root@ip-xx ~]# rvm pkg install openssl
Fetching openssl-1.0.1c.tar.gz to /usr/local/rvm/archives
Extracting openssl to /usr/local/rvm/src/openssl-1.0.1c
Configuring openssl in /usr/local/rvm/src/openssl-1.0.1c.
Compiling openssl in /usr/local/rvm/src/openssl-1.0.1c.
Installing openssl to /usr/local/rvm/usr

Please note that it's required to reinstall all rubies:

    rvm reinstall all --force

Updating openssl certificates
</code></pre>

<p>We can now install ruby2:</p>

<pre><code>[root@ip-xx ~]# rvm reinstall 2.0.0 --with-openssl-dir=/usr/local/rvm/usr
Removing /usr/local/rvm/src/ruby-2.0.0-p0...
Removing /usr/local/rvm/rubies/ruby-2.0.0-p0...
Installing Ruby from source to: /usr/local/rvm/rubies/ruby-2.0.0-p0, this may take a while depending on your cpu(s)...
ruby-2.0.0-p0 - #downloading ruby-2.0.0-p0, this may take a while depending on your connection...
ruby-2.0.0-p0 - #extracting ruby-2.0.0-p0 to /usr/local/rvm/src/ruby-2.0.0-p0
ruby-2.0.0-p0 - #extracted to /usr/local/rvm/src/ruby-2.0.0-p0
ruby-2.0.0-p0 - #configuring
ruby-2.0.0-p0 - #compiling
ruby-2.0.0-p0 - #installing 
Removing old Rubygems files...
Installing rubygems-2.0.3 for ruby-2.0.0-p0 ...
Installation of rubygems completed successfully.
Saving wrappers to '/usr/local/rvm/bin'.
ruby-2.0.0-p0 - #adjusting #shebangs for (gem irb erb ri rdoc testrb rake).
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/gem'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/irb'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/erb'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/ri'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/rdoc'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/testrb'? y
cp: overwrite `/usr/local/rvm/rubies/ruby-2.0.0-p0/bin/rake'? y
ruby-2.0.0-p0 - #importing default gemsets, this may take time ...
Install of ruby-2.0.0-p0 - #complete 
Making gemset ruby-2.0.0-p0 pristine.
Making gemset ruby-2.0.0-p0@global pristine.

[root@ip-xx ~]# ruby -v
ruby 2.0.0p0 (2013-02-24 revision 39474) [x86_64-linux]
</code></pre>

<p>And rails4:</p>

<pre><code>[root@ip-xx ~]# gem install rails -v 4.0.0beta1
...
28 gems installed
[root@ip-xx ~]# gem list

*** LOCAL GEMS ***

actionmailer (4.0.0.beta1)
actionpack (4.0.0.beta1)
activemodel (4.0.0.beta1)
activerecord (4.0.0.beta1)
activerecord-deprecated_finders (0.0.3)
activesupport (4.0.0.beta1)
arel (4.0.0.beta2)
atomic (1.0.1)
bigdecimal (1.2.0)
builder (3.1.4)
bundler (1.3.4)
erubis (2.7.0)
hike (1.2.1)
i18n (0.6.4)
io-console (0.4.2)
json (1.7.7)
mail (2.5.3)
mime-types (1.21)
minitest (4.3.2)
multi_json (1.7.1)
polyglot (0.3.3)
psych (2.0.0)
rack (1.5.2)
rack-test (0.6.2)
rails (4.0.0.beta1)
railties (4.0.0.beta1)
rake (10.0.3, 0.9.6)
rdoc (4.0.0, 3.12.2)
rubygems-bundler (1.1.1)
rvm (1.11.3.6)
sprockets (2.9.0)
sprockets-rails (2.0.0.rc3)
test-unit (2.0.0.0)
thor (0.17.0)
thread_safe (0.1.0)
tilt (1.3.6)
treetop (1.4.12)
tzinfo (0.3.37)
</code></pre>

<p>That's all folks!</p>
",8,CC BY-SA 3.0,15535140.0,2013-03-20T21:32:14.087,27.0,26172.0,2014-04-19T00:30:22.727,2014-04-19T00:30:22.727,Installing Ruby 2.0 and Rails 4.0.0beta on AWS EC2,<ruby-on-rails><amazon-web-services><amazon-ec2><ruby-on-rails-4><ruby-2.0>,2.0,0.0,,30.0,
32810376,19240067,1,skill is obsolete? pkill is something you should look at.,2014-02-11T11:54:22.100,2503715,CC BY-SA 3.0,True,1,skill,?,0,skill is obsolete?,False,False,,,,,,,,,,,,,,,,,,,,
32970975,21752030,1,@ankurverma: Using random AMIs you find on EC2 is highly discouraged. You should only use AMIs where you know and trust the person/company that created the AMI.,2014-02-14T22:44:19.780,111286,CC BY-SA 3.0,True,1,you,.,0,Using random AMIs you find on EC2 is highly discouraged.,False,False,,,,,,,,,,,,,,,,,,,,
33236697,16335372,6,"Looks like get_all_instances() has deprecated functionality and actually returns a list of Reservation objects. So it's more favorable to use:

`instances=conn.get_only_instances(instance_ids=['instance_id'])`

So that you can then:

`instances[0].start()`",2014-02-21T16:54:34.793,174474,CC BY-SA 3.0,True,1,,.,0,Looks like get_all_instances() has deprecated functionality and actually returns a list of Reservation objects.,False,False,16335372.0,16335181.0,2013-05-02T10:08:45.603,11.0,"<p>I had completely missed <a href=""http://boto.readthedocs.org/en/latest/ref/ec2.html#boto.ec2.instance.Instance.start"">this command in the API</a></p>

<p>For future reference, this is how to start a stopped instance:</p>

<pre><code>instance = conn.get_all_instances(instance_ids=['instance_id'])
print instance[0].instances[0].start()
</code></pre>
",1,CC BY-SA 3.0,16335181.0,2013-05-02T09:58:26.363,8.0,8847.0,,2018-07-22T19:02:39.297,Starting a stopped EC2 instance with Boto,<python><amazon-web-services><amazon-ec2><boto>,2.0,0.0,,3.0,
33285703,21956835,0,"hmm but even if I do have the latest SDK installed and updated, if the libraries are deprecated, the tutorial would not work right?",2014-02-23T11:06:49.657,2253918,CC BY-SA 3.0,True,1,,?,1,"hmm but even if I do have the latest SDK installed and updated, if the libraries are deprecated, the tutorial would not work right?",False,False,21956835.0,21933724.0,2014-02-22T16:11:58.480,1.0,"<p>Dynamo DB made many breaking changes (requires code change) when they moved to DynamoDB V2.</p>

<p>You need to ensure that the latest SDK is installed and you re using it - In eclipse, you need to also mention which AWS SDK to use (point it to the latest). Though eclipse AWS plugin downloads the latest SDK automatically, it doesnt update the version number to us the latest. You need to update this yourself.</p>
",2,CC BY-SA 3.0,21933724.0,2014-02-21T11:51:50.497,0.0,3375.0,,2014-07-09T16:46:15.847,AWS DynamoDB tutorial does not import certain classes,<android><eclipse><amazon-web-services><amazon-dynamodb>,4.0,2.0,,,
33550606,18537349,0,"My answer may be out of date, as things change. I'll have to check.",2014-03-01T14:44:52.163,2003763,CC BY-SA 3.0,True,1,answer,.,0,"My answer may be outdated, as things change.",False,False,,,,,,,,,,,,,,,,,,,,
34053592,2734998,0,This solution is outdated. There are no plugins support CodeIgniter.,2014-03-14T07:18:54.667,419500,CC BY-SA 3.0,True,1,solution,.,0,This solution is outdated.,False,False,2734998.0,2337411.0,2010-04-29T05:40:47.467,3.0,"<p>Peng Kong of a3m <a href=""http://code.google.com/p/a3m/"" rel=""nofollow noreferrer"">http://code.google.com/p/a3m/</a> has a nice way of doing it with plugins:</p>

<p>Example twitter_pi.php</p>

<p>

<p>require_once(APPPATH.'modules/account/plugins/libraries/jmathai-twitter-async/EpiCurl.php');
require_once(APPPATH.'modules/account/plugins/libraries/jmathai-twitter-async/EpiOAuth.php');
require_once(APPPATH.'modules/account/plugins/libraries/jmathai-twitter-async/EpiTwitter.php');</p>

<p>/* End of file twitter_pi.php <em>/
/</em> Location: ./system/application/modules/account/plugins/twitter_pi.php */</p>

<p>In controller</p>

<p>$this->load->plugin('twitter');
$twitterObj = new EpiTwitter($this->config->item('twitter_consumer_key'), $this->config->item('twitter_consumer_secret'));</p>

<p>There is one problem with this in Codeigniter 2.0 there are no plugins</p>
",1,CC BY-SA 2.5,2337411.0,2010-02-25T20:33:15.853,25.0,31997.0,,2015-06-04T13:29:41.183,How do I include external Libraries in CodeIgniter?,<codeigniter><amazon-web-services>,4.0,0.0,,4.0,
34405795,22598020,0,Link only answers are discouraged. Please pull relevant portions of your link into this answer so that the information is available even if the link is not.,2014-03-23T22:39:30.983,189134,CC BY-SA 3.0,True,1,answers,.,0,Link only answers are discouraged.,False,False,,,,,,,,,,,,,,,,,,,,
34568149,22668678,0,"No, this is definitely not IE <= 9 friendly. :) I've used SWFUpload (https://code.google.com/p/swfupload/) to upload to s3 in IE <= 9. It's outdated (and I believe is no longer in active development), but it works. I don't think `atob` is supported in oldIE, either, so that would be a bit of trouble for you, too.",2014-03-27T13:50:12.363,78738,CC BY-SA 3.0,True,1,It,.,1,"It's outdated (and I believe is no longer in active development), but it works.",False,False,,,,,,,,,,,,,,,,,,,,
34888143,22864635,1,"Thanks for your help, your links help me to understand what was going wrong :) btw be aware that the -f option is deprecated in the last versions of elasticsearch [link](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/_system_and_settings.html)",2014-04-04T15:40:13.440,1121154,CC BY-SA 3.0,True,1,,),0,btw be aware that the -f option is deprecated in the last versions of elasticsearch [link](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/_system_and_settings.html),False,False,22864635.0,22862716.0,2014-04-04T13:46:20.053,1.0,"<p>Don't know what <code>-Xss256k -Xmx2048m</code> parameters are but <a href=""http://www.elasticsearch.org/tutorials/elasticsearch-on-ec2/"" rel=""nofollow"">this</a> and <a href=""http://chrissimpson.co.uk/using-elasticsearch-on-amazon-ec2.html"" rel=""nofollow"">this one</a> tutorials worked perfect for me.</p>

<p>Both suggest <code>sudo bin/elasticsearch -f</code> so try this one.</p>
",1,CC BY-SA 3.0,22862716.0,2014-04-04T12:23:05.617,1.0,1107.0,,2014-07-04T16:51:13.560,Exception with elasticSearch and AWS plugin,<amazon-ec2><elasticsearch><sbt-aws-plugin>,2.0,0.0,,,
35247917,13773369,2,This is now completely out of date. Heroku returns this: No such feature: user-env-compile,2014-04-14T16:55:54.697,1457231,CC BY-SA 3.0,True,1,This,.,0,This is now completely outdated.,False,False,13773369.0,13750690.0,2012-12-08T01:01:39.493,10.0,"<p>run below from <a href=""https://github.com/rumblelabs/asset_sync"" rel=""noreferrer"">asset_sync docs Labs section</a></p>

<pre><code>heroku labs:enable user-env-compile -a myapp
</code></pre>

<p>Hasn't made it's way into the platform as standard yet!</p>
",1,CC BY-SA 3.0,13750690.0,2012-12-06T19:05:25.943,2.0,2037.0,2012-12-07T13:04:30.347,2012-12-08T01:01:39.493,rails assets:precompile during slug for s3 error: Fog provider and directory can't be blank when env are set,<ruby-on-rails><heroku><amazon-s3><assets>,1.0,2.0,,2.0,
35339190,8676062,4,"Note that as of April 2014, those URLs are deprecated and the pricing info is outdated. They're now at http://a0.awsstatic.com/pricing/1/ec2/linux-od.min.js and http://a0.awsstatic.com/pricing/1/s3/pricing-storage-s3.min.js. If you look in the source of any pricing page and search for ""json"" you'll find the appropriate links.",2014-04-16T18:05:41.607,523568,CC BY-SA 3.0,True,1,,.,0,"Note that as of April 2014, those URLs are deprecated and the pricing info is outdated.",False,False,,,,,,,,,,,,,,,,,,,,
35730070,23326644,0,Ahh... My outdated info about S3.. They support up to 5TB now: http://aws.typepad.com/aws/2010/12/amazon-s3-object-size-limit.html,2014-04-28T06:12:21.343,447502,CC BY-SA 3.0,True,1,,..,0,My outdated info about S3..,False,False,23326644.0,23326125.0,2014-04-27T17:32:41.787,0.0,"<p>""huge""---is it 10s or 100s of GBs? s3 limits the object size to 5GB and uploading may fail if it exceeds the size limitation.</p>
",2,CC BY-SA 3.0,23326125.0,2014-04-27T16:45:33.863,1.0,346.0,,2014-04-27T19:25:02.613,Uploading a huge file from ec2 to s3 fails,<linux><amazon-web-services><amazon-ec2><amazon-s3>,2.0,3.0,,,
35970739,4806767,0,"These questions should be time-sensitive. Many of these answers and the benchmarks are obsolete. New m3 and i2 instances have SSD drives which kill EBS and RDS (essentially EBS with more optimized latency infrastructure).  PLUS - you can now run read-replicas out of cloud to roll your own hardware for super-nasty SSD replicas.  In short - we use multi-az RDS as primary to ease backups, restore etc, then do replica backups for offsite, then use Route53 with short TTL just in  case RDS failover is slow on the DNS side (or worse) so we can elect our own new masters.  Things have changed.",2014-05-05T00:48:30.873,753673,CC BY-SA 3.0,True,1,Many,.,0,Many of these answers and the benchmarks are obsolete.,False,False,4806767.0,4806272.0,2011-01-26T16:06:15.203,135.0,"<p>This is a simple question with a very complicated answer!</p>

<p>In short: EC2 will provide maximum performance if you go with a RAID0 EBS. Doing RAID0 EBS requires a pretty significant amount of maintenance overhead, for example:</p>

<p><a href=""http://alestic.com/2009/06/ec2-ebs-raid"" rel=""noreferrer"">http://alestic.com/2009/06/ec2-ebs-raid</a></p>

<p><a href=""http://alestic.com/2009/09/ec2-consistent-snapshot"" rel=""noreferrer"">http://alestic.com/2009/09/ec2-consistent-snapshot</a></p>

<p>EC2 without RAID0 EBS will provide crappy I/O performance, thus it's not even really an option.</p>

<p>RDS will provide very good (though not maximum) performance out of the box. The management console is fantastic and it's easy to upgrade instances. High availability and read only slaves are a click away. It's REALLY awesome.</p>

<p>Short answer: Go with RDS. Still on the fence? Go with RDS!!! if you enjoy headaches and tuning every last little bit for maximum performance, then you can consider EC2 + EBS RAID 0. Vanilla EC2 is a terrible option for MySQL hosting. </p>
",9,CC BY-SA 2.5,4806272.0,2011-01-26T15:23:39.647,132.0,52536.0,2014-04-10T09:14:07.307,2014-08-24T15:20:19.403,What are the respective advantages/limitations of Amazon RDS vs. EC2 with MySQL?,<mysql><amazon-ec2><amazon-rds>,8.0,2.0,2018-04-28T06:22:06.753,53.0,
36392863,8583282,0,Can anyone clarify the relationship between CloudInit.NET and EC2Config-with-UserData? Does the latter essentially make the former obsolete?,2014-05-15T18:50:18.300,64257,CC BY-SA 3.0,True,1,,?,0,Does the latter essentially make the former obsolete?,False,False,8583282.0,7677578.0,2011-12-20T23:23:28.543,2.0,"<p>I added a new codeplex project that already has this tool built for windows. Looking forward to some feedback.</p>

<p><a href=""http://cloudinitnet.codeplex.com/"" rel=""nofollow"">http://cloudinitnet.codeplex.com/</a></p>
",4,CC BY-SA 3.0,7677578.0,2011-10-06T16:36:25.540,9.0,10593.0,,2015-03-16T17:06:39.583,What is the Cloud-Init equivalent for Windows?,<windows><amazon-ec2><bootstrapping><cloud-init>,6.0,0.0,,1.0,
36392863,8583282,0,Can anyone clarify the relationship between CloudInit.NET and EC2Config-with-UserData? Does the latter essentially make the former obsolete?,2014-05-15T18:50:18.300,64257,CC BY-SA 3.0,True,1,,?,0,Does the latter essentially make the former obsolete?,False,False,8583282.0,7677578.0,2011-12-20T23:23:28.543,2.0,"<p>I added a new codeplex project that already has this tool built for windows. Looking forward to some feedback.</p>

<p><a href=""http://cloudinitnet.codeplex.com/"" rel=""nofollow"">http://cloudinitnet.codeplex.com/</a></p>
",4,CC BY-SA 3.0,7677578.0,2011-10-06T16:36:25.540,9.0,10593.0,,2015-03-16T17:06:39.583,What is the Cloud-Init equivalent for Windows?,<windows><amazon-ec2><bootstrapping><cloud-init>,6.0,0.0,,1.0,
36544586,12844187,10,This answer appears to be obsolete now. @SamuelJack's answer of looking at the log file at this path worked for me: /var/log/cfn-init.log,2014-05-20T17:17:24.457,1325237,CC BY-SA 3.0,True,1,answer,.,0,This answer appears to be obsolete now.,False,False,12844187.0,12836834.0,2012-10-11T16:23:07.687,7.0,"<p>On a Linux 32 bits Tomcat 7 container, I was able to find logs at:  <code>/var/log/eb-tools.log</code>, where I was able to see which of my commands failed.</p>
",1,CC BY-SA 3.0,12836834.0,2012-10-11T09:52:55.437,22.0,14388.0,,2017-03-03T08:30:45.647,where and how to read results of ebextensions execution?,<amazon-web-services><amazon-elastic-beanstalk>,5.0,1.0,,2.0,
36561925,23774922,1,"@user1157751 To expand on the earlier point, when a thread executes `counter += 1` or `counter++`, it's done in 3 stages: read `counter`, add 1 to it, write back to `counter`. The issue is if one thread reads `counter`, then another thread also reads `counter` *before the first thread wrote back to the variable*, you'll end up with a `counter` that only increased by 1 instead of 2. In addition, changes to `counter` aren't guaranteed to be seen by other threads as locally cached copies of `counter` could get out of date.",2014-05-21T06:12:09.380,3580294,CC BY-SA 3.0,True,1,,.,1,"In addition, changes to `counter` aren't guaranteed to be seen by other threads as locally cached copies of `counter` could get outdated.",False,False,,,,,,,,,,,,,,,,,,,,
36875497,23921875,0,You should basically consider the CLT to be outdated and I'm skeptical it will ever be updated. There has been no development in over two years and its functionality is limited. I'd strongly recommend migrating to a more robust API client.,2014-05-29T18:46:30.487,2338862,CC BY-SA 3.0,True,1,,outdated,0,You should basically consider the CLT to be outdated,False,False,23921875.0,23921598.0,2014-05-28T21:06:09.247,6.0,"<p>I fixed it by changing the <code>http</code> in <code>mturk.properties</code> to <code>https</code>:</p>

<pre><code>~/Desktop/aws-mona/aws-mturk-clt-1.3.1/bin$ vim mturk.properties 
</code></pre>

<p>to</p>

<pre><code># use the service_url defined below:
service_url=https://mechanicalturk.amazonaws.com/?Service=AWSMechanicalTurkRequester

~/Desktop/aws-mona/aws-mturk-clt-1.3.1/bin$ ./getBalance.sh 
Your account balance: $2.20
</code></pre>
",7,CC BY-SA 3.0,23921598.0,2014-05-28T20:50:00.927,1.0,595.0,2014-05-28T20:56:45.933,2014-05-28T21:06:09.247,getBalance in Amazon Turk gives error,<java><amazon-web-services><https><amazon><mechanicalturk>,1.0,6.0,2014-06-04T06:15:17.213,,
37307553,24164772,0,"Glad I could help. For the second question I would need to see the code. `ListVerifiedEmailAddresses` is deprecated as of May 15th, 2012. Amazon [recommends](http://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-email-addresses.html) using `ListIdentities` and `GetIdentityVerificationAttributes` in its place. That's not a solution to your problem, just a side note!",2014-06-11T19:31:05.483,1742975,CC BY-SA 3.0,True,1,,.,0,"`ListVerifiedEmailAddresses` is deprecated as of May 15th, 2012.",False,False,24164772.0,24164421.0,2014-06-11T13:54:10.757,3.0,"<p>Are you using credentials associated with an IAM user? If this is the case, you need to add a policy to the IAM user that will allow those specific actions in SES. Something like this:</p>

<pre><code>{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""Stmt1402494612000"",
      ""Effect"": ""Allow"",
      ""Action"": [
        ""ses:ListVerifiedEmailAddresses"",
        ""ses:VerifyEmailAddress""
      ],
      ""Resource"": [
        ""*""
      ]
    }
  ]
}
</code></pre>
",4,CC BY-SA 3.0,24164421.0,2014-06-11T13:37:55.303,0.0,3349.0,2014-08-22T08:08:16.947,2014-08-22T08:08:16.947,User is not authorized to perform: ses:VerifyEmailAddress,<c#><.net><amazon-web-services><amazon-ses>,1.0,0.0,,,
37533440,6693577,0,Great answer. One change I recommend is to use the awesome [requests module](http://docs.python-requests.org/en/latest/) rather than the outdated `urllib` module.,2014-06-18T15:44:11.323,109554,CC BY-SA 3.0,True,1,,.,0,One change I recommend is to use the awesome [requests module](http://docs.python-requests.org/en/latest/) rather than the outdated `urllib` module.,False,False,6693577.0,6685500.0,2011-07-14T13:03:08.363,57.0,"<p>You need to convert your output image into a set of bytes before you can upload to s3.  You can either write the image to a file then upload the file, or you can use a cStringIO object to avoid writing to disk as I've done here:</p>

<pre><code>import boto
import cStringIO
import urllib
import Image

#Retrieve our source image from a URL
fp = urllib.urlopen('http://example.com/test.png')

#Load the URL data into an image
img = cStringIO.StringIO(fp.read())
im = Image.open(img)

#Resize the image
im2 = im.resize((500, 100), Image.NEAREST)  

#NOTE, we're saving the image into a cStringIO object to avoid writing to disk
out_im2 = cStringIO.StringIO()
#You MUST specify the file type because there is no file name to discern it from
im2.save(out_im2, 'PNG')

#Now we connect to our s3 bucket and upload from memory
#credentials stored in environment AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
conn = boto.connect_s3()

#Connect to bucket and create key
b = conn.get_bucket('example')
k = b.new_key('example.png')

#Note we're setting contents from the in-memory string provided by cStringIO
k.set_contents_from_string(out_im2.getvalue())
</code></pre>
",5,CC BY-SA 3.0,6685500.0,2011-07-13T20:52:02.620,18.0,14903.0,2014-10-10T10:35:38.603,2018-08-17T20:47:27.177,Upload resized image to S3,<python><amazon-s3><python-imaging-library><boto>,3.0,0.0,,12.0,
37533440,6693577,0,Great answer. One change I recommend is to use the awesome [requests module](http://docs.python-requests.org/en/latest/) rather than the outdated `urllib` module.,2014-06-18T15:44:11.323,109554,CC BY-SA 3.0,True,1,,.,0,One change I recommend is to use the awesome [requests module](http://docs.python-requests.org/en/latest/) rather than the outdated `urllib` module.,False,False,6693577.0,6685500.0,2011-07-14T13:03:08.363,57.0,"<p>You need to convert your output image into a set of bytes before you can upload to s3.  You can either write the image to a file then upload the file, or you can use a cStringIO object to avoid writing to disk as I've done here:</p>

<pre><code>import boto
import cStringIO
import urllib
import Image

#Retrieve our source image from a URL
fp = urllib.urlopen('http://example.com/test.png')

#Load the URL data into an image
img = cStringIO.StringIO(fp.read())
im = Image.open(img)

#Resize the image
im2 = im.resize((500, 100), Image.NEAREST)  

#NOTE, we're saving the image into a cStringIO object to avoid writing to disk
out_im2 = cStringIO.StringIO()
#You MUST specify the file type because there is no file name to discern it from
im2.save(out_im2, 'PNG')

#Now we connect to our s3 bucket and upload from memory
#credentials stored in environment AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
conn = boto.connect_s3()

#Connect to bucket and create key
b = conn.get_bucket('example')
k = b.new_key('example.png')

#Note we're setting contents from the in-memory string provided by cStringIO
k.set_contents_from_string(out_im2.getvalue())
</code></pre>
",5,CC BY-SA 3.0,6685500.0,2011-07-13T20:52:02.620,18.0,14903.0,2014-10-10T10:35:38.603,2018-08-17T20:47:27.177,Upload resized image to S3,<python><amazon-s3><python-imaging-library><boto>,3.0,0.0,,12.0,
37729874,24392871,0,"Main pitfall is that it's a lot of work to maintain them yourself. When Amazon Linux is patched or releases a new version, you're out of date. You could use the Vagrant AWS provider and just run your dev boxes on EC2 instances. That's what I do.",2014-06-24T18:05:28.217,2430241,CC BY-SA 3.0,True,1,you,.,0,"When Amazon Linux is patched or releases a new version, you're outdated.",False,False,,,,,,,,,,,,,,,,,,,,
38193785,21935218,0,"This answer is wrong. The files in question are not deprecated. You looked at the wrong documentation. You looked at `com.amazonaws.services.**dynamodb**.datamodeling` (http://docs.aws.amazon.com/AWSAndroidSDK/latest/javadoc/com/amazonaws/services/dynamodb/datamodeling/DynamoDBTable.html)
instead of `com.amazonaws.services.**dynamodb2**.datamodeling`  
(http://docs.aws.amazon.com/AWSAndroidSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBTable.html) which is the package you used (dynamodb2). I'm posting my solution right now as I had the same issue.",2014-07-08T20:48:33.373,2288548,CC BY-SA 3.0,True,1,files,.,1,The files in question are not deprecated.,False,False,21935218.0,21933724.0,2014-02-21T13:01:53.433,1.0,"<p>I am not 100% sure, but after digging the <a href=""http://docs.aws.amazon.com/AWSAndroidSDK/latest/javadoc/"" rel=""nofollow"">documentation</a>, it seems like the classes that cannot be resolved are deprecated and are no longer included in the jar files...
They do still exist in the source code folder, but seeing that since they are deprecared, I guess there is no use trying to follow the tutorial...</p>
",1,CC BY-SA 3.0,21933724.0,2014-02-21T11:51:50.497,0.0,3375.0,,2014-07-09T16:46:15.847,AWS DynamoDB tutorial does not import certain classes,<android><eclipse><amazon-web-services><amazon-dynamodb>,4.0,2.0,,,
38745520,7617300,2,This method of setting the headers has been deprecated after S3 version 1. The `TransferUtilityUploadRequest` object no longer has a `AddHeader` method. The correct answer is now danielonthenet's.,2014-07-24T12:30:08.217,1195056,CC BY-SA 3.0,True,1,method,.,0,This method of setting the headers has been deprecated after S3 version 1.,False,False,,,,,,,,,,,,,,,,,,,,
38854446,16446163,0,"I think this answer is deprecated. Ansible 1.3 added a ""check_implicit_admin"" option. Setting that to yes should allow you to just set the password.

Eg:
mysql_user: check_implicit_admin=yes name=root host=localhost login_user=root login_host=localhost login_password={{ mysql_root_password }} password={{ mysql_root_password }} priv=*.*:ALL,GRANT",2014-07-28T08:56:42.030,273826,CC BY-SA 3.0,True,1,,.,0,I think this answer is deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
39374040,25262445,0,Could you please let me know in which file should this property go? Because mapred.min.split.size is deprecated in Hadoop 2x. There is no mapred-site.xml now.,2014-08-12T16:03:05.913,1733310,CC BY-SA 3.0,True,1,mapred.min.split.size,.,0,Because mapred.min.split.size is deprecated in Hadoop 2x.,False,False,25262445.0,25256958.0,2014-08-12T10:52:08.477,0.0,"<p>Couple of things to think about most likely explanation first</p>

<ul>
<li>Check you are correctly passing in the system variables to control the split size of the job, if you don't change this you won't alter the numbers of mappers (which you can check in the jobtracker UI).  If you get the same number of mappers each time your not actually changing anything.  To change the split size, use the system props <code>mapred.min.split.size</code> and <code>mapred.max.split.size</code></li>
<li>Make sure you are really hitting the cluster and not accidentally running locally with 1 process</li>
<li>Be aware that (unlike Spark) Hadoop has a horrifying job initialization time. IME it's around 20 seconds, and therefore for only 1 GB of data your not really seeing much time difference as the majority of the job is spent in initialization.</li>
</ul>
",2,CC BY-SA 3.0,25256958.0,2014-08-12T05:43:47.183,0.0,26.0,,2014-08-12T10:52:08.477,Benchmarking Hadoop on EC2 gives identical performances,<hadoop><amazon-ec2>,1.0,1.0,,,
39477868,22004172,1,"If you installed via a package manager, you should always try and remove via a package manager, or the package manager metadata will remain out of date and you'll always have issues.",2014-08-15T10:38:46.710,619892,CC BY-SA 3.0,True,1,you,.,0,"If you installed via a package manager, you should always try and remove via a package manager, or the package manager metadata will remain outdated and you'll always have issues.",False,True,,,,,,,,,,,,,,,,,,,,
39542065,25343942,0,"Yes, it seems it's not possible to downgrade in our case. We simply created a fresh hardware-virtualized t2-instance and moved our app to the new instance manually. Then, we terminated the now obsolete m1.small instance. From now on we'll be able to use our custom AMI to spin up new instances.",2014-08-18T09:44:22.450,3778854,CC BY-SA 3.0,True,1,,.,0,"Then, we terminated the now obsolete m1.small instance.",False,False,,,,,,,,,,,,,,,,,,,,
39631829,25390036,0,"Hey @emcanes thanks for your response. I tried what you suggested however that seem to have stopped my app from working. What I have in there right now is: gem 'aws-s3', :require => 'aws/s3' ...Also I read some documentation and called the method '.write' on my S3 Object instead of '.store', that solved the issue of getting rid of the Digest::Digest is deprecated; use Digest warning, however now, I immediately get redirected and alerted that the file cannot be uploaded.",2014-08-20T14:53:36.620,3040414,CC BY-SA 3.0,True,1,,.,1,"Also I read some documentation and called the method '.write' on my S3 Object instead of '.store', that solved the issue of getting rid of the Digest::Digest is deprecated; use Digest warning, however now, I immediately get redirected and alerted that the file cannot be uploaded.",False,False,25390036.0,25389624.0,2014-08-19T18:01:07.587,2.0,"<p>Have you tried making sure you have a current version of the gem?</p>

<p>Check your gemfile:</p>

<pre><code>gem ""aws-sdk"", ""~&gt; 1.33.0""
</code></pre>
",1,CC BY-SA 3.0,25389624.0,2014-08-19T17:37:18.493,4.0,3262.0,,2016-01-05T22:45:39.130,"Ruby on Rails: How can I resolve the ""Digest::Digest is deprecated; use Digest"" warning when uploading to AWS-S3?",<ruby-on-rails><ruby><amazon-web-services><amazon-s3>,3.0,2.0,,1.0,
39893740,18261694,0,"I was referring this url to understand how can we support upload - pause and resume: http://docs.aws.amazon.com/mobile/sdkforios/developerguide/s3transfermanager.html, here AWSS3TransferManagerUploadRequest object is used on which we can invoke pause. Surprisingly when I tried to search the same class at this link: http://docs.aws.amazon.com/AWSiOSSDK/latest/ I was unable to find it. Is it deprecated?",2014-08-28T12:58:21.347,217586,CC BY-SA 3.0,True,1,,?,0,Is it deprecated?,False,False,18261694.0,18237594.0,2013-08-15T20:56:45.883,1.0,"<p>Currently, <code>S3TransferManager</code> doesn't support cancel/pause/resume features. You need to use <code>AmazonS3Client</code> and <code>S3PutObjectRequest</code> without <code>S3TransferManager</code> in order to cancel put requests.</p>
",3,CC BY-SA 3.0,18237594.0,2013-08-14T16:46:04.603,2.0,410.0,2014-12-28T11:27:15.267,2014-12-28T11:27:15.267,S3PutObjectRequest failed to upload after cancelling a prior putObjectRequest,<iphone><ios><amazon-web-services><amazon-s3>,1.0,4.0,,1.0,
40192799,25712012,0,"Link only answers are discouraged. Please include the salient aspects of your solution in your answer, or delete this answer and just leave a comment.",2014-09-07T15:54:16.197,1271826,CC BY-SA 3.0,True,1,answers,.,0,Link only answers are discouraged.,False,False,,,,,,,,,,,,,,,,,,,,
40620982,25531722,0,So just setting these options doesn't work for me.Also it looks like the `OptionName`s are out of date now.,2014-09-20T07:34:35.040,240004,CC BY-SA 3.0,True,1,,.,0,Also it looks like the `OptionName`s are outdated now.,False,False,25531722.0,25516491.0,2014-08-27T15:53:21.413,4.0,"<p>When calling the <code>create-environment</code> endpoint, you can also specify the option <code>--option-settings</code> as shown in <a href=""http://docs.aws.amazon.com/cli/latest/reference/elasticbeanstalk/create-environment.html"" rel=""nofollow"">the documentation</a>.</p>

<p>All of the Option Settings can be found <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html"" rel=""nofollow"">on this page</a>. Specifically, look for the <code>aws:rds:dbinstance</code> section to see the RDS specific ones.</p>

<p>An example JSON config might look like this:</p>

<pre><code>[
    {
        ""Namespace"": ""aws:rds:dbinstance"",
        ""OptionName"": ""DBName"",
        ""Value"": ""my-database""
    },
    {
        ""Namespace"": ""aws:rds:dbinstance"",
        ""OptionName"": ""Engine"",
        ""Value"": ""mysql""
    },
    {
        ""Namespace"": ""aws:rds:dbinstance"",
        ""OptionName"": ""MasterUsername"",
        ""Value"": ""user""
    },
    {
        ""Namespace"": ""aws:rds:dbinstance"",
        ""OptionName"": ""MasterUserPassword"",
        ""Value"": ""hunter2""
    },
    {
        ""Namespace"": ""aws:rds:dbinstance"",
        ""OptionName"": ""DBInstanceClass"",
        ""Value"": ""db.m1.small""
    }
    // ...
]
</code></pre>
",3,CC BY-SA 3.0,25516491.0,2014-08-26T23:02:09.920,4.0,337.0,,2014-08-27T15:53:21.413,AWS cli elasticbeanstalk,<amazon-web-services><amazon-elastic-beanstalk><command-line-interface>,1.0,0.0,,,
41126062,26208134,0,"@Dagon The short echo tags `<?=` are generally excepted and enabled by default. However, any short open tag is discouraged.",2014-10-06T18:18:58.763,790224,CC BY-SA 3.0,True,1,tag,.,0,"However, any short open tag is discouraged.",False,False,26208134.0,26207738.0,2014-10-05T23:04:00.173,1.0,"<p>Thanks for the suggestion above, this is php version issue. only php 5.4 or higher version support short tags. it works after upgrading php version to 5.4.33</p>

<p>THanks</p>
",2,CC BY-SA 3.0,26207738.0,2014-10-05T22:10:41.707,0.0,57.0,,2014-10-05T23:04:00.173,EC2 php - Fail to echo variable,<php><amazon-web-services><amazon-ec2>,1.0,4.0,,0.0,
41164045,26186984,0,"This is what the document says: Maximum registered domains: 100

This limit includes both registered and deprecated domains.",2014-10-07T18:24:57.023,1664318,CC BY-SA 3.0,True,1,,.,0,This limit includes both registered and deprecated domains.,False,False,,,,,,,,,,,,,,,,,,,,
41374818,12762326,0,This answer is outdated.,2014-10-14T10:15:51.117,221315,CC BY-SA 3.0,True,1,answer,.,0,This answer is outdated.,False,False,12762326.0,11424481.0,2012-10-06T17:54:17.333,3.0,"<p>The load balancing between different availability zones is done via DNS. When a DNS resolver on the client asks for the IP address of the ELB, it gets two addresses. And chooses to use one of them (usually the first). The DNS server usually responds with a random order, so the first IP is not used at all times but each IP is used only part of the time (half for 2, third of the time for 3, etc ...).</p>

<p>Then behind these IP addresses you have an ELB server in each availability zone that has your instances connected to it. This is the reason why a zone with just a single instance will get the same amount of traffic as all the instances in another zone.</p>

<p>When you get to the point that you have a very large number of instances, ELB can decide to create two such servers in a <em>single</em> availability zone, but in this case it will split your instances for it to have half (or some other equal division) of your instances.</p>
",5,CC BY-SA 3.0,11424481.0,2012-07-11T01:53:46.903,10.0,12044.0,,2014-04-09T17:14:32.297,AWS Elastic Load Balancer and multiple availability zones,<amazon-ec2><amazon-web-services><load-balancing>,2.0,0.0,,2.0,
41555985,26455536,0,Are my amendments sufficient? You could go the other way and say that if I include usage/documentation of the script here that this could get outdated. I do see your point though.,2014-10-20T03:00:05.437,3938906,CC BY-SA 3.0,True,1,,.,0,You could go the other way and say that if I include usage/documentation of the script here that this could get outdated.,False,False,,,,,,,,,,,,,,,,,,,,
41684145,26498577,1,"Never mind, I see the difference now between get_only_instances and get_all_instances, seems the latter is deprecated.",2014-10-23T13:06:38.417,2356374,CC BY-SA 3.0,True,1,,.,1,"Never mind, I see the difference now between get_only_instances and get_all_instances, seems the latter is deprecated.",False,False,26498577.0,26493207.0,2014-10-22T00:29:49.643,2.0,"<p>Using boto, the first thing you'll need to do is call <a href=""https://boto.readthedocs.org/en/latest/ref/ec2.html"" rel=""nofollow"">connect_to_region</a> in order to connect to the AWS region you're interested in. (If you have instances in multiple regions then you'll need to iterate over each region one by one).</p>

<p>Once you've connected to a region you'll need to call <a href=""https://boto.readthedocs.org/en/latest/ref/ec2.html?highlight=get_all_instances#boto.ec2.connection.EC2Connection.get_only_instances"" rel=""nofollow"">get_only_instances</a> which will return a list of instance objects. Go through the list of those objects and look at the ip_address field for the instances public IP (or the private_ip_address field for the private one).</p>

<p>Then you'll want to call <a href=""https://boto.readthedocs.org/en/latest/ref/ec2.html?highlight=get_all_instances#boto.ec2.connection.EC2Connection.get_all_addresses"" rel=""nofollow"">get_all_addresses</a> to get a list of Elastic IP's.  Once again you'll need to loop through the list of EIP objects and look at the public_ip field in this case.  And if you want to determine which instance the EIP is associated with (if any) then the instance_id field will do that.</p>
",2,CC BY-SA 3.0,26493207.0,2014-10-21T18:00:05.263,1.0,932.0,,2014-10-22T00:29:49.643,"How to tell if my AWS account ""owns"" a given IP address in Python boto",<python><amazon-web-services><amazon-ec2><boto>,2.0,0.0,,,
41936003,25149025,0,As of Django 1.7 you can replace `django-admin.py syncdb --migrate --noinput` with `django-admin.py migrate --noinput` as syncdb has been deprecated.,2014-10-31T03:57:04.110,747548,CC BY-SA 3.0,True,1,syncdb,.,0,As of Django 1.7 you can replace `django-admin.py syncdb --migrate --noinput` with `django-admin.py migrate --noinput` as syncdb has been deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
42208996,26230395,1,"I've looked at that as well, but chatter on ""The Internet"" indicates that the S3 snapshot is, if not deprecated, then discouraged. (http://bit.ly/1wEHhuL) When asked what to use instead, the answer I've heard is ""use EBS"". So I guess that leaves us with n nodes with n attached EBS devices holding n snapshots of the state of each node in the cluster. I guess When the cluster goes down you spin up n new nodes and attach the matching EBS to each. Sounds like a fun maintenance exercise. Having said that I'm using the S3 snapshot anyway.  :)",2014-11-08T17:00:46.390,3223711,CC BY-SA 3.0,True,1,I,.,1,"I've looked at that as well, but chatter on ""The Internet"" indicates that the S3 snapshot is, if not deprecated, then discouraged.",False,False,26230395.0,25652709.0,2014-10-07T07:07:56.240,0.0,"<p>I think you are looking for this aws plugin for elasticsearch (I guess you already installed it to configure your cluster) : <a href=""https://github.com/elasticsearch/elasticsearch-cloud-aws#s3-repository"" rel=""nofollow"">https://github.com/elasticsearch/elasticsearch-cloud-aws#s3-repository</a></p>

<p>It will allow you to create a repository mapped to a S3 bucket.
To use (create/restore/whatever) a snapshot, you need to create a repository first. Then when you will do some actions on a snapshot, Elasticsearch will directly manage it on your S3 bucket.</p>
",2,CC BY-SA 3.0,25652709.0,2014-09-03T20:00:21.503,2.0,2396.0,2016-12-19T10:04:47.333,2016-12-19T10:04:47.333,Elasticsearch Snapshot Fails With RepositoryMissingException,<amazon-web-services><amazon-ec2><elasticsearch>,2.0,0.0,,,
42651934,27069121,0,Or maybe Redshift is still using the outdated [Posix escape syntax](http://stackoverflow.com/questions/12316953/insert-varchar-with-single-quotes-in-postgresql/12320729#12320729) for strings by default. Redshift is a weird mix of very old code and new additions. It's certainly not Postgres. Updated my check.,2014-11-21T20:14:06.113,939860,CC BY-SA 3.0,True,1,,.,0,Or maybe Redshift is still using the outdated [Posix escape syntax](http://stackoverflow.com/questions/12316953/insert-varchar-with-single-quotes-in-postgresql/12320729#12320729) for strings by default.,False,False,,,,,,,,,,,,,,,,,,,,
42920818,27194868,3,"HIgly not recommended use AsteriskNOW, in current moment it outdated and have alot of security bugs. You can install pbxinaflash.net on top of centos 6 AMI running on EC2. There are nothing special in that install, not require any inport/export etc.",2014-12-01T02:45:32.657,861388,CC BY-SA 3.0,True,1,,.,1,"HIgly not recommended use AsteriskNOW, in current moment it outdated and have alot of security bugs.",False,False,27194868.0,27190968.0,2014-11-28T19:25:45.830,2.0,"<p>AsteriskNow is a complete distribution based on CentOS available as an ISO file.  There doesn't appear to be an EC2 AMI available for it so you would have to build an image yourself.</p>

<p><a href=""http://www.rittmanmead.com/2014/09/obiee-sampleapp-in-the-cloud-importing-virtualbox-machines-to-aws-ec2/"" rel=""nofollow"">Here's an overview of the process for Oracle Linux</a> which boils down to:</p>

<ul>
<li>Install AsteriskNow onto a VirtualBox or VMWare instance locally.</li>
<li>Configure all the EC2 specifics (This is the fiddly bit)</li>
<li>Export that virtual machine as a VMDK.</li>
<li>Copy the VMDK to S3</li>
<li>Import the VMDK to an EBS volume and launch on Amazon EC2.  </li>
</ul>

<p>Before you export you will have to make sure AsteriskNow has a <a href=""https://aws.amazon.com/blogs/aws/use-your-own-kernel-with-amazon-ec2/"" rel=""nofollow"">kernel that supports EC2</a>. In CentOS this would be the Xen kernel but I don't know if Asterisk would supply one, which means compiling.  The <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/UserProvidedKernels.html"" rel=""nofollow"">PV-GRUB docco</a> also covers a lot of what can and can't be used on EC2. If it doesn't work out of the box it will take some Linux smarts to figure it all out. </p>

<p>It will probably take a number of exports/imports to get it running. Once you have it up on EC2 you can turn that instance into an AMI to quickly create clones in the future without going through the whole export/import process. </p>
",2,CC BY-SA 3.0,27190968.0,2014-11-28T14:38:34.803,1.0,1344.0,2017-11-14T14:08:21.860,2017-11-17T06:15:55.940,How I can install AsteriskNOW on Amazon EC2?,<amazon-web-services><amazon-ec2><server><asterisk><vps>,2.0,1.0,,,
43132197,27337525,0,"Thank you for your prompt response, and clarification. I wanted to avoid .NET as its a bit outdated.",2014-12-06T23:36:40.003,4332940,CC BY-SA 3.0,True,1,,.,0,as its a bit outdated.,False,False,27337525.0,27337358.0,2014-12-06T22:55:48.913,0.0,"<p>A scanner (the hardware) is connected to the computer of the client. The OS uses a driver to work with the scanner. For most scanners this would be a TWAIN driver:</p>

<p><a href=""http://www.twain.org"" rel=""nofollow"">http://www.twain.org</a></p>

<p>Software on the computer can connect to the API of this TWAIN driver and collect data from the scanner. </p>

<p>You want to access the scanner, more or less, through the browser of the client. This is only possible after the installation of some software on the computer of the client.</p>

<p>Have a look here:</p>

<p><a href=""http://www.codeproject.com/Articles/10518/NET-Web-Twain"" rel=""nofollow"">http://www.codeproject.com/Articles/10518/NET-Web-Twain</a></p>

<p>There are other option, just google a bit. </p>
",1,CC BY-SA 3.0,27337358.0,2014-12-06T22:33:00.597,-3.0,1082.0,2015-02-05T15:34:05.390,2015-02-05T15:34:05.390,Connect scanner to server,<javascript><php><html><amazon-web-services><scanning>,1.0,6.0,2014-12-07T06:00:48.540,1.0,
43887449,8676062,0,"@TimDorr can you edit the post, or post your answer if the above is now outdated?",2015-01-02T06:15:35.243,582917,CC BY-SA 3.0,True,1,,?,0,"can you edit the post, or post your answer if the above is now outdated?",False,False,,,,,,,,,,,,,,,,,,,,
43903071,13381890,0,"The problem isn't boto, django-storages is lacking proper python3 support, and is also using deprecated internal util functions in django.",2015-01-02T17:29:43.030,246603,CC BY-SA 3.0,True,1,,.,1,"The problem isn't boto, django-storages is lacking proper python3 support, and is also using deprecated internal util functions in django.",False,False,,,,,,,,,,,,,,,,,,,,
44218232,2102368,1,SimpleDB-Dev is out of date and apparently no longer maintained. The fakesb version mentioned in an answer below however does work.,2015-01-12T19:37:11.973,91144,CC BY-SA 3.0,True,1,Dev,.,1,SimpleDB-Dev is outdated and apparently no longer maintained.,False,False,2102368.0,2095546.0,2010-01-20T15:12:43.827,3.0,"<p>SimpleDB/dev runs on Windows, Linux and Mac.  The Wareseeker.com page that you found has copied the information across incorrectly.  The Google Code project page for the tool is here</p>

<p><a href=""http://code.google.com/p/simpledb-dev/"" rel=""nofollow noreferrer"">http://code.google.com/p/simpledb-dev/</a></p>
",4,CC BY-SA 2.5,2095546.0,2010-01-19T17:33:23.263,10.0,2846.0,2010-01-19T17:40:31.420,2013-11-13T16:50:39.313,Amazon SimpleDB for development environment / local installation,<windows><development-environment><local><amazon-simpledb>,5.0,0.0,,3.0,
44328452,27966051,0,"That's right, but the Module itself creates a dynamo-hash table. In it would create a range table it would be possible to get the outdated sessions by ""query"" instead by a ""scan"". So that would be much cheaper. So in my opinion it is a design error of the session module ""connect-dynamo""",2015-01-15T16:13:43.257,4456463,CC BY-SA 3.0,True,1,,.,0,"In it would create a range table it would be possible to get the outdated sessions by ""query"" instead by a ""scan"".",False,False,27966051.0,27962249.0,2015-01-15T14:48:15.510,0.0,"<p>I'm using node and dynamoDB allot and just looked inside the module connect-dynamo.  </p>

<p>The main problem with this modul is that it uses a table of type ""HASH"".
It should be a ""RANGE"" table with <code>expires</code> as range key.
Then it is possible to do a query instead of a scan, wtich is way cheaper.</p>

<p>So my advice is not to use this module ;-)</p>

<p>Or fork it and change it to RANGE table!</p>
",4,CC BY-SA 3.0,27962249.0,2015-01-15T11:20:06.470,0.0,317.0,2015-01-15T13:48:10.247,2015-01-15T14:48:15.510,dynamodb scan with delay (or limited provision) - nodejs,<node.js><amazon-dynamodb>,1.0,0.0,,,
44553470,28087153,0,"Maybe my answer is outdated, but able to test it as I'm not using elastic beanstalk anymore.",2015-01-22T12:06:23.857,2139452,CC BY-SA 3.0,True,1,answer,.,1,"Maybe my answer is outdated, but able to test it as I'm not using elastic beanstalk anymore.",False,False,28087153.0,21255525.0,2015-01-22T11:02:49.707,4.0,"<p>I did not find the scripts you have mentionned but find another way to reach the same information.</p>

<pre><code>cd /opt/elasticbeanstalk/bin
sudo ./get-config environment
</code></pre>

<p>Get config script allow you to get access to a lot of indormaftion on this categories :</p>

<pre><code>optionsettings                   environment option settings that affect instance
container                        container specific configurations
addons                           addon configurations
environment                      environment variables
meta                             EB environment meta-data
</code></pre>

<p>Hope it helps.</p>
",1,CC BY-SA 3.0,21255525.0,2014-01-21T10:31:56.547,13.0,2254.0,2014-01-21T10:44:22.063,2015-01-22T11:02:49.707,Elastic Beanstalk connect to RDS from shell SSH,<python><ssh><amazon-ec2><amazon-elastic-beanstalk><amazon-rds>,3.0,0.0,,7.0,
45002442,10636339,0,This answer is very out of date. You can call update stack or simply redeploy the app as needed. Details are here: http://docs.aws.amazon.com/AWSToolkitVS/latest/UserGuide/tkv-deployment-tool.html,2015-02-04T17:17:36.227,131818,CC BY-SA 3.0,True,1,answer,.,0,This answer is very outdated.,False,False,10636339.0,10575553.0,2012-05-17T13:08:10.320,6.0,"<p>As far as I understand it the quick answer is no. This is because the recommended process after launching a new instance is to change the local admin password from that set by AWS. Since the publish to cloud formation only has the AccessKey &amp; Secret Access Key it cannot get the new admin password &amp; as a result cannot connect to an existing instance.</p>

<p>The quick way around this would be to set the instance's security group to allow FTP from your public IP address and then just publish via FTP.</p>
",2,CC BY-SA 3.0,10575553.0,2012-05-13T21:53:33.440,8.0,3964.0,2012-05-13T21:58:37.567,2012-05-24T19:40:01.313,Can you publish to an existing Amazon EC2 instance using AWS Toolkit for Visual Studio?,<c#><asp.net-mvc-3><amazon-web-services>,2.0,0.0,,1.0,
45254620,22475496,0,Seems `context.getRouteDefinitions()` from the doc is deprecated. How to use `AdviceWith` now?,2015-02-11T21:05:12.810,428024,CC BY-SA 3.0,True,1,,.,0,Seems `context.getRouteDefinitions()` from the doc is deprecated.,False,False,22475496.0,22473789.0,2014-03-18T09:44:36.537,1.0,"<p>Read the testing documentation at</p>

<ul>
<li><a href=""http://camel.apache.org/testing"" rel=""nofollow"">http://camel.apache.org/testing</a></li>
</ul>

<p>And read about the advice with</p>

<ul>
<li><a href=""http://camel.apache.org/advicewith.html"" rel=""nofollow"">http://camel.apache.org/advicewith.html</a></li>
</ul>

<p>And see the replaceFromWith(uri) how you can replace the from with some other endpoint.</p>
",4,CC BY-SA 3.0,22473789.0,2014-03-18T08:21:23.697,0.0,789.0,,2014-03-18T09:44:36.537,apache camel - mock a remote endpoint on unit test,<amazon-s3><apache-camel><mockito>,1.0,0.0,,,
45255292,22475496,0,"Found solution here, http://camel.465427.n5.nabble.com/Intercepting-documentation-old-adviceWith-deprecated-in-favour-of-td5711952.html, but ""adviceWith"" is still deprecated.",2015-02-11T21:26:42.873,428024,CC BY-SA 3.0,True,1,,.,0,"Found solution here, http://camel.465427.n5.nabble.com/Intercepting-documentation-old-adviceWith-deprecated-in-favour-of-td5711952.html, but ""adviceWith"" is still deprecated.",False,False,22475496.0,22473789.0,2014-03-18T09:44:36.537,1.0,"<p>Read the testing documentation at</p>

<ul>
<li><a href=""http://camel.apache.org/testing"" rel=""nofollow"">http://camel.apache.org/testing</a></li>
</ul>

<p>And read about the advice with</p>

<ul>
<li><a href=""http://camel.apache.org/advicewith.html"" rel=""nofollow"">http://camel.apache.org/advicewith.html</a></li>
</ul>

<p>And see the replaceFromWith(uri) how you can replace the from with some other endpoint.</p>
",4,CC BY-SA 3.0,22473789.0,2014-03-18T08:21:23.697,0.0,789.0,,2014-03-18T09:44:36.537,apache camel - mock a remote endpoint on unit test,<amazon-s3><apache-camel><mockito>,1.0,0.0,,,
45393484,28532413,0,You can't create new non-VPC instances now as EC2 Classic is deprecated. If you don't launch in your custom VPC it will launch in the default VPC.,2015-02-16T10:56:57.690,user1832464,CC BY-SA 3.0,True,1,,.,1,You can't create new non-VPC instances now as EC2 Classic is deprecated.,False,False,28532413.0,28532199.0,2015-02-15T23:18:03.980,2.0,"<p>Remeber that when you create an instance, you specify the VPC that it will be launched in. It is not possible to change the VPC without terminating the instance and re-launching it in the new one. </p>

<p>One possible option would be to create an AMI of your currently running instance, and relaunch it in your preferred VPC using that AMI. </p>
",2,CC BY-SA 3.0,28532199.0,2015-02-15T22:51:34.910,5.0,2303.0,,2015-02-16T01:35:28.427,EC2 - Remove EC2 from VPC,<amazon-web-services><amazon-ec2><amazon-vpc>,2.0,0.0,,,
45457559,26276763,2,"I get a the following warning when running grub-install: `warning: Embedding is not possible.  GRUB can only be installed in this setup by using blocklists.  However, blocklists are UNRELIABLE and their use is discouraged..
/usr/sbin/grub-bios-setup: error: will not proceed with blocklists.`
Then, everything looks good but it doesn't boot or give me any clue, why it is not booting.",2015-02-18T00:52:33.053,227990,CC BY-SA 3.0,True,1,use,"
",0,"their use is discouraged..
",False,False,,,,,,,,,,,,,,,,,,,,
45489036,28437856,0,"Also note that, while the scan operation can do what OP is asking, it does use throughput for every item returned, and as such is very inefficient and discouraged, especially on larger tables.  Generally, scan is only recommended for backup purposes. (I'm sure this is mentioned in the documentation, but it can't be stressed enough).",2015-02-18T18:37:53.433,1848286,CC BY-SA 3.0,True,1,, ,0,"Also note that, while the scan operation can do what OP is asking, it does use throughput for every item returned, and as such is very inefficient and discouraged, especially on larger tables.  ",False,False,28437856.0,28425835.0,2015-02-10T17:20:43.153,0.0,"<p>In order to use the <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html"" rel=""nofollow""><code>Query</code></a> operation, you <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/QueryAndScan.html"" rel=""nofollow"">must provide a hash key.</a></p>

<blockquote>
  <p>You must provide a hash key attribute name and a distinct value to
  search for. You can optionally provide a range key attribute name and
  value, and use a comparison operator to refine the search results.</p>
</blockquote>

<p>If you do not have the hash key, you should look into the <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Scan.html"" rel=""nofollow""><code>Scan</code></a> operation. It evaluates every item in your table for your conditions, as well as <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/QueryAndScanGuidelines.html"" rel=""nofollow"">other considerations</a>.</p>
",1,CC BY-SA 3.0,28425835.0,2015-02-10T06:58:42.097,0.0,730.0,2015-02-10T07:35:51.970,2015-02-10T17:20:43.153,Querying Dynamo DB with out hash or Range Key using Java SDK,<java><amazon-web-services><amazon-dynamodb>,1.0,0.0,,,
45591066,21809223,1,"Link out of date. Best I have been able to come up with recently is [this](http://www.browsenodes.com/node--2000.html), the look of which does not inspire confidence.",2015-02-21T14:12:50.317,1599229,CC BY-SA 3.0,True,1,,.,0,Link outdated.,False,False,,,,,,,,,,,,,,,,,,,,
45602056,28570633,0,"Ok this is great. I realize I should first deal with successfully updating my outdated auth related AngularFire Firebase code. Once that is done I will revisit this but right off the bat after reading through this, I am trying to understand where Firebase auth comes into place here. Also I will have to deal with integrating Express into my current Angular app. So lots to do and lots of questions but if I need to I will ask new ones to stay organized. Thanks! :)",2015-02-21T23:09:06.853,user883807,CC BY-SA 3.0,True,1,,.,0,I realize I should first deal with successfully updating my outdated auth related AngularFire Firebase code.,False,False,28570633.0,26265823.0,2015-02-17T20:34:07.793,2.0,"<p>To connect AWS with an outside application, Cognito is going to be a good solution. It will let you generate an OpenID token using the AWS Node SDK and your secret keys in your backend, that you can then use with the AWS JavaScript SDK and <code>WebIdentityCredentials</code> in your client.</p>

<p>Note that I'm unfamiliar with your specific plugin/tool, but this much will at least get you the OpenID and in my work it does let me connect using <code>WebIdentityCredentials</code>, which I imagine is what they are using.</p>

<ol>
<li>Configure Cognito on AWS</li>
</ol>

<p>Setup on Cognito is fairly easy - it is more or less a walkthrough. It does involve configuring IAM rules on AWS, though. How to set this up is pretty project specific, so I think I need to point you to the official resources. <a href=""https://aws.amazon.com/blogs/aws/cognito-update-sync-store-access-improved-console-more/"" rel=""nofollow"">They recently made some nice updates</a>, but I am admittedly not up to speed on all the changes.</p>

<p>Through the configuration, you will want to setup a 'developer authenticated identity', take note of the 'identity pool id', and the IAM role ARN setup by Cognito.</p>

<ol start=""2"">
<li>Setup a Node Server that can handle incoming routes</li>
</ol>

<p>There are a lot of materials out there on how to accomplish this, but you want to be sure to include and configure the <a href=""http://aws.amazon.com/sdk-for-node-js/"" rel=""nofollow"">AWS SDK</a>. I also recommend using <a href=""https://github.com/expressjs/body-parser"" rel=""nofollow"">body-parser</a> as it will make reading in your POST requests easier.</p>

<pre><code>var app        = express();
var bodyParser = require('body-parser');
var AWS        = require('aws-sdk');
app.use(bodyParser.urlencoded({ extended: true }));
app.use(bodyParser.json());
</code></pre>

<ol start=""2"">
<li>Create POST Function to talk with Cognito</li>
</ol>

<p>Once you have your server setup, you then reach out to Cognito using <a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentity.html#getOpenIdTokenForDeveloperIdentity-property"" rel=""nofollow"">getOpenIdTokenForDeveloperIdentity</a>. In my setup, I use authenticated users because I expect them to come back and want to be able to continue the associations, so that is why I send in a UserID in <code>req.body.UserIDFromAngularApp</code>.</p>

<p>This is my function using <code>express.router()</code>.</p>

<pre><code>.post(function(req, res) {
   if(req.body.UserIDFromAngularApp) {
      var cognitoidentity = new AWS.CognitoIdentity();
      var params = {
         IdentityPoolId: 'your_cognito_identity_pool_id',
         Logins: {
            'your_developer_authenticated_identity_name': req.body.UserIDFromAngularApp
         }
      };
      cognitoidentity.getOpenIdTokenForDeveloperIdentity(params, function(err, data) {
         if (err) { console.log(err, err.stack); res.json({failure: 'Connection failure'}); }
         else {
            console.log(data); // so you can see your result server side
            res.json(data); // send it back
         }
      });
   }
   else { res.json({failure: 'Connection failure'}); }
});
</code></pre>

<p>If all goes well, that will return an OpenID Token back to you. You can then return that back to your Angular application.</p>

<ol start=""3"">
<li>POST from Angular, Collect from Promise</li>
</ol>

<p>At the very least you need to post to your new node server and then collect the OpenID token out of the promise. Using this pattern, that will be found in <code>data.Token</code>.</p>

<p>It sounds like from there you may just need to pass that token on to your plugin/tool.</p>

<p>In case you need to <a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/WebIdentityCredentials.html"" rel=""nofollow"">handle authentication further</a>, I have included code to handle the <code>WebIdentityCredentials</code>.</p>

<pre><code>angular.module('yourApp').factory('AWSmaker', ['$http', function($http) {
   return {
      reachCognito: function(authData) {
         $http.post('http://localhost:8888/simpleapi/aws', {
            'UserIDFromAngularApp': authData.uid,
         })
         .success(function(data, status, headers, config) {
            if(!data.failure) {
               var params = {
                  RoleArn: your_role_arn_setup_by_cognito,
                  WebIdentityToken: data.Token
               };
               AWS.config.credentials = new AWS.WebIdentityCredentials(params, function(err) {
                  console.log(err, err.stack);
               });
            }
         });
      }
}]);
</code></pre>

<p>This should get you on your way. Let me know if I can help further.</p>
",2,CC BY-SA 3.0,26265823.0,2014-10-08T20:27:34.797,7.0,1753.0,2015-02-18T21:05:05.087,2015-02-18T21:05:05.087,How to use Firebase's email & password authentication method to connect with AWS to make Fine Uploader S3 work?,<authentication><amazon-web-services><amazon-s3><fine-uploader><amazon-cognito>,2.0,8.0,,1.0,
45769217,28682878,0,"Ok, I thought so too.. the website is outdated. Thanks.",2015-02-26T13:05:36.023,1209290,CC BY-SA 3.0,True,1,website,.,0,the website is outdated.,False,False,,,,,,,,,,,,,,,,,,,,
45910483,20574036,1,This is actually outdated as of awsebcli 3.0. I added an answer detailing how it is done now.,2015-03-02T21:01:39.180,1543132,CC BY-SA 3.0,True,1,This,.,0,This is actually outdated as of awsebcli 3.0.,False,False,,,,,,,,,,,,,,,,,,,,
46355549,19247650,0,feedback would be appreciated regarding the downvote. let me know if my answer provides outdated / incorrect information.,2015-03-15T10:34:16.777,706436,CC BY-SA 3.0,True,1,,.,0,let me know if my answer provides outdated / incorrect information.,False,False,19247650.0,16488135.0,2013-10-08T12:26:03.603,-1.0,"<p>Based on the details in your question, it seems that you are trying to SSH into the RDS instance. This is not supported on RDS, it only works for EC2 based MySQL servers.</p>

<p>You should instead connect directly to the MySQL server, on port 3306 (or the one you set manually, if this is the case) and using the username and password you created when provisioning the RDS instance. This means using the Connection Type ""Standard TCP/IP"" in WB.</p>

<p>Also, the public IP address of the machine you're using MySQL Workbench on should be allowed in the RDS security group. </p>
",1,CC BY-SA 3.0,16488135.0,2013-05-10T17:55:59.403,2.0,11292.0,,2020-04-01T17:30:11.317,Unable to connect MySQL Workbench to RDS instance,<amazon-web-services><amazon-ec2><mysql-workbench><amazon-rds>,5.0,1.0,,1.0,
47690295,29662653,0,"Welcome to the club. If I may cite my own comment: ""full set of XSDs, which is rather hard to obtain"". I have an outdated set of XSDs I tampered with to remove some non-resolving includes. They now work. Well, kinda. I tried to get release 1.9 to work, but with no success yet.",2015-04-21T18:17:31.263,2097290,CC BY-SA 3.0,True,1,,XSDs,0,I have an outdated set of XSDs,False,False,29662653.0,29615231.0,2015-04-15T23:19:38.790,0.0,"<p><code>&lt;Scent&gt;</code> exists in other XSDs, such as <code>Beauty.xsd</code>. Why they put it into <code>CE.xsd</code> is beyond me, I guess it is a cut-and-paste error. When using two variants, such as <code>Size-Color</code>, you need to specify both <code>&lt;Size&gt;</code> and <code>&lt;Color&gt;</code>. There is no <code>&lt;Size-Color&gt;</code>. Check the documentation for parent/child relations on how to set up variants.</p>
",7,CC BY-SA 3.0,29615231.0,2015-04-13T21:14:20.073,0.0,470.0,,2015-04-15T23:19:38.790,Amazon XSD node definitions not found for VariationThemes,<xml><amazon-web-services><xsd><amazon-mws>,1.0,0.0,,,
48056129,29932557,0,I found https://realpython.com/blog/python/deploying-a-django-app-to-aws-elastic-beanstalk/ to be the best. It's still a little outdated as EB now supports python 3. Also when you get to the section about handling database migrations use http://stackoverflow.com/questions/29540131/django-aws-elastic-beanstalk-migrate-database,2015-04-30T12:54:10.553,2548934,CC BY-SA 3.0,True,1,It,EB,0,It's still a little outdated as EB,False,False,29932557.0,29932363.0,2015-04-29T00:58:35.387,0.0,"<p>If you are able to use a deployment service, take a look at at <a href=""http://aws.amazon.com/es/documentation/elastic-beanstalk/"" rel=""nofollow"">AWS Elastic Beanstalk</a>. It combines EC2, RDS and S3 storage into a Docker and helps keep them together. It's really easy to connect your RDS instance to your EC2 instance(s). I just launched a Django project using that a few weeks ago.</p>
",7,CC BY-SA 3.0,29932363.0,2015-04-29T00:36:33.367,9.0,8684.0,,2017-08-21T09:22:56.200,how to connect Django in EC2 to a Postgres database in RDS?,<django><amazon-web-services><amazon-ec2><amazon-rds>,2.0,1.0,,3.0,
48222399,29987284,2,"When you define a table specifically for the recent Notifications with higher  access demand you can define your provisioned throughput accordingly avoiding the hot partition. In the same way, you can decrease the provisioned throughput from older tables as the data become obsolete. The table is the unit of granularity for Provisioned Throughput, therefore, you have to make sure that the data that you put in the same table has the SAME throughput requirements.",2015-05-05T13:11:29.557,3183795,CC BY-SA 3.0,True,1,you,.,0,"In the same way, you can decrease the provisioned throughput from older tables as the data become obsolete.",False,True,,,,,,,,,,,,,,,,,,,,
48496807,19844272,1,"@k0pernikus RFC 2616 is now obsolete. Please, refer to RFC 7231. https://tools.ietf.org/html/rfc7231",2015-05-12T15:53:00.353,1202421,CC BY-SA 3.0,True,1,@k0pernikus,.,0,@k0pernikus RFC 2616 is now obsolete.,False,False,,,,,,,,,,,,,,,,,,,,
48718913,30310673,0,"Ahh, I gues it means AWS is using an outdated version then. AWS runs a composer update when my app is pushed from git. If they have an old version, it makes sense they repport an error with the new syntax (or is it called semantics?) I have run composer self-update 3 times today followed by composer update, so I'm sure I'm up to date.",2015-05-18T19:58:22.680,3668625,CC BY-SA 3.0,True,1,,.,0,"Ahh, I gues it means AWS is using an outdated version then.",False,False,30310673.0,30304703.0,2015-05-18T18:43:20.477,1.0,"<p>Update the Composer version you are using. The feature of using the <code>^</code> operator has been added in December 2014, so everyone should have gotten an updated copy of Composer by now</p>

<pre><code>composer self-update
</code></pre>

<p>This is the key to prevent incompatibility issues. Note that Composer is still in development, and there are some alpha releases. Using it means to also update it regularly.</p>
",3,CC BY-SA 3.0,30304703.0,2015-05-18T13:37:07.997,0.0,369.0,,2015-05-18T18:43:20.477,Deploying laravel 5 with AWS EB cli: UnexpectedValueException - Invalid version string,<amazon-web-services><deployment><composer-php><laravel-5>,2.0,0.0,,,
49267420,30545474,1,"Thanks for this detail, it helped me get running as well. One note I'll add: my last headache before smooth deploys was updating the RDS instance's security group to allow connections from the ec2 app itself. Found a useful (tho slightly outdated) guide here: http://blog.goforyt.com/laravel-aws-elastic-beanstalk-dev-guide/",2015-06-02T14:49:46.120,860787,CC BY-SA 3.0,True,1,,http://blog.goforyt.com/laravel-aws-elastic-beanstalk-dev-guide/,0,Found a useful (tho slightly outdated) guide here: http://blog.goforyt.com/laravel-aws-elastic-beanstalk-dev-guide/,False,False,,,,,,,,,,,,,,,,,,,,
49408399,30668965,0,"Are you suggesting installing the eb cli separately from the AWS cli? I thought that the specific cli's were being deprecated so you could now run all eb commands using ""aws elasticbeanstalk create-environment..."" etc.",2015-06-05T18:20:18.637,662694,CC BY-SA 3.0,True,1,,.,0,"I thought that the specific cli's were being deprecated so you could now run all eb commands using ""aws elasticbeanstalk create-environment..."" etc.",False,False,30668965.0,30666702.0,2015-06-05T14:22:17.690,0.0,"<p>Go to folder with Dockerrun.aws.json.</p>

<p>In <code>.elasticbeanstalk/config.yml</code> add</p>

<pre><code>global:
  default_platform: 64bit Amazon Linux 2015.03 v1.4.1 running Docker 1.6.0
</code></pre>

<p>Use the <em>eb</em> command: <code>eb init &amp;&amp; eb create</code></p>

<p><a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install.html"" rel=""nofollow"">How to install eb.</a></p>
",2,CC BY-SA 3.0,30666702.0,2015-06-05T12:34:03.600,0.0,264.0,,2018-05-23T14:53:18.597,ElasticBeanstalk with Docker: how to use create-environment from aws cli,<docker><amazon-elastic-beanstalk><aws-cli>,3.0,0.0,,,
49512651,18116741,1,"@DavidRobbins SimpleDB might have been deprecated by AWS, but it still there and fit my data modal perfectly. I didn't migrate my code yet, and not plan to do so. But for new products, I choose other databases like dynamodb or mysql.",2015-06-09T09:32:49.570,224458,CC BY-SA 3.0,True,1,SimpleDB,.,0,"@DavidRobbins SimpleDB might have been deprecated by AWS, but it still there and fit my data modal perfectly.",False,True,,,,,,,,,,,,,,,,,,,,
49626653,30771427,0,I've seen that example but `params[:file].original_filename` is deprecated and was giving me issues. Also `params[:file].read` returns a no method error.,2015-06-11T18:22:23.977,2270375,CC BY-SA 3.0,True,1,,.,0,`params[:file].original_filename` is deprecated and was giving me issues.,False,False,,,,,,,,,,,,,,,,,,,,
49864206,4921866,1,this answer is obsolete. eb cli v3 handles it out of the box.,2015-06-18T11:02:20.700,1802462,CC BY-SA 3.0,True,1,answer,.,0,this answer is obsolete.,False,False,,,,,,,,,,,,,,,,,,,,
49975605,30970415,0,"Had a facepalm moment - I forgot I have a cronjob that calls a third party API, that could very well be it! And as for mod_wsgi I installed it by getting the libapache2-mod-wsgi package, not from source, so maybe that is why it's so out of date...",2015-06-22T04:27:44.650,3762163,CC BY-SA 3.0,True,1,I,...,1,"And as for mod_wsgi I installed it by getting the libapache2-mod-wsgi package, not from source, so maybe that is why it's so outdated...",False,False,30970415.0,30968544.0,2015-06-21T23:43:22.200,2.0,"<p>Does your application call out to any backend services?</p>

<p>If you are getting 503/504 and that message, then it would generally indicate that your code is either hanging on backend services or that your code is blocking indefinitely on thread locks.</p>

<p>So basically all request threads become busy and get used up.</p>

<p>If they didn't provide such an ancient version of mod_wsgi then newer versions do at least have better options to combat this sort of problem in your application and recover automatically, plus log information to help you work out why.</p>

<p>For such an old version, you can set 'inactivity-timeout' option to WSGIDaemonProcess to '60' as a way of recovering, but this will also have the effect of restarting your application after 60 seconds if it wasn't receiving any requests as well, which may in itself not be ideal for some apps. In newer versions the inactivity timeout is separated from the concept of a request timeout.</p>
",1,CC BY-SA 3.0,30968544.0,2015-06-21T19:35:45.600,3.0,3027.0,,2015-06-21T23:43:22.200,"Trouble with Django and mod_wsgi: ""Script timed out before returning headers: wsgi.py""",<django><apache><amazon-ec2><mod-wsgi>,1.0,0.0,,,
50017649,30983433,0,In continuation or my previous comment - One option can be to use a reverse proxy like OpenIG. I am looking at OpenIG v2.1.0 docs Sec 6.5 [here](http://docs.forgerock.org/en/openig/2.1.0/gateway-guide/#chap-usecases). From the docs: _italic_The figure below illustrates the Federation Gateway as a standards based replacement for OpenAM policy agents._italic_. But there is no mention of this approach in latest version of docs. I am trying to understand why this approach is discouraged.,2015-06-23T05:05:34.697,1862396,CC BY-SA 3.0,True,1,,.,0,I am trying to understand why this approach is discouraged.,False,False,30983433.0,30973930.0,2015-06-22T15:05:23.697,0.0,"<p>You are effectively proposing writing your own agent, and embedding it into your project.  There is nothing wrong with this, just be aware of what you are doing.</p>

<p>I would start by taking apart one of the other <a href=""http://sources.forgerock.org/browse/openam/trunk/openam-agents"" rel=""nofollow"">OpenAM agents</a> and decide how much you can leverage.   If you can't leverage what's there, take a look at the <a href=""http://openam.forgerock.org/doc/bootstrap/dev-guide/#authentication-logic-sample-auth-module"" rel=""nofollow"">REST API</a>, where they have some decent examples.</p>
",2,CC BY-SA 3.0,30973930.0,2015-06-22T07:10:37.753,0.0,118.0,,2015-06-23T13:35:41.240,OpenAM Agentless Architecture options,<java><architecture><agent><openam><amazon-iam>,2.0,0.0,,,
50739463,7650095,0,"This code snipped is outdated, refer to my [answer](http://stackoverflow.com/questions/3142388/how-to-make-10-000-files-in-s3-public/31379500#31379500)",2015-07-13T10:22:25.277,2336662,CC BY-SA 3.0,True,1,,),0,"This code snipped is outdated, refer to my [answer](http://stackoverflow.com/questions/3142388/how-to-make-10-000-files-in-s3-public/31379500#31379500)",False,False,7650095.0,3142388.0,2011-10-04T15:12:13.127,34.0,"<p>I had to change several hundred thousand objects. I fired up an EC2 instance to run this, which makes it all go faster. You'll want to install the <code>aws-sdk</code> gem first.</p>

<p>Here's the code:</p>

<pre class=""lang-ruby prettyprint-override""><code>require 'rubygems'
require 'aws-sdk'


# Change this stuff.
AWS.config({
    :access_key_id =&gt; 'YOURS_HERE',
    :secret_access_key =&gt; 'YOURS_HERE',
})
bucket_name = 'YOUR_BUCKET_NAME'


s3 = AWS::S3.new()
bucket = s3.buckets[bucket_name]
bucket.objects.each do |object|
    puts object.key
    object.acl = :public_read
end
</code></pre>
",2,CC BY-SA 3.0,3142388.0,2010-06-29T15:49:54.340,92.0,52945.0,2014-03-21T16:09:27.523,2018-12-09T21:19:48.170,"How to make 10,000 files in S3 public",<amazon-s3><amazon-web-services>,9.0,1.0,,32.0,
50739463,7650095,0,"This code snipped is outdated, refer to my [answer](http://stackoverflow.com/questions/3142388/how-to-make-10-000-files-in-s3-public/31379500#31379500)",2015-07-13T10:22:25.277,2336662,CC BY-SA 3.0,True,1,,),0,"This code snipped is outdated, refer to my [answer](http://stackoverflow.com/questions/3142388/how-to-make-10-000-files-in-s3-public/31379500#31379500)",False,False,7650095.0,3142388.0,2011-10-04T15:12:13.127,34.0,"<p>I had to change several hundred thousand objects. I fired up an EC2 instance to run this, which makes it all go faster. You'll want to install the <code>aws-sdk</code> gem first.</p>

<p>Here's the code:</p>

<pre class=""lang-ruby prettyprint-override""><code>require 'rubygems'
require 'aws-sdk'


# Change this stuff.
AWS.config({
    :access_key_id =&gt; 'YOURS_HERE',
    :secret_access_key =&gt; 'YOURS_HERE',
})
bucket_name = 'YOUR_BUCKET_NAME'


s3 = AWS::S3.new()
bucket = s3.buckets[bucket_name]
bucket.objects.each do |object|
    puts object.key
    object.acl = :public_read
end
</code></pre>
",2,CC BY-SA 3.0,3142388.0,2010-06-29T15:49:54.340,92.0,52945.0,2014-03-21T16:09:27.523,2018-12-09T21:19:48.170,"How to make 10,000 files in S3 public",<amazon-s3><amazon-web-services>,9.0,1.0,,32.0,
50817672,31408584,0,"That is not what I meant. We are migrating the whole account. We were using the old deprecated T1 instances and we are getting rid of them. In order to be able to create the new T2 instances, we have to migrate the account first, this should not affect the S3 storage since it mainly concerns EC2, nobody guaranteed to us, however, that something bad couldnt happen... And by ""migrate"", I mean transfer the whole aws account into another account (which supports T2 generation)",2015-07-15T06:30:18.743,1262024,CC BY-SA 3.0,True,1,,.,0,We were using the old deprecated T1 instances and we are getting rid of them.,False,False,,,,,,,,,,,,,,,,,,,,
51096134,28819348,2,"You said in a comment above that the old .config way is outdated; but all the documentation still refers to that as the standard way to alter the environment. In fact, in my case the WSGIPath appears to be getting set by the saved configuration I use, but it doesn't appear at all in the local file I see when I use `eb config`.",2015-07-22T18:01:12.127,28875,CC BY-SA 3.0,True,1,,.,0,You said in a comment above that the old .config way is outdated; but all the documentation still refers to that as the standard way to alter the environment.,False,False,28819348.0,20558747.0,2015-03-02T21:00:39.313,5.0,"<p>As of awsebcli 3.0, you can actually edit your configuration settings to represent your <code>WSGI</code> path via <code>eb config</code>. The <code>config</code> command will then pull (and open it in your default command line text editor, i.e nano) an editable config based on your current configuration settings. You'll then search for <code>WSGI</code> and update it's path that way. After saving the file and exiting, your <code>WSGI</code> path will be updated automatically.       </p>
",5,CC BY-SA 3.0,20558747.0,2013-12-13T03:49:31.327,15.0,17170.0,2013-12-13T04:09:12.643,2019-05-20T17:23:10.220,How to deploy structured Flask app on AWS elastic beanstalk,<python><amazon-web-services><flask><amazon-elastic-beanstalk>,7.0,0.0,,12.0,
51111506,31564194,0,"**Nynuzoud** still appreciate the reply! I tried many times on the sample project provided by Amazon [link](https://github.com/awslabs/aws-sdk-android-samples) but their project are all outdated and I cant even run it on the latest android studio. This is kind of frustrating, I am pretty sure I follow all their steps. ( I tried Cognito Sync and it actually work, so this must be a dynamoDB problem...)",2015-07-23T05:27:09.660,3961656,CC BY-SA 3.0,True,1,,outdated,0,I tried many times on the sample project provided by Amazon [link](https://github.com/awslabs/aws-sdk-android-samples) but their project are all outdated,False,False,,,,,,,,,,,,,,,,,,,,
51316917,9605808,0,Also out of date on the comparison link: DynamoDB supports items of up to 400 KB (up from 64 KB) https://aws.amazon.com/blogs/aws/dynamodb-update-json-and-more/,2015-07-28T21:40:23.383,2601671,CC BY-SA 3.0,True,1,,:,0,Also outdated on the comparison link:,False,False,,,,,,,,,,,,,,,,,,,,
51455174,31760931,0,direct_to_template seems to be deprecated.,2015-08-01T11:28:57.617,938884,CC BY-SA 3.0,True,1,,.,0,direct_to_template seems to be deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
51500126,27928360,1,"I have never gone through that issue. Changing the SSL factory targets the case in where you are using HttpUrlConnection to connect to your server - the factory will create HttpUrlConnection objects that will not used the obsolete SSLLv3 protocol. However, I don't know if that will work for a webview. I don't know which HTTP client does the webview use under the hood.",2015-08-03T11:09:51.863,789110,CC BY-SA 3.0,True,1,,.,1,Changing the SSL factory targets the case in where you are using HttpUrlConnection to connect to your server - the factory will create HttpUrlConnection objects that will not used the obsolete SSLLv3 protocol.,False,True,27928360.0,26633349.0,2015-01-13T17:51:28.347,18.0,"<p>I think I have solved this. The fundamental idea is the same than in the code in the question (avoid SSLv3 as the only protocol available), but the code performing it is different:</p>

<pre><code>import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.lang.reflect.InvocationTargetException;
import java.lang.reflect.Method;
import java.net.InetAddress;
import java.net.Socket;
import java.net.SocketAddress;
import java.net.SocketException;
import java.nio.channels.SocketChannel;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import javax.net.ssl.HandshakeCompletedListener;
import javax.net.ssl.HttpsURLConnection;
import javax.net.ssl.SSLParameters;
import javax.net.ssl.SSLSession;
import javax.net.ssl.SSLSocket;
import javax.net.ssl.SSLSocketFactory;

    /**
     * {@link javax.net.ssl.SSLSocketFactory} that doesn't allow {@code SSLv3} only connections
     * &lt;p&gt;fixes https://github.com/koush/ion/issues/386&lt;/p&gt;
     *
     * &lt;p&gt; see https://code.google.com/p/android/issues/detail?id=78187 &lt;/p&gt;
     */
    public class NoSSLv3Factory extends SSLSocketFactory {
        private final SSLSocketFactory delegate;

        public NoSSLv3Factory() {
            this.delegate = HttpsURLConnection.getDefaultSSLSocketFactory();
        }

        @Override
        public String[] getDefaultCipherSuites() {
            return delegate.getDefaultCipherSuites();
        }

        @Override
        public String[] getSupportedCipherSuites() {
            return delegate.getSupportedCipherSuites();
        }

        private static Socket makeSocketSafe(Socket socket) {
            if (socket instanceof SSLSocket) {
                socket = new NoSSLv3SSLSocket((SSLSocket) socket);
            }
            return socket;
        }

        @Override
        public Socket createSocket(Socket s, String host, int port, boolean autoClose) throws IOException {
            return makeSocketSafe(delegate.createSocket(s, host, port, autoClose));
        }

        @Override
        public Socket createSocket(String host, int port) throws IOException {
            return makeSocketSafe(delegate.createSocket(host, port));
        }

        @Override
        public Socket createSocket(String host, int port, InetAddress localHost, int localPort) throws IOException {
            return makeSocketSafe(delegate.createSocket(host, port, localHost, localPort));
        }

        @Override
        public Socket createSocket(InetAddress host, int port) throws IOException {
            return makeSocketSafe(delegate.createSocket(host, port));
        }

        @Override
        public Socket createSocket(InetAddress address, int port, InetAddress localAddress, int localPort) throws IOException {
            return makeSocketSafe(delegate.createSocket(address, port, localAddress, localPort));
        }

        /**
         * Created by robUx4 on 25/10/2014.
         */
        private static class DelegateSSLSocket extends SSLSocket {

            protected final SSLSocket delegate;

            DelegateSSLSocket(SSLSocket delegate) {
                this.delegate = delegate;
            }

            @Override
            public String[] getSupportedCipherSuites() {
                return delegate.getSupportedCipherSuites();
            }

            @Override
            public String[] getEnabledCipherSuites() {
                return delegate.getEnabledCipherSuites();
            }

            @Override
            public void setEnabledCipherSuites(String[] suites) {
                delegate.setEnabledCipherSuites(suites);
            }

            @Override
            public String[] getSupportedProtocols() {
                return delegate.getSupportedProtocols();
            }

            @Override
            public String[] getEnabledProtocols() {
                return delegate.getEnabledProtocols();
            }

            @Override
            public void setEnabledProtocols(String[] protocols) {
                delegate.setEnabledProtocols(protocols);
            }

            @Override
            public SSLSession getSession() {
                return delegate.getSession();
            }

            @Override
            public void addHandshakeCompletedListener(HandshakeCompletedListener listener) {
                delegate.addHandshakeCompletedListener(listener);
            }

            @Override
            public void removeHandshakeCompletedListener(HandshakeCompletedListener listener) {
                delegate.removeHandshakeCompletedListener(listener);
            }

            @Override
            public void startHandshake() throws IOException {
                delegate.startHandshake();
            }

            @Override
            public void setUseClientMode(boolean mode) {
                delegate.setUseClientMode(mode);
            }

            @Override
            public boolean getUseClientMode() {
                return delegate.getUseClientMode();
            }

            @Override
            public void setNeedClientAuth(boolean need) {
                delegate.setNeedClientAuth(need);
            }

            @Override
            public void setWantClientAuth(boolean want) {
                delegate.setWantClientAuth(want);
            }

            @Override
            public boolean getNeedClientAuth() {
                return delegate.getNeedClientAuth();
            }

            @Override
            public boolean getWantClientAuth() {
                return delegate.getWantClientAuth();
            }

            @Override
            public void setEnableSessionCreation(boolean flag) {
                delegate.setEnableSessionCreation(flag);
            }

            @Override
            public boolean getEnableSessionCreation() {
                return delegate.getEnableSessionCreation();
            }

            @Override
            public void bind(SocketAddress localAddr) throws IOException {
                delegate.bind(localAddr);
            }

            @Override
            public synchronized void close() throws IOException {
                delegate.close();
            }

            @Override
            public void connect(SocketAddress remoteAddr) throws IOException {
                delegate.connect(remoteAddr);
            }

            @Override
            public void connect(SocketAddress remoteAddr, int timeout) throws IOException {
                delegate.connect(remoteAddr, timeout);
            }

            @Override
            public SocketChannel getChannel() {
                return delegate.getChannel();
            }

            @Override
            public InetAddress getInetAddress() {
                return delegate.getInetAddress();
            }

            @Override
            public InputStream getInputStream() throws IOException {
                return delegate.getInputStream();
            }

            @Override
            public boolean getKeepAlive() throws SocketException {
                return delegate.getKeepAlive();
            }

            @Override
            public InetAddress getLocalAddress() {
                return delegate.getLocalAddress();
            }

            @Override
            public int getLocalPort() {
                return delegate.getLocalPort();
            }

            @Override
            public SocketAddress getLocalSocketAddress() {
                return delegate.getLocalSocketAddress();
            }

            @Override
            public boolean getOOBInline() throws SocketException {
                return delegate.getOOBInline();
            }

            @Override
            public OutputStream getOutputStream() throws IOException {
                return delegate.getOutputStream();
            }

            @Override
            public int getPort() {
                return delegate.getPort();
            }

            @Override
            public synchronized int getReceiveBufferSize() throws SocketException {
                return delegate.getReceiveBufferSize();
            }

            @Override
            public SocketAddress getRemoteSocketAddress() {
                return delegate.getRemoteSocketAddress();
            }

            @Override
            public boolean getReuseAddress() throws SocketException {
                return delegate.getReuseAddress();
            }

            @Override
            public synchronized int getSendBufferSize() throws SocketException {
                return delegate.getSendBufferSize();
            }

            @Override
            public int getSoLinger() throws SocketException {
                return delegate.getSoLinger();
            }

            @Override
            public synchronized int getSoTimeout() throws SocketException {
                return delegate.getSoTimeout();
            }

            @Override
            public boolean getTcpNoDelay() throws SocketException {
                return delegate.getTcpNoDelay();
            }

            @Override
            public int getTrafficClass() throws SocketException {
                return delegate.getTrafficClass();
            }

            @Override
            public boolean isBound() {
                return delegate.isBound();
            }

            @Override
            public boolean isClosed() {
                return delegate.isClosed();
            }

            @Override
            public boolean isConnected() {
                return delegate.isConnected();
            }

            @Override
            public boolean isInputShutdown() {
                return delegate.isInputShutdown();
            }

            @Override
            public boolean isOutputShutdown() {
                return delegate.isOutputShutdown();
            }

            @Override
            public void sendUrgentData(int value) throws IOException {
                delegate.sendUrgentData(value);
            }

            @Override
            public void setKeepAlive(boolean keepAlive) throws SocketException {
                delegate.setKeepAlive(keepAlive);
            }

            @Override
            public void setOOBInline(boolean oobinline) throws SocketException {
                delegate.setOOBInline(oobinline);
            }

            @Override
            public void setPerformancePreferences(int connectionTime, int latency, int bandwidth) {
                delegate.setPerformancePreferences(connectionTime, latency, bandwidth);
            }

            @Override
            public synchronized void setReceiveBufferSize(int size) throws SocketException {
                delegate.setReceiveBufferSize(size);
            }

            @Override
            public void setReuseAddress(boolean reuse) throws SocketException {
                delegate.setReuseAddress(reuse);
            }

            @Override
            public synchronized void setSendBufferSize(int size) throws SocketException {
                delegate.setSendBufferSize(size);
            }

            @Override
            public void setSoLinger(boolean on, int timeout) throws SocketException {
                delegate.setSoLinger(on, timeout);
            }

            @Override
            public synchronized void setSoTimeout(int timeout) throws SocketException {
                delegate.setSoTimeout(timeout);
            }

            @Override
            public void setSSLParameters(SSLParameters p) {
                delegate.setSSLParameters(p);
            }

            @Override
            public void setTcpNoDelay(boolean on) throws SocketException {
                delegate.setTcpNoDelay(on);
            }

            @Override
            public void setTrafficClass(int value) throws SocketException {
                delegate.setTrafficClass(value);
            }

            @Override
            public void shutdownInput() throws IOException {
                delegate.shutdownInput();
            }

            @Override
            public void shutdownOutput() throws IOException {
                delegate.shutdownOutput();
            }

            @Override
            public String toString() {
                return delegate.toString();
            }

            @Override
            public boolean equals(Object o) {
                return delegate.equals(o);
            }
        }

        /**
         * An {@link javax.net.ssl.SSLSocket} that doesn't allow {@code SSLv3} only connections
         * &lt;p&gt;fixes https://github.com/koush/ion/issues/386&lt;/p&gt;
         */
        private static class NoSSLv3SSLSocket extends DelegateSSLSocket {

            private NoSSLv3SSLSocket(SSLSocket delegate) {
                super(delegate);

                String canonicalName = delegate.getClass().getCanonicalName();
                if (!canonicalName.equals(""org.apache.harmony.xnet.provider.jsse.OpenSSLSocketImpl"")) {
                    // try replicate the code from HttpConnection.setupSecureSocket()
                    try {
                        Method msetUseSessionTickets = delegate.getClass().getMethod(""setUseSessionTickets"", boolean.class);
                        if (null != msetUseSessionTickets) {
                            msetUseSessionTickets.invoke(delegate, true);
                        }
                    } catch (NoSuchMethodException ignored) {
                    } catch (InvocationTargetException ignored) {
                    } catch (IllegalAccessException ignored) {
                    }
                }
            }

            @Override
            public void setEnabledProtocols(String[] protocols) {
                if (protocols != null &amp;&amp; protocols.length == 1 &amp;&amp; ""SSLv3"".equals(protocols[0])) {
                    // no way jose
                    // see issue https://code.google.com/p/android/issues/detail?id=78187
                    List&lt;String&gt; enabledProtocols = new ArrayList&lt;String&gt;(Arrays.asList(delegate.getEnabledProtocols()));
                    if (enabledProtocols.size() &gt; 1) {
                        enabledProtocols.remove(""SSLv3"");
                    }
                    protocols = enabledProtocols.toArray(new String[enabledProtocols.size()]);
                }
                super.setEnabledProtocols(protocols);
            }
        }

    }
</code></pre>

<p>and somewhere in your code, before creating the Connection:</p>

<pre><code>    static {
    HttpsURLConnection.setDefaultSSLSocketFactory(new NoSSLv3Factory());
}
</code></pre>

<p>This code is taken from <a href=""https://code.google.com/p/android/issues/detail?id=78187"">https://code.google.com/p/android/issues/detail?id=78187</a>, where you can find a fully explanation on why this is happening in Android 4.X.</p>

<p>I've had this in production from one week and seems to have done the trick.</p>
",8,CC BY-SA 3.0,26633349.0,2014-10-29T14:44:27.807,16.0,26579.0,,2018-07-11T12:33:21.323,Disable SSL as a protocol in HttpsURLConnection,<android><ssl><amazon-web-services><poodle-attack><sslsocketfactory>,6.0,6.0,,14.0,
51563041,31781192,2,Please note that v1 of the SDK is deprecated.,2015-08-04T20:44:52.730,1572233,CC BY-SA 3.0,True,1,,.,0,Please note that v1 of the SDK is deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
51563663,31781192,0,"If it is deprecated, it would still be good information to have.  Based on the existing answer, it appeared that some couple users expected it to work with v2 or v3.  I think the edit improves the post by making it more specific and does not go against author's intention.  While, downvoting is an option, I don't think it is warranted as it does not meet the criteria of being an ""egregiously sloppy, no-effort-expended post, or an answer that is clearly and perhaps dangerously incorrect.""",2015-08-04T21:05:24.043,1059070,CC BY-SA 3.0,True,1,, ,0,"If it is deprecated, it would still be good information to have.  ",False,False,,,,,,,,,,,,,,,,,,,,
51760611,31923716,0,"Note that [link-only answers](http://meta.stackoverflow.com/tags/link-only-answers/info) are discouraged, SO answers should be the end-point of a search for a solution (vs. yet another stopover of references, which  tend to get stale over time). Please consider adding a stand-alone synopsis here, keeping the link as a reference.",2015-08-10T15:55:53.243,203657,CC BY-SA 3.0,True,1,,.,0,"Note that [link-only answers](http://meta.stackoverflow.com/tags/link-only-answers/info) are discouraged, SO answers should be the end-point of a search for a solution (vs. yet another stopover of references, which  tend to get stale over time).",False,False,,,,,,,,,,,,,,,,,,,,
51787012,31929014,0,"Thanks @shacarsol, but why do you use both amazon-webservices-bundle and aws-sdk-php? It seems that the first one is deprecated. And which adapter do you use? Because on the KnpGaufretteBundle doc there are two (AwsS3 and Amazon S3) https://github.com/KnpLabs/KnpGaufretteBundle",2015-08-11T09:43:36.260,3611204,CC BY-SA 3.0,True,1,,.,0,It seems that the first one is deprecated.,False,False,31929014.0,31925231.0,2015-08-10T20:56:50.137,0.0,"<p>I'm using this set of bundles in latest symfony and it works great:</p>

<pre><code>    ""knplabs/gaufrette"":                    ""dev-master"",
    ""knplabs/knp-gaufrette-bundle"":         ""dev-master"",
    ""amazonwebservices/aws-sdk-for-php"": ""dev-master"",
    ""cybernox/amazon-webservices-bundle"": ""dev-master"",
    ""aws/aws-sdk-php"": ""~2.7@dev""
</code></pre>
",2,CC BY-SA 3.0,31925231.0,2015-08-10T17:02:46.067,0.0,136.0,,2015-08-10T20:56:50.137,What is the difference difference between AwsS3 and Amazon S3 (amazon_s3) for KnpGaugretteBundle?,<symfony><amazon-web-services><amazon-s3>,1.0,0.0,,,
51993304,1693376,1,"I'm guessing you're right, as the API still exists! I wonder why I thought this... I'm sure I received an email saying this. Or maybe it was only deprecated for certain usages.",2015-08-17T09:21:53.950,28190,CC BY-SA 3.0,True,1,it,.,0,Or maybe it was only deprecated for certain usages.,False,False,,,,,,,,,,,,,,,,,,,,
52134394,9461956,3,"Although this is set as the right/selected answer, it should be noted that https://github.com/SaltwaterC/aws2js has been deprecated. Upon npm install it informs one that ""aws2js is deprecated. Please use aws-sdk.""",2015-08-20T14:31:16.190,3407994,CC BY-SA 3.0,True,2,,.,0,"Although this is set as the right/selected answer, it should be noted that https://github.com/SaltwaterC/aws2js has been deprecated.",False,False,,,,,,,,,,,,,,,,,,,,
52289601,29359050,1,Nice answer. Its worth noting that AttributeUpdates has been deprecated now so UpdateExpression is definitely the way to go. http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateItem.html#DDB-UpdateItem-request-AttributeUpdates,2015-08-25T11:27:25.110,590974,CC BY-SA 3.0,True,1,,.,0,Its worth noting that AttributeUpdates has been deprecated now so UpdateExpression is definitely the way to go.,False,False,29359050.0,29275219.0,2015-03-31T01:22:20.837,5.0,"<p>Looks like the for format of the request was missing the 'Action' and 'Value' parameters. E.g. the following is working for me:</p>

<pre><code>$response = $client-&gt;updateItem(array(
    ""TableName"" =&gt; ""PlayerInfo"",
    ""Key"" =&gt; array(
        ""PlayerId"" =&gt; array('N' =&gt; '201503261435580358849074082'),
    ),
    ""ReturnValues"" =&gt; \Aws\DynamoDb\Enum\ReturnValue::ALL_NEW,

    ""AttributeUpdates"" =&gt; array(
        'PlayerPrice' =&gt; array(
            'Action' =&gt; \Aws\DynamoDb\Enum\AttributeAction::PUT,
            'Value' =&gt; array('N' =&gt; '5'),
        )
    )
));
print_r($response);
</code></pre>

<p>You can also use an <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.Modifying.html"" rel=""noreferrer"">UpdateExpression</a> to achieve the same effect (UpdateExpressions also provide greater flexibility than AttributeUpdates so they are generally recommended):</p>

<pre><code>$response = $client-&gt;updateItem(array(
    ""TableName"" =&gt; ""PlayerInfo"",
    ""Key"" =&gt; array(
        ""PlayerId"" =&gt; array('N' =&gt; '201503261435580358849074082'),
    ),
    ""ReturnValues"" =&gt; \Aws\DynamoDb\Enum\ReturnValue::ALL_NEW,

    ""UpdateExpression"" =&gt; ""SET #pp = :val"",
    ""ExpressionAttributeNames"" =&gt; array(
        ""#pp"" =&gt; ""PlayerPrice"",
    ),
    ""ExpressionAttributeValues"" =&gt; array(
        ':val' =&gt; array('N' =&gt; '5')
    )
));
print_r($response);
</code></pre>
",1,CC BY-SA 3.0,29275219.0,2015-03-26T09:43:11.977,5.0,7950.0,2015-03-26T10:03:21.430,2015-03-31T01:22:20.837,Error: One or more parameter values were invalid--DynamoDb,<amazon-dynamodb>,1.0,0.0,,1.0,
52704936,32419384,0,"Thanks for the link. I've already seen it before, but it seemed a bit outdated and it seems like Watir is run in the ""headless"" setting. I am looking for something where I can log in to the server and Watir run on the actual browser.",2015-09-06T02:35:48.410,1005396,CC BY-SA 3.0,True,1,,.,0,"I've already seen it before, but it seemed a bit outdated and it seems like Watir is run in the ""headless"" setting.",False,False,32419384.0,32417036.0,2015-09-06T02:14:26.857,0.0,"<p>Its definitely possible. </p>

<p><a href=""http://watirmelon.com/2011/08/29/running-your-watir-webdriver-tests-in-the-cloud-for-free/"" rel=""nofollow"">http://watirmelon.com/2011/08/29/running-your-watir-webdriver-tests-in-the-cloud-for-free/</a></p>

<p>The link is a few years old, but should at least be a good starting point.</p>
",2,CC BY-SA 3.0,32417036.0,2015-09-05T19:45:32.907,0.0,326.0,,2015-09-06T02:14:26.857,Running watir-webdriver on a server,<amazon-web-services><automated-tests><watir><watir-webdriver><digital-ocean>,1.0,0.0,,,
52987664,32514746,0,"Solved it, see my answer [here](http://stackoverflow.com/a/32566091/949328). I'm going to accept my answer but award you the bounty for the insights/references/effort; really helped. But some of your suggestions would be misleading to someone facing this issue in the future so I'd rather not highlight your answer in general: (1) having enqueued jobs does not confirm a deleted directory theory (2) retries are not related to this issue and should not be discouraged (3) switching deployment scripts (capistrano gems) sidesteps the issue. But those aside, thanks again!",2015-09-14T13:40:46.627,949328,CC BY-SA 3.0,True,1,,.,1,(1) having enqueued jobs does not confirm a deleted directory theory (2) retries are not related to this issue and should not be discouraged (3) switching deployment scripts (capistrano gems) sidesteps the issue.,False,False,,,,,,,,,,,,,,,,,,,,
53041071,29831330,1,"Everyone links to this page, but it has no ""Access Keys"" section. I think this answer might be out of date.",2015-09-15T19:06:25.217,773754,CC BY-SA 3.0,True,1,,.,0,I think this answer might be outdated.,False,False,,,,,,,,,,,,,,,,,,,,
53174703,27350635,22,I got node 0.10.36 and npm 1.3.6 out of this. These are very out of date.,2015-09-19T04:44:47.287,1048668,CC BY-SA 3.0,True,1,These,.,0,These are very outdated.,False,False,,,,,,,,,,,,,,,,,,,,
53534499,27866359,0,Unfortunately that project seems to be abandoned. The information is out of date and pull requests containing updates are sitting there from last year... It would be a good starting point for a fork anyhow.,2015-09-29T17:36:23.747,1407473,CC BY-SA 3.0,True,1,information,...,0,The information is outdated and pull requests containing updates are sitting there from last year...,False,False,27866359.0,27863460.0,2015-01-09T17:54:24.250,5.0,"<p>Not officially. But the incredible Mitch Garnaat <a href=""https://github.com/garnaat/missingcloud"">has a Github repository with ""missingcloud"" bits</a>. On that list is instance information. You can pick that out with your favorite language. Here's an example with a bit of <code>jq</code>. (this is imperfect, maybe someone can help split these into instance:ramMB rows?)</p>

<pre><code>$ curl --silent https://raw.githubusercontent.com/garnaat/missingcloud/master/aws.json | jq '[.services.""Elastic Compute Cloud"".instance_types|to_entries|.[]|.key,.value.ramMB]' | head -9
[
  ""c1.medium"",
  1700,
  ""c1.xlarge"",
  7000,
  ""c3.2xlarge"",
  15000,
  ""c3.4xlarge"",
  30000,
</code></pre>
",1,CC BY-SA 3.0,27863460.0,2015-01-09T15:05:23.977,3.0,698.0,,2015-01-09T17:54:24.250,Query EC2 instance type attributes,<amazon-web-services><amazon-ec2><ec2-api-tools>,1.0,2.0,,2.0,
53662246,31778418,1,"Oh, and here is deprecation conflict in your solution: 
          |SPARK_CLASSPATH was detected (set to '$value').
          |This is deprecated in Spark 1.0+.
          |
          |Please instead use:
          | - ./spark-submit with --driver-class-path to augment the driver classpath
          | - spark.executor.extraClassPath to augment the executor classpath",2015-10-02T23:29:44.940,1732418,CC BY-SA 3.0,True,1,|This,1.0,0,|This is deprecated in Spark 1.0,False,False,31778418.0,30385981.0,2015-08-03T01:56:37.443,4.0,"<p>Using Spark 1.4.1 pre-built with Hadoop 2.6, I am able to get s3a:// to work when deploying to a Spark Standalone cluster by adding the hadoop-aws and aws-java-sdk jar files from the Hadoop 2.7.1 distro (found under $HADOOP_HOME/share/hadoop/tools/lib of Hadoop 2.7.1) to my SPARK_CLASSPATH environment variable in my $SPARK_HOME/conf/spark-env.sh file.</p>
",4,CC BY-SA 3.0,30385981.0,2015-05-21T23:24:05.323,53.0,70659.0,2016-11-30T09:27:08.267,2020-08-17T07:45:20.970,How to access s3a:// files from Apache Spark?,<hadoop><apache-spark><amazon-s3>,10.0,3.0,,28.0,
53878469,33027492,0,"I added the actual algorithm in a step by step list.  I wrote the python implementation wading through posts on how to do it all day, most of them full of incorrect or outdated information.",2015-10-09T00:35:18.613,5425358,CC BY-SA 3.0,True,1,,.,0,"I wrote the python implementation wading through posts on how to do it all day, most of them full of incorrect or outdated information.",False,False,,,,,,,,,,,,,,,,,,,,
54106780,33147692,0,"That solved my problem of decision task-- Thanks a lot. I still don't know why the decision task is not able start simple-activity. When I check in the AWS console I get this ""ScheduleActivityTaskFailed"" as it is calling activity whose version is 1.0 (this is deprecated now). Where as it should have called activity with version 2.0. Where do I mention that it should use 2.0. **Thing I don't understand is how SWF comes to know which version to call as both activity poller and decider poller don't let you mention the version.**",2015-10-15T12:38:05.250,1445826,CC BY-SA 3.0,True,1,,.,0,"When I check in the AWS console I get this ""ScheduleActivityTaskFailed"" as it is calling activity whose version is 1.0 (this is deprecated now).",False,False,33147692.0,33144757.0,2015-10-15T11:46:41.843,2.0,"<p>It stands out that in the decider you are using <code>""test-domain""</code>, but in the rest of the code you are using<code>""test-domain-newspecies""</code>.</p>

<p>If the domain <code>""test-domain""</code> is not registered you should get an <a href=""http://docs.aws.amazon.com/amazonswf/latest/apireference/API_PollForDecisionTask.html#API_PollForDecisionTask_Errors"" rel=""nofollow""><code>UnknownResourceFault</code></a> error when polling for a decision task.</p>
",3,CC BY-SA 3.0,33144757.0,2015-10-15T09:29:10.313,1.0,580.0,2015-10-15T09:51:37.323,2015-10-15T11:46:41.843,Why is the descisionTask not receiving any task from AWS-SWF service (SWF)?,<node.js><amazon-web-services><amazon-swf>,1.0,0.0,,,
54512624,23842621,1,AWS now provides mobile services https://aws.amazon.com/mobile/ so this answer is obsolete.,2015-10-27T01:22:10.523,991806,CC BY-SA 3.0,True,1,AWS,.,0,AWS now provides mobile services https://aws.amazon.com/mobile/ so this answer is obsolete.,False,False,,,,,,,,,,,,,,,,,,,,
54997760,33611380,1,"The official ""Getting Started"" guide gave information on how to set up in VS2005. I thought it was a typing mistake - but no... it is really that out of date.",2015-11-09T14:52:42.850,1778482,CC BY-SA 3.0,True,1,,.,0,it is really that outdated.,False,False,33611380.0,33611249.0,2015-11-09T14:30:40.637,0.0,"<p>Take a look to <a href=""https://aws.amazon.com/sdk-for-net/"" rel=""nofollow"">AWS SDK for .Net</a>. Also you can find <a href=""http://docs.aws.amazon.com/AWSSdkDocsNET/latest/V3/DeveloperGuide/welcome.html"" rel=""nofollow"">some guides</a> and how to work with it's APIs.</p>

<hr>

<p>The AWS SDK for .NET includes the following:</p>

<ul>
<li>The current version of the AWS SDK for .NET.</li>
<li>All previous major versions of the AWS SDK for .NET.</li>
<li>Sample code that demonstrates how to use the AWS SDK for .NET with several AWS services.</li>
</ul>
",5,CC BY-SA 3.0,33611249.0,2015-11-09T14:23:45.723,2.0,2034.0,2015-11-09T16:02:11.093,2017-12-19T21:42:32.817,Is there a more efficient way to deal with Amazon Product advertising API on C# ASP.NET (esp MVC 5)?,<c#><asp.net><asp.net-mvc><amazon-web-services>,4.0,0.0,,,
55018227,33616264,0,"Thank you Vineet. Should you please tell me why the ideal number should be 38? I did some research and found [this doc](http://wiki.apache.org/hadoop/HowManyMapsAndReduces), it seems that  **reduce task = p*( nodes * mapred.tasktracker.reduce.tasks.maximum)**, where p =0.75 or 0.95,  nodes = 3, mapred.tasktracker.reduce.tasks.maximum=# of cpu cores in every node = 4, therefore, **mapreduce.job.reduces** = 9 to 11 (FYI: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces)",2015-11-10T02:35:45.740,5397906,CC BY-SA 3.0,True,1,mapred.reduce.tasks,.,0,(FYI: mapred.reduce.tasks is deprecated.,False,False,33616264.0,33598202.0,2015-11-09T19:02:11.427,1.0,"<p>Your input file size should conclude the number of reducers. Standard is 1 Reducer per 1 GB unless you are compressing the Mapper output data. So in this case ideal number should have been at least 38. Try passing the command line option as -D mapred.reduce.tasks=40 and see if there is any change.</p>
",3,CC BY-SA 3.0,33598202.0,2015-11-08T19:42:08.593,0.0,1069.0,2015-11-09T15:17:21.320,2015-11-09T19:02:11.427,How to tune the Hadoop MapReduce parameters on Amazon EMR?,<hadoop><memory><hadoop2><emr><amazon-emr>,2.0,0.0,,1.0,
55132676,33679457,0,`AmazonS3Client` is not deprecated.,2015-11-12T19:33:50.370,1721492,CC BY-SA 3.0,True,1,AmazonS3Client,.,1,`AmazonS3Client` is not deprecated.,False,False,33679457.0,33679187.0,2015-11-12T19:16:01.680,0.0,"<p>The transfer manager currently does not support ACL; however, you can use <code>AmazonS3Client</code> to perform <a href=""http://docs.aws.amazon.com/AWSAndroidSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html#putObject%28com.amazonaws.services.s3.model.PutObjectRequest%29"" rel=""nofollow""><code>putObject()</code></a>. You can take a look at the AWS Mobile SDK for Android <a href=""http://docs.aws.amazon.com/AWSAndroidSDK/latest/javadoc/com/amazonaws/services/s3/model/PutObjectRequest.html#setCannedAcl%28com.amazonaws.services.s3.model.CannedAccessControlList%29"" rel=""nofollow"">API Reference</a> for more details.</p>
",3,CC BY-SA 3.0,33679187.0,2015-11-12T18:59:33.967,0.0,523.0,,2015-11-12T19:16:01.680,How to specify public-read policy on a complete bucket in aws?,<android><amazon-web-services><amazon-s3>,2.0,0.0,,,
55274672,25862505,0,It looks like this fix is now out of date.  The runner up answer from Rubén Durá Tarí works (if you fix the typo) and seems on the face of it to be more robust.,2015-11-17T09:41:26.230,2347231,CC BY-SA 3.0,True,1,, ,0,It looks like this fix is now outdated.  ,False,False,,,,,,,,,,,,,,,,,,,,
55632414,32955365,0,The `RegisterProfile` approach is also discouraged by Amazon: https://blogs.aws.amazon.com/net/post/Tx3VJIHQF8Q0YSX/RegisterProfile,2015-11-26T10:37:58.077,3205,CC BY-SA 3.0,True,1,approach,https://blogs.aws.amazon.com/net/post/Tx3VJIHQF8Q0YSX/RegisterProfile,0,The `RegisterProfile` approach is also discouraged by Amazon: https://blogs.aws.amazon.com/net/post/Tx3VJIHQF8Q0YSX/RegisterProfile,False,False,32955365.0,32461275.0,2015-10-05T18:26:30.577,1.0,"<p>I got the same issue and what worked for me was to use basic AWS credentials instead of using store ones.</p>

<pre><code>    public static AmazonS3Client GetAwsS3Client(string accessKey, string secretKey)
    {
        var credentials = new BasicAWSCredentials(accessKey, secretKey);

        return new AmazonS3Client(credentials, Amazon.RegionEndpoint.USEast1);
    }
</code></pre>
",1,CC BY-SA 3.0,32461275.0,2015-09-08T15:07:09.930,2.0,956.0,2015-09-08T15:30:48.990,2015-10-05T18:26:30.577,AWS SDK CryptProtectData failed,<.net><amazon-web-services><aws-sdk>,1.0,0.0,,,
56012882,33418364,0,Thanks but I did this using the invoke method because the invokeAsync method is deprecated.  (http://docs.aws.amazon.com/lambda/latest/dg/API_InvokeAsync.html),2015-12-07T11:37:54.610,5359715,CC BY-SA 3.0,True,1,, ,0,I did this using the invoke method because the invokeAsync method is deprecated.  ,False,False,33418364.0,33411014.0,2015-10-29T15:28:20.773,0.0,"<p>Just call the next Lambda function at the end of each function?</p>

<p>Use <a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Lambda_20141111.html#invokeAsync-property"" rel=""nofollow"">http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Lambda_20141111.html#invokeAsync-property</a> if you are using Node.js/JavaScript.</p>
",4,CC BY-SA 3.0,33411014.0,2015-10-29T09:57:36.133,4.0,6457.0,2018-05-21T05:42:34.577,2018-06-22T00:38:40.997,Invoke multiple aws lambda functions,<amazon-web-services><amazon-s3><aws-lambda><aws-sdk><amazon-kinesis-firehose>,4.0,0.0,,5.0,
56036396,3988825,0,"The part of this answer about network performance is fairly obsolete - for quite a while now, there have existed a variety of instances that can be ""EBS-optimized"" at a small extra cost, and some that are such by default (with no surcharge), which have dedicated network interfaces towards EBS, cf. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html",2015-12-07T22:01:43.527,4617744,CC BY-SA 3.0,True,1,part,http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html,0,"The part of this answer about network performance is fairly obsolete - for quite a while now, there have existed a variety of instances that can be ""EBS-optimized"" at a small extra cost, and some that are such by default (with no surcharge), which have dedicated network interfaces towards EBS, cf. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html",False,False,,,,,,,,,,,,,,,,,,,,
56123139,34174888,0,"[Instance: i-36f77380] Command failed on instance. Return code: 1 Output: [CMD-Startup/StartupStage0/AppDeployPreHook/03deploy.py] command failed with error code 1: /opt/elasticbeanstalk/hooks/appdeploy/pre/03deploy.py New python executable in /opt/python/run/venv/bin/python27 Also creating executable in /opt/python/run/venv/bin/python Installing setuptools, pip...done. Running virtualenv with interpreter /usr/bin/python27 --use-mirrors has been deprecated and will be removed in the future. Explicit uses of --index-url and/or --extra-index-url is suggested.",2015-12-09T20:31:11.197,3928035,CC BY-SA 3.0,True,1,mirrors,.,0,/python27 --use-mirrors has been deprecated and will be removed in the future.,False,False,34174888.0,34173732.0,2015-12-09T09:18:53.810,0.0,"<p>When you redeployed, you opted into the latest beanstalk version, which uses a different AMI than the one originally used. If you're familiar with the concept of <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html"" rel=""nofollow"">ebextensions</a> then you're probably looking to add a file like this:</p>

<p>.ebextensions/python.config:<br>
<code>ln -s /opt/python/run/venv/bin/python27 /opt/python/run/venv/bin/python</code></p>
",9,CC BY-SA 3.0,34173732.0,2015-12-09T08:11:18.173,1.0,50.0,,2015-12-09T09:18:53.810,Error in AWS ElasticBeanStalk python 27 application that was running fine,<amazon-elastic-beanstalk>,1.0,0.0,,,
56172223,34213578,0,Also note that invoke-async is deprecated ... they recommend using invoke instead (I believe there is a parameter to set it to async though),2015-12-10T23:48:11.010,1315053,CC BY-SA 3.0,True,1,,),0,Also note that invoke-async is deprecated ... they recommend using invoke instead (I believe there is a parameter to set it to async though),False,False,34213578.0,34213464.0,2015-12-10T23:20:51.627,2.0,"<p>You could use API Gateway to invoke your Lambda function asynchronously by configuring it as an AWS service proxy. The configuration is basically the same you see in <a href=""https://github.com/awslabs/api-gateway-secure-pet-store/blob/master/src/main/resources/swagger.yaml#L38"" rel=""nofollow"">this GitHub sample</a>, with the exception that the uri for the Lambda invocation changes to <a href=""http://docs.aws.amazon.com/lambda/latest/dg/API_InvokeAsync.html"" rel=""nofollow"">/invoke-async/</a> instead of just /invoke/</p>
",3,CC BY-SA 3.0,34213464.0,2015-12-10T23:11:42.537,19.0,12211.0,2015-12-13T00:04:25.137,2019-05-07T18:10:45.460,AWS API-Gateway communicating to SNS,<amazon-web-services><amazon-sns><aws-api-gateway>,6.0,4.0,,6.0,
56206354,34231222,0,Please quit adding a signature to every post; [it's discouraged.](http://meta.stackexchange.com/questions/2950/should-hi-thanks-taglines-and-salutations-be-removed-from-posts),2015-12-11T19:43:50.807,1950231,CC BY-SA 3.0,True,1,,/,0,Please quit adding a signature to every post; [it's discouraged.](http://meta.stackexchange.com/questions/2950/,False,False,34231222.0,33794753.0,2015-12-11T19:36:07.777,0.0,"<p>API Gateway currently does not support matrix parameters. As a workaround, you could use query parameters as already mentioned and parse them in your backend.</p>

<p>Best,
Jurgen</p>
",1,CC BY-SA 3.0,33794753.0,2015-11-19T03:28:29.450,0.0,119.0,,2015-12-11T19:36:07.777,How to setup AWS API gateway resource to take matrix parameters?,<amazon-web-services><aws-api-gateway>,2.0,0.0,,,
56785278,34513997,0,"Hi. Infact, I have indeed tried bluebird but I didn't get it working well. I followed and had my code identical to [this guide](https://www.rethinkdb.com/blog/s3-batch-upload/). Is the code outdated or something? `var s3 = bluebird.promisifyAll(new aws.S3());` doesn't seem to make any changes and `bluebird.all(uploadToS3, function(data){uploadToMongo})` did not wait for s3 upload to finish, either.",2015-12-30T01:57:46.543,user4950757,CC BY-SA 3.0,True,1,code,?,0,Is the code outdated or something?,False,False,,,,,,,,,,,,,,,,,,,,
57128492,25965387,1,those methods look deprecated,2016-01-09T08:06:44.160,505259,CC BY-SA 3.0,True,1,,deprecated,0,those methods look deprecated,False,False,25965387.0,25257078.0,2014-09-22T00:56:24.270,14.0,"<p>You can also scrap using Cognito and use the Access Key/Secret:</p>

<pre><code>AWSStaticCredentialsProvider *credentialsProvider = [AWSStaticCredentialsProvider credentialsWithAccessKey:ACCESS_KEY_ID secretKey:SECRET_KEY];
AWSServiceConfiguration *configuration = [AWSServiceConfiguration configurationWithRegion:AWSRegionUSWest2
                                                                      credentialsProvider:credentialsProvider];

[AWSServiceManager defaultServiceManager].defaultServiceConfiguration = configuration;
</code></pre>

<p>Make sure you use the region your bucket is actually in.</p>
",2,CC BY-SA 3.0,25257078.0,2014-08-12T05:54:55.627,4.0,4374.0,2017-05-23T11:46:12.973,2015-05-11T09:01:41.273,Connecting to S3 Using AWS SDK v2 (iOS),<ios><amazon-web-services><amazon-s3>,3.0,0.0,,5.0,
57356104,27097261,0,"S3Client::factory is deprecated in SDK 3.x, otherwise the solution is valid",2016-01-15T08:28:32.330,984213,CC BY-SA 3.0,True,1,,valid,0,"S3Client::factory is deprecated in SDK 3.x, otherwise the solution is valid",False,True,27097261.0,18683206.0,2014-11-24T03:08:16.617,29.0,"<p>The answer is above however i figured i would supply a complete working example that can be copied and pasted directly into a php file and ran</p>

<pre><code>use Aws\S3\S3Client;

require_once('PATH_TO_API/aws-autoloader.php');

$s3 = S3Client::factory(array(
    'key'    =&gt; 'YOUR_KEY',
    'secret' =&gt; 'YOUR_SECRET',
    'region' =&gt; 'us-west-2'
));

$bucket = 'YOUR_BUCKET_NAME';

$objects = $s3-&gt;getIterator('ListObjects', array(
    ""Bucket"" =&gt; $bucket,
    ""Prefix"" =&gt; 'some_folder/' //must have the trailing forward slash ""/""
));

foreach ($objects as $object) {
    echo $object['Key'] . ""&lt;br&gt;"";
}
</code></pre>
",2,CC BY-SA 3.0,18683206.0,2013-09-08T11:36:32.983,32.0,40011.0,2014-01-15T12:59:55.233,2019-01-02T19:39:22.933,List objects in a specific folder on Amazon S3,<php><sdk><amazon-s3>,3.0,0.0,,9.0,
57520012,34890818,0,Sorry for the newbie question: I see that TransferManger is deprecated and is superseded by TransferUtility. I am guessing the same claims apply. Is that correct?,2016-01-20T03:16:24.333,5641865,CC BY-SA 3.0,True,1,,.,0,Sorry for the newbie question: I see that TransferManger is deprecated and is superseded by TransferUtility.,False,False,,,,,,,,,,,,,,,,,,,,
57630829,34913442,0,"I appreciate you answering my question, but this is still not the one that's good enough. This call you're mentioning, It's deprecated and should not be used anymore (as I already mentioned in the question).",2016-01-22T15:02:51.763,1859462,CC BY-SA 3.0,True,1,It,.,1,"This call you're mentioning, It's deprecated and should not be used anymore (as I already mentioned in the question).",False,False,,,,,,,,,,,,,,,,,,,,
57631131,34913442,0,"I see, I cannot find any information on a V2 API.  Can you post a link to the docs so I can poke around?  Also, you should change the text in your OP to `myUpload.waitForUploadResult()` to avoid confusion.  I did read your Post, and you wrote `myUpload.waitForCompletion()`, hence the confusion. I posted the method from the docs I found, I didn't see any text mentioning it was deprecated.",2016-01-22T15:10:11.030,1359578,CC BY-SA 3.0,True,1,,.,1,"I posted the method from the docs I found, I didn't see any text mentioning it was deprecated.",False,False,,,,,,,,,,,,,,,,,,,,
57631519,34913442,0,"And this is the api v2 page -> http://docs.aws.amazon.com/AWSAndroidSDK/latest/javadoc/. If you look at this page -> https://aws.amazon.com/articles/3002109349624271 it clearly say that we should use v2. In Android studio, all the Classes from v1 are deprecated.",2016-01-22T15:19:37.470,1859462,CC BY-SA 3.0,True,1,Classes,.,0,"In Android studio, all the Classes from v1 are deprecated.",False,False,,,,,,,,,,,,,,,,,,,,
57632066,34913442,0,"When I try to use UploadResult, it gives me -> ""Deprecated - The Transfer Manager is now deprecated in favor of the com.amazonaws.mobileconnectors.s3.transferutility.TransferUtility""",2016-01-22T15:32:27.680,1859462,CC BY-SA 3.0,True,1,Manager,.,0,"""Deprecated - The Transfer Manager is now deprecated in favor of the com.amazonaws.mobileconnectors.s3.transferutility.",False,False,,,,,,,,,,,,,,,,,,,,
57632774,34913442,0,"Thanks Andrew, it really is driving me mad and I'm thinking of switching back to deprecated methods :/",2016-01-22T15:48:17.663,1859462,CC BY-SA 3.0,True,1,,:/,0,and I'm thinking of switching back to deprecated methods :/,False,False,,,,,,,,,,,,,,,,,,,,
57807460,16595208,0,"Hmm, why does s3cmd have so much more functionality than aws cli yet it's deprecated????",2016-01-27T15:34:21.557,1586965,CC BY-SA 3.0,True,1,it,?,0,yet it's deprecated????,False,False,16595208.0,8975959.0,2013-05-16T18:51:46.850,36.0,"<p><a href=""http://s3tools.org/s3cmd"" rel=""noreferrer"">s3cmd</a> can show you this by running <code>s3cmd du</code>, optionally passing the bucket name as an argument.</p>
",7,CC BY-SA 3.0,8975959.0,2012-01-23T17:31:28.750,114.0,104090.0,2019-12-26T19:27:22.130,2019-12-26T19:27:22.130,AWS S3: how do I see how much disk space is using,<amazon-s3><amazon-web-services>,18.0,2.0,,40.0,
58101370,35185831,0,"I'm not using ALAssetsLibrary, I'm just trying to pass the NSURL. That sample does use ALAssetsLibrary though, and it seems it's been deprecated as of iOS 9.0. :(",2016-02-04T03:50:56.730,4964612,CC BY-SA 3.0,True,1,,:(,0,"That sample does use ALAssetsLibrary though, and it seems it's been deprecated as of iOS 9.0. :(",False,False,35185831.0,35166668.0,2016-02-03T19:21:56.007,1.0,"<p><code>AWSS3TransferManager</code> does not support <code>ALAssetsLibrary</code>. You need to copy the file to your app disk space and pass the file <code>NSURL</code>. See <a href=""https://github.com/awslabs/aws-sdk-ios-samples/tree/master/S3TransferManager-Sample"" rel=""nofollow"">this sample</a> as an example.</p>
",2,CC BY-SA 3.0,35166668.0,2016-02-03T00:58:25.173,3.0,2933.0,2016-02-03T08:19:11.167,2017-12-15T12:36:29.953,No such file location - Uploading image from Swift to S3,<ios><swift><amazon-web-services><amazon-s3><uiimagepickercontroller>,4.0,4.0,,1.0,
58927510,33262467,1,"This is the best answer now, as aws 1 is deprecated.",2016-02-25T09:05:49.150,993592,CC BY-SA 3.0,True,1,,.,0,"This is the best answer now, as aws 1 is deprecated.",False,False,33262467.0,28374401.0,2015-10-21T14:47:16.060,18.0,"<p>There is official solution
Use paperclip from this branch:
it works with aws-sdk versions above 2</p>

<pre><code>gem 'paperclip', :git=&gt; 'https://github.com/thoughtbot/paperclip', :ref =&gt; '523bd46c768226893f23889079a7aa9c73b57d68'
</code></pre>

<p>just add :s3_region parameter to your paperclip s3 config</p>

<p>works for me</p>
",1,CC BY-SA 3.0,28374401.0,2015-02-06T20:38:43.700,91.0,26280.0,2015-02-11T00:47:45.117,2017-01-03T10:59:50.737,NameError (uninitialized constant Paperclip::Storage::S3::AWS):,<ruby-on-rails><ruby><amazon-web-services><paperclip>,4.0,7.0,,9.0,
59058154,35684837,0,"Yeah, that's definitely weird; its probably obsolete and someone didn't realize it's misleading (and wrong) for the current PHP SDK, or I'm sure they would have put a notice somewhere that there was a new location for the latest SDK version's docs. Anyway, if you go to https://aws.amazon.com/ (very bottom of the page, a ""PHP on AWS"" link under ""Resources & Training""), or https://aws.amazon.com/tools/ (""Documentation"", under ""PHP""), then you'll get linked to the correct page, which you can follow down to http://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.CloudFront.CloudFrontClient.html .",2016-02-29T00:35:33.643,44637,CC BY-SA 3.0,True,1,,and,0,"Yeah, that's definitely weird; its probably obsolete and",False,False,35684837.0,32719717.0,2016-02-28T16:06:00.237,2.0,"<p>I took a look at what was going on here and apparently you're using an invalid method signature.</p>

<p>First of all, the AWS PHP SDK uses something called ""service description models"" to get an API interface for the version you specified. In this case that means they are using some meta-programming techniques to provide an interface to what  looks like normal PHP function calls, but the functions are not hard-coded in the SDK. They don't really exist in the sense we generally think of, at least not as regular PHP functions. Some PHP magic is going on underneath.</p>

<p>When you call <code>$cdn-&gt;create_invalidation()</code> it runs on the <code>Aws\CloudFront\CloudFrontClient</code> instance which inherits from <code>Aws\AwsClient</code>. Neither of those classes (nor any of their ancestors) actually have a <code>::create_invalidation()</code> method implementation. But <code>AwsClient</code> does implement the PHP magic method <code>::__call()</code>. You can see <a href=""http://php.net/manual/en/language.oop5.overloading.php#object.call"" rel=""nofollow"">the PHP docs</a> for the full info on this magic method, but basically when you call any method that doesn't exist on an object, if its class implements <code>::__call()</code>, then <code>::__call()</code> will be invoked instead.</p>

<p>So now we're inside of <code>::__call()</code> (you can see the code <a href=""https://github.com/aws/aws-sdk-php/blob/3.3.8/src/AwsClient.php#L157"" rel=""nofollow"">here</a>) and this method ends up calling <code>::getCommand()</code>.  <a href=""https://github.com/aws/aws-sdk-php/blob/3.3.8/src/AwsClient.php#L211"" rel=""nofollow"">Inside of <code>::getCommand()</code></a> you can see that the method signature requires an array to be passed in (the first argument, <code>$name</code>, is the only other method parameter and it's going to have the value <code>""create_invalidation""</code> because of the way <code>::__call()</code> was implemented above). So that's where the the first problem crops up: you need to pass in an array, not individual strings or timestamps or anything else.</p>

<p>But there's one other thing; the actual method you want to call is not called <code>create_invalidation</code>, but <code>createInvalidation</code>. There are full API docs for the SDK <a href=""http://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.CloudFront.CloudFrontClient.html"" rel=""nofollow"">here</a> - just make sure you pick the right version. For the version I'm looking at, you can find the API docs and method signature for creating invalidations <a href=""http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-cloudfront-2015-07-27.html#createinvalidation"" rel=""nofollow"">here</a>.</p>

<p>You can find lots of information about the AWS PHP SDK including links to a User Guide, API Docs, and more, in the project's <a href=""https://github.com/aws/aws-sdk-php"" rel=""nofollow"">GitHub readme</a>. Good luck and happy coding :)</p>
",3,CC BY-SA 3.0,32719717.0,2015-09-22T14:33:15.260,10.0,3589.0,2016-02-28T23:27:03.810,2016-02-28T23:27:03.810,"Argument 2 passed to Aws\AwsClient::getCommand() must be of the type array, string given",<php><api><amazon-web-services>,1.0,1.0,,,
59202994,35759415,0,"You are using outdated packages or an outdated operating system. Your original question is answered, if you do not know how to use your operating system ask it in a different question.",2016-03-03T07:12:17.010,127508,CC BY-SA 3.0,True,1,,.,0,You are using outdated packages or an outdated operating system.,False,False,35759415.0,35758317.0,2016-03-02T22:25:47.113,3.0,"<p>You have to use the latest version 1.6.1</p>

<p>Just upgrade s3cmd the following way or any other way you might want to use.</p>

<pre><code>pip install --upgrade
</code></pre>
",4,CC BY-SA 3.0,35758317.0,2016-03-02T21:21:24.767,1.0,3598.0,2017-05-23T12:09:42.380,2017-04-06T10:18:18.200,s3cmd put - The authorization mechanism you have provided is not supported. Please use AWS4-HMAC-SHA256,<amazon-web-services><amazon-s3><amazon-ec2>,2.0,0.0,,,
59217239,20574036,0,@davetw12 can you provide documentation that says that config files are out of date?,2016-03-03T13:05:43.340,57344,CC BY-SA 3.0,True,1,,?,0,@davetw12 can you provide documentation that says that config files are outdated?,False,False,,,,,,,,,,,,,,,,,,,,
59360092,31226593,2,"@HaveAGuess thanks. I don't have time to do a lot of research on this, so I just updated my answer saying that what I have posted does work, but not that ebextensions is outdated. It's all I have time for right now.",2016-03-07T15:18:23.900,3757322,CC BY-SA 3.0,True,1,,.,1,"I don't have time to do a lot of research on this, so I just updated my answer saying that what I have posted does work, but not that ebextensions is outdated.",False,False,,,,,,,,,,,,,,,,,,,,
59507530,35898205,0,@TheRedSeth that's a separate issue. Your Boto certificates are out of date. I think you need to update the cacerts.txt in the boto folder.,2016-03-10T18:38:39.683,13070,CC BY-SA 3.0,True,1,certificates,.,0,Your Boto certificates are outdated.,False,False,35898205.0,35897798.0,2016-03-09T17:14:02.027,4.0,"<p>Move <code>boto.connect_ec2</code> into the first for loop, and pass it the region. Something like this:</p>

<pre><code>import boto.ec2
import os
import sys

ACCESS_KEY = raw_input(""Enter your access key &gt; "")
SECRET_KEY = raw_input(""Enter your secret key &gt; "")
if not ACCESS_KEY:
    sys.exit(""ERROR: You did not enter anything for ACCESS KEY or SECRET KEY. Exiting..."")

regions = boto.ec2.regions()
for x in regions:
    ec2_conn = boto.connect_ec2(ACCESS_KEY, SECRET_KEY, region=x)
    reservations = ec2_conn.get_all_reservations()
    for r in reservations:
        for i in r.instances:
            print r.instances
            print 'Tags: ',i.tags['Name']
            print 'Public IP Address: ',i.ip_address
            print 'Private IP address: ',i.private_ip_address
            if (i.virtualization_type == 'hvm'):
                platform = 'Windows'
            else:
                platform = 'Linux'
                print 'Platform: ',platform
            print 'State: ',i.state
            print
</code></pre>
",3,CC BY-SA 3.0,35897798.0,2016-03-09T16:55:58.903,0.0,2553.0,,2016-03-09T17:14:02.027,How do I loop through regions using boto with python?,<python><amazon-web-services><amazon-ec2><boto>,1.0,0.0,,,
59561114,22738217,4,"No, it's not a custom function.  Looks like you are referring to Postgres as opposed to Redshift which has it.  In Postgres I believe it was deprecated and the equivalent function is strpos.",2016-03-12T00:54:34.703,3204134,CC BY-SA 3.0,True,1,,.,0,In Postgres I believe it was deprecated and the equivalent function is strpos.,False,False,22738217.0,22715053.0,2014-03-29T23:21:43.760,14.0,"<p>The privileges are stored in the nspacl field of pg_namespace.  Since it's an array field, you have to do a little fancy coding to parse it.  This query will give you the grant statements used for users and groups:</p>

<pre><code>select 
'grant ' || substring(
          case when charindex('U',split_part(split_part(array_to_string(nspacl, '|'),pu.usename,2 ) ,'/',1)) &gt; 0 then ',usage ' else '' end 
          ||case when charindex('C',split_part(split_part(array_to_string(nspacl, '|'),pu.usename,2 ) ,'/',1)) &gt; 0 then ',create ' else '' end 
       , 2,10000)
|| ' on schema '||nspname||' to ""'||pu.usename||'"";' 
from pg_namespace pn,pg_user pu
 where  array_to_string(nspacl,',') like '%'||pu.usename||'%' --and pu.usename='&lt;username&gt;' 
and nspowner &gt; 1 
union
select 
'grant ' || substring(
          case when charindex('U',split_part(split_part(array_to_string(nspacl, '|'),pg.groname,2 ) ,'/',1)) &gt; 0 then ',usage ' else '' end 
          ||case when charindex('C',split_part(split_part(array_to_string(nspacl, '|'),pg.groname,2 ) ,'/',1)) &gt; 0 then ',create ' else '' end 
       , 2,10000)
|| ' on schema '||nspname||' to group ""'||pg.groname||'"";' 
from pg_namespace pn,pg_group pg
 where array_to_string(nspacl,',') like '%'||pg.groname||'%' --and pg.groname='&lt;username&gt;' 
 and nspowner &gt; 1 
</code></pre>
",2,CC BY-SA 3.0,22715053.0,2014-03-28T14:04:31.067,37.0,50050.0,,2020-06-11T13:39:23.137,postgresql - view schema privileges,<sql><postgresql><amazon-redshift>,9.0,1.0,,9.0,
59748227,36049767,0,"Awesome amount of detail Mike! Just need to add some info about possibly using range keys to solve it, and my answer becomes obsolete...",2016-03-17T01:17:56.693,836214,CC BY-SA 3.0,True,1,answer,...,0,"Just need to add some info about possibly using range keys to solve it, and my answer becomes obsolete...",False,False,,,,,,,,,,,,,,,,,,,,
59860672,36097324,0,I just found out myself. Thanks for responding and sorry for the outdated post.,2016-03-20T01:43:19.897,2880784,CC BY-SA 3.0,True,1,,.,0,Thanks for responding and sorry for the outdated post.,False,False,36097324.0,33279485.0,2016-03-19T03:01:52.080,0.0,"<p>How about using a timer at 30 mins., then calling the signInSilently?</p>
",2,CC BY-SA 3.0,33279485.0,2015-10-22T11:03:18.163,5.0,4318.0,2015-10-30T16:40:29.250,2016-03-19T03:01:52.080,How to refresh authentication.idToken with GIDSignIn or GIDAuthentication?,<ios><oauth-2.0><google-plus><amazon-cognito><google-signin>,3.0,2.0,,2.0,
59878771,27387153,0,`initWithConfiguration` is now deprecated. Use `registerS3WithConfiguration` to initialize and `S3ForKey` to access. No need to keep a reference.,2016-03-20T20:42:11.660,58505,CC BY-SA 3.0,True,1,,.,0,`initWithConfiguration` is now deprecated.,False,False,27387153.0,27350068.0,2014-12-09T19:24:35.240,1.0,"<p><code>defaultServiceConfiguration</code> can only be set once (only the first configuration assigned will be valid). In order to switch between configurations, you need to call <code>- initWithConfiguration:</code> on <code>AWSS3</code> and <code>AWSDynamoDB</code> instead of <code>+ defaultS3</code> and <code>+ defaultDynamoDB</code>.</p>

<p>Please note that when you use <code>- initWithConfiguration:</code>, you need to retain a strong reference to the service client object. For <code>+ defaultS3</code> and <code>+ defaultDynamoDB</code>, the SDK maintains the strong references (these are singleton methods). Just like you are making configuration objects <code>@property</code>, you can make the service client <code>@property</code> as well.</p>
",2,CC BY-SA 3.0,27350068.0,2014-12-08T01:29:52.550,0.0,432.0,,2014-12-09T19:24:35.240,AWS Switch Region from East1 to West2,<ios><amazon-web-services><amazon-dynamodb>,1.0,2.0,,,
60084366,33223399,0,"I think this chart is very out of date. Two things quickly jumped out at me: Hortonworks easily handles the ingestion of streaming data and also supports rolling upgrades. It's also available on Azure, where the OP already does some work.",2016-03-25T19:27:36.113,3470739,CC BY-SA 3.0,True,1,I,.,0,I think this chart is very outdated.,False,False,33223399.0,28696601.0,2015-10-19T20:32:46.337,2.0,"<p>Comparison among different hadoop distributors
  <a href=""https://i.stack.imgur.com/2fTVa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2fTVa.png"" alt=""Distributors""></a>
<a href=""https://i.stack.imgur.com/2fTVa.png"" rel=""nofollow noreferrer"">1</a>]<a href=""https://i.stack.imgur.com/2fTVa.png"" rel=""nofollow noreferrer"">1</a> </p>

<p>You can find reference about Microsoft distribution at this <a href=""http://www.dezyre.com/article/-top-6-hadoop-vendors-providing-big-data-solutions-in-open-data-platform/93"" rel=""nofollow noreferrer"">article</a></p>
",2,CC BY-SA 3.0,28696601.0,2015-02-24T13:04:54.220,0.0,534.0,,2018-02-13T03:44:55.327,Comparisons between BigData Solutions.,<azure><amazon-web-services><bigdata><azure-hdinsight>,4.0,3.0,2018-02-13T07:07:32.717,2.0,
60705350,36545939,0,"Since `~` is a character that can require special handling in a URL via url-encoding (or may be given special handling when it isn't required) and the signing code must generate a signature that matches the encoding, this seems like a likely candidate for an explanation of the problem, and the deprecated state of boto... well, yeah. It should have worked. +1",2016-04-11T15:04:12.033,1695906,CC BY-SA 3.0,True,1,,...,1,"Since `~` is a character that can require special handling in a URL via url-encoding (or may be given special handling when it isn't required) and the signing code must generate a signature that matches the encoding, this seems like a likely candidate for an explanation of the problem, and the deprecated state of boto...",False,False,36545939.0,36545111.0,2016-04-11T10:20:03.747,3.0,"<p>AWS S3 shall be fine with <code>~</code> in file name.</p>

<p>Using AWSCLI <code>aws</code> command I was able to upload file named <code>data.txt~</code>.</p>

<p>Anyway, you talk about <code>boto</code> being up to date. If you check README.rst of this project, you will find there: </p>

<pre><code>boto 2.39.0

Released: 9-Apr-2015
</code></pre>

<p>It is a year and 2 days today without an update.</p>

<p>Number of issues in this package is growing and according to Mitch Garnaat (author of <code>boto</code>) <a href=""https://github.com/boto/boto/issues/3306#issuecomment-132625810"" rel=""nofollow"">comment to my issue</a> <strong>""There is no way home""</strong>, the future development is only with <code>boto3</code>. </p>

<p>So my advice is: try to <strong>rewrite the script to <code>boto3</code></strong>, it is likely to work better and shall stay working longer.</p>
",3,CC BY-SA 3.0,36545111.0,2016-04-11T09:43:46.100,0.0,49.0,,2016-04-11T10:20:03.747,AWS S3 403 server error due to backup file,<python><amazon-s3><backup><boto><http-status-code-403>,1.0,0.0,,,
60730019,1786158,0,Links are obsolete by now!,2016-04-12T06:24:56.560,3305978,CC BY-SA 3.0,True,1,Links,!,0,Links are obsolete by now!,False,False,,,,,,,,,,,,,,,,,,,,
61384211,36906033,0,"Nice. When you finish making your API with `WWW`, I suggest you port it to `UnityWebRequest` The newest way of communicating with servers. It solves many problems `WWW` has. It is very likely `WWW` will be deprecated.  Don't worry about that now, just when you finish and the porting is easy if you read the link below. http://docs.unity3d.com/Manual/UnityWebRequest.html",2016-04-28T09:42:24.147,3785314,CC BY-SA 3.0,True,1,, ,0,It is very likely `WWW` will be deprecated.  ,False,False,36906033.0,36905561.0,2016-04-28T05:32:52.267,1.0,"<p>I found the answer myself.x-am-expires must also be in the same format as date. 
string expires = DateTime.Now.AddMinutes (10).ToString (""ddd,dd MMM yyyy HH:mm:ss "")+""GMT"";</p>
",4,CC BY-SA 3.0,36905561.0,2016-04-28T04:55:51.870,1.0,269.0,2016-04-28T05:18:08.410,2018-05-29T19:18:11.143,Adding x-amz-expires to query authentication header gives error(AccessDenied),<unity3d><amazon-s3>,1.0,0.0,,1.0,
61491195,15180632,0,"I believe that this answer it's out of date, and the considerations are no longer true...",2016-05-01T14:33:39.113,2738155,CC BY-SA 3.0,True,1,,...,1,"I believe that this answer it's outdated, and the considerations are no longer true...",False,False,15180632.0,15177908.0,2013-03-02T23:28:42.943,6.0,"<p>We use both approaches (EMR and EC2) at my job. </p>

<p>The advantages of EMR that Amar mentioned are more or less true: so if you want simplicity it may be the way to go.</p>

<p>But there are other considerations:</p>

<ul>
<li>the version of EMR is far  behind apache head. it is approximately 0.20.205 whereas head is at 2.X, which is essentially 3 versions up (1.0, 1.1, 2.0..)  </li>
</ul>

<p>hadoop@domU-12-31-39-07-B9-97:~$ ll hadoop*.jar
lrwxrwxrwx 1 hadoop hadoop 73 Feb  5 12:00 hadoop-examples-0.20.205.jar -> /home/hadoop/.versions/0.20.205/share/hadoop/hadoop-examples-0.20.205.jar
lrwxrwxrwx 1 hadoop hadoop 69 Feb  5 12:00 hadoop-test-0.20.205.jar -> /home/hadoop/.versions/0.20.205/share/hadoop/hadoop-test-0.20.205.jar
lrwxrwxrwx 1 hadoop hadoop 69 Feb  5 12:00 hadoop-core-0.20.205.jar -> /home/hadoop/.versions/0.20.205/share/hadoop/hadoop-core-0.20.205.jar
lrwxrwxrwx 1 hadoop hadoop 70 Feb  5 12:00 hadoop-tools-0.20.205.jar -> /home/hadoop/.versions/0.20.205/share/hadoop/hadoop-tools-0.20.205.jar
lrwxrwxrwx 1 hadoop hadoop 68 Feb  5 12:00 hadoop-ant-0.20.205.jar -> /home/hadoop/.versions/0.20.205/share/hadoop/hadoop-ant-0.20.205.jar</p>

<ul>
<li><p>As a direct consequence I had to re-code /restructure my Map/reduce program due to missing contrib modules in the older version running on EMR</p></li>
<li><p>You do not have as much of an opportunity to use non-Map/Reduce algorithms as if you were using updated version of M/R.</p></li>
<li><p>Flexibility to mix and match versions of hadoop ecosystem.  </p></li>
</ul>
",2,CC BY-SA 3.0,15177908.0,2013-03-02T18:31:55.507,16.0,6408.0,,2013-08-28T21:18:54.207,Hadoop on EC2 vs Elastic Map Reduce,<hadoop><amazon-web-services>,2.0,1.0,,5.0,
61629297,13578804,1,"fs.default.name is deprecated, every reader of this question should use fs.defaultFS instead.",2016-05-04T23:25:19.293,2738155,CC BY-SA 3.0,True,1,,.,0,"fs.default.name is deprecated, every reader of this question should use fs.defaultFS instead.",False,False,,,,,,,,,,,,,,,,,,,,
62100596,26889083,1,"At the moment, CollectionFS is deprecated. Another solution has to be used.",2016-05-17T23:30:10.913,100272,CC BY-SA 3.0,True,1,CollectionFS,.,0,"At the moment, CollectionFS is deprecated.",False,False,26889083.0,26817395.0,2014-11-12T14:14:34.813,2.0,"<p>Seems I was following the wrong path. CollectionFS has everything I need and more. I now have this working with plenty of scope to do more later. This is one brilliant collection of packages with clear guides on respective Github pages.</p>

<p>Here are the packages I ended up usings:</p>

<ul>
<li><a href=""https://github.com/CollectionFS/Meteor-CollectionFS"" rel=""nofollow"">cfs:standard-packages</a> - base</li>
<li><a href=""https://github.com/CollectionFS/Meteor-cfs-gridfs"" rel=""nofollow"">cfs:gridfs</a> - required for some reason, not sure why</li>
<li><a href=""https://github.com/CollectionFS/Meteor-cfs-graphicsmagick/"" rel=""nofollow"">cfs:graphicsmagick</a> - thumbnailing/cropping</li>
<li><a href=""https://github.com/CollectionFS/Meteor-cfs-s3"" rel=""nofollow"">cfs:s3</a> - S3 upload</li>
</ul>

<p><a href=""https://gist.github.com/onepixelsolid/3890f7e1be1a76d316a5"" rel=""nofollow"">Code sample →</a> </p>
",3,CC BY-SA 3.0,26817395.0,2014-11-08T12:59:14.577,5.0,2005.0,,2017-08-28T10:19:24.387,Meteor Amazon S3 image upload with thumbnails,<file-upload><amazon-s3><meteor><thumbnails>,4.0,0.0,,3.0,
62336115,35952580,0,man you saved me countless hours ! aws documentation sucks and so out of date .. I have been looking for hours in their documentation with no luck !,2016-05-24T13:25:47.487,3492881,CC BY-SA 3.0,True,1,,..,0,aws documentation sucks and so outdated ..,False,False,35952580.0,35949709.0,2016-03-12T01:18:41.803,4.0,"<p>You may want look at the SNS sample in <a href=""https://github.com/awslabs/aws-sdk-net-samples/tree/master/XamarinSamples/SNS"" rel=""nofollow"">GitHub</a> or via the <a href=""https://components.xamarin.com/view/aws-simplenotificationservice-sdk"" rel=""nofollow"">Xamarin component store</a> to augment the getting started, you may find some things that you are missing and are not fully covered in the getting started guide. Something that calls out to me that is present in the example but not in your code is <code>RegisterForGCM()</code> in the main activity:</p>

<pre><code>private void RegisterForGCM()
{
    string senders = Constants.GoogleConsoleProjectId;
    Intent intent = new Intent(""com.google.android.c2dm.intent.REGISTER"");
    intent.SetPackage(""com.google.android.gsf"");
    intent.PutExtra(""app"", PendingIntent.GetBroadcast(this, 0, new Intent(), 0));
    intent.PutExtra(""sender"", senders);
    StartService(intent);
}
</code></pre>
",6,CC BY-SA 3.0,35949709.0,2016-03-11T20:53:05.200,1.0,1023.0,,2016-03-12T01:18:41.803,how to enable push notifications on Android (Xamarin) for AWS SNS,<android><amazon-web-services><xamarin><push-notification><amazon-sns>,1.0,0.0,,,
62378066,37427657,0,"Thanks for that, I shall try. It was my understand that it was just the admin console on 28017 that was deprecated, and not the RESTful interface on 27017. Is that a misunderstanding?",2016-05-25T12:51:59.183,343159,CC BY-SA 3.0,True,1,,.,1,"It was my understand that it was just the admin console on 28017 that was deprecated, and not the RESTful interface on 27017.",False,False,,,,,,,,,,,,,,,,,,,,
63283245,37880232,1,"SDKGlobalConfiguration.ENABLE_S3_SIGV4_SYSTEM_PROPERTY method was deprecated, so how should i configure that.  is there is any other alternate way.",2016-06-20T06:46:20.860,5349679,CC BY-SA 3.0,True,1,, ,0,"ENABLE_S3_SIGV4_SYSTEM_PROPERTY method was deprecated, so how should i configure that.  ",False,False,,,,,,,,,,,,,,,,,,,,
63870994,38224110,1,"To expand on this answer, the version of the AWS CLI in package repositories is often out of date and is really best installed by downloading it from AWS.",2016-07-06T12:56:01.707,1428388,CC BY-SA 3.0,True,1,version,.,0,"To expand on this answer, the version of the AWS CLI in package repositories is often outdated and is really best installed by downloading it from AWS.",False,False,,,,,,,,,,,,,,,,,,,,
63934782,38256739,0,"Link-only answers are discouraged, and are subject to deletion, as links can change or be removed. Please use the edit link below your post to include the pertintent information for the solution here; you can still include the link for context, but an answer needs to stand on its own.",2016-07-07T23:43:54.783,1982136,CC BY-SA 3.0,True,1,answers,.,0,"Link-only answers are discouraged, and are subject to deletion, as links can change or be removed.",False,False,,,,,,,,,,,,,,,,,,,,
63973335,25814973,9,"This answer is a little outdated, the AWSSDK was the big assembly that held all Aws logic. In Aws 3.0, there are many smaller assemblies for each feature of Aws you want to use. Awssdk.s3 is probably what you want to use instead of awssdk. I think awssdk still exists though, although parts of the API have changed.",2016-07-09T00:20:34.513,468778,CC BY-SA 3.0,True,1,answer,.,0,"This answer is a little outdated, the AWSSDK was the big assembly that held all Aws logic.",False,False,,,,,,,,,,,,,,,,,,,,
64004952,38293614,0,"My opinion may be outdated. For years RDS could not tune the InnoDB log file size, or offer query logs except for table-based logs. That seems to have changed. But I guess I have a prejudice against a database server I can't log into! :-)",2016-07-10T16:21:16.130,20860,CC BY-SA 3.0,True,1,opinion,.,0,My opinion may be outdated.,False,False,,,,,,,,,,,,,,,,,,,,
64042965,38308008,0,"Based on the hadoop-project commit [which updated the aws-java-sdk version](https://github.com/apache/hadoop/commit/f7b0f292e722fa819900f455a070be1d7bf97072), there are some minor changes in the S3 interface that had to be made. However, checking the javadoc it does appear the newer versions are at least backwards-compatible even though the [method is deprecated](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/model/ObjectMetadata.html#getServerSideEncryption--).",2016-07-11T17:55:21.163,3777211,CC BY-SA 3.0,True,1,,.,0,"However, checking the javadoc it does appear the newer versions are at least backwards-compatible even though the [method is deprecated](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/model/ObjectMetadata.html#getServerSideEncryption--).",False,True,,,,,,,,,,,,,,,,,,,,
64469451,37631433,0,@MarkMercurio - It'd be neat if you could update this page too: http://docs.aws.amazon.com/cognito/latest/developerguide/facebook.html. It was last updated 4/17/2016 so it's not extremely old but appears to be outdated since using the Swift code sample gave me the deprecated error also.,2016-07-23T04:26:04.610,1359088,CC BY-SA 3.0,True,2,,sample,1,so it's not extremely old but appears to be outdated since using the Swift code sample,False,False,,,,,,,,,,,,,,,,,,,,
64591979,31972324,0,This is the most accurate answer. all others are outdated.+1,2016-07-27T01:10:55.743,1276136,CC BY-SA 3.0,True,1,,outdated.+1,0,all others are outdated.+1,False,False,,,,,,,,,,,,,,,,,,,,
64631683,26889083,0,This package is deprecated now. lepozepo:s3 package or tomi:upload-server are great options.,2016-07-27T20:57:48.503,1424473,CC BY-SA 3.0,True,1,package,.,0,This package is deprecated now.,False,False,26889083.0,26817395.0,2014-11-12T14:14:34.813,2.0,"<p>Seems I was following the wrong path. CollectionFS has everything I need and more. I now have this working with plenty of scope to do more later. This is one brilliant collection of packages with clear guides on respective Github pages.</p>

<p>Here are the packages I ended up usings:</p>

<ul>
<li><a href=""https://github.com/CollectionFS/Meteor-CollectionFS"" rel=""nofollow"">cfs:standard-packages</a> - base</li>
<li><a href=""https://github.com/CollectionFS/Meteor-cfs-gridfs"" rel=""nofollow"">cfs:gridfs</a> - required for some reason, not sure why</li>
<li><a href=""https://github.com/CollectionFS/Meteor-cfs-graphicsmagick/"" rel=""nofollow"">cfs:graphicsmagick</a> - thumbnailing/cropping</li>
<li><a href=""https://github.com/CollectionFS/Meteor-cfs-s3"" rel=""nofollow"">cfs:s3</a> - S3 upload</li>
</ul>

<p><a href=""https://gist.github.com/onepixelsolid/3890f7e1be1a76d316a5"" rel=""nofollow"">Code sample →</a> </p>
",3,CC BY-SA 3.0,26817395.0,2014-11-08T12:59:14.577,5.0,2005.0,,2017-08-28T10:19:24.387,Meteor Amazon S3 image upload with thumbnails,<file-upload><amazon-s3><meteor><thumbnails>,4.0,0.0,,3.0,
64715520,38667224,0,thanks but still didn't work (and actually it would surprise me as ansible_ssh_user and ssh_host were deprecated on 2.x http://docs.ansible.com/ansible/intro_inventory.html),2016-07-29T20:56:40.597,3486807,CC BY-SA 3.0,True,1,ssh_host,),0,ssh_host were deprecated on 2.x http://docs.ansible.com/ansible/intro_inventory.html),False,False,38667224.0,38666251.0,2016-07-29T20:47:22.347,0.0,"<p>The key pair should be enough to allow you to connect with your EC2 instance. I've just tried it with my setup and it seems to be working.</p>

<p>Have you tried using the full path to the keypair in question? For me it did the trick.</p>
",4,CC BY-SA 3.0,38666251.0,2016-07-29T19:31:50.980,4.0,2854.0,2016-07-29T20:49:07.643,2016-07-30T21:44:14.343,can I run ansible playbook on ec2 just with ssh keyfile no aws key/pair?,<amazon-ec2><ansible><ansible-2.x>,2.0,1.0,,,
64715614,38667224,0,"Could be you're right, I did set up this hostfile a long time ago, and it might have still been an earlier version. Anyway works for me so that syntax is still valid, even if deprecated.
Can you try adding your own key (id_rsa.pub) to the servers authorized_keys file and see if you can use that to connect? It will help you differentiate between problems with ansible and problems with the key.",2016-07-29T21:00:13.567,3833773,CC BY-SA 3.0,True,1,,"
",0,"Anyway works for me so that syntax is still valid, even if deprecated.
",False,False,38667224.0,38666251.0,2016-07-29T20:47:22.347,0.0,"<p>The key pair should be enough to allow you to connect with your EC2 instance. I've just tried it with my setup and it seems to be working.</p>

<p>Have you tried using the full path to the keypair in question? For me it did the trick.</p>
",4,CC BY-SA 3.0,38666251.0,2016-07-29T19:31:50.980,4.0,2854.0,2016-07-29T20:49:07.643,2016-07-30T21:44:14.343,can I run ansible playbook on ec2 just with ssh keyfile no aws key/pair?,<amazon-ec2><ansible><ansible-2.x>,2.0,1.0,,,
64737242,38678368,0,"while running the following command in Docker container eval $(aws ecr get-login --region us-west-2) it shows login succeeded but while running in ""git bash"" it throws following error Warning: '-e' is deprecated, it will be removed soon. **See usage.
An error occurred trying to connect: Post http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.23/auth: open //./pipe/docker_engine: The system cannot find the file specified.** since i am using windows i don't have terminal.Your help should be appreciable.",2016-07-30T22:00:27.677,3656056,CC BY-SA 3.0,True,1,,.,0,"it shows login succeeded but while running in ""git bash"" it throws following error Warning: '-e' is deprecated, it will be removed soon.",False,False,38678368.0,38678141.0,2016-07-30T20:50:06.540,0.0,"<p>Your command is not pointing to your ECR endpoint, but to DockerHub. Using Linux, normally I would simply run:</p>

<pre><code>$ eval $(aws ecr get-login --region us-west-2)
</code></pre>

<p>This is possible because the <code>get-login</code> command is a wrapper that retrieves a new authorization token and formats the <code>docker login</code> command. You only need to execute the formatted command (in this case with <code>eval</code>)</p>

<p>But if you really want to run the <code>docker login</code> manually, you'll have to specify the authorization token and the endpoint of your repository:</p>

<pre><code>$ docker login -u AWS -p &lt;password&gt; -e none https://&lt;aws_account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com
</code></pre>

<p>Where <code>&lt;password&gt;</code> is actually the authorization token (which can be generated by the <code>aws ecr get-authorization-token</code> command).</p>

<p>Please refer to the documentation for more details: <a href=""http://docs.aws.amazon.com/cli/latest/reference/ecr/index.html"" rel=""nofollow"">http://docs.aws.amazon.com/cli/latest/reference/ecr/index.html</a></p>
",6,CC BY-SA 3.0,38678141.0,2016-07-30T20:19:30.150,0.0,778.0,2016-07-31T17:22:25.673,2016-07-31T17:22:25.673,Docker unable to connect AWS EC2 cloud,<amazon-web-services><amazon-ec2><docker><spring-boot><devops>,1.0,3.0,,,
64950818,35420319,0,It appears that `RequestHandler` is now deprecated,2016-08-05T13:21:55.643,653708,CC BY-SA 3.0,True,1,,deprecated,0,It appears that `RequestHandler` is now deprecated,False,False,35420319.0,31789078.0,2016-02-15T22:33:08.233,0.0,"<p>Your code caused the following exception in CloudWatch log,</p>

<p><strong>Class does not implement an appropriate handler interface</strong>....</p>

<p>Once I changed the code to the following, I can get the SNS message just fine.</p>

<pre><code>public class RecomFunction implements RequestHandler&lt;SNSEvent, Void&gt; {

    public Void handleRequest(SNSEvent event, Context context) {

        ...
        return null;
    }

}
</code></pre>
",2,CC BY-SA 3.0,31789078.0,2015-08-03T13:47:56.850,6.0,8633.0,2017-05-23T10:29:24.320,2016-05-14T04:24:06.213,AWS DynamoDB trigger using Lambda in JAVA,<java><amazon-web-services><stream><amazon-dynamodb><aws-lambda>,5.0,1.0,,1.0,
64962400,35068909,0,It's a shame all of the Wercker tutorials are outdated. I fell victim to the same problem when deploying my Jekyll project in Wercker. Thanks for posting the answer.,2016-08-05T18:54:48.547,1508105,CC BY-SA 3.0,True,1,,.,0,It's a shame all of the Wercker tutorials are outdated.,False,False,35068909.0,35067388.0,2016-01-28T18:11:31.233,7.0,"<p>Solved!</p>

<p>It appears that the tutorial is outdated.</p>

<p>I needed to update wercker.yml to work with Wercker v2. </p>

<p>To do this, I changed:
<code>box: wercker/ruby</code>
to
<code>box: ruby</code>.</p>
",2,CC BY-SA 3.0,35067388.0,2016-01-28T16:55:35.460,3.0,570.0,2020-06-20T09:12:55.060,2016-01-28T18:11:31.233,Wercker: Build failing on 'set up environment'. Why?,<ruby><amazon-web-services><amazon-s3><jekyll><wercker>,1.0,0.0,,,
64995440,38814083,1,"I would be suspicious of code that old. In addition to needing to be updated for ARC, It's likely to use deprecated frameworks and be otherwise out of date. AWS is still actively supported. You should look for updated sample code rather than working with code that's that outdated.",2016-08-07T12:24:57.310,205185,CC BY-SA 3.0,True,2,,.,0,"In addition to needing to be updated for ARC, It's likely to use deprecated frameworks and be otherwise outdated.",False,False,38814083.0,38813313.0,2016-08-07T12:22:01.453,2.0,"<p>The error message is correct: the call to <code>retain</code> is no longer valid. </p>

<p><a href=""https://developer.apple.com/library/ios/releasenotes/ObjectiveC/RN-TransitioningToARC/Introduction/Introduction.html"" rel=""nofollow"">Automatic Reference Counting</a>, or ARC, was added to the Objective-C language in 2011 with iOS 5 and Mac OS X 10.7. Prior to ARC, you had to manage the memory usage of your app manually with calls to methods such as <code>-retain</code>, <code>-release</code>, and <code>-autorelease</code>. ARC manages these calls automatically at compile time, and, as such, does not allow you to call them yourself.</p>

<p>As @Paulw11 mentions in his comment, you should be able to replace that line with</p>

<pre><code>parentViewController = aViewController
</code></pre>

<p>and ARC will do the correct thing automatically.</p>
",2,CC BY-SA 3.0,38813313.0,2016-08-07T10:44:03.760,-1.0,688.0,2016-08-07T12:53:14.447,2016-08-07T15:54:42.097,ARC forbids explicit message send of 'retain',<ios><objective-c><amazon-web-services>,1.0,2.0,,,
65174259,38862314,1,AWSCognitoLoginProviderKey.Facebook.rawValue is deprecated,2016-08-12T00:07:24.483,4396943,CC BY-SA 3.0,True,1,,deprecated,0,Facebook.rawValue is deprecated,False,False,38862314.0,38780852.0,2016-08-10T00:09:08.910,0.0,"<p>The key in your logins map should be <strong>graph.facebook.com</strong>. Try using <code>AWSCognitoLoginProviderKey.Facebook.rawValue</code> instead of <code>AWSIdentityProviderFacebook</code></p>
",3,CC BY-SA 3.0,38780852.0,2016-08-05T03:56:00.470,1.0,1969.0,,2018-06-11T06:10:50.793,ios swift AWS cognito and Facebook Authentication,<ios><swift><amazon-web-services><aws-sdk><facebook-sdk-4.0>,3.0,0.0,,1.0,
65314033,10899935,0,"@nurettin is it outdated or not? The year is 2016 and have Amazon provided with an API for placing orders? I've seen their Order API, but it seems to only provide readable actions, not writable.",2016-08-16T17:06:29.570,1014588,CC BY-SA 3.0,True,1,,?,0,@nurettin is it outdated or not?,False,False,,,,,,,,,,,,,,,,,,,,
65325902,38986073,0,"I checked again, and the version of WSGI I found from the error log was actually outdated (since I scrolled very far up in the error log to find that line). I actually had installed a WSGI version from source for python 2.7 between then and now, which is why the code ends up running.",2016-08-17T01:04:15.113,1376311,CC BY-SA 3.0,True,1,,.,0,"I checked again, and the version of WSGI I found from the error log was actually outdated (since I scrolled very far up in the error log to find that line).",False,False,38986073.0,38982107.0,2016-08-16T23:39:13.300,0.0,"<p>Your setup is broken because mod_wsgi is compiled for Python 2.6 and not specifically for the Python 2.7 installation you want to use. You should not force the <code>site-packages</code> and <code>dict-packages</code> from your Python 2.7 installation into the module search path for what is a Python 2.6 environment. First up you are still running the wrong Python version and second, any extension modules in those directories will likely fail and possibly crash the processes.</p>

<p>You must uninstall the mod_wsgi you are using from system packages and install a version compiled for Python 2.7. Because you are using a non standard Python installation you likely will need to build mod_wsgi from source code.</p>
",3,CC BY-SA 3.0,38982107.0,2016-08-16T18:32:16.987,0.0,617.0,2016-08-17T01:05:07.473,2016-08-17T01:05:07.473,ImportError: No module named cv2 — WSGI + python + apache,<python><apache><opencv><amazon-web-services><amazon-ec2>,2.0,0.0,,,
65330568,38986651,0,"It is not outdated per se because it is still valid for IAM authentication, which we are using, and IAM authentication is still used for social logins (Facebook etc.)",2016-08-17T06:08:41.573,495796,CC BY-SA 3.0,True,1,It,),1,"It is not outdated per se because it is still valid for IAM authentication, which we are using, and IAM authentication is still used for social logins (Facebook etc.)",False,False,38986651.0,38897266.0,2016-08-17T01:04:32.693,1.0,"<p>API gateway now has native integration with 'Cognito Your User Pool', so you can pass the identity token directly - <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html"" rel=""nofollow"">api gateway docs</a>. The post you have linked is outdated</p>
",2,CC BY-SA 3.0,38897266.0,2016-08-11T13:05:49.980,6.0,4013.0,,2019-04-09T10:33:31.400,Sign AWS requests using Cognito Your User Pool user using Postman,<amazon-web-services><postman><amazon-iam><amazon-cognito><aws-api-gateway>,2.0,0.0,,2.0,
65675374,38495671,0,"Still, the solution you suggested requires all configuration (secret keys, etc) for all deployments (dev, staging, production) to be in the repository. This violates the separation between code and configuration and is typically discouraged due to the (obvious) security implications involved",2016-08-26T12:40:30.633,2756682,CC BY-SA 3.0,True,1,,involved,0,This violates the separation between code and configuration and is typically discouraged due to the (obvious) security implications involved,False,False,,,,,,,,,,,,,,,,,,,,
65682181,39152781,1,"Please edit with more information. Code-only and ""try this"" answers are discouraged, because they contain no searchable content, and don't explain why someone should ""try this"".",2016-08-26T15:32:24.343,4579155,CC BY-SA 3.0,True,1,answers,.,1,"Code-only and ""try this"" answers are discouraged, because they contain no searchable content, and don't explain why someone should ""try this"".",False,False,39152781.0,39108234.0,2016-08-25T18:53:48.103,1.0,"<pre><code>// Check if it is PutItem event
if (record.eventName == ""INSERT"") {
    ...
    // Get item's id
    var id = record.dynamodb.Keys.Id.N;
    ...
}
</code></pre>
",1,CC BY-SA 3.0,39108234.0,2016-08-23T18:22:38.360,1.0,598.0,2016-08-24T13:31:00.293,2016-08-25T18:53:48.103,How to detect PutItem operation in DynamoDB using lambda trigger?,<amazon-web-services><amazon-dynamodb><aws-lambda><amazon-sns>,1.0,0.0,,,
65934539,39299398,0,"Aah yes I am ... as I was alluding to, my app server is responsible for user authentication, so I have my own app-specific username, etc. Sorry one more question - is this the right blog for me to follow / implement the flow I mentioned. I ask because I couldn't locate BFTask and read somewhere that its been deprecated...? Thanks again!   http://mobile.awsblog.com/post/Tx1YVAQ4NZKBWF5/Amazon-Cognito-Announcing-Developer-Authenticated-Identities",2016-09-02T20:30:06.007,5216913,CC BY-SA 3.0,True,1,,?,1,I ask because I couldn't locate BFTask and read somewhere that its been deprecated...?,False,False,39299398.0,39299068.0,2016-09-02T19:18:45.213,2.0,"<p>The identity pool id is normally embedded in the app, you're not missing a step on that front. The risk can be alleviated by using your auth and unauth roles to tightly define what each user and type of user is allowed to do.</p>

<p>The credentials provider automatically manages credentials for you. When they're expired, it will get new ones and use those. All you have to do is pass it on to whatever AWS client your app needs at the time.</p>
",5,CC BY-SA 3.0,39299068.0,2016-09-02T18:55:12.590,0.0,115.0,,2018-10-08T23:22:23.857,Understanding AWS Cognito usage in iOS apps,<ios><amazon-web-services><amazon-s3><ejabberd><amazon-cognito>,1.0,0.0,,,
66007911,30131929,2,"@theLastNightTrain - Agreed, it honestly pisses me off. Furthermore, all the documentation is months out of date and none of the examples are relevant anymore. They also don't deprecate things, they just remove entire classes which breaks everyone's code when they run a pod update. The more I try to work with the AWS Mobile SDK the more I think it's some cruel joke on us devs actually trying to work with it.",2016-09-05T22:26:17.127,793709,CC BY-SA 3.0,True,1,documentation,.,0,"Furthermore, all the documentation is months outdated and none of the examples are relevant anymore.",False,False,,,,,,,,,,,,,,,,,,,,
66110914,36037124,1,"@MarkusKöhler: Correct, had the same issue. Their docs on some places say otherwise: http://docs.aws.amazon.com/lambda/latest/dg/setup-awscli.html (probably outdated)",2016-09-08T13:35:23.837,682114,CC BY-SA 3.0,True,1,,),0,Their docs on some places say otherwise: http://docs.aws.amazon.com/lambda/latest/dg/setup-awscli.html (probably outdated),False,False,,,,,,,,,,,,,,,,,,,,
66663678,35991573,0,"The problem is, this is now deprecated.",2016-09-24T23:01:51.903,2199852,CC BY-SA 3.0,True,1,,.,0,"The problem is, this is now deprecated.",False,False,35991573.0,35986973.0,2016-03-14T15:27:42.493,2.0,"<p>Somehow <code>python-amazon-product-api</code> is not considering the config file i.e. <code>~/.amazon-product-api</code>. Alternatively you can specify your credentials directly when instantiating <code>API</code> object by</p>

<pre><code>api = API(AWS_KEY, SECRET_KEY, ""us"")
</code></pre>
",1,CC BY-SA 3.0,35986973.0,2016-03-14T11:59:48.747,0.0,432.0,2016-03-14T12:02:02.270,2016-03-14T15:27:42.493,Python: Invalid AWS Access key error,<php><python><amazon-web-services>,1.0,7.0,,1.0,
66799415,34078869,4,The LITERAL type is being deprecated... which unfortunately makes Alexa look even more limited compared to its competitors.,2016-09-28T15:23:26.530,600548,CC BY-SA 3.0,True,1,type,.,0,The LITERAL type is being deprecated... which unfortunately makes Alexa look even more limited compared to its competitors.,False,False,,,,,,,,,,,,,,,,,,,,
66853620,39779480,0,The API docs don't seem to indicate the changeObjectStorageClass method is deprecated. Do you have a link to where you found this information?,2016-09-29T20:46:50.003,1448412,CC BY-SA 3.0,True,1,,.,1,The API docs don't seem to indicate the changeObjectStorageClass method is deprecated.,False,False,39779480.0,39737414.0,2016-09-29T20:32:17.197,2.0,"<p>The accepted solution works, but the method <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3.html#changeObjectStorageClass-java.lang.String-java.lang.String-com.amazonaws.services.s3.model.StorageClass-"" rel=""nofollow"">changeObjectStorageClass</a> is actually deprecated.
The deprecation note suggests to use the method <a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3.html#copyObject-com.amazonaws.services.s3.model.CopyObjectRequest-"" rel=""nofollow"">copyObject</a>, as in the following snippet:</p>

<pre><code>    CopyObjectRequest copyObjectRequest= 
        new CopyObjectRequest(bucket, key, bucket, key)
        .withStorageClass(StorageClass.StandardInfrequentAccess);
    s3.copyObject(copyObjectRequest);
</code></pre>

<p>I could not notice any performance difference between the two approaches.</p>
",2,CC BY-SA 3.0,39737414.0,2016-09-28T03:10:19.277,1.0,593.0,,2016-09-29T20:32:17.197,How to change storage class of an S3 object with the Java SDK?,<java><amazon-web-services><amazon-s3>,2.0,0.0,,,
66856454,37250663,0,"I don't think think this works anymore. ""DepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.""",2016-09-29T22:52:50.057,1718505,CC BY-SA 3.0,True,1,,.,0,DepInterpreter(%dep) deprecated.,False,False,37250663.0,36769539.0,2016-05-16T09:29:09.430,5.0,"<p>You could download jars from S3 and put it on the local FS. It could be done inside <em>%dep</em> interpreter like this:</p>

<pre class=""lang-scala prettyprint-override""><code>%dep
import com.amazonaws.services.s3.AmazonS3Client
import java.io.File
import java.nio.file.{Files, StandardCopyOption}

val dest = ""/tmp/dependency.jar""
val s3 = new AmazonS3Client()
val stream = s3.getObject(""buckename"", ""path.jar"").getObjectContent

Files.copy(stream, new File(dest).toPath, StandardCopyOption.REPLACE_EXISTING)

z.load(dest)
</code></pre>

<p><em>Note:</em> You must generate fat jar, i.e. include all custom dependencies not provided by default (for example when you have multiple modules in your project). In maven it could be implemented with maven-shade-plugin:</p>

<pre><code>&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
    &lt;version&gt;2.4.2&lt;/version&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
                &lt;goal&gt;shade&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
                &lt;artifactSet&gt;
                    &lt;includes&gt;
                        &lt;include&gt;com.yourcompany:*&lt;/include&gt;
                    &lt;/includes&gt;
                &lt;/artifactSet&gt;
            &lt;/configuration&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
&lt;/plugin&gt;
</code></pre>
",4,CC BY-SA 3.0,36769539.0,2016-04-21T12:23:46.203,2.0,5033.0,,2019-06-24T23:45:41.813,How to use dependencies from S3 in Zeppelin?,<amazon-s3><apache-zeppelin>,1.0,0.0,,2.0,
66875003,34078869,0,"@JasonSteele It is being deprecated, but only because you can still get any value you want from the user using the custom slot type - you give a few sample values, but it will still return any phrase said by the user.",2016-09-30T11:59:34.843,1008563,CC BY-SA 3.0,True,1,It,.,0,"It is being deprecated, but only because you can still get any value you want from the user using the custom slot type - you give a few sample values, but it will still return any phrase said by the user.",False,False,,,,,,,,,,,,,,,,,,,,
67099103,9003134,0,FYI the Bamboo AWS Plugin is deprecated: https://marketplace.atlassian.com/plugins/net.utoolity.atlassian.bamboo.tasks-for-aws/server/overview.,2016-10-06T22:54:59.250,33204,CC BY-SA 3.0,True,1,Plugin,.,0,FYI the Bamboo AWS Plugin is deprecated: https://marketplace.atlassian.com/plugins/net.utoolity.atlassian.bamboo.tasks-for-aws/server/overview.,False,False,,,,,,,,,,,,,,,,,,,,
67329781,36992096,1,"@zzztimbo I took his comment to mean the workaround that was pointed out performed as well as the deprecated DirectParquetOutputCommitter (and thus better than the out of the box way of writing parquet files). But, I have yet to try it.",2016-10-13T16:23:10.313,5827767,CC BY-SA 3.0,True,1,,deprecated,0,@zzztimbo I took his comment to mean the workaround that was pointed out performed as well as the deprecated,False,False,,,,,,,,,,,,,,,,,,,,
67383420,40033120,1,"Something uses the `six` library, and it is now deprecated (or at least I often get errors). So, I find that specifically excluding it helps the upgrade succeed.",2016-10-15T00:40:04.007,174777,CC BY-SA 3.0,True,1,,.,0,"Something uses the `six` library, and it is now deprecated (or at least I often get errors).",False,False,40033120.0,40032411.0,2016-10-14T00:28:40.113,1.0,"<p>Your code worked fine for me!</p>

<p>I suggest you check that you are running the latest version of boto:</p>

<pre><code>sudo pip install boto3 --upgrade --ignore six

Successfully installed boto3-1.4.1 botocore-1.4.61 docutils-0.12 futures-3.0.5 jmespath-0.9.0 python-dateutil-2.5.3 s3transfer-0.1.7 six-1.10.0
</code></pre>
",2,CC BY-SA 3.0,40032411.0,2016-10-13T23:03:11.690,0.0,677.0,2017-05-23T12:06:56.190,2016-10-14T00:28:40.113,boto3 error unable to get tags for instances,<amazon-web-services><amazon-ec2><boto3>,1.0,0.0,,,
67500917,20890421,0,"@User23890 The base concept is that when you write to a key it will be stored on multiple replicas. Once the write succeeds on a quorum of the replicas then the call returns as successful, but some of the replicas may be out of date until they get caught up. If you read the key there's a chance you'll get an out-of-date value for a while, hence the ""eventual consistency"". You can get a consistent read immediately by reading the value from all replicas and choosing the value that is present on the majority of the nodes.",2016-10-18T17:43:44.980,2811887,CC BY-SA 3.0,True,1,,.,0,"Once the write succeeds on a quorum of the replicas then the call returns as successful, but some of the replicas may be outdated until they get caught up.",False,False,,,,,,,,,,,,,,,,,,,,
67589008,28078115,2,"Now the annotation [DynamoDBMarshalling](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBMarshalling.html) is deprecated, you can use the annotation [DynamoDBTypeConverted](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBTypeConverted.html) instead. See my answer for more information.",2016-10-20T16:45:36.597,4698204,CC BY-SA 3.0,True,1,,.,0,"Now the annotation [DynamoDBMarshalling](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBMarshalling.html) is deprecated, you can use the annotation [DynamoDBTypeConverted](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBTypeConverted.html) instead.",False,True,,,,,,,,,,,,,,,,,,,,
67685449,33974245,0,I'm still seeing the same problem that @QuantumTiger is seeing. It's unfortunate that Amazon's documentation seems to be a bit outdated.,2016-10-24T04:27:35.350,1148547,CC BY-SA 3.0,True,1,It,.,0,It's unfortunate that Amazon's documentation seems to be a bit outdated.,False,False,33974245.0,33954150.0,2015-11-28T17:28:47.633,8.0,"<p>Unfortunately there is a limitation when using TestInvoke with API Gateway proxying to Amazon S3 (and some other AWS services) within the same region. This will not be the case once deployed, but if you want to test from the console you will need to use a bucket in a different region.</p>

<p>We are aware of the issue, but I can't commit to when this issue would be resolved.</p>
",7,CC BY-SA 3.0,33954150.0,2015-11-27T09:25:28.007,6.0,6120.0,2015-11-30T18:49:01.623,2017-01-15T23:47:49.417,AWS API Gateway Method to Serve static content from S3 Bucket,<amazon-web-services><amazon-s3><aws-api-gateway>,3.0,0.0,,2.0,
68025536,37900890,0,Are someone able to call Lambda in async mode? It seems this doc is outdated with current AWS Gateway/Lambda interface.,2016-11-02T18:10:19.517,656984,CC BY-SA 3.0,True,1,,.,0,It seems this doc is outdated with current AWS Gateway/Lambda interface.,False,False,37900890.0,33861072.0,2016-06-18T19:32:36.420,4.0,"<p>As of Apr/2016 is it is possible to create async Lambda execution through API Gateway by using AWS Service Proxy. See <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-lambda.html"" rel=""nofollow"">http://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-lambda.html</a></p>
",1,CC BY-SA 3.0,33861072.0,2015-11-22T22:33:58.890,7.0,4446.0,2019-04-28T15:41:37.157,2019-04-28T15:41:37.157,Handing back a response to API Gateway from Lambda,<amazon-web-services><aws-lambda><aws-api-gateway>,3.0,0.0,,2.0,
68301199,40530478,0,"1. It's an old blog post and even has a warning at the top that says it might be outdated, so don't take his advice on using the IAM service via the AWS CLI. That's just the old method that you had to use before the ACM service became available. 2. He is telling you ""paste the output from the following commands"" into some files, and that is also exactly what you need to paste into the AWS ACM web console. 3. If you look at the certificate chain command, he is actually chaining two commands together to create the certificate chain from those two files.",2016-11-10T14:59:34.957,13070,CC BY-SA 3.0,True,1,,.,1,"It's an old blog post and even has a warning at the top that says it might be outdated, so don't take his advice on using the IAM service via the AWS CLI.",False,False,40530478.0,40530052.0,2016-11-10T14:50:11.507,2.0,"<p>First, don't do this through the old IAM service method, use the new AWS ACM service to manage the certificate. The ACM service also has a nice web interface for doing this. </p>

<p>The company you got the cert from should be able to tell you which file is the chain file, and yes the chain file is required if you want browsers to properly see your SSL certificate as valid. I did a little searching and found <a href=""http://www.robertbrewitz.com/2014/09/aws-and-setting-up-a-custom-ssl-certificate/"" rel=""nofollow noreferrer"">this blog post</a> which discusses using Digicert certificates with AWS. It looks like your files are as follows:</p>

<ul>
<li>wildcard_example_com.key = private key file</li>
<li>wildcard_example_com.crt = public key file</li>
<li>DigiCertCA2.pem and TrustedRoot.pem combined = certificate chain</li>
</ul>
",3,CC BY-SA 3.0,40530052.0,2016-11-10T14:31:10.083,1.0,139.0,2016-11-10T14:39:29.970,2016-11-10T14:50:11.507,Terminating SSL at an AWS ELB instance,<amazon-web-services><ssl><amazon-elb><pki>,1.0,0.0,,,
68680195,40725792,0,THANK YOU SO MUCH! Adding the `OrdinaryCallingFormat()` in the parameter works!! I couldn't believe there is no mentioning of that in the doc or anywhere... Maybe Boto doc for 2.43 (the one that I linked in my question above) is outdated??,2016-11-21T17:58:57.950,1330974,CC BY-SA 3.0,True,1,one,?,0,Maybe Boto doc for 2.43 (the one that I linked in my question above) is outdated??,False,False,,,,,,,,,,,,,,,,,,,,
68719313,3434952,0,SimpleDB seems to be deprecated now; it's fallen off the AWS console and isn't listed among their 'Services and Solutions' anymore.  Got an implementation that uses DynamoDB? :),2016-11-22T16:19:29.247,8002,CC BY-SA 3.0,True,1,, ,1,SimpleDB seems to be deprecated now; it's fallen off the AWS console and isn't listed among their 'Services and Solutions' anymore.  ,False,False,3434952.0,3431418.0,2010-08-08T15:53:44.907,11.0,"<p>Ok, I spent some time this morning playing with boto and I think I have a solution that works using SimpleDB.  You need the latest boto release so that conditional puts and consistent reads are supported.</p>

<p>Example code here: <a href=""http://pastebin.com/3XzhPqfY"" rel=""noreferrer"">http://pastebin.com/3XzhPqfY</a></p>

<p>Please post comments/suggestions.  I believe this code should be fairly safe -- my test in main() tries it with 10 threads.</p>

<p>One thing I haven't addressed is that S3 reads are not consistent (right?), so in theory a thread may be operating on an old copy of the S3 value.  It looks like there may be a workaround for that as described here:</p>

<p><a href=""http://www.shlomoswidler.com/2009/12/read-after-write-consistency-in-amazon.html"" rel=""noreferrer"">http://www.shlomoswidler.com/2009/12/read-after-write-consistency-in-amazon.html</a></p>
",1,CC BY-SA 2.5,3431418.0,2010-08-07T17:44:24.310,22.0,13302.0,,2010-08-09T21:37:38.733,Locking with S3,<locking><amazon-s3><amazon-web-services>,2.0,0.0,,12.0,
68886793,37486820,1,"The s3 clients spark uses don't support accessing different buckets with different credentials. You could cheat by using the s3n client for one set of credentials, s3a for another. There is another strongly deprecated hack of using the username:password in the URL, as in s3n://awsID@escapedkey:bucket/path

Be aware: do that and your AWS secrets will be scattered through your logs",2016-11-27T22:23:32.867,2261274,CC BY-SA 3.0,True,1,,:,0,"There is another strongly deprecated hack of using the username:password in the URL, as in s3n://awsID@escapedkey:",False,False,37486820.0,37479650.0,2016-05-27T15:07:54.710,0.0,"<p>s3n doesn't support the aws credentials stored in <code>~/.aws/credentials</code>, you should try to use hadoop 2.7 and the new hadoop s3 impl: <code>s3a</code>, it is using aws sdk.</p>

<p>not sure if the current spark release 1.6.1 works well with hadoop 2.7, but spark 2.0 is definitely no problem with hadoop 2.7 and s3a.</p>

<p>for spark 1.6.x, we made some dirty hack, with the s3 driver from EMR... you can take a look this doc: <a href=""https://github.com/zalando/spark-appliance#emrfs-support"" rel=""nofollow"">https://github.com/zalando/spark-appliance#emrfs-support</a></p>
",3,CC BY-SA 3.0,37479650.0,2016-05-27T09:20:17.607,7.0,1816.0,,2017-07-12T11:33:58.173,PySpark s3 Access with Multiple AWS Credential Profiles?,<amazon-web-services><amazon-s3><apache-spark><pyspark>,2.0,0.0,,3.0,
68890764,40836783,0,"So, the entire AWSLambdaAsyncClient is deprecated?  The Javadoc for AWSLambdaClient says that all service calls will block...",2016-11-28T03:35:27.660,4426091,CC BY-SA 3.0,True,1,AWSLambdaAsyncClient, ,0,"So, the entire AWSLambdaAsyncClient is deprecated?  ",False,False,40836783.0,40836686.0,2016-11-28T03:31:52.900,5.0,"<p>The documentation regarding the deprecation of this method is confusing. What they are trying to say is to use <code>AWSLambdaClient.invoke(request)</code>. You will need to set the <code>InvocationType</code> to <code>Event</code> on the request object in order to invoke the Lambda function without waiting for a response.</p>
",4,CC BY-SA 3.0,40836686.0,2016-11-28T03:18:44.470,8.0,3860.0,2016-11-28T04:14:03.177,2016-11-28T04:14:03.177,Replacement for AWS Lambda invokeAsync (deprecated),<java><amazon-web-services><aws-lambda><aws-sdk>,1.0,0.0,,1.0,
68890909,40836783,0,You didn't mention you were using the `AWSLambdaAsyncClient` in your question. Looking at the javadocs for that class it says only `invokeAsyncAsync()` is deprecated. The `invokeAsync()` method you are using should be fine. http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/lambda/AWSLambdaAsyncClient.html,2016-11-28T03:45:00.307,13070,CC BY-SA 3.0,True,1,,.,0,Looking at the javadocs for that class it says only `invokeAsyncAsync()` is deprecated.,False,False,40836783.0,40836686.0,2016-11-28T03:31:52.900,5.0,"<p>The documentation regarding the deprecation of this method is confusing. What they are trying to say is to use <code>AWSLambdaClient.invoke(request)</code>. You will need to set the <code>InvocationType</code> to <code>Event</code> on the request object in order to invoke the Lambda function without waiting for a response.</p>
",4,CC BY-SA 3.0,40836686.0,2016-11-28T03:18:44.470,8.0,3860.0,2016-11-28T04:14:03.177,2016-11-28T04:14:03.177,Replacement for AWS Lambda invokeAsync (deprecated),<java><amazon-web-services><aws-lambda><aws-sdk>,1.0,0.0,,1.0,
69041167,40913609,0,I saw it but it seems a little bit out of date because last update is in date 2013...,2016-12-01T15:29:56.857,1695091,CC BY-SA 3.0,True,1,,...,0,but it seems a little bit outdated because last update is in date 2013...,False,False,40913609.0,40874090.0,2016-12-01T14:49:53.690,0.0,"<p>Have you looked at the <a href=""https://developer.amazonservices.com/doc/orders/orders/v20130901/php.html/154-8806927-8947634"" rel=""nofollow noreferrer"">PHP client library</a> for the Orders API?  I use the C# version, but I would assume the PHP library is similar.  Most of the work is done for you.</p>
",2,CC BY-SA 3.0,40874090.0,2016-11-29T19:20:55.390,3.0,1080.0,2017-03-25T16:27:14.117,2017-03-25T16:27:14.117,Amazon MWS ListOrders from Scratchpad to request,<php><amazon-web-services><amazon-mws>,2.0,0.0,,,
69177578,40983938,0,"Ubuntu asks me to install the `jq`utility. However, I posted an answer to my dilemma which doesn't require this. Im giving you an upvote because `describe-instances` was the way to go, though I used `ec2-describe-instances` which is maybe an outdated API...",2016-12-05T21:50:59.170,220949,CC BY-SA 3.0,True,1,,...,0,"Im giving you an upvote because `describe-instances` was the way to go, though I used `ec2-describe-instances` which is maybe an outdated API...",False,False,,,,,,,,,,,,,,,,,,,,
69308319,41049985,0,Surprisingly the ruby gems I found for this one are all outdated and not working on recent rubies...,2016-12-08T23:06:01.410,2832282,CC BY-SA 3.0,True,1,gems,...,1,Surprisingly the ruby gems I found for this one are all outdated and not working on recent rubies...,False,False,,,,,,,,,,,,,,,,,,,,
69401418,41085178,0,yeap that is what i ended up doing. BTW the exception arose from an ID that was never on AWS (a made up testing one) and not with an obsolete id,2016-12-12T10:20:19.077,645421,CC BY-SA 3.0,True,1,,d,1,BTW the exception arose from an ID that was never on AWS (a made up testing one) and not with an obsolete id,False,False,41085178.0,41084707.0,2016-12-11T10:27:24.753,0.0,"<p>Depending of the number of instances you have, you might consider then to perform your DescribeInstances request for each individual IDs.</p>

<pre><code>$reservations = [];
foreach($this-&gt;_InstanceIDs as $anInstance){
    try {
        $requesltArray = ['Filters' =&gt; $this-&gt;_Filters, 'InstanceIds' =&gt; $anInstance];
        $aReservation = $this-&gt;_EC2Client-&gt;DescribeInstances($requesltArray)-&gt;toArray();
        $reservations[] = $aReservation;
    } catch (Ec2Exception $exc) {
        // --&gt; Delete the instance from your database?
        continue;
    }
}

dd($reservations);
</code></pre>
",5,CC BY-SA 3.0,41084707.0,2016-12-11T09:26:07.597,1.0,296.0,,2017-06-19T17:57:15.097,"AWS SDK EC2 - ""DescribeInstances"" Exception, skip instance in list",<php><amazon-web-services><exception><amazon-ec2><aws-sdk>,1.0,0.0,,,
69466383,17941145,10,"Some of this is out of date. For example, 1 is no longer true.",2016-12-13T21:43:56.793,2083907,CC BY-SA 3.0,True,1,Some,.,0,Some of this is outdated.,False,False,,,,,,,,,,,,,,,,,,,,
69625385,27917079,0,"There's also a 'policy generator' on S3 console where you can use this policy as a guide, provide * for principal, * for the keys in the resource ARN, and generate a new policy. That way even if the version is out of date, you'll get a working policy",2016-12-18T21:54:00.720,49153,CC BY-SA 3.0,True,1,,policy,0,"That way even if the version is outdated, you'll get a working policy",False,False,,,,,,,,,,,,,,,,,,,,
69650324,13351607,3,"This issue with the link being broken, fixed, and broken again highlight one possible reason why link only answers are discouraged.",2016-12-19T15:22:20.603,5437272,CC BY-SA 3.0,True,1,,.,0,"This issue with the link being broken, fixed, and broken again highlight one possible reason why link only answers are discouraged.",False,False,13351607.0,12989162.0,2012-11-12T21:10:24.113,0.0,"<p>Here is a guide on how to get a Godaddy ssl certificate working on Amazon Elastic Load Balancer (ELB) <a href=""http://cloudarch.co.uk/2011/10/elastic-load-balancer-ssl-setup-guide-pem-encoded-csr/#.UKFla2nGU_8"" rel=""nofollow noreferrer"">http://cloudarch.co.uk/2011/10/elastic-load-balancer-ssl-setup-guide-pem-encoded-csr/#.UKFla2nGU_8</a></p>
",4,CC BY-SA 3.0,12989162.0,2012-10-20T14:02:38.703,24.0,33927.0,2016-08-19T19:51:25.043,2017-08-27T02:02:43.217,How to install godaddy ssl certificate on aws elb?,<amazon-web-services><ssl><https>,6.0,1.0,,9.0,
69830436,41292202,0,You might have an outdated SDK then. Grab the latest Python SDK at https://aws.amazon.com/sdk-for-python/,2016-12-24T13:16:20.953,1958151,CC BY-SA 3.0,True,1,,.,0,You might have an outdated SDK then.,False,False,41292202.0,41292005.0,2016-12-22T21:54:43.950,5.0,"<p>You are confusing S3 metadata, stored by AWS S3 along with an object, and EXIF metadata, stored inside the file itself.</p>

<p><code>download_file()</code> doesn't get object attributes from S3. You should use <code>get_object()</code> instead: <a href=""https://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.get_object"" rel=""noreferrer"">https://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.get_object</a></p>

<p>Then you can use <code>put_objects()</code> with the same attributes to upload new file: <a href=""https://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.put_object"" rel=""noreferrer"">https://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.put_object</a></p>
",5,CC BY-SA 3.0,41292005.0,2016-12-22T21:37:05.027,2.0,1652.0,2016-12-23T15:58:32.343,2020-01-17T15:20:53.297,AWS S3 image saving loses metadata,<python><amazon-s3><python-imaging-library>,2.0,0.0,,,
69842429,41318145,1,"I will have to check which ones do by testing each one, but I can tell you that no new regions have supported owner names in quite some time. So their usage is kind of in a deprecated state. I say deprecated because you can not use owner names reliably in all regions and this is especially true for all new regions as of a few years ago. There doesn't appear to have been any official deprecation.",2016-12-25T07:33:52.913,2585788,CC BY-SA 3.0,True,2,,.,1,So their usage is kind of in a deprecated state.,False,False,,,,,,,,,,,,,,,,,,,,
70140442,41455779,0,"You are right, I spent 5 hours with an AWS support guy and finally we resolved the issue using my own httpd.conf in .ebextensions. He discouraged using BeansTalk for this kind of mess.",2017-01-04T15:31:02.293,2488478,CC BY-SA 3.0,True,1,,.,0,He discouraged using BeansTalk for this kind of mess.,False,False,41455779.0,39931198.0,2017-01-04T03:30:44.693,2.0,"<p>From my investigation, this seems only related to an initial instance install/build using Elastic Beanstalk. The ssl.conf file in question actually comes from the initial Apache install and is not part of the Elastic Beanstalk configuration.</p>

<p>The issue is happening because AWS no longer installs mod_setenvif.so as a module in httpd.conf and therefore this error is being thrown.</p>

<p>In order to remove this ongoing issue from my builds and due to the fact that I already customize my Apache configuration, I copied a working AWS httpd.conf file from another instance and added the mod_setenvif.so module to it. Using the directions at <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/java-tomcat-platform.html#java-tomcat-proxy-apache"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/java-tomcat-platform.html#java-tomcat-proxy-apache</a>, Elastic Beanstalk configuration scripts use the new httpd.conf file and the error is no longer thrown.</p>

<p>I consider this a workaround answer since the main scripts is controlled by AWS. It my be a defect in the script or it may not. Regardless, I just wanted to find a solution quickly.</p>
",2,CC BY-SA 3.0,39931198.0,2016-10-08T10:18:57.247,2.0,896.0,,2017-01-04T03:30:44.693,Apache config syntax failing in Elastic Beanstalk,<apache><amazon-elastic-beanstalk>,1.0,1.0,,,
70781548,26528986,0,Note that the prepare() method has been deprecated since Scala 2.12,2017-01-22T22:04:53.197,1173529,CC BY-SA 3.0,True,1,,2.12,0,Note that the prepare() method has been deprecated since Scala 2.12,False,False,26528986.0,26005304.0,2014-10-23T13:15:55.547,5.0,"<p>If you create your context like this (do not copy-paste this blindly - it's configured for blocking operations):</p>

<pre><code>val blockingContext: ExecutionContext = {
    val executor = new ThreadPoolExecutor(100, 100, 1, TimeUnit.MINUTES, new LinkedBlockingQueue(1000))
    executor.allowCoreThreadTimeOut(true)
    ExecutionContext.fromExecutorService(executor) // main part
}
</code></pre>

<p>Then you will be able to get <code>ExecutorService</code> instance from it:</p>

<pre><code>val executor: ExecutorService = blockingContext.prepare().asInstanceOf[ExecutorService]
</code></pre>
",3,CC BY-SA 3.0,26005304.0,2014-09-23T21:52:21.677,3.0,1342.0,,2014-10-23T13:15:55.547,Use Scala ExecutionContext when 3rd Party Library asks for ExecutorService,<java><scala><amazon-s3><playframework-2.3>,2.0,1.0,,,
70911478,19983670,0,This code seems outdated. Could you update it using boto3? Thanks!,2017-01-26T00:07:43.393,364772,CC BY-SA 3.0,True,1,,.,0,This code seems outdated.,False,False,19983670.0,19838506.0,2013-11-14T16:56:38.520,2.0,"<p>After initially requesting spot instances using the <code>request_spot_instances</code> method, you need to then monitor the progress of your request by periodically calling <code>get_all_spot_instance_requests</code> to see if your request has been fulfilled.  For example, this call:</p>

<pre><code>import boto.ec2
conn = boto.ec2.connect_to_region('us-west-2')
fulfilled = conn.get_all_spot_instance_requests(filters={'status-code': 'fulfilled'})
</code></pre>

<p>Would return a list of spot instance requests that have been fulfilled.  Each of the <code>SpotInstanceRequest</code> objects in that list will have an attribute called <code>instance_id</code> which will be the ID of the instance created by the spot instance request.  To turn that into an Instance object, do something like this:</p>

<pre><code>reservations = conn.get_all_instances(instance_ids=fulfilled[0].instance_id)
instance = reservations[0].instances[0]
</code></pre>

<p>The <code>instance</code> variable should now be an Instance object representing the instance created in your spot instance requests.</p>
",1,CC BY-SA 3.0,19838506.0,2013-11-07T14:32:14.023,0.0,889.0,,2013-11-14T16:56:38.520,SpotInstanceRequest to Instance object?,<python><amazon-web-services><sdk><amazon-ec2><boto>,1.0,0.0,,1.0,
71032699,41921604,0,"strangely enough, the EC2 instance that is spun up has an outdated CLI, so the sync command isn't working. How do I ensure it has an updated CLI?",2017-01-29T21:24:03.337,981177,CC BY-SA 3.0,True,1,,.,1,"strangely enough, the EC2 instance that is spun up has an outdated CLI, so the sync command isn't working.",False,False,,,,,,,,,,,,,,,,,,,,
71506092,42162916,0,"`Expires` is also read by browsers, but it is deprecated in favor of `Cache-Control`.",2017-02-11T00:07:04.587,1695906,CC BY-SA 3.0,True,1,,.,0,"`Expires` is also read by browsers, but it is deprecated in favor of `Cache-Control`.",False,True,42162916.0,42162153.0,2017-02-10T15:23:55.273,3.0,"<p>That info is <a href=""http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html"" rel=""nofollow noreferrer"">used by CloudFront</a> to know how long to cache objects from S3. Or if you are serving a static website from S3 without CloudFront, it would be used by browsers to determine how long to cache a file in the local browser cache.</p>
",1,CC BY-SA 3.0,42162153.0,2017-02-10T14:46:07.153,1.0,1437.0,,2017-02-10T15:23:55.273,"How S3 putObject ""expires"" works?",<php><amazon-web-services><amazon-s3>,1.0,0.0,,,
71755770,42279299,0,"Any idea, what is the working version for the fix you suggsted.Later version deprecated many things.",2017-02-17T14:41:39.743,7395437,CC BY-SA 3.0,True,1,,.,0,Later version deprecated many things.,False,False,,,,,,,,,,,,,,,,,,,,
72246905,42553939,0,"Yes agreed, this class does't have this parameter. However, class Amazon.DynamoDBv2.Model.QueryRequest class has parameter ProjectionExpression and clearly mentioned that it is a deprecated parameter. http://docs.aws.amazon.com/sdkfornet/v3/apidocs/Index.html. Gets and sets the property AttributesToGet.
This is a legacy parameter. Use ProjectionExpression instead. For more information, see AttributesToGet in the Amazon DynamoDB Developer Guide.",2017-03-02T13:07:39.633,6337748,CC BY-SA 3.0,True,1,,.,0,QueryRequest class has parameter ProjectionExpression and clearly mentioned that it is a deprecated parameter.,False,False,42553939.0,42553441.0,2017-03-02T11:16:40.883,0.0,"<p>Firstly, you are using the legacy <code>AttributesToGet</code> parameter. </p>

<blockquote>
  <p>This is a legacy parameter. Use ProjectionExpression instead.</p>
</blockquote>

<p><strong>Answer:-</strong></p>

<p>DynamoDB doesn't guarantee the order of the attributes in results. Primarily, DynamoDB is a <strong>key-value store</strong>. You need to get the value for the specific key from the result.</p>

<p>Please note that this is not like SELECT statement on RDBMS. This is a NoSQL database and it is completely different from RDBMS.</p>
",3,CC BY-SA 3.0,42553441.0,2017-03-02T10:54:35.357,0.0,470.0,,2017-03-02T11:16:40.883,What is the order of Attributes in the result of the Query in DynamoDB (.net)?,<amazon-dynamodb>,1.0,0.0,,1.0,
72257726,11090727,4,This answer is now out of date. See Rohit's answer below.,2017-03-02T17:17:32.393,839128,CC BY-SA 3.0,True,1,answer,.,0,This answer is now outdated.,False,False,11090727.0,10700870.0,2012-06-18T20:43:25.177,3.0,"<p>I don't believe it is possible to store an ordered list as an attribute, as DynamoDB only supports single-valued and (unordered) set attributes.  However, the performance overhead of storing a string of comma-separated values (or some other separator scheme) is probably pretty minimal given the fact that all the attributes for row must together be under 64KB.</p>

<p>(source: <a href=""http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/DataModel.html"" rel=""nofollow"">http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/DataModel.html</a>)</p>
",2,CC BY-SA 3.0,10700870.0,2012-05-22T11:10:37.890,13.0,15071.0,,2016-11-29T00:27:13.317,DynamoDB ordered list,<amazon-dynamodb>,3.0,0.0,,1.0,
72349651,26272328,0,Is this blog post out of date now? The code in it doesn't look like anything else I've seen anywhere,2017-03-05T15:28:50.583,5144469,CC BY-SA 3.0,True,1,post,?,0,Is this blog post outdated now?,False,False,26272328.0,14087070.0,2014-10-09T07:09:02.193,3.0,"<p>DynamoDB now supports json object direct storing. read: <a href=""http://aws.amazon.com/blogs/aws/dynamodb-update-json-and-more/"" rel=""nofollow"">http://aws.amazon.com/blogs/aws/dynamodb-update-json-and-more/</a></p>
",1,CC BY-SA 3.0,14087070.0,2012-12-30T00:01:08.927,9.0,14778.0,2017-07-03T19:49:22.000,2017-07-06T16:18:41.290,DynamoDB + Store Values as Items or JSON,<database-design><amazon-dynamodb><amazon>,6.0,0.0,,1.0,
72423972,19126090,1,"Granting permission for ""Authenticated Users"" - IS NOT SECURE !
It lets access to your bucket for everyone (just need any aws account).
You need to spend huge amount of time trying to find working solution for you.
Access validation is so buggy on aws. It consist from 3 parts:
1) IAM role permissions
2) Bucket access policies.
3) ACL - obsolete system - that just ruining whole security and able to compromise your data.

Everything is just multiplying with bugs in APIs like boto.
I really recommend to switch to other system than s3.",2017-03-07T12:58:45.553,4182817,CC BY-SA 3.0,True,1,,"

",0,"3) ACL - obsolete system - that just ruining whole security and able to compromise your data.

",False,True,19126090.0,12412115.0,2013-10-01T21:16:31.640,6.0,"<p>Some things to check...</p>

<ul>
<li><p>If you are using boto, be sure you are using conn.get_bucket(bucket_name) to access only the bucket you have permission to access.</p></li>
<li><p>In your IAM (user) policy, if you are restricting access to a single 
bucket, be sure that the policy includes adequate permissions to the 
bucket and do not include a trailing slash+asterisks for the ARN name (see example below).</p></li>
<li><p>Be sure to set ""Upload/Delete"" permissions for ""Authenticated Users"" in S3 for the bucket.</p></li>
</ul>

<p><strong>Permissions sample:</strong></p>

<p><img src=""https://i.stack.imgur.com/r6p5H.png"" alt=""enter image description here""></p>

<p><strong>IAM policy sample:</strong></p>

<p><em>NOTE: The SID will be automatically generated when using the policy generator</em></p>

<pre><code>{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": [
        ""s3:*""
      ],
      ""Sid"": ""Stmt0000000000001"",
      ""Resource"": [
        ""arn:aws:s3:::myBucketName""
      ],
      ""Effect"": ""Allow""
    }
  ]
}
</code></pre>
",4,CC BY-SA 3.0,12412115.0,2012-09-13T17:59:47.377,6.0,7227.0,,2018-01-20T01:46:30.753,Why might boto be denied access to S3 with proper IAM keys?,<amazon-s3><boto><amazon-iam>,3.0,0.0,,,
72471455,42672325,0,"`context.fail()` is the old, deprecated method. `callback(err)` is the new recommended method.",2017-03-08T14:05:18.073,13070,CC BY-SA 3.0,True,1,,.,0,"`context.fail()` is the old, deprecated method.",False,False,42672325.0,42672023.0,2017-03-08T13:13:00.190,-1.0,"<p>Use context.fail() instead of callback(err) to get handler failing</p>
",2,CC BY-SA 3.0,42672023.0,2017-03-08T12:57:46.140,4.0,2250.0,2018-07-10T19:10:50.983,2020-01-07T12:40:40.173,Why is AWS.Lambda.invoke `error` callback argument never populated?,<javascript><node.js><amazon-web-services><lambda>,3.0,4.0,,2.0,
72508088,13293041,1,"small comment: replace the version string with ""2012-10-17"" as it's obsolete",2017-03-09T10:30:15.843,124110,CC BY-SA 3.0,True,1,,obsolete,0,"small comment: replace the version string with ""2012-10-17"" as it's obsolete",False,False,,,,,,,,,,,,,,,,,,,,
72519588,35627652,0,"`sudo` is deprecated in ansible, `become` and `become_user` are supported to be used.",2017-03-09T15:12:43.710,99834,CC BY-SA 3.0,True,1,sudo,.,0,"`sudo` is deprecated in ansible, `become` and `become_user` are supported to be used.",False,False,35627652.0,35597076.0,2016-02-25T12:41:46.390,3.0,"<p>I have found the following solutions for myself:<br>
 1. Change requiretty in <code>/etc/sudoers</code> with <code>sed</code> run playbooks and change it back.</p>

<pre><code> ""#!/bin/bash\n"", ""\n"", ""  
 echo 'Installing Git'\n"","" 
 yum --nogpgcheck -y install git ansible htop nano wget\n"",  
 ""wget https://s3.eu-central-1.amazonaws.com/xx/ansible -O /root/.ssh/id_rsa\n"",  
 ""chmod 600 /root/.ssh/id_rsa\n"",   
  ""ssh-keyscan 172.31.9.231 &gt;&gt; /root/.ssh/known_hosts\n"",   
  ""git clone git@172.31.5.254:somerepo/dev.git /root/dev\n"",   
  ""sed -i 's/Defaults    requiretty/Defaults    !requiretty/g' /etc/sudoers\n"", 
  ""\n"", 
  ""ansible-playbook /root/dev/env/ansible/uk.yml\n"",   
  ""\n"",   
  ""sed -i 's/Defaults    !requiretty/Defaults    requiretty/g' /etc/sudoers\n""   
</code></pre>

<p>OR
2.  In ansible playbook specify variable:</p>

<pre><code> - name: Setup 
 hosts: 127.0.0.1
 connection: local
 sudo: {{ require_sudo }}
 roles:
    - generic
</code></pre>

<p>Run in AWS Cloud Formation template would be  </p>

<pre><code>   ""ansible-playbook -e require_sudo=False /root/dev/env/ansible/uk.yml\n""  
</code></pre>

<p>And for Vagrant in ansible.cfg it can be specified  </p>

<pre><code>  require_sudo=True
</code></pre>

<ol start=""3"">
<li><p>Also in CF template may identify who is running and the pass variable  </p>

<p><code>ansible-playbook -e$(id -u |egrep '^0$' &gt; /dev/null &amp;&amp; require_sudo=False || require_sudo=True; echo ""require_sudo=$require_sudo"") /apps/ansible/uk.yml</code></p></li>
</ol>
",1,CC BY-SA 3.0,35597076.0,2016-02-24T08:36:43.543,10.0,16640.0,2016-02-24T10:10:58.877,2020-06-12T17:08:54.957,"ansible sudo: sorry, you must have a tty to run sudo",<ansible><sudo><amazon-cloudformation>,3.0,1.0,,1.0,
72528170,10736262,0,"This answer is outdated. Safari now allows file uploads, but S3 appears to require an alternative file upload method that is still not supported for Safari on iOS devices.",2017-03-09T18:43:39.673,3305145,CC BY-SA 3.0,True,1,answer,.,0,This answer is outdated.,False,False,,,,,,,,,,,,,,,,,,,,
72563802,8257583,0,"Update for 2017, almost all of the omegahat Amazon packages are woefully out of date and may not work at all.  If you are comfortable with Java consider https://cran.r-project.org/web/packages/awsjavasdk/index.html.  Otherwise consider one of the cloudyr packages (https://github.com/cloudyr).",2017-03-10T15:29:03.183,169095,CC BY-SA 3.0,True,1,, ,1,Amazon packages are woefully outdated and may not work at all.  ,False,False,8257583.0,8251632.0,2011-11-24T13:08:16.717,0.0,"<p>Check  <a href=""http://www.omegahat.org/"" rel=""nofollow"">http://www.omegahat.org/</a> . There are several Amazon-related packages there, and even if Product API might not be among these, you should be able to copy the basic functions.</p>
",1,CC BY-SA 3.0,8251632.0,2011-11-24T02:39:10.027,12.0,4881.0,2016-06-08T11:48:32.900,2017-05-27T22:38:44.353,Amazon Product API with R,<r><api><rest><amazon-web-services><amazon-product-api>,5.0,0.0,,5.0,
72563802,8257583,0,"Update for 2017, almost all of the omegahat Amazon packages are woefully out of date and may not work at all.  If you are comfortable with Java consider https://cran.r-project.org/web/packages/awsjavasdk/index.html.  Otherwise consider one of the cloudyr packages (https://github.com/cloudyr).",2017-03-10T15:29:03.183,169095,CC BY-SA 3.0,True,1,, ,1,Amazon packages are woefully outdated and may not work at all.  ,False,False,8257583.0,8251632.0,2011-11-24T13:08:16.717,0.0,"<p>Check  <a href=""http://www.omegahat.org/"" rel=""nofollow"">http://www.omegahat.org/</a> . There are several Amazon-related packages there, and even if Product API might not be among these, you should be able to copy the basic functions.</p>
",1,CC BY-SA 3.0,8251632.0,2011-11-24T02:39:10.027,12.0,4881.0,2016-06-08T11:48:32.900,2017-05-27T22:38:44.353,Amazon Product API with R,<r><api><rest><amazon-web-services><amazon-product-api>,5.0,0.0,,5.0,
73220486,12724670,0,"Hi! Although this code works, the method setRequestCredentials() is showing as deprecated. Is there a newer version for this implementation available?",2017-03-28T11:42:00.340,6042824,CC BY-SA 3.0,True,1,,.,0,"Although this code works, the method setRequestCredentials() is showing as deprecated.",False,False,12724670.0,12711975.0,2012-10-04T09:52:25.880,6.0,"<pre><code>CreateUserRequest user = new CreateUserRequest(""userName"");

CreateAccessKeyRequest key = new CreateAccessKeyRequest();

BasicAWSCredentials cred = new BasicAWSCredentials(""access"", ""secret"");

key.withUserName(user.getUserName());
key.setRequestCredentials(cred);

user.setRequestCredentials(key.getRequestCredentials());
user.setPath(""/"");
AmazonIdentityManagementClient client =  new AmazonIdentityManagementClient(cred);
CreateUserResult result = client.createUser(user);
</code></pre>
",5,CC BY-SA 3.0,12711975.0,2012-10-03T15:40:31.487,6.0,2764.0,2012-10-04T09:44:22.527,2012-10-04T09:53:21.117,How to create a new user in AWS,<java><amazon-web-services><amazon-iam>,2.0,1.0,,2.0,
73364246,27231752,0,"They just added a newer version of Node.js, they now support Node.js v4.3.2 and 6.10 (v0.10.32 is now deprecated).",2017-03-31T14:43:02.663,1345244,CC BY-SA 3.0,True,1,v0.10.32,.,0,(v0.10.32 is now deprecated).,False,False,27231752.0,27220837.0,2014-12-01T15:34:15.963,4.0,"<p>According to <a href=""http://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction.html#current-supported-versions"" rel=""nofollow"">the docs</a>, Lambda currently (at the time of this writing) supports only v0.10.32. In the future they will likely have an option when creating the cloud function specify the language and version. Lambda will ensure it runs in the correct execution environment (which, by the way, is <a href=""http://alestic.com/2014/11/aws-lambda-environment"" rel=""nofollow"">probably not Docker</a>).</p>
",5,CC BY-SA 3.0,27220837.0,2014-12-01T02:29:04.380,5.0,2342.0,2017-06-30T00:49:31.677,2017-06-30T00:49:31.677,How AWS lambda supports different versions of NodeJS,<node.js><amazon-web-services><version><aws-lambda>,1.0,0.0,,1.0,
73510222,39088822,4,"For those using newer versions of Spark  `sqlContext.jsonFile(""..."")` is deprecated. Use `sqlContext.read.json(""..."")` instead.",2017-04-04T23:51:42.083,138642,CC BY-SA 3.0,True,1,,.,0,"For those using newer versions of Spark  `sqlContext.jsonFile(""..."")` is deprecated.",False,False,39088822.0,38214633.0,2016-08-22T21:13:59.777,7.0,"<p>If your JSON is uniformly structured I would advise you to give Spark the schema for your JSON files and this should speed up processing tremendously.</p>

<p>When you don't supply a schema Spark will read all of the lines in the file first to infer the schema which, as you have observed, can take a while.</p>

<p>See this documentation for how to create a schema: <a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema"" rel=""noreferrer"">http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema</a></p>

<p>Then you'd just have to add the schema you created to the jsonFile call:</p>

<pre><code>val df = sqlContext.jsonFile(""s3://testData/*/*/*"", mySchema)
</code></pre>

<p>At this time (I'm using Spark 1.6.2) it seems as if <code>jsonFile</code> has been deprecated, so switching to <code>sqlContext.read.schema(mySchema).json(myJsonRDD)</code> (where <code>myJsonRDD</code> is of type <code>RDD[String]</code>) might be preferable.</p>
",2,CC BY-SA 3.0,38214633.0,2016-07-06T00:00:08.393,5.0,11151.0,2016-07-06T02:53:41.783,2016-08-22T21:13:59.777,Which is the fastest way to read Json Files from S3 : Spark,<json><scala><apache-spark><amazon-s3><pyspark>,1.0,1.0,,3.0,
73714212,42812666,0,"For the record, the ec2ConfigService in not just not included, it is deprecated in windows 2016. This question is about the manual configuration via UserData script I showed in the original question. You only need to copy the a json file such as the one @mahdi shows here to ```C:\Program Files\Amazon\SSM\Plugins\awsCloudWatch``` and restart the ssm again, ```Restart-Service AmazonSSMAgent```",2017-04-10T13:49:57.097,558277,CC BY-SA 3.0,True,1,it,.,1,"For the record, the ec2ConfigService in not just not included, it is deprecated in windows 2016.",False,False,,,,,,,,,,,,,,,,,,,,
73823930,43380913,0,That GoDaddy documentation for creating an A record is outdated. I can't find the equivalent panel in the new UI. :(,2017-04-12T22:57:01.583,7127000,CC BY-SA 3.0,True,1,documentation,.,0,That GoDaddy documentation for creating an A record is outdated.,False,False,43380913.0,43380393.0,2017-04-12T22:56:09.400,1.0,"<p>To point your domain to a specific EC2 instance:</p>

<ul>
<li>Create an <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html"" rel=""nofollow noreferrer"">Elastic IP Address</a>. This is a static IP address that will remain associated with your EC2 instance. If you use an auto-assigned IP address, it might change when your instance is stopped/started.</li>
<li>In GoDaddy, <a href=""https://au.godaddy.com/help/add-an-a-record-19238"" rel=""nofollow noreferrer"">create an <code>A</code> record for your domain</a> that points to the Elastic IP address of your instance</li>
</ul>

<p>Alternatively, you could configure GoDaddy to send all DNS requests to <strong>Amazon Route 53</strong> and you could do the above configuration within Route 53 itself. See: <a href=""http://blog.sefindustries.com/redirect-a-subdomain-to-route-53-from-godaddy/"" rel=""nofollow noreferrer"">Old article showing how to redirect a subdomain from GoDaddy to Route 53</a></p>
",2,CC BY-SA 3.0,43380393.0,2017-04-12T22:04:55.663,-1.0,38.0,,2017-04-12T22:56:09.400,How to make a domain point to my site hosted on AWS,<amazon-web-services><iis><dns><windows-services><ip>,1.0,0.0,,,
74415745,43681991,0,"and you are right, looks like `key_file` is deprecated now.",2017-04-28T16:26:06.390,6524890,CC BY-SA 3.0,True,1,,.,0,"and you are right, looks like `key_file` is deprecated now.",False,False,,,,,,,,,,,,,,,,,,,,
74444336,36510727,0,"@JeremyThomas check the GitHub link I posted above, the code might be outdated tho",2017-04-29T19:31:27.200,6141946,CC BY-SA 3.0,True,1,code,tho,0,"@JeremyThomas check the GitHub link I posted above, the code might be outdated tho",False,False,,,,,,,,,,,,,,,,,,,,
74487679,43701148,0,Answers which are simply links to offsite blog posts are discouraged.,2017-05-01T15:56:29.180,5216668,CC BY-SA 3.0,True,1,posts,.,0,Answers which are simply links to offsite blog posts are discouraged.,False,False,,,,,,,,,,,,,,,,,,,,
74505671,43701148,3,Thanks @johni but I think that this link is deprecated. I didn't find the dynamodb-to-elasticsearch Lambda blueprint. I follwed that link and then I realized that the blueprint wasn't there. Then I saw this link: https://forums.aws.amazon.com/thread.jspa?threadID=240647. Tha's why I made the question here :),2017-05-02T06:18:10.303,3713919,CC BY-SA 3.0,True,1,,.,0,but I think that this link is deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
74603042,36339430,2,"As of January 2017, `AMAZON.LITERAL` is not deprecated anymore: https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-interaction-model-reference#literal",2017-05-04T11:20:31.560,593036,CC BY-SA 3.0,True,1,AMAZON.LITERAL,https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-interaction-model-reference#literal,1,"As of January 2017, `AMAZON.LITERAL` is not deprecated anymore: https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-interaction-model-reference#literal",False,False,,,,,,,,,,,,,,,,,,,,
74635277,43794424,0,"My AWS-CLI version was `aws-cli/1.10.34`. After upgrading to `aws-cli/1.11.82` I no longer had the issue. For me, the issue seems to be an outdated CLI installation.",2017-05-05T06:02:56.430,3166160,CC BY-SA 3.0,True,1,,.,0,"For me, the issue seems to be an outdated CLI installation.",False,False,43794424.0,43793382.0,2017-05-04T23:51:33.390,0.0,"<p>Are you sure you found the root cause because of <code>out of date</code>?</p>

<p>The problem you reported is about <a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/bucket-policy-s3-sigv4-conditions.html"" rel=""nofollow noreferrer"">Amazon S3 Signature Version 4 Authentication Specific Policy Keys</a> </p>

<p>You should be fine to fix with the command</p>

<pre><code>aws configure set profile.jacoblambert.s3.signature_version s3v4
</code></pre>

<p>or add below lines to that profile <code>[jacoblambert]</code> in ~/.aws/config</p>

<pre><code>s3 =
    signature_version = s3v4
</code></pre>

<p>Refer:</p>

<p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version"" rel=""nofollow noreferrer"">Specifying Signature Version in Request Authentication</a></p>
",2,CC BY-SA 3.0,43793382.0,2017-05-04T21:55:14.837,0.0,646.0,,2017-05-04T23:51:33.390,AWS CLI not reading config file,<amazon-web-services><aws-cli>,2.0,0.0,,,
74787179,38183310,0,"This API is deprecated. They recommend you use Invoke API,  with InvocationType set to Event  rather than the default of RequestResponse. 
http://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html",2017-05-09T17:20:53.160,1227074,CC BY-SA 3.0,True,1,API,.,0,This API is deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
74880399,10730428,1,This is outdated answer. You indeed can assign tags at instance create time - see my answer: http://stackoverflow.com/a/43723682/4988742,2017-05-11T19:04:51.313,4988742,CC BY-SA 3.0,True,1,,.,0,This is outdated answer.,False,False,,,,,,,,,,,,,,,,,,,,
74990525,43889585,0,"s3n (native s3 file system) is now deprecated, in its place s3a is to be used. In Spark 2.0, following properties need to be configured in order to make use of s3a                                                                                                                     1. fs.s3a.access.key                                                                                   
2. fs.s3a.secret.key                                                                                               3. org.apache.hadoop.fs.s3a.S3AFileSystem",2017-05-15T13:26:24.893,6093375,CC BY-SA 3.0,True,1,,.,0,"s3n (native s3 file system) is now deprecated, in its place s3a is to be used.",False,False,43889585.0,43883077.0,2017-05-10T10:16:38.887,0.0,"<p>Not sure, how did you manage to upload .part files to s3 without authentication even if you have tweaked s3 policies. I guess you might have added aws keys in the system environment as properties or in conf files.
In order to access aws resource, atleast its required to supply access key and secret key. Also, s3 scheme is deprecated now.
Following code works with hadoop-aws-2.8.0.jar and spark 2.1.
(note: I should have used s3a scheme as its the preferred over s3n (native scheme).</p>

<pre><code>val spark = SparkSession
              .builder
              .appName(""SparkS3Integration"")
              .master(""local[*]"")
              .getOrCreate()
            spark.sparkContext.hadoopConfiguration.set(""fs.s3n.awsAccessKeyId"", awsAccessKey)
            spark.sparkContext.hadoopConfiguration.set(""fs.s3n.awsSecretAccessKey"", awsSecretKey)

 val rdd = spark.sparkContext.parallelize(Seq(1,2,3,4))
 rdd.saveAsTextFile(""s3n://&lt;bucket_name&gt;/&lt;path&gt;"")
</code></pre>
",1,CC BY-SA 3.0,43883077.0,2017-05-10T03:20:01.043,2.0,2315.0,2017-05-11T15:01:58.283,2017-05-11T15:01:58.283,Saving a file to Amazon Web Service s3,<python><amazon-web-services><apache-spark><amazon-s3><amazon-emr>,2.0,0.0,,,
75002487,42298730,0,"Which part? The context functions were part of the AWS Lambda Programming Model (http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html#nodejs-prog-model-oldruntime-context-methods), but though it is outdated it still exists for backwards compatibility and you may test this point out. The `setTimeout` delay is a undocumented trivia.",2017-05-15T18:41:47.033,2029902,CC BY-SA 3.0,True,1,,compatibility,0,"The context functions were part of the AWS Lambda Programming Model (http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html#nodejs-prog-model-oldruntime-context-methods), but though it is outdated it still exists for backwards compatibility",False,False,42298730.0,42285813.0,2017-02-17T13:04:25.353,0.0,"<p>You may use the old <code>context.done</code> function to return immediately or more specifically <code>context.succeed</code>/<code>context.fail</code>. This function is still available on Node 4.</p>

<p>Though it doesn't abruptly ends the running Lambda but gives a response to the caller (like API Gateway) and keep running on the background, if needed, for at most ~15 seconds.</p>

<p>Funny extra: if you schedules a function to run a little later using <code>setTimeout</code> you have those ~15 seconds to run free of charge, because Lambda only holds charge to explicitly asynchronous functions calls.</p>
",2,CC BY-SA 3.0,42285813.0,2017-02-16T22:22:05.963,7.0,774.0,2017-02-17T07:19:49.397,2017-02-17T13:04:25.353,AWS Lamba with poor performance when using RDS,<node.js><aws-lambda><amazon-rds><serverless-framework><serverless-architecture>,2.0,0.0,,,
75043979,41103343,2,`AWSLogger` - deprecated. Use `AWSDDLog.sharedInstance.logLevel = .verbose`,2017-05-16T17:28:16.887,4747788,CC BY-SA 3.0,True,1,,.,0,`AWSLogger` - deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
75075784,33623644,0,"Amazon.EC2.Util is now deprecated, see my post above for current location of the properties in the .NET SDK",2017-05-17T12:23:56.473,4757635,CC BY-SA 3.0,True,1,EC2.Util,SDK,0,"EC2.Util is now deprecated, see my post above for current location of the properties in the .NET SDK",False,False,33623644.0,13274353.0,2015-11-10T05:40:46.260,5.0,"<p>You can also use <a href=""http://docs.aws.amazon.com/sdkfornet/v3/apidocs/Index.html"" rel=""noreferrer"">AWS's Dot Net SDK</a> for example <code>Amazon.EC2.Util.EC2Metadata.InstanceId.ToString();</code></p>
",2,CC BY-SA 3.0,13274353.0,2012-11-07T16:50:45.987,11.0,4998.0,,2019-02-06T17:54:12.070,Easiest way to get EC2 instance attributes within the instance itself,<c#><amazon-ec2><amazon-web-services>,3.0,0.0,,1.0,
75100508,33123751,1,"Great idea. A few notes, though. Unfortunately that kind of link is not official, and risks to become obsolete. This link is dead, now. Another problem is security, when not officially supported. Note that the official FFMPEG site does provide [static builds](https://ffmpeg.org/download.html), although from a third party. Problem is the compression format is `xz`. EB accepts only ZIP and TAR (gz).",2017-05-18T00:59:12.047,2050,CC BY-SA 3.0,True,1,kind,.,1,"Unfortunately that kind of link is not official, and risks to become obsolete.",False,False,,,,,,,,,,,,,,,,,,,,
75357306,44170730,0,@errate isnt put deprecated?,2017-05-25T01:09:40.207,4794571,CC BY-SA 3.0,True,1,,?,1,@errate isnt put deprecated?,False,False,,,,,,,,,,,,,,,,,,,,
75611299,44295647,0,"@prayagupd You are right about SDK API and this is exactly my point. If it holds some old stuff (e.g. for compatibility) I would expect this to be marked as deprecated. If it isn't, it is supposed to work, according to what its interface and documentation says. I really appreciate your response, but I wouldn't like to take the question/answer from the context of AWS SDK to REST API.",2017-06-01T09:02:15.747,1295402,CC BY-SA 3.0,True,1,,.,0,If it holds some old stuff (e.g. for compatibility) I would expect this to be marked as deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
75696257,5983499,6,"This response is now out of date. @Kainax's response is up to date, though.",2017-06-03T17:37:57.960,984515,CC BY-SA 3.0,True,1,response,.,0,This response is now outdated.,False,False,5983499.0,5123208.0,2011-05-12T19:37:05.963,39.0,"<p>The Amazon S3 Console now supports uploading entire folder hierarchies.  Enable the Ehanced Uploader in the Upload dialog and then add one or more folders to the upload queue.</p>

<p><a href=""http://console.aws.amazon.com/s3"" rel=""noreferrer"">http://console.aws.amazon.com/s3</a></p>
",2,CC BY-SA 3.0,5123208.0,2011-02-25T22:01:50.057,74.0,93494.0,2011-07-14T03:55:24.863,2020-03-11T05:22:17.047,Upload folder with subfolders using S3 and the AWS console,<amazon-s3>,14.0,0.0,,8.0,
75696257,5983499,6,"This response is now out of date. @Kainax's response is up to date, though.",2017-06-03T17:37:57.960,984515,CC BY-SA 3.0,True,1,response,.,0,This response is now outdated.,False,False,5983499.0,5123208.0,2011-05-12T19:37:05.963,39.0,"<p>The Amazon S3 Console now supports uploading entire folder hierarchies.  Enable the Ehanced Uploader in the Upload dialog and then add one or more folders to the upload queue.</p>

<p><a href=""http://console.aws.amazon.com/s3"" rel=""noreferrer"">http://console.aws.amazon.com/s3</a></p>
",2,CC BY-SA 3.0,5123208.0,2011-02-25T22:01:50.057,74.0,93494.0,2011-07-14T03:55:24.863,2020-03-11T05:22:17.047,Upload folder with subfolders using S3 and the AWS console,<amazon-s3>,14.0,0.0,,8.0,
75761025,8344200,3,"This is now out of date as there is no ""DB Security Groups"" section in the left panel.",2017-06-06T01:53:49.900,1053,CC BY-SA 3.0,True,1,This,.,0,"This is now outdated as there is no ""DB Security Groups"" section in the left panel.",False,False,8344200.0,8344057.0,2011-12-01T15:52:40.470,30.0,"<p>If you are using MySql on AWS via an RDS instance you must add the IP address you want to connect from to the ""DB Security Groups"". To do this go to your AWS Managment Console and select RDS.
<br />
1. Select ""DB Security Groups"" on the left panel
<br />
2. Select ""default""
<br />
3. Select ""CIDR/IP"" from the select box and enter your workstations public IP address. Example: <br />
23.234.192.123/32 (<i>dont forget the /32 for a single ip</i>)
<br />
4. Click ""Add""
<br />
5. Wait a few minutes for it to go into effect and then connect your MySql client.
<br />
<br />
This only applies for RDS instances, if you are using MySql installed on an EC2 instance then the instructions are the same as accessing MySql from any remote machine.</p>
",3,CC BY-SA 3.0,8344057.0,2011-12-01T15:42:32.213,33.0,60630.0,,2019-02-13T07:34:07.513,Connect to MySQL on AWS from local machine,<mysql><amazon-web-services><remote-access>,6.0,4.0,,15.0,
75761025,8344200,3,"This is now out of date as there is no ""DB Security Groups"" section in the left panel.",2017-06-06T01:53:49.900,1053,CC BY-SA 3.0,True,1,This,.,0,"This is now outdated as there is no ""DB Security Groups"" section in the left panel.",False,False,8344200.0,8344057.0,2011-12-01T15:52:40.470,30.0,"<p>If you are using MySql on AWS via an RDS instance you must add the IP address you want to connect from to the ""DB Security Groups"". To do this go to your AWS Managment Console and select RDS.
<br />
1. Select ""DB Security Groups"" on the left panel
<br />
2. Select ""default""
<br />
3. Select ""CIDR/IP"" from the select box and enter your workstations public IP address. Example: <br />
23.234.192.123/32 (<i>dont forget the /32 for a single ip</i>)
<br />
4. Click ""Add""
<br />
5. Wait a few minutes for it to go into effect and then connect your MySql client.
<br />
<br />
This only applies for RDS instances, if you are using MySql installed on an EC2 instance then the instructions are the same as accessing MySql from any remote machine.</p>
",3,CC BY-SA 3.0,8344057.0,2011-12-01T15:42:32.213,33.0,60630.0,,2019-02-13T07:34:07.513,Connect to MySQL on AWS from local machine,<mysql><amazon-web-services><remote-access>,6.0,4.0,,15.0,
76359057,44685468,1,"I don't think so. `Expires` is essentially deprecated in RFC-7234 in favor of `Cache-Control`.   [*""If a response includes a `Cache-Control` field with the `max-age`
   directive, a recipient `MUST` ignore the Expires
   field.""*](https://tools.ietf.org/html/rfc7234#section-5.3)     See also  https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching.",2017-06-21T22:59:37.527,1695906,CC BY-SA 3.0,True,1,Expires,  ,0,`Expires` is essentially deprecated in RFC-7234 in favor of `Cache-Control`.   ,False,False,44685468.0,44674892.0,2017-06-21T20:12:43.667,2.0,"<p><code>$params['ContentEncoding'] = 'gzip';</code> doesn't actually gzip your content -- it is used to indicate that you already gzipped it yourself before uploading it.  </p>

<p>If you set that, and you didn't gzip the content, then <code>ERR_CONTENT_DECODING_FAILED</code> would be correct.</p>

<p>You can store the content in S3 uncompressed and just let CloudFront gzip it if you set Compress Objects Automatically to Yes in the CloudFront cache behavior.</p>

<p>After setting up correctly, do a CloudFront cache invalidation for <code>*</code>, then test again.</p>

<p>Also, don't set the <code>Expires</code> header.  You almost certainly don't want that.</p>

<p>When examining the headers from CloudFront, watch for <code>Age</code>.  This is how long ago, in seconds, CloudFront fetched the object from S3.</p>
",4,CC BY-SA 3.0,44674892.0,2017-06-21T11:31:34.897,2.0,425.0,,2017-06-21T20:12:43.667,"Issue with gzipping, sync of AWS S3, cloudfront and my server",<amazon-web-services><amazon-s3><yii2><aws-sdk><amazon-cloudfront>,1.0,0.0,,,
76390855,33522444,0,"Looks like currently `context.done` etc methods are either deprecated or not supported at all. They are not mentioned in the docs; instead they suppose using `callback` which is the third argument to the handler. Its signature is identical to `context.done`, i.e. it accepts `error` as the first argument and `result` as the second one.",2017-06-22T15:43:23.563,2267932,CC BY-SA 3.0,True,1,,.,1,Looks like currently `context.done` etc methods are either deprecated or not supported at all.,False,False,,,,,,,,,,,,,,,,,,,,
76886998,14930504,5,This answer is now deprecated now that AWS supports online resizing for EBS volumes.,2017-07-06T17:49:05.317,267455,CC BY-SA 3.0,True,1,answer,.,0,This answer is now deprecated now that AWS supports online resizing for EBS volumes.,False,False,,,,,,,,,,,,,,,,,,,,
76933482,44976089,0,"I am using this to upload objects to S3. Can I do: `AmazonS3Client s3Client = new AmazonS3Client(new BasicAWSCredentials(accessKey, secretKey); Regions region = Region.fromName(""us-west-1""); s3Client.setRegion(regions); s3Client.putObject(""bucketname"", ""somefilename.txt"", is, meta); ` ? Is there a better way to do it since `setRegion` on `AmazonS3Client` is deprecated.",2017-07-07T20:24:33.807,654203,CC BY-SA 3.0,True,1,AmazonS3Client,.,0,Is there a better way to do it since `setRegion` on `AmazonS3Client` is deprecated.,False,False,44976089.0,44975588.0,2017-07-07T16:53:43.083,4.0,"<p>Notice that if you have the region as a string, you can do this:</p>

<pre><code>Regions regions = Regions.fromName('us-west-1');
</code></pre>

<p>And you can create a credentials provider in a variety of ways, including this:</p>

<pre><code>AWSStaticCredentialsProvider credentialsProvider = new AWSStaticCredentialsProvider(
        new BasicAWSCredentials(ACCESS_KEY, SECRET_KEY));
</code></pre>

<p>and then (for whatever builder you're doing), you can do this:</p>

<pre><code>AmazonSNS snsClient = AmazonSNSClientBuilder.standard()
    .withRegion(region)
    .withCredentials(credentialsProvider)
    .build();
</code></pre>
",2,CC BY-SA 3.0,44975588.0,2017-07-07T16:21:55.757,3.0,1902.0,2017-07-07T22:38:38.770,2020-01-09T02:09:55.077,How to specify region name in AWS S3 Java API?,<java><amazon-web-services><aws-sdk>,3.0,1.0,,,
77109227,45065328,0,"As I indicated (we're cross posting comments here) they would have to hook their variable expansion into the parser to be able to distinguish between plain scalar and literal scalar (or quoted scalar). I recently looked at their software to see how they use YAML and if extra tagged objects could be loaded, but it is in Ruby and that discouraged me to investigate (nothing against Ruby, but I am not proficient enough in it to read sources).",2017-07-12T21:39:49.390,1307905,CC BY-SA 3.0,True,1,,.,1,"I recently looked at their software to see how they use YAML and if extra tagged objects could be loaded, but it is in Ruby and that discouraged me to investigate (nothing against Ruby, but I am not proficient enough in it to read sources).",False,False,45065328.0,45064517.0,2017-07-12T18:41:29.017,0.0,"<p>In a plain scalar (without quotes), as in your first YAML example, you cannot have sequences like <code>:</code> (colon+space) as these would indicate a key-value pair.</p>

<p>If you make that into a double quoted scalar, you have to escape the double quotes and any existing backslashes with backslashes.<br>
If you make that into a single quotes scalar, you have to escape any single quote by doubling it.</p>

<p>The easiest way to get your string into a scalar is to make it a block style literal scalar:</p>

<pre><code>update_public_dns:
  script: 
  - |-
    TARGET_ZONEID=""Z2T1234""; TARGET_FQDN=""appName.domain.com.""; echo '{ ""Comment"": ""DDNS update"", ""Changes"":[ { ""Action"": ""UPSERT"", ""ResourceRecordSet"": { ""ResourceRecords"": [ { ""Value"": ""web_server.domain.com"" } ], ""Name"": ""'""$TARGET_FQDN""'"", ""Type"": ""CNAME"", ""TTL"": 60} } ] }'
</code></pre>

<p>Within such a scalar no escapes are done and <code>:</code> can occur without causing problems. The the dash after <code>|</code> indicates that the final newline needs to be stripped.</p>
",5,CC BY-SA 3.0,45064517.0,2017-07-12T17:51:16.880,1.0,344.0,,2017-07-12T18:41:29.017,gitlab-ci.yml bash script to update AWS route53 invalid yaml,<bash><amazon-web-services><yaml><gitlab><amazon-route53>,1.0,0.0,,,
77233508,27700924,1,This answer was right at that moment but is obsolete now.,2017-07-16T18:43:28.137,2357411,CC BY-SA 3.0,True,1,answer,.,0,This answer was right at that moment but is obsolete now.,False,False,27700924.0,27557235.0,2014-12-30T06:56:11.923,0.0,"<p>Volumes are not supported at the moment. But AWS seems to be working on this feature.</p>

<p><a href=""https://twitter.com/mndoci/status/549647072358440964"" rel=""nofollow"">https://twitter.com/mndoci/status/549647072358440964</a></p>
",1,CC BY-SA 3.0,27557235.0,2014-12-18T23:04:08.913,4.0,5073.0,2015-06-25T14:21:14.283,2015-06-25T14:21:14.283,Does Amazon AWS EC2 Container Service support volumes?,<amazon-web-services><amazon-ecs>,2.0,0.0,,,
77465670,45250341,0,"I tried running `sudo apt-get install default-jre` and got this in return:

    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    Package default-jre is not available, but is referred to by another package.
    This may mean that the package is missing, has been obsoleted, or
is only available from another source
    
    E: Package 'default-jre' has no installation candidate",2017-07-22T03:47:25.000,8348259,CC BY-SA 3.0,True,1,,candidate,0,"This may mean that the package is missing, has been obsoleted, or
is only available from another source
    
    E: Package 'default-jre' has no installation candidate",False,False,45250341.0,45249993.0,2017-07-22T03:26:37.057,2.0,"<p><strong>Installing RJava (Ubuntu)</strong></p>

<p>First, we need Java itself, check if it's installed</p>

<p>Write in Terminal: <code>java -version</code></p>

<p>but you already checked it, so you need to install it.</p>

<p>If it returns The program java can be found in the following packages, then Java hasn't been installed yet, so execute the following command: <code>sudo apt-get install default-jre</code>. This will install the Java Runtime Environment (JRE).</p>

<p><strong>Then Install JDK</strong></p>

<p>Write in Terminal: <code>sudo apt-get install default-jdk</code></p>

<p><strong>Then assotiate the JDK installed with R</strong></p>

<p>Run in Terminal: <code>sudo R CMD javareconf</code></p>

<p><strong>Install RJava and Rgdal</strong></p>

<p>Execute: <code>sudo apt-get install r-cran-rjava</code></p>

<p>Then: <code>sudo apt-get install libgdal1-dev libproj-dev</code></p>

<p><strong>Install package in RStudio</strong></p>

<p>Run in RStudio: <code>install.packages(""rJava"")</code></p>

<p>Done!</p>
",2,CC BY-SA 3.0,45249993.0,2017-07-22T02:16:44.870,1.0,390.0,,2018-03-02T03:06:05.530,Installing and using rJava-dependent libraries on AWS EC2,<java><r><amazon-ec2><rjava>,1.0,0.0,,,
77552562,18790493,0,This link is outdated at this point. `eb init` does not add any git commands.,2017-07-25T06:30:13.017,898905,CC BY-SA 3.0,True,1,link,.,0,This link is outdated at this point.,False,False,18790493.0,18788896.0,2013-09-13T15:53:11.357,-2.0,"<p>You can customize your environment with Elastic Beanstalk Extensions</p>

<p><a href=""http://ruby.awsblog.com/post/Tx2AK2MFX0QHRIO/Deploying-Ruby-Applications-to-AWS-Elastic-Beanstalk-with-Git"" rel=""nofollow"">http://ruby.awsblog.com/post/Tx2AK2MFX0QHRIO/Deploying-Ruby-Applications-to-AWS-Elastic-Beanstalk-with-Git</a></p>
",2,CC BY-SA 3.0,18788896.0,2013-09-13T14:32:50.857,7.0,3706.0,,2013-09-13T15:53:11.357,How does elastic beanstalk update my Rails database?,<ruby-on-rails><amazon-web-services><amazon-elastic-beanstalk>,1.0,0.0,,2.0,
77642734,33485737,2,"The ""each connection has an overhead of ~10MB"" comment is very outdated. Looks like it was reduced to around 1MB around July 2011 for MongoDB 2.0",2017-07-27T02:46:36.977,108359,CC BY-SA 3.0,True,1,connection,.,0,"The ""each connection has an overhead of ~10MB"" comment is very outdated.",False,False,33485737.0,33357518.0,2015-11-02T19:54:34.500,7.0,"<p>Although pool size of 50 isn't large for a machine with 64GB RAM, 800 certainly is. That's because you have 16 instances of your node process running with 50 each. The default number for max connections is 80% of the available file descriptors. If you are using Linux, the default is 1024 so you already have nearly max connections opened. Furthermore, each connection has an overhead of ~10MB, so you are using around 8GB for connections alone. This is obviously not ideal.</p>

<p>Ideally, You are supposed to reuse those connections in your connection pool as much as possible. So, start off your load testing with setting the poolSize to the default of 5. (i.e. 16*5=80 actually). You can trust pm2 to nicely handle the load in a round-robin fashion and that poolsize of 5 for each instance should be perfectly fine and give you an optimal performance. In case, 5 isn't enough, go up a little bit until you find something suitable.</p>
",9,CC BY-SA 3.0,33357518.0,2015-10-26T23:43:26.710,17.0,2194.0,2015-11-03T00:03:42.060,2015-11-03T00:03:42.060,Calculating needed memory for n connection pools for mongodb running on node.js app,<node.js><performance><mongodb><amazon-ec2>,2.0,7.0,,4.0,
77935623,17239899,3,"AmazonRDSClient is deprecated, you should use AmazonRDSClientBuilder now.",2017-08-03T14:35:45.613,1001573,CC BY-SA 3.0,True,1,,.,0,"AmazonRDSClient is deprecated, you should use AmazonRDSClientBuilder now.",False,False,,,,,,,,,,,,,,,,,,,,
78232720,45573161,0,not really (where Composer is concerned) - these are 'standard' nodejs messages seen during the install.  This warning is displayed because one of your direct dependency is out of date or one the npm packages in the dependency tree.  This article will help explain -> https://www.triplet.fi/blog/how_node_package_deprecation_works/,2017-08-11T13:46:31.110,7581325,CC BY-SA 3.0,True,1,, ,0,This warning is displayed because one of your direct dependency is outdated or one the npm packages in the dependency tree.  ,False,False,45573161.0,45499454.0,2017-08-08T16:07:23.183,0.0,"<p>we recommend a memory footprint of at least 3.75Gb, so I suspect its failing to build the Fabric environment as part of the composer install (tutorial includes standing up a Dev Fabric environment). </p>
",2,CC BY-SA 3.0,45499454.0,2017-08-04T06:16:59.143,0.0,301.0,2017-08-04T07:43:46.940,2017-08-08T16:07:23.183,Frozen trying to install Hyperledger composer-rest-server on AWS Lightsail,<amazon-web-services><hyperledger-composer><amazon-lightsail>,1.0,5.0,,1.0,
78655374,20144728,1,"I should point out that Mark's information is out of date. From his link: ""Amazon S3 buckets in all Regions provide read-after-write consistency for PUTS of new objects and eventual consistency for overwrite PUTS and DELETES.""",2017-08-23T20:17:49.700,365719,CC BY-SA 3.0,True,1,,.,0,I should point out that Mark's information is outdated.,False,False,20144728.0,20143216.0,2013-11-22T12:16:52.513,8.0,"<p>That's problematic on a different level. </p>

<p>S3 has only eventual consistency. You don't immediately see/can read after something was written by your code (e.g. a <code>close()</code> or <code>flush()</code>) , as the write process is delayed. I think this might be due to the allocation of free resources for the data you write. So it is not a problem of performance, but of the consistency you really want/need. </p>

<p>What do I do on EMR? I startup a Hadoop cluster and put everything into HDFS what is needed by the job(s). Reads are much more expensive in time on S3 and the eventual consistency makes ist basically useless for buffering items between jobs. </p>

<p>However S3 is great when backing up files from your HDFS or making them available for other instances or services (e.g. CloudFront). </p>
",2,CC BY-SA 3.0,20143216.0,2013-11-22T11:00:21.767,6.0,11431.0,,2017-12-21T08:34:10.887,AWS EMR performance HDFS vs S3,<hadoop><amazon-s3><mapreduce><hdfs><amazon-emr>,3.0,0.0,,2.0,
78954028,41905731,0,"I'm confused though, the docs clearly state that ""The ReturnValues parameter is used by several DynamoDB operations; however, PutItem does not recognize any values other than NONE or ALL_OLD ."". I understand you're saying PutItem does accept 'ALL_NEW'. Are the docs outdated then?",2017-09-01T09:22:41.860,3325322,CC BY-SA 3.0,True,1,,?,0,Are the docs outdated then?,False,False,,,,,,,,,,,,,,,,,,,,
79009218,46023064,0,thought about it as well. DB connection is set up with slick. Changed the deprecated version this morning. gonna try if the new profile driver will fix it. thanks,2017-09-03T15:22:46.617,4367019,CC BY-SA 3.0,True,1,,.,0,Changed the deprecated version this morning.,False,False,46023064.0,46022266.0,2017-09-03T11:38:56.110,1.0,"<p>They are indeed connections, to double check this login to the RDS instance and run this:</p>

<pre><code>SHOW PROCESSLIST;
</code></pre>

<p>What could be happening is that your applications are not closing the connection after using it.</p>

<p>Check also this answer <a href=""https://stackoverflow.com/a/4284212/1135424"">https://stackoverflow.com/a/4284212/1135424</a> could help to set better values to the <code>timeout</code> variables:</p>

<pre><code>set global wait_timeout=3;
set global interactive_timeout=3;
</code></pre>
",1,CC BY-SA 3.0,46022266.0,2017-09-03T10:03:16.347,-1.0,94.0,2017-09-03T11:01:14.830,2017-09-03T11:38:56.110,"aws rds connects, what are they and how can I reduce the number of connection",<amazon-web-services><amazon-rds>,1.0,0.0,,,
79130458,694413,0,"FYI, `from_blob` is now deprecated in favor of `read`. See https://github.com/probablycorey/mini_magick/blob/f309fbf390cd21a845264bca9bec95b9bdae8029/lib/mini_magick.rb#L82",2017-09-06T20:58:09.953,1019369,CC BY-SA 3.0,True,1,FYI,.,0,"FYI, `from_blob` is now deprecated in favor of `read`.",False,False,,,,,,,,,,,,,,,,,,,,
79724465,46382132,2,"MyISAM also is also strongly discouraged by the RDS docs, since it appears to be fundamentally incompatible with the mechanisms underlying RDS snapshot backups and point-in-time recovery -- these appear to be based at least in part on InnoDB's ability to recover from the crash-like state that results when MySQL first wakes up after being restored from a backup created by a disk snapshot. This wouldn't be handled well by MyISAM tables.  (Anecdotal observations suggest that point in time and snapshot restorations in RDS appear to go through crash recovery on startup.)",2017-09-23T18:50:08.710,1695906,CC BY-SA 3.0,True,1,MyISAM,.,0,"MyISAM also is also strongly discouraged by the RDS docs, since it appears to be fundamentally incompatible with the mechanisms underlying RDS snapshot backups and point-in-time recovery -- these appear to be based at least in part on InnoDB's ability to recover from the crash-like state that results when MySQL first wakes up after being restored from a backup created by a disk snapshot.",False,True,46382132.0,46381942.0,2017-09-23T17:03:34.947,2.0,"<p>Apparently RDS doesn't allow you to change that parameter. </p>

<p>You can change each of your tables to MyISAM, one at a time:</p>

<pre><code>ALTER TABLE MyTable ENGINE=MyISAM;
</code></pre>

<p>You can get a list of your tables you need to change:</p>

<pre><code>SELECT TABLE_SCHEMA, TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE ENGINE='InnoDB';
</code></pre>

<p>For future tables, don't rely on the default engine. Create tables with the engine named explicitly in the CREATE TABLE statement.</p>

<p>I would caution you, however: MyISAM is being phased out by MySQL. It doesn't support concurrent writes, row-level locking, transactions, or atomic changes. InnoDB is shown over and over to be superior in performance, to MyISAM, except in a vanishingly few cases:</p>

<ul>
<li><code>SELECT COUNT(*) FROM MyTable;</code> because MyISAM keeps this stat built into each table. This is admittedly the big weakness of InnoDB or any MVCC architecture.</li>
<li>Table-scans. But you shouldn't be doing table-scans anyway, you should be doing index lookups. An optimized query against InnoDB will generally be much better for performance than a table-scan against MyISAM.</li>
</ul>

<p>I would encourage you to figure out your query optimization and stick with InnoDB.</p>
",1,CC BY-SA 3.0,46381942.0,2017-09-23T16:42:43.470,1.0,820.0,,2017-09-23T17:23:02.097,Changing Default Storage Engine Amazon RDS MariaDB,<mysql><amazon-web-services><mariadb><myisam><rds>,2.0,0.0,,,
79880449,46441191,0,"No. You would need to use another method, such as a VPN service, which makes your traffic emerge at a different location. Please note that you are trying to do something that is probably against the Terms of Service and should therefore be discouraged.",2017-09-28T07:01:49.107,174777,CC BY-SA 3.0,True,1,,.,0,Please note that you are trying to do something that is probably against the Terms of Service and should therefore be discouraged.,False,False,46441191.0,46440098.0,2017-09-27T06:46:39.217,0.0,"<p>AWS does not have ""a service for us to register instance IPs as being in a different country"".</p>

<p>However, an AWS account can launch an Amazon EC2 instance in any region around the world and it will receive a public IP address that is (usually) mapped to that part of the world. If you use that EC2 instance, it will ""appear"" be be in that part of the world (because it is!).</p>

<p>However, please note that many online services block Amazon EC2 instance IP address ranges to prevent such practices.</p>
",2,CC BY-SA 3.0,46440098.0,2017-09-27T05:27:52.070,0.0,2119.0,,2017-09-27T06:46:39.217,How can I setup EC2 instance as proxy server in another country?,<amazon-web-services><amazon-ec2><proxy>,1.0,2.0,,,
80046141,46533878,0,"I figured it out. I was setting the region explicitly (with another deprecated method, setRegion) on the client. The right way to do this is to use withRegion while creating the client. This solved the problem: AWSCognitoIdentityProvider cognitoClient = AWSCognitoIdentityProviderClientBuilder.standard().withRegion(Regions.US_WEST_2).defaultClient();",2017-10-03T13:42:20.020,2134768,CC BY-SA 3.0,True,1,,.,0,"I was setting the region explicitly (with another deprecated method, setRegion) on the client.",False,False,46533878.0,46533670.0,2017-10-02T21:19:12.587,4.0,"<p>In general all of the old client constructors are deprecated in the newer AWS libraries.  You'll need to do something like:</p>

<pre><code>AWSCognitoIdentityProvider provider = 
        AWSCognitoIdentityProviderClientBuilder.standard().defaultClient();
</code></pre>

<p>This is the bare bones version - if you need to pass a different credentials provider or region you'll need to add some more parameters.  See <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/cognitoidp/AWSCognitoIdentityProviderClientBuilder.html"" rel=""nofollow noreferrer"">AWSCognitoIdentityProviderClientBuilder</a> and <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/cognitoidp/AWSCognitoIdentityProvider.html"" rel=""nofollow noreferrer"">AWSCognitoIdentityProvider</a> for more details.</p>
",3,CC BY-SA 3.0,46533670.0,2017-10-02T21:02:03.100,1.0,2211.0,,2017-10-02T21:19:12.587,Authentication with AWS Incognito,<java><amazon-web-services><sdk><amazon-cognito><aws-cognito>,1.0,0.0,,,
80083194,31311171,0,"`context.succeed` and `context.fail` are deprecated in Node versions greater than v0.10.42 in Lambda. You should now just use `callback(error, result)`. Source: [AWS Lambda docs](http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html#nodejs-prog-model-oldruntime-context-methods)",2017-10-04T12:27:40.350,1554860,CC BY-SA 3.0,True,1,context.fail,.,0,`context.succeed` and `context.fail` are deprecated in Node versions greater than v0.10.42 in Lambda.,False,False,,,,,,,,,,,,,,,,,,,,
80089186,46566702,0,"I tried it with stage keys as well, but it didn't change a thing. Stage keys are deprecated too, since I am using usage plans.",2017-10-04T14:42:53.167,7751692,CC BY-SA 3.0,True,1,keys,.,0,"Stage keys are deprecated too, since I am using usage plans.",False,False,46566702.0,46563692.0,2017-10-04T13:53:27.070,0.0,"<p>Parameters are not matching as documented,</p>

<pre><code>var params = {
  customerId: 'STRING_VALUE',
  description: 'STRING_VALUE',
  enabled: true || false,
  generateDistinctId: true || false,
  name: 'STRING_VALUE',
  stageKeys: [
    {
      restApiId: 'STRING_VALUE',
      stageName: 'STRING_VALUE'
    },
    /* more items */
  ],
  value: 'STRING_VALUE'
};
apigateway.createApiKey(params, function(err, data) {
  if (err) console.log(err, err.stack); // an error occurred
  else     console.log(data);           // successful response
});
</code></pre>

<p>stageKeys is missing. </p>

<p>Hope it helps.</p>
",1,CC BY-SA 3.0,46563692.0,2017-10-04T11:29:16.207,0.0,295.0,2017-10-04T12:18:35.990,2017-10-06T17:15:30.673,Creating API gateway api keys in Lambda,<amazon-web-services><lambda><aws-api-gateway><api-gateway>,2.0,2.0,,,
80148011,46596655,0,"Your Lambda@Edge example is incorrect and will result in an error. Specifically, it is obsolete.  The data structures changed a few days before L@E became generally available.",2017-10-06T03:21:37.040,1695906,CC BY-SA 3.0,True,1,it, ,0,"Specifically, it is obsolete.  ",False,False,46596655.0,46596143.0,2017-10-06T00:34:01.167,2.0,"<p>It is currently not possible to send multiple cookies with lambda integration.</p>

<p>If you send multiple set-cookie, then it will take the last one. ok, such a junk implementation right.</p>

<p>Reference, <a href=""https://stackoverflow.com/questions/39769222/how-can-i-send-multiple-set-cookie-headers-from-api-gateway-using-a-proxied-lamb"">How can I send multiple Set-Cookie headers from API Gateway using a proxied Lambda</a></p>

<p>Let us see other avaialable options,</p>

<p><strong>Lambda@Edge:</strong></p>

<p>Here is what I find working with Lambda@Edge,</p>

<p>You can create a lambda function for viewer response and modify the header to set cookies.</p>

<pre><code>'use strict';

exports.handler = (event, context, callback) =&gt; {
   const response = event.Records[0].cf.response;
   const headers = response.headers;

   // send via a single header and split it into multiple set cookie here.
   headers['set-Cookie'] = 'cookie1';
   headers['Set-Cookie'] = 'cookie2';

   callback(null, response);
};
</code></pre>

<p><strong>API Gateway Integration Request mapping:</strong></p>

<p>Here is what I found and got working with integration request,</p>

<p><a href=""https://i.stack.imgur.com/0Rc6d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Rc6d.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/lRAJ3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lRAJ3.png"" alt=""enter image description here""></a></p>

<p>Hope it helps.</p>
",2,CC BY-SA 3.0,46596143.0,2017-10-05T23:20:22.200,0.0,582.0,,2018-10-23T16:16:46.213,Use multiple same header field in Lambda Proxy Integration,<amazon-web-services><aws-lambda><aws-api-gateway>,2.0,0.0,,,
80299166,46479087,1,"Are you using the included boto3 library, or did you have to import your own? I am having trouble connecting to ecs to trigger a task using boto3 and I'm curious if it's because the included library is outdated.",2017-10-10T20:27:52.757,1368038,CC BY-SA 3.0,True,1,,.,0,and I'm curious if it's because the included library is outdated.,False,False,46479087.0,46475671.0,2017-09-28T22:17:31.130,5.0,"<p>I found the answer! When I established the IAM role for my Glue services, I didn't realize I was opening that up to boto3 as well.</p>

<p>The answer is that I don't need to pass my credentials.  I simply use this:</p>

<pre><code>mySession = boto3.Session(region_name='my_region_name')
</code></pre>

<p>and it works like a charm!</p>
",3,CC BY-SA 3.0,46475671.0,2017-09-28T18:05:53.330,1.0,1518.0,,2017-09-28T22:17:31.130,AWS Glue Job Python Script Boto3 - want to hide credentials,<python-3.x><amazon-web-services><boto3>,1.0,2.0,,,
80340067,46671315,0,"Dots in a bucket name are only good for this purpose.  Otherwise, you don't want them -- if you want to use the built-in S3 SSL certs on the REST endpoints, or you want to use transfer acceleration, dots are not allowed in the bucket name.  Note also if you are really using something like `appImageBucket` with capital letters in it -- you should also reconsider that.  Mixed case names and bucket names longer than 63 characters are both deprecated features and aren't allowed in most regions.  See [Bucket Restrictions](http://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html).",2017-10-11T19:35:16.353,1695906,CC BY-SA 3.0,True,1,, ,1,Mixed case names and bucket names longer than 63 characters are both deprecated features and aren't allowed in most regions.  ,False,False,46671315.0,46669728.0,2017-10-10T16:13:02.983,0.0,"<blockquote>
  <p>so that I can easily setup a DNS redirect to it</p>
</blockquote>

<p>That isn't exactly how it works.  Simply pointing DNS to the hostname of a bucket does not cause the bucket to respond to requests for that hostname.</p>

<p>In order to point a hostname to a bucket in DNS, you have two options:</p>

<p>You can create a bucket named <em>exactly</em> the same as the hostname -- for example, if you want to point images.example.com to a bucket, the actual name of the bucket must be <code>images.example.com</code>. (Option 1)</p>

<p>Or, you can create a CloudFront distribution, point it to the bucket, set the desired hostname as an Alternate Domain Name on the CloudFront distribution, and point your DNS to CloudFront.  In this configuration,  the bucket name doesn't matter. (Option 2)</p>

<p>Only option 2 supports SSL.</p>

<p>Enabling the web site hosting feature is only indirectly related to what you are trying to do -- this will enable features like redirects and index documents, and will also change the endpoint you need to map to, into a format similar to <code>example-bucket.s3-website-us-west-1.amazonaws.com</code> (depending on the bucket region) and this value will be displayed in the console.  You'll need this for either your DNS settings (option 1, above) or your CloudFront configuration (this will be your origin domain name, option 2, above).</p>
",3,CC BY-SA 3.0,46669728.0,2017-10-10T14:49:33.440,0.0,46.0,2018-10-13T16:30:34.930,2018-10-13T16:30:34.930,"s3 bucket name as part os subdomain but not hosting a website, just proxy",<amazon-s3><proxy>,1.0,0.0,,,
80367094,9427598,1,5yrs on as you'd expect this answer is out of date and wrong. Currently  the Amazon app store supports multiple .apks,2017-10-12T13:03:05.627,31751,CC BY-SA 3.0,True,1,,.,0,5yrs on as you'd expect this answer is outdated and wrong.,False,False,,,,,,,,,,,,,,,,,,,,
80378813,46704673,0,"Thank you! So I was able to enable the Mongo PHP driver on the server. I enabled traffic UDP/TCP on IC2 and changed bind_ip to 0.0.0.0 . I checked on IC2 and mongoDB instance is up and running. However, I get the following error message “Fatal error: Uncaught exception 'MongoConnectionException' with message 'Failed to connect to: ip:port: Connection refused”.I am using following command “$m = new MongoClient(""mongodb://ip"",array(""username""=>""bitnami"",""password""=> $key));”I know that this call is deprecated but the other command does not generate error message.Any idea what can be the problem?",2017-10-12T18:12:23.540,6654351,CC BY-SA 3.0,True,1,,but,0,"I am using following command “$m = new MongoClient(""mongodb://ip"",array(""username""=>""bitnami"",""password""=> $key));”I know that this call is deprecated but",False,False,46704673.0,46690487.0,2017-10-12T08:16:18.763,0.0,"<p>Hi Bitnami Engineer here, </p>

<p>Apart from this <a href=""https://stackoverflow.com/a/46691826/4709625"">tip</a>, please note that the database port for the nodes in this solution cannot be accessed over a public IP address by default. For security reasons, we do not recommend making the database port accessible over a public IP address.</p>

<p><a href=""https://docs.bitnami.com/aws/infrastructure/mongodb/#how-to-connect-to-mongodb-from-a-different-machine"" rel=""nofollow noreferrer"">https://docs.bitnami.com/aws/infrastructure/mongodb/#how-to-connect-to-mongodb-from-a-different-machine</a></p>

<p>You can open the MongoDB port in the server by accessing the AWS console and modifying the access group configuration</p>

<p><a href=""https://docs.bitnami.com/aws/faq/#how-to-open-the-server-ports-for-remote-access"" rel=""nofollow noreferrer"">https://docs.bitnami.com/aws/faq/#how-to-open-the-server-ports-for-remote-access</a></p>

<p>You will also need to modify the MongoDB configuration file (<code>/opt/bitnami/mongodb/mongodb.conf</code>) to allow the database to not to work only on 127.0.0.1.</p>

<p>Regards,
Jota</p>
",4,CC BY-SA 3.0,46690487.0,2017-10-11T14:12:33.370,0.0,462.0,2017-10-12T10:23:05.200,2018-09-26T10:55:45.090,Connect to mongoDb Bitnami server hosted on Amazon EC2,<php><mongodb><amazon-web-services><amazon-ec2><bitnami>,3.0,0.0,,,
80723701,46888079,0,"I think this info is outdated, the size of the zip file doesn't matter, what matters is the unzipped file size, which must be less than 250 Mb.",2017-10-23T11:35:43.280,3736779,CC BY-SA 3.0,True,1,,.,1,"I think this info is outdated, the size of the zip file doesn't matter, what matters is the unzipped file size, which must be less than 250 Mb.",False,False,46888079.0,46887794.0,2017-10-23T11:18:37.100,-5.0,"<p>(Are you sure you need <code>pandas</code> and <code>numpy</code> in a microservice? There is nothing ""micro"" in those libraries).</p>

<p>There is a way. Deploy you Lambda with Zappa <a href=""https://github.com/Miserlou/Zappa"" rel=""nofollow noreferrer"">https://github.com/Miserlou/Zappa</a>. It's a convenient way to write, deploy and manage your Python Lambdas anyway. But with Zappa you can specify an option called <code>slim_handler</code>. If set to <code>true</code>, most of your code will reside at S3 and will be pulled once a Lambda is executed:</p>

<blockquote>
  <p>AWS currently limits Lambda zip sizes to 50 megabytes. If your project
  is larger than that, set slim_handler: true in your
  zappa_settings.json. In this case, your fat application package will
  be replaced with a small handler-only package. The handler file then
  pulls the rest of the large project down from S3 at run time! The
  initial load of the large project may add to startup overhead, but the
  difference should be minimal on a warm lambda function. Note that this
  will also eat into the memory space of your application function.</p>
</blockquote>
",5,CC BY-SA 3.0,46887794.0,2017-10-23T11:03:19.310,5.0,2185.0,2017-10-23T11:27:39.097,2019-04-11T19:38:00.280,Reduce size of serverless deploy package,<amazon-web-services><aws-lambda><serverless-framework>,2.0,0.0,,,
80733107,46890992,0,"Thanks for your answer. I adjusted the mapping, but still no success. Could you please provide me the URL you would use? I'm trying `GET /myindex/_search`. `?search_type=count`, as seen in the other solution, is deprecated.",2017-10-23T15:22:24.903,1792858,CC BY-SA 3.0,True,1,count,.,0,"search_type=count`, as seen in the other solution, is deprecated.",False,False,46890992.0,46890574.0,2017-10-23T13:48:09.123,1.0,"<p>The suggest answer with .keyword is the correct one. </p>

<pre><code>{
    ""aggs"": {
        ""group"": {
            ""terms"": {
                ""field"": ""groupId.raw""
            },
            ""aggs"": {
                ""group_docs"": {
                    ""top_hits"": {
                        ""size"": 1,
                        ""sort"": [
                            {
                                ""timestamp (or wathever you want to sort)"": {
                                    ""order"": ""desc""
                                }
                            }
                        ]
                    }
                }
            }
        }
    }
}
</code></pre>

<p>with a mapping like that:</p>

<pre><code>       ""groupId"": {
           ""type"": ""text"",
           ""fields"": {
              ""raw"": {
                 ""type"": ""keyword""
              }
           }
        }
</code></pre>
",2,CC BY-SA 3.0,46890574.0,2017-10-23T13:28:20.803,2.0,695.0,,2017-10-23T13:48:09.123,Elasticsearch: can I avoid enabling fielddata on text fields?,<javascript><amazon-web-services><elasticsearch>,1.0,0.0,,,
80768780,27330752,0,"Using `available()` is not recommended, in fact it is quite discouraged. See the javadoc for an explanation: [link](https://docs.oracle.com/javase/7/docs/api/java/io/InputStream.html#available()). Basically you can't count on an implementation returning the total number of bytes in the stream.",2017-10-24T12:50:57.577,1545759,CC BY-SA 3.0,True,1,it,.,1,"Using `available()` is not recommended, in fact it is quite discouraged.",False,False,27330752.0,27318587.0,2014-12-06T10:39:47.140,8.0,"<p>First you should get the object <code>InputStream</code> to do your need.</p>

<pre><code>S3Object object = s3Client.getObject(new GetObjectRequest(bucketName, key));
InputStream objectData = object.getObjectContent();
</code></pre>

<p>Pass the <code>InputStream</code>, <code>File Name</code> and the <code>path</code> to the below method to download your stream.</p>

<pre><code>public void saveFile(String fileName, String path, InputStream objectData) throws Exception {
    DataOutputStream dos = null;
    OutputStream out = null;
    try {
        File newDirectory = new File(path);
        if (!newDirectory.exists()) {
            newDirectory.mkdirs();
        }

        File uploadedFile = new File(path, uploadFileName);
        out = new FileOutputStream(uploadedFile);
        byte[] fileAsBytes = new byte[inputStream.available()];
        inputStream.read(fileAsBytes);

        dos = new DataOutputStream(out);
        dos.write(fileAsBytes);
    } catch (IOException io) {
        io.printStackTrace();
    } catch (Exception e) {
        e.printStackTrace();
    } finally {
        try {
            if (out != null) {
                out.close();
            }
            if (dos != null) {
                dos.close();
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
</code></pre>

<p>After you Download your object read the file and make it to <code>JSON</code> and write it to <code>.txt</code> file after that you can upload the <code>txt</code> file to the desired bucket in <code>S3</code></p>
",4,CC BY-SA 3.0,27318587.0,2014-12-05T14:53:39.947,7.0,27166.0,2015-12-24T10:48:55.263,2018-09-20T12:47:30.800,How do I read the content of a file in Amazon S3,<java><amazon-web-services><file-io><amazon-s3>,2.0,2.0,,2.0,
81390211,47178015,0,"You don't have API access to the billing console. You need to export your billing information to S3 using the DBR (almost deprecated) or the CUR. It is a CSV file containing all the billing information you need, including under which account the billing was made.",2017-11-10T10:15:24.297,1495765,CC BY-SA 3.0,True,1,,.,0,You need to export your billing information to S3 using the DBR (almost deprecated) or the CUR.,False,False,,,,,,,,,,,,,,,,,,,,
81413249,47230543,0,Perhaps your `awscli` is out of date? I did this using version 1.11.183. `pip install -U awscli`,2017-11-10T21:18:28.930,1706504,CC BY-SA 3.0,True,1,awscli,?,0,Perhaps your `awscli` is outdated?,False,False,,,,,,,,,,,,,,,,,,,,
81461205,47213323,0,"This error is referring to the serialized JSON response from Lambda (which is *always* a JSON response, and this has nothing to do with your payload content).  It means you returned something that was not a JavaScript object.  If you insist on using the deprecated `context` methods, don't include the leading null.  `context.succeed(request)` is equivalent to `callback(null,request)`.",2017-11-12T23:45:10.220,1695906,CC BY-SA 3.0,True,1,, ,1,"If you insist on using the deprecated `context` methods, don't include the leading null.  ",False,False,47213323.0,47207065.0,2017-11-09T23:26:09.687,0.0,"<p>To allow request processing by CloudFront to continue from a Viewer Request or Origin Request trigger:</p>

<pre><code>return callback(null,request);
</code></pre>
",4,CC BY-SA 3.0,47207065.0,2017-11-09T16:34:08.380,0.0,1508.0,2017-11-09T16:49:48.827,2018-05-07T03:42:17.883,Amazon Lambda@edge response,<amazon-web-services><aws-lambda><amazon-cloudfront>,2.0,3.0,,,
81657920,46673808,0,"Fine, but Primitive Roles are kind of deprecated. How about the more granular roles - any documentation about which one is required for a given Firebase feature?",2017-11-17T14:28:17.613,1824548,CC BY-SA 3.0,True,1,,.,0,"Fine, but Primitive Roles are kind of deprecated.",False,False,46673808.0,46670334.0,2017-10-10T18:44:49.760,0.0,"<p><a href=""https://support.google.com/firebase/answer/7000272?hl=en"" rel=""nofollow noreferrer"">Firebase Help: Manage Project Members</a></p>

<p>Every Firebase project is backed by a Google Cloud project. That's why you get sent to the IAM page.</p>

<p>You can set the new member's role to any of the <a href=""https://cloud.google.com/iam/docs/understanding-roles#primitive_role_definitions"" rel=""nofollow noreferrer"">primitive roles</a> (viewer, editor, or owner).</p>
",1,CC BY-SA 3.0,46670334.0,2017-10-10T15:17:56.773,1.0,238.0,2017-10-10T15:55:57.760,2017-10-10T18:44:49.760,How to grant Firebase Specific or Firebase Admin role to a user in Google Cloud Platform?,<firebase><roles><amazon-iam><firebase-console>,1.0,0.0,,1.0,
81732748,36871764,0,"I think at this point StatefulSets can do it best, this answer might be outdated.",2017-11-20T10:32:45.797,3232601,CC BY-SA 3.0,True,1,answer,.,0,"I think at this point StatefulSets can do it best, this answer might be outdated.",False,False,36871764.0,36857097.0,2016-04-26T17:26:07.657,3.0,"<p>The idea is to use a different 'service' and 'deployment' for each of the node you want to create. </p>

<p>As you said, you have to create a custom NODENAME for each i.e:</p>

<pre><code>RABBITMQ_NODENAME=rabbit@rabbitmq-1
</code></pre>

<p>Also <code>rabbitmq-1,rabbitmq-2,rabbitmq-3</code> have to be resolved from each nodes.  For that you can use kubedns. The <code>/etc/resolv.conf</code> will look like:</p>

<pre><code>search rmq.svc.cluster.local 
</code></pre>

<p>and <code>/etc/hosts</code> must contains:</p>

<pre><code>127.0.0.1 rabbitmq-1  # or rabbitmq-2 on node 2...
</code></pre>

<p>The services are here to create a stable network identity for each nodes</p>

<pre><code>rabbitmq-1.svc.cluster.local
rabbitmq-2.svc.cluster.local
rabbitmq-3.svc.cluster.local
</code></pre>

<p>The different <code>deployments</code> resources will allow you to mount a different volume on each node.</p>

<p>I'm working on a deployment tool to simplify those actions:
I've done a demo on how I scale and deploy rabbitmq from 1 to 3 nodes on kubernetes:
<a href=""https://asciinema.org/a/2ktj7kr2d2m3w25xrpz7mjkbu?speed=1.5"" rel=""nofollow"">https://asciinema.org/a/2ktj7kr2d2m3w25xrpz7mjkbu?speed=1.5</a></p>

<p>More generally, the complexity your facing to deploy a clustered application is addressed in the 'petset proposal': <a href=""https://github.com/kubernetes/kubernetes/pull/18016"" rel=""nofollow"">https://github.com/kubernetes/kubernetes/pull/18016</a></p>
",3,CC BY-SA 3.0,36857097.0,2016-04-26T06:40:05.310,8.0,2610.0,2016-04-26T12:09:28.287,2016-04-28T12:44:54.390,Changing hostname breaks Rabbitmq when running on Kubernetes,<amazon-ec2><dns><rabbitmq><kubernetes><hostname>,2.0,0.0,,2.0,
82104699,5495462,0,"this answer is no out of date, see below",2017-11-30T12:54:56.307,1153938,CC BY-SA 3.0,True,1,answer,below,0,"this answer is no outdated, see below",False,False,,,,,,,,,,,,,,,,,,,,
82447349,47723592,2,Can you maybe elaborate a little bit more on this? I am trying something similar and this class seems to have been deprecated.,2017-12-10T21:24:45.533,2568851,CC BY-SA 3.0,True,1,,.,0,I am trying something similar and this class seems to have been deprecated.,False,False,47723592.0,47722295.0,2017-12-08T23:27:20.837,2.0,"<p>The class you should be using is <a href=""https://github.com/aws/aws-sdk-android/blob/master/aws-android-sdk-core/src/main/java/com/amazonaws/services/cognitoidentity/AmazonCognitoIdentityClient.java"" rel=""nofollow noreferrer"">AmazonCognitoIdentityClient</a>, that is the class implementing the <code>GetCredentialsForIdentity</code> API.</p>
",2,CC BY-SA 3.0,47722295.0,2017-12-08T21:19:21.677,4.0,1040.0,2017-12-08T21:23:16.680,2017-12-21T00:38:43.670,Getting Cognito Credentials on Android,<java><android><amazon-web-services><amazon-cognito><aws-cognito>,2.0,0.0,,,
82764013,47824622,0,"Thanks for your answer, @s-cherry, but it didn't get me very far from where I was. When I tried your suggestions in step 1, ELB console complained about deprecated instructions in `secure-listener.config`. Anyway, I manually changed those settings in ELB console configuration. About the rest, it's no different from what I've tried. Do I need to install Really Simple SSL plugin after this or it isn't mandatory? Thanks in advance.",2017-12-20T04:12:31.190,1061340,CC BY-SA 3.0,True,1,,in,0,"When I tried your suggestions in step 1, ELB console complained about deprecated instructions in",False,False,,,,,,,,,,,,,,,,,,,,
82792296,47824622,0,"Bummer, sorry that didn't work. Can you share the exact message about deprecated instructions? I've had trouble in the past going the manual configuration route so using the config file would be preferable if we can get it working. The above won't require the Really Simple SSL plugin.",2017-12-20T18:32:51.517,9101504,CC BY-SA 3.0,True,1,,?,0,Can you share the exact message about deprecated instructions?,False,False,,,,,,,,,,,,,,,,,,,,
82800297,47897459,0,"Hello, why exactly are you trying to use deprecated functionality?",2017-12-21T00:12:45.463,4785056,CC BY-SA 3.0,True,1,,?,0,"Hello, why exactly are you trying to use deprecated functionality?",False,False,,,,,,,,,,,,,,,,,,,,
82808795,8601736,4,this answer is extremely out of date. You can simply use the built in django configuration to do this now.,2017-12-21T08:16:55.033,8207,CC BY-SA 3.0,True,1,answer,.,0,this answer is extremely outdated.,False,False,8601736.0,8580754.0,2011-12-22T09:22:40.060,28.0,"<p>Thanks everyone for the recommendations but I finally found a much simpler solution that would allow me to use Django's built-in mail classes so I can still get my admin error email reports etc.</p>

<p>Thanks to this little beauty I was able to use SES SMTP without any problems:</p>

<p><a href=""https://github.com/bancek/django-smtp-ssl"" rel=""noreferrer"">https://github.com/bancek/django-smtp-ssl</a></p>

<p>Download and install (python setup.py install)</p>

<p>Then just change your settings to use this new email backend:</p>

<pre><code>EMAIL_BACKEND = 'django_smtp_ssl.SSLEmailBackend'
</code></pre>

<p>The rest of the settings are as per normal:</p>

<pre><code>EMAIL_HOST = 'email-smtp.us-east-1.amazonaws.com'
EMAIL_PORT = 465
EMAIL_HOST_USER = 'my_smtp_username'
EMAIL_HOST_PASSWORD = 'my_smtp_password'
EMAIL_USE_TLS = True
</code></pre>

<p>Nice.</p>

<p>G</p>
",3,CC BY-SA 3.0,8580754.0,2011-12-20T19:18:04.770,28.0,17130.0,2011-12-20T19:23:08.007,2020-05-18T11:25:51.380,Amazon SES SMTP with Django,<python><django><smtp><amazon-web-services><amazon-ses>,7.0,3.0,,19.0,
82808795,8601736,4,this answer is extremely out of date. You can simply use the built in django configuration to do this now.,2017-12-21T08:16:55.033,8207,CC BY-SA 3.0,True,1,answer,.,0,this answer is extremely outdated.,False,False,8601736.0,8580754.0,2011-12-22T09:22:40.060,28.0,"<p>Thanks everyone for the recommendations but I finally found a much simpler solution that would allow me to use Django's built-in mail classes so I can still get my admin error email reports etc.</p>

<p>Thanks to this little beauty I was able to use SES SMTP without any problems:</p>

<p><a href=""https://github.com/bancek/django-smtp-ssl"" rel=""noreferrer"">https://github.com/bancek/django-smtp-ssl</a></p>

<p>Download and install (python setup.py install)</p>

<p>Then just change your settings to use this new email backend:</p>

<pre><code>EMAIL_BACKEND = 'django_smtp_ssl.SSLEmailBackend'
</code></pre>

<p>The rest of the settings are as per normal:</p>

<pre><code>EMAIL_HOST = 'email-smtp.us-east-1.amazonaws.com'
EMAIL_PORT = 465
EMAIL_HOST_USER = 'my_smtp_username'
EMAIL_HOST_PASSWORD = 'my_smtp_password'
EMAIL_USE_TLS = True
</code></pre>

<p>Nice.</p>

<p>G</p>
",3,CC BY-SA 3.0,8580754.0,2011-12-20T19:18:04.770,28.0,17130.0,2011-12-20T19:23:08.007,2020-05-18T11:25:51.380,Amazon SES SMTP with Django,<python><django><smtp><amazon-web-services><amazon-ses>,7.0,3.0,,19.0,
82939679,47980841,2,@PrathameshDatar see also [T2 Instances](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/t2-instances.html).  You can remove the cap for an additional fee by enabling the T2 Unlimited feature.  This class and the now obsolete (but still available) T1 are the only classes with burstable behavior.,2017-12-27T01:33:02.997,1695906,CC BY-SA 3.0,True,1,,),0,This class and the now obsolete (but still available),False,False,47980841.0,47979806.0,2017-12-26T15:50:54.003,3.0,"<p>If you want consistent CPU performance, you should avoid the burstable performance instances (the T2 family). All other families of instances (M5, C5, etc) will have consistent CPU performance over time. You can use any instance family with Elastic Beanstalk. No need for a dedicated host.</p>
",2,CC BY-SA 3.0,47979806.0,2017-12-26T14:14:29.800,1.0,473.0,,2017-12-26T15:50:54.003,How to get consistent CPU utilization on AWS,<amazon-web-services><amazon-ec2><amazon-sqs>,1.0,0.0,,,
83417630,48217376,0,"I did just that, but people said that urllib is rather outdated and not very compatible with 3+. Ideally I need a way to change the simple product api library to use requests instead, but this is also proving to be a challenge as they are so based around urllib",2018-01-12T00:19:47.573,5655267,CC BY-SA 3.0,True,1,I,3,1,"I did just that, but people said that urllib is rather outdated and not very compatible with 3",False,False,48217376.0,48217312.0,2018-01-11T23:21:18.987,0.0,"<p>The AWS has their own python CLI package.</p>

<p><a href=""https://aws.amazon.com/cli/"" rel=""nofollow noreferrer"">https://aws.amazon.com/cli/</a></p>

<p>pip install awscli </p>
",4,CC BY-SA 3.0,48217312.0,2018-01-11T23:15:00.640,0.0,3489.0,2018-01-12T01:44:48.430,2018-01-12T01:44:48.430,Amazon API In Python,<python><amazon-web-services><python-requests>,3.0,1.0,,,
83705590,48359452,0,"also, here is an example, however it is old and there are several deprecated items it will give you an ideahttps://aws.amazon.com/blogs/developer/building-a-serverless-developer-authentication-api-in-java-using-aws-lambda-amazon-dynamodb-and-amazon-cognito-part-2/ or here https://github.com/awslabs/aws-cognito-java-desktop-app",2018-01-20T18:06:27.093,7962689,CC BY-SA 3.0,True,1,,ideahttps://aws.amazon.com/blogs/developer/building-a-serverless-developer-authentication-api-in-java-using-aws-lambda-amazon-dynamodb-and-amazon-cognito-part-2/,0,"also, here is an example, however it is old and there are several deprecated items it will give you an ideahttps://aws.amazon.com/blogs/developer/building-a-serverless-developer-authentication-api-in-java-using-aws-lambda-amazon-dynamodb-and-amazon-cognito-part-2/",False,False,,,,,,,,,,,,,,,,,,,,
83842271,24709263,0,"this answer is very old and probably out of date, I think there are probably better ways to do this now, but I now use Azure instead of AWS",2018-01-24T15:34:29.427,1341806,CC BY-SA 3.0,True,1,,AWS,0,"this answer is very old and probably outdated, I think there are probably better ways to do this now, but I now use Azure instead of AWS",False,False,24709263.0,24567203.0,2014-07-12T03:12:08.920,1.0,"<p>The best way I have found to host a consumer program is using EMR, but not as a hadoop cluster. Package your program as a jar, and place it in s3. Launch an emr cluster and have it run your jar. Using the data pipeline you can schedule this job flow to run at regular intervals. You can also scale an emr cluster, or use a actual EMR job to process the stream if you choose to get the high tech. </p>
",2,CC BY-SA 3.0,24567203.0,2014-07-04T05:23:57.287,10.0,4205.0,2017-07-15T11:16:01.920,2017-07-15T11:18:17.527,How to deploy and Run Amazon Kinesis Application on Amazon Kinesis service,<amazon-web-services><amazon-kinesis>,3.0,3.0,,3.0,
84355897,30882309,0,"@mkobit This works great, but I'm getting the error as DynamoDBMarshaller is deprecated..",2018-02-08T06:56:41.860,7073340,CC BY-SA 3.0,True,1,,..,0,"This works great, but I'm getting the error as DynamoDBMarshaller is deprecated..",False,False,,,,,,,,,,,,,,,,,,,,
84430858,48713189,0,"Thanks, but surely the delay is just a few seconds at most? I’m getting an out of date listing maybe 10 minutes later, sometimes half an hour.",2018-02-10T00:00:51.377,3110357,CC BY-SA 3.0,True,1,,.,0,"I’m getting an outdated listing maybe 10 minutes later, sometimes half an hour.",False,False,48713189.0,48710517.0,2018-02-09T19:56:16.507,0.0,"<p>That is due to <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html"" rel=""nofollow noreferrer"">Amazon S3 Data Consistency Model</a>. S3 provides read-after-write consistency for PUTs, however other requests - including listObjects are eventually consistent which means there could be a delay in propagation.</p>
",1,CC BY-SA 3.0,48710517.0,2018-02-09T16:48:44.720,1.0,590.0,,2018-11-21T01:15:56.823,Results of S3 function call are being cached by my Lambda function,<amazon-s3><lambda><aws-lambda><aws-sdk>,2.0,2.0,,,
84510883,48427226,1,"@VikashGupta If I can make a tangential suggestion… in my experience, it tends to be easier to deal with dependencies by *always* keeping them up to date. If you have a good QA process where (mostly automated) tests cover your whole project, you will automatically test the incorporated changes as you release. if you leave dependencies out of date, you eventually wind up with a difficult, painful upgrade process because you have years' worth of changes to absorb all at once. To use a metaphor, it's easier to do regular maintenance on your car instead of waiting until the engine seizes up.",2018-02-12T20:31:16.623,647373,CC BY-SA 3.0,True,1,,.,0,"if you leave dependencies outdated, you eventually wind up with a difficult, painful upgrade process because you have years' worth of changes to absorb all at once.",False,False,,,,,,,,,,,,,,,,,,,,
84587898,37264137,0,"The AMAZON.LITERAL type is deprecated and skills that use it will not pass certification after Nov 30, 2016. It's a privacy issue to not allow apps to listen to random conversation. If that was allowed, then an app could appear innocent, say provide the weather, but then secretly stay active while listening and recording whatever is said in the room. It could stay in that state until being forced to quit by the inactivity timeout.",2016-10-14T22:51:54.383,1404723,CC BY-SA 3.0,True,1,type,.,1,"The AMAZON.LITERAL type is deprecated and skills that use it will not pass certification after Nov 30, 2016.",False,False,37264137.0,37249475.0,2016-05-16T22:17:03.953,15.0,"<p>To capture free-form speech input (rather than a defined list of possible values), you'll need to use the <code>AMAZON.LITERAL</code> slot type. The <a href=""https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-interaction-model-reference#LITERAL%20Slot%20Type%20Reference"" rel=""noreferrer"">Amazon documentation for the Literal slot type</a> describes a use case similar to yours, where a skill is created to take any phrase and post it to a Social Media site. This is done by creating a StatusUpdate intent:</p>

<pre><code>{
  ""intents"": [
    {
      ""intent"": ""StatusUpdate"",
      ""slots"": [
        {
          ""name"": ""UpdateText"",
          ""type"": ""AMAZON.LITERAL""
        }
      ]
    }
  ]
}
</code></pre>

<p>Since it uses the <code>AMAZON.LITERAL</code> slot type, this intent will be able to capture any arbitrary phrase. However, to ensure that the speech engine will do a decent job of capturing real-world phrases, you need to provide a variety of example utterances that resemble the sorts of things you expect the user to say.  </p>

<p>Given that in your described scenario, you're trying to capture <em>very</em> dynamic phrases, there's a couple things in the documentation you'll want to give extra consideration to:</p>

<blockquote>
  <p>If you are using the AMAZON.LITERAL type to collect free-form text
  with wide variations in the number of words that might be in the slot,
  note the following:</p>
  
  <ul>
  <li>Covering this full range (minimum, maximum, and all in between) will
  require a very large set of samples. Try to provide several hundred
  samples or more to address all the variations in slot value words as
  noted above. </li>
  <li>Keep the phrases within slots short enough that users can
  say the entire phrase without needing to pause.</li>
  </ul>
  
  <p>Lengthy spoken input can lead to lower accuracy experiences, so avoid
  designing a spoken language interface that requires more than a few
  words for a slot value. A phrase that a user cannot speak without
  pausing is too long for a slot value.</p>
</blockquote>

<p>That said, here's the example Sample Utterances from the documentation, again: </p>

<blockquote>
  <p>StatusUpdate    post the update {arrived|UpdateText} </p>
  
  <p>StatusUpdate post the update {dinner time|UpdateText} </p>
  
  <p>StatusUpdate post the update {out at lunch|UpdateText}   </p>
  
  <p>...(more samples showing phrases with  4-10 words)</p>
  
  <p>StatusUpdate post the update {going to stop by the grocery store this evening|UpdateText}</p>
</blockquote>

<p>If you provide enough examples of different lengths to give an accurate picture of the range of expected user utterances, then your intent will be able to accurately capture dynamic phrases in real uses cases, which you can access in the <code>UpdateText</code> slot. Based on this, you should be able to implement an intent specific to your needs.</p>
",12,CC BY-SA 3.0,37249475.0,2016-05-16T08:16:41.400,20.0,12540.0,2016-05-16T08:48:36.530,2019-01-15T08:03:28.463,Amazon Alexa: store user's words,<amazon><alexa-skills-kit><alexa-skill>,6.0,0.0,,6.0,
84587901,37264137,1,Looks like it is no longer deprecated based on developer feedback: https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/migrating-to-the-improved-built-in-and-custom-slot-types#literal,2017-03-19T05:26:52.093,154142,CC BY-SA 3.0,True,1,,https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/migrating-to-the-improved-built-in-and-custom-slot-types#literal,1,Looks like it is no longer deprecated based on developer feedback: https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/migrating-to-the-improved-built-in-and-custom-slot-types#literal,False,False,37264137.0,37249475.0,2016-05-16T22:17:03.953,15.0,"<p>To capture free-form speech input (rather than a defined list of possible values), you'll need to use the <code>AMAZON.LITERAL</code> slot type. The <a href=""https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-interaction-model-reference#LITERAL%20Slot%20Type%20Reference"" rel=""noreferrer"">Amazon documentation for the Literal slot type</a> describes a use case similar to yours, where a skill is created to take any phrase and post it to a Social Media site. This is done by creating a StatusUpdate intent:</p>

<pre><code>{
  ""intents"": [
    {
      ""intent"": ""StatusUpdate"",
      ""slots"": [
        {
          ""name"": ""UpdateText"",
          ""type"": ""AMAZON.LITERAL""
        }
      ]
    }
  ]
}
</code></pre>

<p>Since it uses the <code>AMAZON.LITERAL</code> slot type, this intent will be able to capture any arbitrary phrase. However, to ensure that the speech engine will do a decent job of capturing real-world phrases, you need to provide a variety of example utterances that resemble the sorts of things you expect the user to say.  </p>

<p>Given that in your described scenario, you're trying to capture <em>very</em> dynamic phrases, there's a couple things in the documentation you'll want to give extra consideration to:</p>

<blockquote>
  <p>If you are using the AMAZON.LITERAL type to collect free-form text
  with wide variations in the number of words that might be in the slot,
  note the following:</p>
  
  <ul>
  <li>Covering this full range (minimum, maximum, and all in between) will
  require a very large set of samples. Try to provide several hundred
  samples or more to address all the variations in slot value words as
  noted above. </li>
  <li>Keep the phrases within slots short enough that users can
  say the entire phrase without needing to pause.</li>
  </ul>
  
  <p>Lengthy spoken input can lead to lower accuracy experiences, so avoid
  designing a spoken language interface that requires more than a few
  words for a slot value. A phrase that a user cannot speak without
  pausing is too long for a slot value.</p>
</blockquote>

<p>That said, here's the example Sample Utterances from the documentation, again: </p>

<blockquote>
  <p>StatusUpdate    post the update {arrived|UpdateText} </p>
  
  <p>StatusUpdate post the update {dinner time|UpdateText} </p>
  
  <p>StatusUpdate post the update {out at lunch|UpdateText}   </p>
  
  <p>...(more samples showing phrases with  4-10 words)</p>
  
  <p>StatusUpdate post the update {going to stop by the grocery store this evening|UpdateText}</p>
</blockquote>

<p>If you provide enough examples of different lengths to give an accurate picture of the range of expected user utterances, then your intent will be able to accurately capture dynamic phrases in real uses cases, which you can access in the <code>UpdateText</code> slot. Based on this, you should be able to implement an intent specific to your needs.</p>
",12,CC BY-SA 3.0,37249475.0,2016-05-16T08:16:41.400,20.0,12540.0,2016-05-16T08:48:36.530,2019-01-15T08:03:28.463,Amazon Alexa: store user's words,<amazon><alexa-skills-kit><alexa-skill>,6.0,0.0,,6.0,
84784974,41957744,2,"@ManishMehra The answer would be better if you edited it to clarify colintobing's point of confusion; it's non-obvious without checking the docs which parameters refer to local paths and which ones to S3 paths without checking the docs or reading the comments. (Once that's done, you can flag to have all the comments here purged, since they'll be obsolete.)",2018-02-20T16:30:59.997,1709587,CC BY-SA 3.0,True,1,you,),0,"(Once that's done, you can flag to have all the comments here purged, since they'll be obsolete.)",False,False,,,,,,,,,,,,,,,,,,,,
84870002,48908295,0,You are using the Root user of an Account to access resources? This is discouraged because Root users have total access that cannot be restricted. You should always use IAM Users.,2018-02-22T15:51:51.017,174777,CC BY-SA 3.0,True,1,This,.,1,This is discouraged because Root users have total access that cannot be restricted.,False,False,48908295.0,48906143.0,2018-02-21T14:24:38.787,0.0,"<p>You are granting access to an <em>Account</em>, which does not mean all its users also gain access.</p>

<p>Try a principal like:</p>

<pre><code>arn:aws:iam::ACCOUNT:user/*
</code></pre>
",3,CC BY-SA 3.0,48906143.0,2018-02-21T12:37:46.073,0.0,87.0,2018-02-22T15:50:27.960,2018-02-22T15:50:27.960,Can't access the Amazon S3 data created by the master account in the member account,<amazon-web-services><amazon-s3>,2.0,3.0,,,
84907504,33381032,0,unfortunately this example is out of date,2018-02-23T14:53:22.783,4332207,CC BY-SA 3.0,True,1,,outdated,0,unfortunately this example is outdated,False,False,33381032.0,27334846.0,2015-10-28T00:45:40.413,5.0,"<pre><code>// Set the AWS credentials provider to use Facebook's auth token
let credentialProvider = AWSCognitoCredentialsProvider(
    regionType: CognitoRegionType, 
    identityPoolId: CognitoIdentityPoolId)
let logins: NSDictionary = NSDictionary(dictionary: 
    [""graph.facebook.com"" : self.fbToken])
credentialProvider.logins = logins as [NSObject : AnyObject]
credentialProvider.refresh()
let configuration = AWSServiceConfiguration(
    region: DefaultServiceRegionType,
    credentialsProvider: credentialProvider)

AWSServiceManager.defaultServiceManager().defaultServiceConfiguration = configuration
</code></pre>

<p>Where self.fbToken is the Facebook token, and CognitoRegionType, CognitoIdentityPoolId, and DefaultServiceRegionType are all defined constants.</p>
",3,CC BY-SA 3.0,27334846.0,2014-12-06T18:17:17.083,3.0,3771.0,,2015-10-28T00:45:40.413,Example for cognito login using SWIFT & Facebook,<ios><swift><amazon-web-services><amazon-s3><amazon-cognito>,2.0,0.0,,2.0,
84937423,17941145,0,"#1 is not completely out of date...You can add/delete GSI's on the fly, however you cannot modify the primary key (hash/sort).",2018-02-24T17:53:13.710,4426091,CC BY-SA 3.0,True,1,1,...,1,#1 is not completely outdated...,False,False,,,,,,,,,,,,,,,,,,,,
84945348,48969794,0,I also tried your other command in the terminal but BUILD FAILED in 1m 43s Configuration 'compile' in project ':app' is deprecated. Use 'implementation' instead.,2018-02-25T03:30:04.143,4683153,CC BY-SA 3.0,True,1,app,.,0,Configuration 'compile' in project ':app' is deprecated.,False,False,48969794.0,48969368.0,2018-02-25T02:58:38.947,0.0,"<p>Replace </p>

<pre><code>implementation files('libs/aws-android-sdk-apigateway-core-2.6.15.jar')
</code></pre>

<p>with </p>

<pre><code> compile group: 'com.amazonaws', name: 'aws-android-sdk-apigateway-core', version: '2.6.15'
</code></pre>

<p>if not work,try below command for identify the issue</p>

<pre><code>gradlew -q dependencies yourProject:dependencies --configuration compile
</code></pre>
",7,CC BY-SA 3.0,48969368.0,2018-02-25T01:29:13.863,1.0,814.0,2018-02-25T05:26:19.493,2018-02-25T05:26:19.493,More than one file was found with OS independent path 'com/amazonaws/services/sqs/request.handlers',<android><amazon-web-services><android-gradle-plugin>,1.0,0.0,,,
84945468,48969794,0,"Configuration 'compile' in project ':app' is deprecated. Use 'implementation' instead. simply replace compile with implementation keyword for example `compile group: 'com.amazonaws', name: 'aws-android-sdk-apigateway-core', version: '2.6.15'` updated like implementation `group: 'com.amazonaws', name: 'aws-android-sdk-apigateway-core', version: '2.6.15'`",2018-02-25T03:40:34.193,8603832,CC BY-SA 3.0,True,1,app,.,0,Configuration 'compile' in project ':app' is deprecated.,False,False,48969794.0,48969368.0,2018-02-25T02:58:38.947,0.0,"<p>Replace </p>

<pre><code>implementation files('libs/aws-android-sdk-apigateway-core-2.6.15.jar')
</code></pre>

<p>with </p>

<pre><code> compile group: 'com.amazonaws', name: 'aws-android-sdk-apigateway-core', version: '2.6.15'
</code></pre>

<p>if not work,try below command for identify the issue</p>

<pre><code>gradlew -q dependencies yourProject:dependencies --configuration compile
</code></pre>
",7,CC BY-SA 3.0,48969368.0,2018-02-25T01:29:13.863,1.0,814.0,2018-02-25T05:26:19.493,2018-02-25T05:26:19.493,More than one file was found with OS independent path 'com/amazonaws/services/sqs/request.handlers',<android><amazon-web-services><android-gradle-plugin>,1.0,0.0,,,
84973603,48951784,0,"Thanks Sudharsan for your answer! It worked! I had to do a few changes because the Amazon example is deprecated. I will past the code I used.
Thanks for all the clarifications!",2018-02-26T08:50:24.667,6892193,CC BY-SA 3.0,True,1,,.,0,I had to do a few changes because the Amazon example is deprecated.,False,False,48951784.0,48950637.0,2018-02-23T15:59:25.690,1.0,"<p>SES uses S3 Client-side encryption, you must use the Amazon S3 encryption client to decrypt the email after retrieving it from Amazon S3, Here is the code sample and reference from AWS Documentation. </p>

<p><a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/client-side-using-kms-java.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonS3/latest/dev/client-side-using-kms-java.html</a></p>

<p><a href=""https://docs.aws.amazon.com/ses/latest/DeveloperGuide/receiving-email-action-s3.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/ses/latest/DeveloperGuide/receiving-email-action-s3.html</a></p>

<blockquote>
  <p>Your mail is encrypted by Amazon SES using the Amazon S3 encryption
  client before the mail is submitted to Amazon S3 for storage. It is
  not encrypted using Amazon S3 server-side encryption. This means that
  you must use the Amazon S3 encryption client to decrypt the email
  after retrieving it from Amazon S3, as the service has no access to
  use your AWS KMS keys for decryption. This encryption client is
  available in the AWS SDK for Java</p>
</blockquote>
",1,CC BY-SA 3.0,48950637.0,2018-02-23T14:58:41.107,4.0,814.0,2018-02-23T15:13:17.963,2018-02-26T10:51:32.653,Amazon S3: Encrypt messages coming from Amazon SES,<amazon-web-services><amazon-s3><amazon-kms>,3.0,0.0,,,
85175739,49076831,0,"Well, I did try this and just added a do nothing for if there is an err, that did get the website to pull up, just with none of the galleries, I feel like there has to be something outdated in the mongo connections? I tried reverting to an earlier version of mongoose and mongodb, but it didn't seem to help. It's just confusing because everything works perfectly fine locally on windows, it's only when I upload the files to the EC2 Ubuntu instance that these problems occur.",2018-03-03T14:32:23.030,7103600,CC BY-SA 3.0,True,1,,?,0,"just with none of the galleries, I feel like there has to be something outdated in the mongo connections?",False,False,49076831.0,49076542.0,2018-03-02T20:25:30.810,0.0,"<p>docs in <code>for (var i = 0; i &lt; docs.length; i += chunkSize)</code>seems to be undefined, an error might have occurred which is not being handled.
I suggest you handle the error case as well.
You may include an if-then-else case as follows:</p>

<pre><code>router.get('/', function(req, res, next) {
    Photo.find({ galleryHeader: true },function (err, docs) {
      if(err){
        //handle error case
      } else {
        var photoChunks = [];
        var chunkSize = 3;
        for (var i = 0; i &lt; docs.length; i += chunkSize) {
          photoChunks.push(docs.slice(i, i + chunkSize));
        }
        res.render('photoSite/index', {title: 'Luminosity Photography', 
photos: photoChunks});
      }
    });
});
</code></pre>
",2,CC BY-SA 3.0,49076542.0,2018-03-02T20:05:39.397,0.0,18.0,,2018-03-02T20:25:30.810,Cannot find property length of undefined - MEAN Stack web app on AWS EC2 instance,<javascript><linux><amazon-web-services><amazon-ec2><mean>,1.0,1.0,,,
85323311,28569038,1,"Seems it is deprecated, Use `AmazonS3 s3Client = AmazonS3ClientBuilder.standard().build()`",2018-03-07T18:44:00.657,269292,CC BY-SA 3.0,True,1,,AmazonS3,0,"Seems it is deprecated, Use `AmazonS3",False,False,28569038.0,28568635.0,2015-02-17T18:58:14.350,31.0,"<p>The 'File' class from Java doesn't understand that S3 exists. <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/RetrievingObjectUsingJava.html"" rel=""noreferrer"">Here's an example of reading a file from the AWS documentation</a>:</p>

<pre><code>AmazonS3 s3Client = new AmazonS3Client(new ProfileCredentialsProvider());        
S3Object object = s3Client.getObject(new GetObjectRequest(bucketName, key));
InputStream objectData = object.getObjectContent();
// Process the objectData stream.
objectData.close();
</code></pre>
",7,CC BY-SA 3.0,28568635.0,2015-02-17T18:34:22.970,16.0,43141.0,,2020-08-25T17:41:43.703,Read AWS s3 File to Java code,<java><amazon-web-services><amazon-s3>,3.0,0.0,,3.0,
85403164,44341051,0,"Indeed, I had to change it to make it work. I submitted a [pull request](https://github.com/aws/aws-sdk-cpp/pull/822) even if I understand Amazon does not like the idea of including this deprecated process in the SDK.",2018-03-09T17:28:20.003,1847677,CC BY-SA 3.0,True,1,,.,1,even if I understand Amazon does not like the idea of including this deprecated process in the SDK.,False,False,44341051.0,44325418.0,2017-06-03T06:29:49.530,1.0,"<p>Signature Version 2 has been deprecated in favor of the more secure version 4. We don't support it, and the only reason it is supported in other sdks is for backwards compatibility. For Amazon S3, you can turn off body signing for v4. This is the default option for the S3 Client in the C++ SDK.</p>
",3,CC BY-SA 3.0,44325418.0,2017-06-02T09:29:11.253,1.0,731.0,,2017-06-03T06:29:49.530,How can I set signature to v2 by using AWS S3 C++ SDK,<amazon-s3><aws-sdk><aws-sdk-cpp>,1.0,3.0,,,
85522960,46145794,0,"@JustinHelgerson Possibly my answer is now obsolete and the approach you've linked to is superior, but I can't tell just by looking, and since I no longer work with a .NET stack I don't fancy burning the hours that are probably needed to figure it all out. But thanks for the link nonetheless; hopefully somebody will try out the new documented solution and either post their own answer describing it or offer up a comment explaining why it doesn't work.",2018-03-13T15:21:30.660,1709587,CC BY-SA 3.0,True,1,@JustinHelgerson,stack,1,"@JustinHelgerson Possibly my answer is now obsolete and the approach you've linked to is superior, but I can't tell just by looking, and since I no longer work with a .NET stack",False,False,,,,,,,,,,,,,,,,,,,,
85617607,49307463,1,"Please, avoid link-only answers. They become obsolete if the remote resource is not available. Try to describe the most important aspects of the solution in your post.",2018-03-15T19:31:44.293,7158505,CC BY-SA 3.0,True,1,They,.,1,They become obsolete if the remote resource is not available.,False,False,49307463.0,32970790.0,2018-03-15T19:08:50.650,0.0,"<p>Try this link : <a href=""https://aws.amazon.com/blogs/mobile/use-amazon-cognito-in-your-website-for-simple-aws-authentication/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/mobile/use-amazon-cognito-in-your-website-for-simple-aws-authentication/</a></p>

<p>It explains everything for Java SDK. Similar to the reporter I had a harrowing experience going through loads of documentations before arriving here.</p>
",1,CC BY-SA 3.0,32970790.0,2015-10-06T13:06:07.237,5.0,6730.0,,2018-03-15T19:08:50.650,Can I authenticate with AWS Cognito from plain Java?,<java><android><amazon-web-services><amazon-cognito>,2.0,0.0,,3.0,
85629426,27917079,1,"Not anymore works. Looks like they have deprecated 'Principal' field.

Error : ```This policy contains the following error: Has prohibited field Principal For more information about the IAM policy grammar, see AWS IAM Policies```",2018-03-16T05:53:42.880,2251418,CC BY-SA 3.0,True,1,,"

",0,"Looks like they have deprecated 'Principal' field.

",False,False,,,,,,,,,,,,,,,,,,,,
85784534,49370391,0,thank you. It seems the guide I was using is rather outdated.,2018-03-20T18:19:01.463,user9487981,CC BY-SA 3.0,True,1,It,.,0,It seems the guide I was using is rather outdated.,False,False,49370391.0,49359953.0,2018-03-19T18:53:12.340,0.0,"<p>You are definitely using Signature Version 2, not V4.</p>

<pre><code>signature = base64.b64encode(hmac.new(aws_secret, policy, hashlib.sha256).digest())
</code></pre>

<p>...and here...</p>

<pre><code>fd.append('AWSAccessKeyId',awspolicy.key);
</code></pre>

<p>This parameter is <code>X-Amz-Credential</code>, and contains additional information along with the aws-access-key-id.</p>

<blockquote>
  <p>according to the docs ... we need to add the headers to our request.</p>
</blockquote>

<p>You are confusing two different interfaces.</p>

<p>What you are doing is a form <code>POST</code> upload, which does not use the <code>Authorization</code> header.</p>

<p>Review <a href=""https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html</a>.</p>
",1,CC BY-SA 3.0,49359953.0,2018-03-19T09:50:24.193,0.0,48.0,2018-03-19T10:57:05.990,2018-03-19T18:53:12.340,amazon complians I am not using correct algorithm to sign requests I am,<python><angular><amazon-s3><sha256>,1.0,0.0,,,
85785312,4048235,0,This link is no longer active. They have removed it from their blog (probably for good reason since it was likely out of date).,2018-03-20T18:40:46.607,293280,CC BY-SA 3.0,True,1,They,.,0,They have removed it from their blog (probably for good reason since it was likely outdated).,False,False,,,,,,,,,,,,,,,,,,,,
85794793,49395856,0,"@JonScott Thank you very much for the reference. Referring it got added on March 8th. With cloud, you are always outdated.",2018-03-21T01:00:07.903,299462,CC BY-SA 3.0,True,1,you,.,0,"With cloud, you are always outdated.",False,False,,,,,,,,,,,,,,,,,,,,
86215566,43277404,0,"npm i @types/aws-sdk --save-dev
npm WARN deprecated @types/aws-sdk@2.7.0: This is a stub types definition for aws-sdk (https://github.com/aws/aws-sdk-js). aws-sdk provides its own type definitions, so you don't need @types/aws-sdk installed!",2018-04-02T01:25:58.557,225630,CC BY-SA 3.0,True,1,,aws,0,npm WARN deprecated @types/aws,False,False,,,,,,,,,,,,,,,,,,,,
86909636,20697082,0,best to delete obsolete info,2018-04-20T17:06:06.850,3576984,CC BY-SA 3.0,True,1,,info,0,best to delete obsolete info,False,False,,,,,,,,,,,,,,,,,,,,
87081364,50032619,0,"ah yes I see that it is deprecated. thanks! I was able to use 
    res.sendFile('/web/index.html', { root: '.' }) and that worked! thank you!",2018-04-25T23:20:27.557,3827252,CC BY-SA 3.0,True,1,,.,0,ah yes I see that it is deprecated.,False,False,50032619.0,50032521.0,2018-04-25T23:16:15.143,3.0,"<blockquote>
  <p>path must be absolute or specify root to res.sendFile</p>
</blockquote>

<p>You can use the <code>root</code> option for it:</p>

<pre><code>res.sendFile('index.html', { root: '.' })
</code></pre>

<p>By the way, <code>sendfile</code> is deprecated, use <code>sendFile</code>.</p>
",1,CC BY-SA 3.0,50032521.0,2018-04-25T23:02:33.523,1.0,160.0,2018-04-26T05:32:54.610,2018-04-26T05:32:54.610,Express can't load index,<node.js><amazon-web-services><express>,1.0,0.0,,,
87483690,50232995,0,Think the main problem here is that the tutorials you are following are a bit outdated. AWS splitted up the functionality between CognitoIdentityClient  and CognitoIdentityProviderClient quite some time ago. The only example I could find is here: https://forums.aws.amazon.com/thread.jspa?threadID=248662,2018-05-08T12:05:16.217,1054268,CC BY-SA 4.0,True,1,problem,.,0,Think the main problem here is that the tutorials you are following are a bit outdated.,False,False,50232995.0,50232690.0,2018-05-08T11:55:12.213,1.0,"<p>The problem here is that <a href=""https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.CognitoIdentity.CognitoIdentityClient.html"" rel=""nofollow noreferrer"">CognitoIdentityClient</a> does not contain the adminInitiateAuth functionality.</p>

<p>You will need to use the <a href=""https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.CognitoIdentityProvider.CognitoIdentityProviderClient.html"" rel=""nofollow noreferrer"">CognitoIdentityProviderClient</a></p>
",4,CC BY-SA 4.0,50232690.0,2018-05-08T11:38:50.230,0.0,430.0,,2018-05-08T11:55:12.213,Unable to log in to AWS cognito user pool using PHP SDK,<php><amazon-web-services><aws-sdk><amazon-cognito>,1.0,0.0,,,
87549698,45961625,0,TransferManager and MultipleFileUpload are deprecated in java,2018-05-10T05:14:10.730,8874958,CC BY-SA 4.0,True,1,TransferManager,java,0,TransferManager and MultipleFileUpload are deprecated in java,False,False,,,,,,,,,,,,,,,,,,,,
87717852,50344896,1,"Does it mean your buckets are not visible to Redshift? In that case, also Glue will be of no help, since internally it uses Redshift `COPY` command :/ This leaves you with `INSERT`s via JDBC interface, which is discouraged for large datasets.",2018-05-15T13:06:33.967,1680826,CC BY-SA 4.0,True,1,,.,0,"This leaves you with `INSERT`s via JDBC interface, which is discouraged for large datasets.",False,True,,,,,,,,,,,,,,,,,,,,
88087684,38626398,0,"It seems that this unfortunately is not working anymore since google GCM was deprecated on last april, I am contacting aws support to confirm what is their position regards this. Does anyone has issues with SNS FCM notifications?",2018-05-25T23:09:17.700,4148439,CC BY-SA 4.0,True,1,,.,1,"It seems that this unfortunately is not working anymore since google GCM was deprecated on last april, I am contacting aws support to confirm what is their position regards this.",False,False,,,,,,,,,,,,,,,,,,,,
88091429,50526609,0,"Why did you call it file.mp3 then? Also then using a traditional web server is highly discouraged, resource inefficient and hard to set up in a stable way.",2018-05-26T06:23:58.490,2648865,CC BY-SA 4.0,True,1,,.,0,"Also then using a traditional web server is highly discouraged, resource inefficient and hard to set up in a stable way.",False,False,,,,,,,,,,,,,,,,,,,,
88157311,42905817,7,This answer is outdated. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cloudfront-cloudfrontoriginaccessidentity.html,2018-05-28T22:06:44.720,7471599,CC BY-SA 4.0,True,1,answer,.,0,This answer is outdated.,False,False,42905817.0,20632828.0,2017-03-20T14:10:19.873,1.0,"<p>An Origin Access Identity cannot be created with CloudFormation. The only CloudFront resource available through Cloudformation is the <a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html"" rel=""nofollow noreferrer"">AWS::CloudFront::Distribution</a> resource.</p>

<p>You can avoid hard coding a reference to an OAI in your template by using a parameter to pass in an existing OAI when your stack is created. You can then use this parameter as the value for <code>OriginAccessIdentity</code> in the <code>S3Origin</code> type associated with the <code>S3OriginConfig</code> key. </p>

<p>This isn't ideal, but it allows you to make your templates more generic.</p>
",1,CC BY-SA 3.0,20632828.0,2013-12-17T11:24:09.707,22.0,9467.0,2016-06-03T00:36:37.693,2019-01-07T13:36:38.487,AWS - Cloud formation Script to create S3 bucket and Distribution,<amazon-web-services><amazon-cloudfront>,3.0,2.0,,2.0,
88210104,50598906,0,"(node:34792) UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'S' of undefined
    aernal/process/next_tick.js:228:7)
(node:34792) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 1)
(node:34792) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.",2018-05-30T09:02:09.460,2463570,CC BY-SA 4.0,True,1,,.,0,Unhandled promise rejections are deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
88510708,50747955,0,"Manage to ssh in and saw the file is there. Also researched that config.serve_static_assets is deprecated, rails 5 is using config.public_file_server.enabled = true. Anyway, it still doesn't work for me !",2018-06-07T23:28:21.690,3762304,CC BY-SA 4.0,True,1,,.,0,"Also researched that config.serve_static_assets is deprecated, rails 5 is using config.public_file_server.enabled = true.",False,False,,,,,,,,,,,,,,,,,,,,
88584288,42096438,0,"@StefanoBuliani lets say I have `myapi.com/v1`, `myapi.com/v2` and `myapi.com/v3`. Lets say v1 is my deprecated version, v2 is the stable version of my API and v3 is beta.  I do not want version numbers in the public stable version --per REST API Guidelines. So  myapi.com/v2/names and myapi.com/names should point to the same endpoint (or lets say Lambda). Is there a way to do this via base path mapping, any other ways --  within API Gateway ?",2018-06-10T23:50:22.850,1034442,CC BY-SA 4.0,True,1,, ,0,"Lets say v1 is my deprecated version, v2 is the stable version of my API and v3 is beta.  ",False,False,42096438.0,42095592.0,2017-02-07T17:41:06.617,13.0,"<p>Do not create the version path (/v1) as a resource in your API. Instead, simply call you API ""Names V1"" and start creating the resources (/names). When you want to make a breaking change and create a new version of the API, we recommend you create an entirely new API called ""Names V2"". Once again, simply create your resources without the version path.</p>

<p>To bring the two APIs together, you can use custom domain names. A custom domain name in API Gateway includes both a fully qualified domain name and a base path. Create two custom domain names:</p>

<ul>
<li>myapi.com/v1 -> points to the prod stage of the Names V1 API</li>
<li>myapi.com/v2 -> points to the prod stage of the Names V2 API</li>
</ul>

<p>This way you can keep bug-fixing v1 while making changes to v2 and deploy the two APIs independently. The custom domain name will bring them together and make them appear under the same domain (myapi.com/v2/names).</p>

<p>Hope this helps.</p>
",14,CC BY-SA 3.0,42095592.0,2017-02-07T16:54:58.720,11.0,5720.0,,2020-06-26T14:46:46.587,URI-based Versioning for AWS API Gateway,<api><amazon-web-services><aws-api-gateway>,2.0,0.0,,2.0,
88657023,28075622,1,"If you're running Spark on EMR, `s3://` is the correct scheme to use. Otherwise, `s3n://` has been deprecated in favor of `s3a://`. I updated [my answer](https://stackoverflow.com/a/27494547/877069) accordingly.",2018-06-12T21:32:40.703,877069,CC BY-SA 4.0,True,1,,.,0,"Otherwise, `s3n://` has been deprecated in favor of `s3a://`.",False,False,28075622.0,27477730.0,2015-01-21T19:48:54.943,7.0,"<p>Note: Under Spark 1.2, the proper format would be as follows:</p>

<pre><code>val rdd = sc.textFile(""s3n://&lt;bucket&gt;/&lt;foo&gt;/bar.*.gz"")
</code></pre>

<p>That's <code>s3n://</code>, <strong>not</strong> <code>s3://</code></p>

<p>You'll also want to put your credentials in <code>conf/spark-env.sh</code> as <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>.</p>
",3,CC BY-SA 3.0,27477730.0,2014-12-15T05:10:39.013,15.0,15140.0,2016-08-23T08:49:05.190,2018-06-12T21:40:21.757,How to read multiple gzipped files from S3 into a single RDD?,<amazon-s3><apache-spark>,3.0,0.0,,4.0,
88896039,50948793,0,is there any good tutorial which is not outdated how to deploy a spring application with database?,2018-06-20T13:21:06.967,6624525,CC BY-SA 4.0,True,1,,?,1,is there any good tutorial which is not outdated how to deploy a spring application with database?,False,True,50948793.0,50948591.0,2018-06-20T12:56:06.227,1.0,"<blockquote>
  <p>When I click on the Configuration menu entry, I wanted to enable a mysql instance / db, but I am not able to modify it (select an instance class etc.) or get access to the database with e.g SequelPro. </p>
</blockquote>

<p>Beanstalk is container-based service, you can't run MySQL on same instance, you should start corresponding one from RDS or use a separate EC2 instance. Beanstalk instances should be stateless as any moment new instance can be added or removed. </p>

<blockquote>
  <p>I can not find the log for the spring application deployed</p>
</blockquote>

<p>You can access logs and access the instance via EB CLI  <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb3-ssh.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb3-ssh.html</a>
Tou should be able to test DB connectivity same way</p>
",3,CC BY-SA 4.0,50948591.0,2018-06-20T12:44:13.583,0.0,37.0,,2018-06-20T12:56:06.227,Deploying Spring on AWS Elastic Beanstalk,<java><spring><amazon-web-services>,1.0,0.0,,,
88995861,50975081,0,"Schema merging has been disabled by default since Spark 1.5.
Also `sqlContext` is deprecated, prefer

`val mergedDF = spark.read.option(""mergeSchema"", ""true"").parquet(""data/test_table"")`

when writing new code.",2018-06-23T12:14:29.007,5813962,CC BY-SA 4.0,True,1,,"

",0,"Also `sqlContext` is deprecated, prefer

",False,False,50975081.0,50970704.0,2018-06-21T18:08:54.703,0.0,"<p>There is another idea: instead of changing the type of the existing field (field_string), add a new field of the long type (field_long) and update the code that reads the data to something like this (in pseudocode) and enable schema merging. I believe it it enabled by default but this is a good case to be explicit about it:</p>

<pre><code>sqlContext.read.option(""mergeSchema"", ""true"").parquet(&lt;parquet_file&gt;)

...

if isNull(field_long) 
  field_value_long = field_string.value.to_long
else
  field_value_long = field_long.value
</code></pre>
",1,CC BY-SA 4.0,50970704.0,2018-06-21T14:13:05.090,0.0,2576.0,,2018-07-06T04:32:28.530,How to load mixed Parquet schema into DataFrame using Apache Spark?,<apache-spark><dataframe><amazon-s3><parquet>,2.0,0.0,,1.0,
89287333,50518832,3,The **Windows Server 2016 with Containers AMI** come with an extremely outdated version of `docker`.  As soon as you install the latest version the `docker daemon` refuses to start as it doesn't have hyper-v enabled.  Has anyone got a solution for that?,2018-07-03T10:13:50.940,1769485,CC BY-SA 4.0,True,1,, ,0,** come with an extremely outdated version of `docker`.  ,False,False,50518832.0,46248193.0,2018-05-24T22:05:17.123,13.0,"<p>You don't actually <em>need</em> to install Docker for Windows (formerly known as the Docker Toolbox) in order to utilize Docker on Windows Server.</p>

<p>First, it's important to understand that there are two different types of containers on the Windows Server 2016 platform: Windows Containers and Hyper-V containers.</p>

<ul>
<li>Windows Containers - runs on top of the Windows Server kernel, no virtual machines used here</li>
<li>Hyper-V Containers - virtual machine containers, each with their own kernel</li>
</ul>

<p>There's also a third option that runs on top of Hyper-V called Linux Containers on Windows (LCOW), but we won't get into that, as it appears you're specifically asking about Windows containers.</p>

<p>Here are a couple options you can look at:</p>

<h3>Bare Metal Instances on AWS</h3>

<p>If you absolutely need to run Windows Hyper-V containers on AWS, or want to run Linux containers with Docker for Windows, you can provision the <code>i3.metal</code> EC2 instance type, which is a bare metal instance. You can deploy Windows Server 2016 onto the <code>i3.metal</code> instance type, install Hyper-V, and install Docker for Windows. This will give you the ability to run both Linux containers (under a Hyper-V Linux guest), Hyper-V containers, and Windows containers.</p>

<h3>ECS-Optimized AMI</h3>

<p>Amazon provides an Amazon Machine Image (AMI) that you can deploy EC2 instances from, which contains optimizations for the Amazon Elastic Container Service (ECS). ECS is a cloud-based clustering service that enables you to deploy container-based applications across an array of worker nodes running in EC2.</p>

<p>Generally you'll use ECS and the ECS-optimized AMI together to build a production-scale cluster to deploy your applications onto.</p>

<h3>Windows Server 2016 with Containers AMI</h3>

<p>There's also a ""<em>Windows Server 2016 with Containers</em>"" AMI available, which isn't the same as the ECS-optimized AMI, but does include support for running Docker containers on Windows Server 2016. All you have to do is deploy a new EC2 instance, using this AMI, and you can log into it and start issuing Docker commands to launch Windows containers. <strong>This option is most likely the easiest option for you, if you're new to Windows containers.</strong></p>
",5,CC BY-SA 4.0,46248193.0,2017-09-15T22:24:57.033,13.0,11646.0,2020-08-26T22:07:28.810,2020-08-26T22:07:28.810,How can I run Docker in a AWS Windows Server environment?,<amazon-web-services><docker><windows-server-2012><windows-server-2016>,3.0,6.0,,4.0,
89543737,33106542,0,I think this may be slightly out of date now. It appears that for a single bucket you can disable versioning now which means you can opt out of this behaviour. https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-versioning.html,2018-07-11T10:34:52.700,472987,CC BY-SA 4.0,True,1,,.,0,I think this may be slightly outdated now.,False,False,33106542.0,33106248.0,2015-10-13T15:22:15.860,11.0,"<p>It's not possible to append to an existing file on AWS S3. When you upload an object it creates a new version if it already exists:</p>

<blockquote>
  <p>If you upload an object with a key name that already exists in the
  bucket, Amazon S3 creates another version of the object instead of
  replacing the existing object</p>
</blockquote>

<p>Source: <a href=""http://docs.aws.amazon.com/AmazonS3/latest/UG/ObjectOperations.html"" rel=""noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/UG/ObjectOperations.html</a></p>

<p>The objects are immutable.</p>

<p>It's also mentioned in these AWS Forum threads:</p>

<p><a href=""https://forums.aws.amazon.com/message.jspa?messageID=179375"" rel=""noreferrer"">https://forums.aws.amazon.com/message.jspa?messageID=179375</a>
<a href=""https://forums.aws.amazon.com/message.jspa?messageID=540395"" rel=""noreferrer"">https://forums.aws.amazon.com/message.jspa?messageID=540395</a></p>
",6,CC BY-SA 3.0,33106248.0,2015-10-13T15:08:21.550,3.0,22380.0,,2019-06-10T22:53:37.307,Updating a file in Amazon S3 bucket,<amazon-web-services><amazon-s3>,2.0,0.0,,3.0,
89626312,51307879,0,"Using Cognito is a best practice as it gives you temporary credentials that are short lived and helps with keeping your app secure. Another option that I can think of, which is definitely not something that I recommend from a security standpoint, is to use the com.amazonaws.auth.BasicAWSCredentials that you can use to pass in the accessKey and secretKey of the IAM user into the app. As these keys are not short-lived, their usage in most circumstances is discouraged, but you can try it out to see if it fits what you are looking to do.",2018-07-13T12:59:00.090,9282303,CC BY-SA 4.0,True,1,usage,.,1,"As these keys are not short-lived, their usage in most circumstances is discouraged, but you can try it out to see if it fits what you are looking to do.",False,False,51307879.0,51295263.0,2018-07-12T14:12:15.257,0.0,"<p>One way to accomplish what you are looking to do is to add user login to your app using a Cognito Identity Pool in conjunction with a SignInProvider. For SignInProviders you have multiple choices a) you can use Cognito User Pools to setup your own user store with the ability to sign-in/sign-up etc b) Or you can federate from a social login provider such as FaceBook, Google etc (See <a href=""https://docs.aws.amazon.com/aws-mobile/latest/developerguide/add-aws-mobile-user-sign-in.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/aws-mobile/latest/developerguide/add-aws-mobile-user-sign-in.html</a> for more information)</p>

<p>As part of the setup of a Cognito Identity Pool and a SignProvider, two roles will be created for you - one for auth users ( for users who have signed in) and an unauth role (for users who haven't signed in). You can configure the CloudWatchLogReadOnly access on the auth role and that should get you properly setup. </p>
",2,CC BY-SA 4.0,51295263.0,2018-07-11T23:01:38.257,0.0,84.0,,2018-07-12T14:12:15.257,How to allow users to sign in to AWS Console from an android app,<android><amazon-web-services>,1.0,0.0,,1.0,
89641347,51332879,0,"Yeah, I think I had tried to update both, but found there was something else that was deprecated that had broken another chunk of code. So I had ended up at what I thought was a working state. I did find that if I created a new project with .net core 2 and downloaded the newest packages, I was able to get things working. So in the end the solution might just be to port everything to the newer framework. Assuming I can get approval to spend time on that. :)",2018-07-13T21:24:27.983,6363273,CC BY-SA 4.0,True,1,,.,0,"Yeah, I think I had tried to update both, but found there was something else that was deprecated that had broken another chunk of code.",False,False,51332879.0,51315906.0,2018-07-13T21:13:48.130,2.0,"<p>Short answer is I think you need to update to a newer version of AWSSDK.DynamoDBv2.</p>

<p>We had a pull request that removed the need for AsyncRunner from AWSSDK.Core that AWSSDK.DynamoDBv2 needed and so it was removed. In hindsight seeing your problem it wasn't the right decision to remove. If you update both your AWSSDK.Core and AWSSDK.DynamoDBv2 packages you would have been fine but in this case you updated just AWSSDK.Core but your older version of AWSSDK.DynamoDBv2 is still referencing AsyncRunner from AWSSDK.Core.</p>

<p>I'll have a discussion with the team and see about adding back AsyncRunner for cases like this.</p>
",3,CC BY-SA 4.0,51315906.0,2018-07-12T23:48:37.223,0.0,1407.0,2018-07-12T23:54:50.917,2018-07-13T21:13:48.130,AWS .NET SDK DynamoDB LoadAsync returning 'Could not load type' error,<c#><amazon-web-services><aws-lambda><amazon-dynamodb>,1.0,0.0,,,
89685345,51348542,1,"Yep, if you can't connect and continue to see the same issue with outdated DNS results, I'd talk to your server team.",2018-07-16T07:27:56.167,7529276,CC BY-SA 4.0,True,1,,.,1,"Yep, if you can't connect and continue to see the same issue with outdated DNS results, I'd talk to your server team.",False,False,51348542.0,51346145.0,2018-07-15T12:47:28.623,2.0,"<p>Looks like your DNS is returning the wrong IP address, that is the old IP address for your website. Given WhatsmyDNS (in my comment above - ie <a href=""https://www.whatsmydns.net/#A/schoolvoice.com"" rel=""nofollow noreferrer"">https://www.whatsmydns.net/#A/schoolvoice.com</a>) shows that DNS records around the world are all set with the new IP, its something to do with your local computer/network not seeing the new IP address. As you've already flushed the DNS cache on your computer and that hasn't fixed it, I suggest updating the DNS servers you use. </p>

<p>You can do this at a computer level by following the guide here (for windows - sorry not sure what OS you are using on your computer): <a href=""https://www.teamknowhow.com/kit-guide/computing/laptops/dell/inspiron-13-5000/how-to-change-your-dns-settings-windows-10"" rel=""nofollow noreferrer"">https://www.teamknowhow.com/kit-guide/computing/laptops/dell/inspiron-13-5000/how-to-change-your-dns-settings-windows-10</a> - I'd suggest using Google DNS servers 8.8.8.8 and 8.8.4.4 (reference: <a href=""https://developers.google.com/speed/public-dns/"" rel=""nofollow noreferrer"">https://developers.google.com/speed/public-dns/</a>)</p>

<p>You can test the results that using Google DNS will produce. This is instructions for Windows but you can do similar on other OSs:</p>

<ol>
<li>Open Command Prompt</li>
<li>Type <code>nslookup</code> -> enter</li>
<li>Type <code>server 8.8.8.8</code> -> enter (this tells nslookup to use that server, ie google's DNS server for this query)</li>
<li>Type <code>schoolvoice.com</code> -> enter. This should show you the latest IP for the A record for that domain</li>
</ol>

<p>(reference: <a href=""https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/nslookup"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/nslookup</a> )</p>
",3,CC BY-SA 4.0,51346145.0,2018-07-15T07:14:17.853,1.0,1598.0,,2018-07-15T12:47:28.623,Error 1001 : DNS resolution error for my website after removing domain from Cloudflare,<amazon-web-services><dns><cloudflare>,1.0,4.0,,,
89735318,12891164,0,"The entire domain seems offline but here's a wayback link:
https://web.archive.org/web/20160328183724/http://johan.heapsource.com/post/31047804966/the-state-of-websockets-ssl-and-sticky-sessions

This information is very outdated though, so I don't know how much it'll help anymore.",2018-07-17T12:48:23.920,268939,CC BY-SA 4.0,True,1,information,.,1,"This information is very outdated though, so I don't know how much it'll help anymore.",False,False,,,,,,,,,,,,,,,,,,,,
89804827,42441997,0,This answer is outdated. You can now certainly use SSL Certificate with Custom domains with S3 along with Route53 and Cloudfront. The SSL Certificate can be requested from Certificate Manager for free.,2018-07-19T07:35:36.720,1137672,CC BY-SA 4.0,True,1,answer,.,0,This answer is outdated.,False,False,42441997.0,42441828.0,2017-02-24T15:10:05.297,23.0,"<p>Unfortunately you can't use an SSL certificate with your custom domain with S3. You can use the S3 domain with the Amazon SSL certificate like: <code>https://my-example-bucket.s3-website-us-east-1.amazonaws.com</code>.</p>

<p>If you want to use a custom domain with SSL, and you can't use CloudFront, then you will need to look into placing some other proxy in front of S3 like your own Nginx server or something.</p>
",8,CC BY-SA 3.0,42441828.0,2017-02-24T15:01:58.323,36.0,19012.0,,2019-05-05T13:40:23.263,https on S3 WITHOUT cloudfront possible?,<amazon-web-services><amazon-s3><https><ssl-certificate><lets-encrypt>,4.0,0.0,,1.0,
90282200,40836165,1,"Note I had initially gone to the wiki page to get the AMI IDs but they were outdated and my cloudFormation template errors mentioned ""not authorized for images"". I had to use the AWS CLI with the query here to get the latest 7.x image ID values.",2018-08-02T17:37:13.030,874803,CC BY-SA 4.0,True,1,,.,1,"Note I had initially gone to the wiki page to get the AMI IDs but they were outdated and my cloudFormation template errors mentioned ""not authorized for images"".",False,False,,,,,,,,,,,,,,,,,,,,
90328315,39625697,0,"marshalling has been deprecated, use `DynamoDBTypeConverter ` instead:  https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBTypeConverted.html",2018-08-04T05:41:48.403,985088,CC BY-SA 4.0,True,1,marshalling,use,0,"marshalling has been deprecated, use",False,False,39625697.0,39588299.0,2016-09-21T20:08:48.353,9.0,"<p>Used DynamoDbMapper's -  marshalIntoObjects(..)</p>

<p><a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBMapper.html"" rel=""noreferrer"">http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBMapper.html</a></p>
",1,CC BY-SA 3.0,39588299.0,2016-09-20T07:37:14.253,1.0,4626.0,2017-04-04T04:40:40.500,2017-04-04T04:40:40.500,"Is there a library to map Dynamo Db Map<String ,AttributeValue> to an Object?",<jackson><gson><amazon-dynamodb><aws-sdk>,1.0,2.0,,1.0,
90351984,37121376,2,Note that using `AWS.config.accessKeyId=` or `AWS.config.secretAccessKey=` is now deprecated,2018-08-05T14:43:21.230,5177017,CC BY-SA 4.0,True,1,,deprecated,0,Note that using `AWS.config.accessKeyId=` or `AWS.config.secretAccessKey=` is now deprecated,False,False,37121376.0,37096719.0,2016-05-09T16:55:25.513,19.0,"<p>As Frederick mentioned hardcoding is not an AWS recommended standard, and this is not something you would want to do in a production environment. However, for testing purpose, and learning purposes, it can be the simplest way. </p>

<p>Since your request was specific to AWS EC2, here is a small example that should get you started.</p>

<p>To get a list of all the methods available to you for Node.js reference this <a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EC2.html"" rel=""noreferrer""> AWS documentation</a>.</p>

<pre><code>var AWS = require('aws-sdk'); 

AWS.config = new AWS.Config();
AWS.config.accessKeyId = ""accessKey"";
AWS.config.secretAccessKey = ""secretKey"";
AWS.config.region = ""us-east-1"";

var ec2 = new AWS.EC2();

var params = {
  InstanceIds: [ /* required */
    'i-4387dgkms3',
    /* more items */
  ],
  Force: true
};
ec2.stopInstances(params, function(err, data) {
  if (err) console.log(err, err.stack); // an error occurred
  else     console.log(data);           // successful response
});
</code></pre>
",2,CC BY-SA 3.0,37096719.0,2016-05-08T05:55:29.543,15.0,19915.0,2019-10-28T15:52:24.413,2019-10-28T15:52:24.413,Cant set AWS credentials in nodejs,<node.js><amazon-web-services><amazon-ec2>,3.0,1.0,,1.0,
90748483,51886858,0,"Thank you for your response, cause I see in this tutorial [link](https://towardsdatascience.com/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722)  that   Windows user, and you’re one of those unfortunates to run the outdated Docker toolbox, make sure you use some directory in the C:\Users directory as your project home folder. Otherwise, you’ll run into a very ugly experience of mounting the folder to the container.",2018-08-17T15:39:55.340,9360453,CC BY-SA 4.0,True,1,,.,0,"Thank you for your response, cause I see in this tutorial [link](https://towardsdatascience.com/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722)  that   Windows user, and you’re one of those unfortunates to run the outdated Docker toolbox, make sure you use some directory in the C:\Users directory as your project home folder.",False,True,,,,,,,,,,,,,,,,,,,,
91308115,42021271,0,This should be the accepted answer now. Current accepted answer is obsolete,2018-09-05T07:06:39.470,7061173,CC BY-SA 4.0,True,1,,obsolete,0,Current accepted answer is obsolete,False,False,42021271.0,7679342.0,2017-02-03T09:46:34.507,2.0,"<p>The accepted answer was right at the time but the API updated in 2014 so this is the new way</p>

<pre><code>PutObjectRequest request = new PutObjectRequest
{
    BucketName = bucketName,
    ContentType = ""text/css"",
    Key = filename
};
request.Headers[""Content-Encoding""] = ""gzip"";
</code></pre>
",1,CC BY-SA 3.0,7679342.0,2011-10-06T19:20:00.547,4.0,3022.0,2011-10-06T19:36:45.773,2019-07-23T19:34:14.993,Setting content-encoding and content-type with Amazon API for .NET,<c#><asp.net><api><header><amazon-s3>,3.0,0.0,,2.0,
91358154,47324631,0,"I've heard StreamX is also being deprecated, or at least not maintained or updated",2018-09-06T12:51:46.097,2308683,CC BY-SA 4.0,True,1,,updated,1,"I've heard StreamX is also being deprecated, or at least not maintained or updated",False,False,47324631.0,37685114.0,2017-11-16T08:14:39.157,1.0,"<p>From the description, it looks like what you are looking for is 
1) Avro data write to S3</p>

<p>2) Data to be partitioned in S3</p>

<p>3) Exactly-once support while writing.</p>

<p>Qubole <a href=""https://github.com/qubole/streamx"" rel=""nofollow noreferrer"">StreamX</a> supports rich variety of format conversions, avro being one of them, along with data partition. 
And, exactly once is in our pipeline which will be out soon. </p>

<p>Whereas secor is getting deprecated(mentioned in one of their responses on google group) and it also do not support avro.</p>

<p>So you can use qubole streamx to start with.</p>
",1,CC BY-SA 3.0,37685114.0,2016-06-07T16:56:41.567,1.0,1096.0,,2017-11-16T08:14:39.157,Avro Records -> Kafka -> Kafka Connect Sink -> Amazon S3 Storage. Idempotency?,<amazon-s3><apache-kafka><idempotent>,3.0,0.0,,,
91731585,47681908,0,There are a couple ways to reach this error. Two that I found were: 1) using the deprecated api in lieu of KeyConditionExpression and 2) a typo in my GSI definition. there was a mismatch between the value I wanted the GSI to be an index on and what the index actually was on. Deleting and re-defining the index fixed it.,2018-09-18T19:17:31.907,584947,CC BY-SA 4.0,True,1,,.,0,Two that I found were: 1) using the deprecated api in lieu of KeyConditionExpression and 2) a typo in my GSI definition.,False,False,47681908.0,47681108.0,2017-12-06T19:18:03.380,41.0,"<p>You're trying to run a query using a condition that does not include the primary key. This is <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html"" rel=""noreferrer"">how queries work in DynamoDB</a>. You would need to do a <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html"" rel=""noreferrer"">scan for the info in your case</a>, however, I don't think that is the best option.</p>

<p>I think you want to set up a <a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html"" rel=""noreferrer"">global secondary index</a> and use that to query for the <code>processing</code> status.</p>
",5,CC BY-SA 3.0,47681108.0,2017-12-06T18:27:54.280,35.0,47286.0,,2019-10-29T18:39:14.663,Query condition missed key schema element : Validation Error,<node.js><amazon-web-services><nosql><amazon-dynamodb>,3.0,0.0,,3.0,
92124419,32318270,0,"Can also just run `knex migrate:latest` without the node node_modules stuff. But is `prestart` the right place to do this? If there are table conflicts, the server will fail to start. Are you guys using `createTableIfNotExists`? It's said to be deprecated, so not sure how to handle tables that already exist.",2018-10-01T16:01:41.310,1736601,CC BY-SA 4.0,True,1,,.,1,"It's said to be deprecated, so not sure how to handle tables that already exist.",False,False,,,,,,,,,,,,,,,,,,,,
92506798,47679666,1,@vipertherapper AWS is not only incredibly huge but has lots of outdated documentation and few docs are well written. When a company lets you earn degrees to know how to use their services... buckle up!,2018-10-13T12:59:07.380,2495341,CC BY-SA 4.0,True,1,,.,1,@vipertherapper AWS is not only incredibly huge but has lots of outdated documentation and few docs are well written.,False,False,,,,,,,,,,,,,,,,,,,,
92595400,52839467,0,"This page is out of date. There are new rules. They ask how this key will be used (public, private, other), how many employees you have, EIN, and etc. Also they ask to describe your app. My app is quite easy but they decied to decline and told me about how cool are their 3rd-party apps in Seller Central section (ofcourse they are not free). And also they ask to wait up to 30 days for their decision.",2018-10-16T16:09:32.860,7520450,CC BY-SA 4.0,True,1,page,.,0,This page is outdated.,False,False,,,,,,,,,,,,,,,,,,,,
92683330,49521915,1,Passing access key and secret key as part of Bulk load is deprecated. You should first attach an IAM Role using AddRoleToCluster API and then mention the role name as an argument to bulk load. https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html,2018-10-19T02:41:19.900,3069919,CC BY-SA 4.0,True,1,,.,0,Passing access key and secret key as part of Bulk load is deprecated.,False,False,49521915.0,49088195.0,2018-03-27T20:30:47.840,0.0,"<p>Yes, AWS Neptune end point can be connected using postman and I am able to successfully load the TinkerPop modern graph from S3 to neptune db instance using following command.</p>

<pre><code>'Content-Type: application/json' \
    http://your-neptune-endpoint:8182/loader -d '
    { 
      ""source"" : ""s3://neptune-us-east-1/tinkerpopmodern/"", 
      ""format"" : ""csv"", 
      ""accessKey"" : ""access-key-id"", 
      ""secretKey"" : ""secret-key"", 
      ""region"" : ""us-east-1"", 
      ""failOnError"" : ""FALSE"",
      ""mode"" : ""NEW""
    }'
</code></pre>
",1,CC BY-SA 3.0,49088195.0,2018-03-03T19:16:18.890,1.0,1193.0,,2018-10-24T07:18:39.057,How to Connect/Query to AWS Neptune instance using HTTP POST/GET request,<amazon-neptune>,3.0,1.0,,1.0,
92733032,52903573,1,The last line in my code (above) apparently both configures and starts the agent. You should not use `awslogs` -- that is old and outdated.,2018-10-21T00:23:49.253,174777,CC BY-SA 4.0,True,1,,.,1,You should not use `awslogs` -- that is old and outdated.,False,False,,,,,,,,,,,,,,,,,,,,
92758688,50800155,0,"While this link may answer the question, link only answers are discouraged on Stack Overflow, you can improve this answer by taking vital parts of the link and putting it into your answer, this makes sure your answer is still an answer if the link gets changed or removed :)",2018-10-22T07:58:35.353,7147233,CC BY-SA 4.0,True,1,,:),0,"While this link may answer the question, link only answers are discouraged on Stack Overflow, you can improve this answer by taking vital parts of the link and putting it into your answer, this makes sure your answer is still an answer if the link gets changed or removed :)",False,False,50800155.0,38334744.0,2018-06-11T14:31:15.780,1.0,"<p>Cloudwatch supports integration with a few common logging frameworks - <a href=""https://aws.amazon.com/blogs/developer/amazon-cloudwatch-logs-and-net-logging-frameworks/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/developer/amazon-cloudwatch-logs-and-net-logging-frameworks/</a></p>

<p>You can find configuration samples in their github repo - <a href=""https://github.com/aws/aws-logging-dotnet"" rel=""nofollow noreferrer"">https://github.com/aws/aws-logging-dotnet</a></p>
",2,CC BY-SA 4.0,38334744.0,2016-07-12T16:45:38.180,3.0,3426.0,,2019-07-15T09:48:33.797,How do I write a custom event string to Aws cloudwatch logs,<c#><amazon-web-services><cloudwatch><amazon-cloudwatchlogs>,3.0,6.0,,,
92903357,52999332,0,link only answers are discouraged.,2018-10-25T23:20:00.500,user10226920,CC BY-SA 4.0,True,1,answers,.,0,link only answers are discouraged.,False,False,52999332.0,52999281.0,2018-10-25T23:13:42.390,0.0,"<p>You can achieve this with Amazon <code>Route 53</code> and <code>EC2</code>.</p>

<p><a href=""https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/TutorialAddingLBRRegion.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/TutorialAddingLBRRegion.html</a></p>
",4,CC BY-SA 4.0,52999281.0,2018-10-25T23:05:31.413,-2.0,51.0,2018-10-26T00:41:13.893,2018-10-26T01:13:24.323,How to use the minimum latency in Amazon EC2 around the globe?,<php><amazon-web-services><amazon-ec2><amazon-cloudfront>,2.0,2.0,,,
92967459,53015106,0,Thanks for the help. Turns out Lambda uses an out of date SDK version,2018-10-28T16:28:19.027,4096899,CC BY-SA 4.0,True,1,,version,0,Turns out Lambda uses an outdated SDK version,False,False,53015106.0,53014542.0,2018-10-26T19:21:54.303,2.0,"<p>Could you clarify how you're deploying your lambda function?</p>

<p>This can happen if you've accidentally included a version of the aws-sdk (rather than using the latest already provided by Amazon in the container).</p>

<p>The <code>getMetricWidgetImage</code> function was only added in 2.318 (end of September; <a href=""https://github.com/aws/aws-sdk-js/blob/01039b3e5b1299ede23776a9bb56db9aeded1c2d/.changes/2.318.0.json"" rel=""nofollow noreferrer"">change log</a>.) So it's likely that any cached version you have locally could be behind.</p>
",5,CC BY-SA 4.0,53014542.0,2018-10-26T18:37:42.603,0.0,436.0,2018-10-27T14:27:46.680,2018-10-27T14:27:46.680,AWS Lambda cloudWatch.getMetricWidgetImage is not a function,<amazon-web-services><aws-lambda>,1.0,0.0,,,
93001977,26111627,0,"Yes to what @ShuheiKagawa said; `ContentEncoding` is not necessary. Also, according to the latest Node.js docs, that form of the `Buffer` constructor is deprecated, replaced by `Buffer.from(..., 'base64')`.",2018-10-29T17:54:42.610,396414,CC BY-SA 4.0,True,1,,.,0,"Also, according to the latest Node.js docs, that form of the `Buffer` constructor is deprecated, replaced by `Buffer.from(..., 'base64')`.",False,False,,,,,,,,,,,,,,,,,,,,
93068700,53064629,0,So just set the bid price to be the same as the on demand price for the same behaviour? Also the documentation is simply outdated if the AWS API now allows you to add spot MASTER instances to your EMR cluster because there isn't currently a check in the Terraform code to prevent that from happening.,2018-10-31T14:40:28.543,2291321,CC BY-SA 4.0,True,1,documentation,.,1,Also the documentation is simply outdated if the AWS API now allows you to add spot MASTER instances to your EMR cluster because there isn't currently a check in the Terraform code to prevent that from happening.,False,False,53064629.0,53064338.0,2018-10-30T12:46:17.520,-1.0,"<p>You can specify a bid price for the instances your EMR cluster uses under the <a href=""https://www.terraform.io/docs/providers/aws/r/emr_cluster.html#instance_group-1"" rel=""nofollow noreferrer""><code>instance_group</code> block</a> to have Terraform use spot instances for the instances.</p>

<p>The <a href=""https://www.terraform.io/docs/providers/aws/r/emr_cluster.html#example-usage"" rel=""nofollow noreferrer"">example in the <code>aws_emr_cluster</code> resource documentation</a> shows how this should be specified:</p>

<pre><code>resource ""aws_emr_cluster"" ""emr-test-cluster"" {
  name          = ""emr-test-arn""
  release_label = ""emr-4.6.0""
  applications  = [""Spark""]
  additional_info = &lt;&lt;EOF
{
  ""instanceAwsClientConfiguration"": {
    ""proxyPort"": 8099,
    ""proxyHost"": ""myproxy.example.com""
  }
}
EOF

  termination_protection = false
  keep_job_flow_alive_when_no_steps = true

  ec2_attributes {
    subnet_id                         = ""${aws_subnet.main.id}""
    emr_managed_master_security_group = ""${aws_security_group.sg.id}""
    emr_managed_slave_security_group  = ""${aws_security_group.sg.id}""
    instance_profile                  = ""${aws_iam_instance_profile.emr_profile.arn}""
  }

  instance_group {
      instance_role = ""CORE""
      instance_type = ""c4.large""
      instance_count = ""1""
      ebs_config {
        size = ""40""
        type = ""gp2""
        volumes_per_instance = 1
      }
      bid_price = ""0.30""
      autoscaling_policy = &lt;&lt;EOF
{
""Constraints"": {
  ""MinCapacity"": 1,
  ""MaxCapacity"": 2
},
""Rules"": [
  {
    ""Name"": ""ScaleOutMemoryPercentage"",
    ""Description"": ""Scale out if YARNMemoryAvailablePercentage is less than 15"",
    ""Action"": {
      ""SimpleScalingPolicyConfiguration"": {
        ""AdjustmentType"": ""CHANGE_IN_CAPACITY"",
        ""ScalingAdjustment"": 1,
        ""CoolDown"": 300
      }
    },
    ""Trigger"": {
      ""CloudWatchAlarmDefinition"": {
        ""ComparisonOperator"": ""LESS_THAN"",
        ""EvaluationPeriods"": 1,
        ""MetricName"": ""YARNMemoryAvailablePercentage"",
        ""Namespace"": ""AWS/ElasticMapReduce"",
        ""Period"": 300,
        ""Statistic"": ""AVERAGE"",
        ""Threshold"": 15.0,
        ""Unit"": ""PERCENT""
      }
    }
  }
]
}
EOF
}
  ebs_root_volume_size     = 100

  master_instance_type = ""m3.xlarge""
  core_instance_type   = ""m3.xlarge""
  core_instance_count  = 1

  tags {
    role     = ""rolename""
    env      = ""env""
  }

  bootstrap_action {
    path = ""s3://elasticmapreduce/bootstrap-actions/run-if""
    name = ""runif""
    args = [""instance.isMaster=true"", ""echo running on master node""]
  }

  configurations_json = &lt;&lt;EOF
  [
    {
      ""Classification"": ""hadoop-env"",
      ""Configurations"": [
        {
          ""Classification"": ""export"",
          ""Properties"": {
            ""JAVA_HOME"": ""/usr/lib/jvm/java-1.8.0""
          }
        }
      ],
      ""Properties"": {}
    },
    {
      ""Classification"": ""spark-env"",
      ""Configurations"": [
        {
          ""Classification"": ""export"",
          ""Properties"": {
            ""JAVA_HOME"": ""/usr/lib/jvm/java-1.8.0""
          }
        }
      ],
      ""Properties"": {}
    }
  ]
EOF  

  service_role = ""${aws_iam_role.iam_emr_service_role.arn}""
}
</code></pre>
",2,CC BY-SA 4.0,53064338.0,2018-10-30T12:29:11.850,1.0,472.0,,2018-10-30T12:46:17.520,Amazon EMR use spot instances as core and Master Nodes using Terraform,<amazon-web-services><terraform><amazon-emr>,1.0,0.0,,,
93336685,29894552,0,To create an instance of TransferManager better to use TransferManagerBuilder.defaultTransferManager() as the constructor of TransferManager is deprecated,2018-11-09T11:34:12.457,379173,CC BY-SA 4.0,True,1,,deprecated,0,To create an instance of TransferManager better to use TransferManagerBuilder.defaultTransferManager() as the constructor of TransferManager is deprecated,False,False,29894552.0,8749151.0,2015-04-27T11:32:53.033,17.0,"<p>HI This is the simple way to upload the Directory into S3 bucket.</p>

<pre><code>BasicAWSCredentials awsCreds = new BasicAWSCredentials(access_key_id,
            secret_access_key);
    AmazonS3 s3Client = new AmazonS3Client(awsCreds);

    TransferManager tm = new TransferManager(s3Client);


    MultipleFileUpload upload = tm.uploadDirectory(existingBucketName,
            ""BuildNumber#1"", ""FilePathYouWant"", true);
</code></pre>
",2,CC BY-SA 3.0,8749151.0,2012-01-05T20:16:47.270,9.0,12728.0,2012-01-05T22:29:52.957,2015-04-27T11:32:53.033,Upload Directory with files to S3 using Java,<java><file-upload><amazon-s3><jets3t>,2.0,1.0,,4.0,
93408637,53259973,0,This option is deprecated in version 6.x and will be removed in 7. Since I am using dfs_query_then_fetch it should ideally query all the shards.,2018-11-12T12:29:36.847,7862558,CC BY-SA 4.0,True,1,option,.,0,This option is deprecated in version 6.x and will be removed in 7.,False,False,,,,,,,,,,,,,,,,,,,,
93645651,47593328,0,"Directly constructing the config has been deprecated in favor of using the builder:

```DynamoDBMapperConfig.builder().withSaveBehavior(DynamoDBMapperConfig.SaveBehavior.CLOBBER).build()```",2018-11-20T01:14:35.053,1418271,CC BY-SA 4.0,True,1,,.,0,"Directly constructing the config has been deprecated in favor of using the builder:

```DynamoDBMapperConfig.builder().withSaveBehavior(DynamoDBMapperConfig.",False,False,47593328.0,38532729.0,2017-12-01T12:22:24.160,2.0,"<p>You can delete the item by specifying the SaveBehaviour as CLOBBER without worrying about the version.</p>

<pre><code>DynamoDBMapper mapper;
mapper.delete(object, new DynamoDBMapperConfig(DynamoDBMapperConfig.SaveBehavior.CLOBBER)
</code></pre>
",1,CC BY-SA 3.0,38532729.0,2016-07-22T18:01:09.097,1.0,374.0,,2017-12-01T12:22:24.160,Force delete on table that uses optimistic locking with version number,<java><amazon-web-services><amazon-dynamodb>,2.0,0.0,,,
93698357,53379175,0,"I see two possibilities here, either you use outdated Kubernetes dashboard - in that case please delete it and install from [official GitHub](https://github.com/kubernetes/dashboard) page. Or try to change https to http - but I recommend deleting old dashboard and installing the proper one (you can do it for example by `kubectl delete -f *enter old yaml used for dashboard deployment*`. 
If this will not fix the problem please provide results of `kubectl get services --all-namespaces`",2018-11-21T12:15:20.833,9892073,CC BY-SA 4.0,True,1,,.,0,"I see two possibilities here, either you use outdated Kubernetes dashboard - in that case please delete it and install from [official GitHub](https://github.com/kubernetes/dashboard) page.",False,False,53379175.0,53354734.0,2018-11-19T16:45:33.893,3.0,"<p>If you only want to reach the dashboard then it is pretty easy, get the IP address of your EC2 instance and the Port on which it is serving dashboard (<code>kubectl get services --all-namespaces</code>) and then reach it using:
First:</p>

<p><code>kubectl proxy --address 0.0.0.0 --accept-hosts '.*'</code></p>

<p>And in your browswer:</p>

<p><code>http://&lt;IP&gt;:&lt;PORT&gt;/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login</code></p>

<p><strong>Note that this is a possible security vulnerability as you are  accepting all traffic</strong> (AWS firewall rules) and also all connections for your <code>kubectl proxy</code> (<code>--address 0.0.0.0 --accept-hosts '.*'</code>) so please narrow it down or use different approach. If you have more questions feel free to ask. </p>
",6,CC BY-SA 4.0,53354734.0,2018-11-17T19:24:49.447,3.0,2034.0,2018-11-19T05:59:26.970,2018-11-27T04:12:59.717,Kubernetes dashboard in aws EC2 instance?,<amazon-web-services><docker><amazon-ec2><kubernetes>,3.0,0.0,,,
94459983,47686027,1,This information is outdated. See https://aws.amazon.com/about-aws/whats-new/2018/09/aws-fargate-now-supports-private-registry-authentication/,2018-12-16T19:30:21.530,407257,CC BY-SA 4.0,True,1,information,.,0,This information is outdated.,False,False,,,,,,,,,,,,,,,,,,,,
94515424,8819466,0,That's deprecated. Use the official SDK: https://docs.aws.amazon.com/de_de/sdk-for-php/v3/developer-guide/s3-presigned-url.html,2018-12-18T13:05:27.213,1331671,CC BY-SA 4.0,True,1,That,.,0,That's deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
94628086,7115527,0,"Yes, this answer is now outdated.",2018-12-21T20:36:47.830,396138,CC BY-SA 4.0,True,1,answer,.,0,"Yes, this answer is now outdated.",False,False,7115527.0,3459177.0,2011-08-18T23:55:20.607,0.0,"<p>The aws-s3 gem does not have the ability to copy files in between buckets without moving files to your local machine. If that's acceptable to you, then the following will work:</p>

<pre><code>AWS::S3::S3Object.store 'dest-key', open('http://url/to/source.file'), 'dest-bucket'
</code></pre>
",2,CC BY-SA 3.0,3459177.0,2010-08-11T14:09:25.087,12.0,11536.0,2020-02-10T05:33:31.663,2020-02-10T05:33:31.663,How to copy file across buckets using aws-s3 or aws-sdk gem in ruby on rails,<ruby-on-rails><ruby><ruby-on-rails-3><amazon-s3><aws-sdk>,9.0,0.0,,6.0,
94628086,7115527,0,"Yes, this answer is now outdated.",2018-12-21T20:36:47.830,396138,CC BY-SA 4.0,True,1,answer,.,0,"Yes, this answer is now outdated.",False,False,7115527.0,3459177.0,2011-08-18T23:55:20.607,0.0,"<p>The aws-s3 gem does not have the ability to copy files in between buckets without moving files to your local machine. If that's acceptable to you, then the following will work:</p>

<pre><code>AWS::S3::S3Object.store 'dest-key', open('http://url/to/source.file'), 'dest-bucket'
</code></pre>
",2,CC BY-SA 3.0,3459177.0,2010-08-11T14:09:25.087,12.0,11536.0,2020-02-10T05:33:31.663,2020-02-10T05:33:31.663,How to copy file across buckets using aws-s3 or aws-sdk gem in ruby on rails,<ruby-on-rails><ruby><ruby-on-rails-3><amazon-s3><aws-sdk>,9.0,0.0,,6.0,
94901068,54001095,0,"Major version upgrade also gets easily done with above approach. Only problem is we need to take care of what things have changed in newer version. Ex - I was getting some errors such as usage of command has changed, some options were deprecated.",2019-01-04T02:42:38.910,6280515,CC BY-SA 4.0,True,1,options,.,0,"Ex - I was getting some errors such as usage of command has changed, some options were deprecated.",False,False,54001095.0,53897521.0,2019-01-02T04:10:06.267,0.0,"<p>Finally, I was able to resolve this which means a K8s minor upgrade is successful. Below steps were followed during the course: </p>

<ul>
<li>Deploy a K8s cluster running version 1.11.2</li>
<li>Double the node count, change version to 1.11.5 and re-deploy</li>
<li>New nodes get created with updated version</li>
<li>Remove nodes running old version i.e. 1.11.2</li>
<li>Run <code>terraform refresh</code> so as to sync statefile with real world running Infrastructure</li>
<li>Change the node count to 3 or half it. </li>
<li>Run <code>terraform plan</code> and verify (multiple runs of refresh might be needed)</li>
<li>Run <code>terraform apply</code> to apply changes. </li>
<li>Statefile should be in sync with remote </li>
<li>Run terraform plan which shouldn’t show any resources to be created</li>
</ul>

<p>I will be trying a major version upgrade shortly and post the results here. </p>
",1,CC BY-SA 4.0,53897521.0,2018-12-22T16:45:21.450,0.0,251.0,2018-12-24T06:09:29.533,2019-01-02T04:10:06.267,Upgrade Kubernetes Cluster using Terraform's provisioner,<amazon-web-services><kubernetes><terraform>,1.0,2.0,,,
95102631,13685835,0,This is outdated,2019-01-10T19:22:12.827,717267,CC BY-SA 4.0,True,1,,outdated,0,This is outdated,False,False,13685835.0,13685764.0,2012-12-03T14:58:22.743,7.0,"<p>Follow the tutorial at <a href=""http://carltonbale.com/how-to-alias-a-domain-name-or-sub-domain-to-amazon-s3/"" rel=""noreferrer"">http://carltonbale.com/how-to-alias-a-domain-name-or-sub-domain-to-amazon-s3/</a> to alias your domain to your S3 bucket.</p>
",3,CC BY-SA 3.0,13685764.0,2012-12-03T14:54:39.030,10.0,7212.0,,2019-01-10T19:21:19.617,Custom url for images hosted on Amazon S3,<amazon-s3>,1.0,0.0,2019-01-11T08:14:17.107,6.0,
95255983,23698987,0,note : $ eb cli is the now preferred elastic beanstalk deployment cli tool of choice and is not deprecated one refereed to above.,2019-01-16T10:36:32.200,1081275,CC BY-SA 4.0,True,1,,.,1,$ eb cli is the now preferred elastic beanstalk deployment cli tool of choice and is not deprecated one refereed to above.,False,False,,,,,,,,,,,,,,,,,,,,
95411763,9648866,2,"I just want to add that the tips from the first link are obsolete now, and you might want to add that to your answer:
https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/
""This S3 request rate performance increase removes any previous guidance to randomize object prefixes to achieve faster performance. That means you can now use logical or sequential naming patterns in S3 object naming without any performance implications. """,2019-01-21T18:16:33.573,6700830,CC BY-SA 4.0,True,1,I,"
",0,"I just want to add that the tips from the first link are obsolete now, and you might want to add that to your answer:
",False,False,,,,,,,,,,,,,,,,,,,,
95411831,52445252,8,"This guide is obsolete. It does not matter if you put a random prefix or not now because S3 will now hash that internally:

https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/ 
""This S3 request rate performance increase removes any previous guidance to randomize object prefixes to achieve faster performance. That means you can now use logical or sequential naming patterns in S3 object naming without any performance implications. """,2019-01-21T18:19:14.463,6700830,CC BY-SA 4.0,True,1,guide,.,0,This guide is obsolete.,False,False,,,,,,,,,,,,,,,,,,,,
95433893,48509471,0,"@SteveW. you're right! In my own code, I'd only used Strings but I got carried away in writing up my answer. I've made the correction. This type of API from AWS appears to be dropped by Amazon. AWSCognitoSync, for example, is also limited to String values and has recently been deprecated and replaced by AWSAppSync - a far more involving API.",2019-01-22T11:16:22.057,42756,CC BY-SA 4.0,True,1,,.,0,"AWSCognitoSync, for example, is also limited to String values and has recently been deprecated and replaced by AWSAppSync - a far more involving API.",False,False,,,,,,,,,,,,,,,,,,,,
95491812,54329221,0,"@LovekushVishwakarma Maybe you can implement the multiple logic on your side ( or propose a feature request on the github forum). In any case, using TransferManager is problematic as support for it has been deprecated a few years ago - see https://aws.amazon.com/blogs/mobile/aws-sdk-for-android-transfer-manager-to-transfer-utility-migration-guide/",2019-01-23T23:37:24.250,9282303,CC BY-SA 4.0,True,1,,https://aws.amazon.com/blogs/mobile/aws-sdk-for-android-transfer-manager-to-transfer-utility-migration-guide/,0,"In any case, using TransferManager is problematic as support for it has been deprecated a few years ago - see https://aws.amazon.com/blogs/mobile/aws-sdk-for-android-transfer-manager-to-transfer-utility-migration-guide/",False,False,54329221.0,54184611.0,2019-01-23T14:15:25.297,0.0,"<p>The TransferManager component in the AWS Android SDK has been deprecated in favor of the TransferUtility component. The TransferUtility component allows you to pause and resume transfers. It also has support for network monitoring and will automatically pause and resume transfers when the network goes down and comes back up. Here is the link to the TransferUtility documentation - <a href=""https://aws-amplify.github.io/docs/android/storage"" rel=""nofollow noreferrer"">https://aws-amplify.github.io/docs/android/storage</a></p>
",4,CC BY-SA 4.0,54184611.0,2019-01-14T15:40:57.953,2.0,612.0,2020-04-03T13:09:48.827,2020-04-03T13:09:48.827,How to resume upload with AWS S3 Android,<java><android><amazon-web-services><amazon-s3>,1.0,2.0,,,
95544006,48603353,0,AWSS3TransferManager is deprecated please suggest latest one.,2019-01-25T12:16:53.367,4201337,CC BY-SA 4.0,True,1,AWSS3TransferManager,.,0,AWSS3TransferManager is deprecated please suggest latest one.,False,False,48603353.0,48595543.0,2018-02-04T00:02:24.970,1.0,"<p>Here is the code that worked based on sqlbot's comment:  The configuration and transfer manager definitions are moved to the initializer for the helper class.  If picture files are stored in the document file system then creating a temporary file will also not be needed gaining further advantage however I use coreData so need to keep that.</p>

<p>initialize an instance of MediaHelper (let mediaHelper = MediaHelper()) early on in your viewController and maintain a reference to it so that the initialization is done early on and only once.</p>

<pre><code>class MediaHelper {

    var transferManager: AWSS3TransferManager!

    init() {
        let credentialsProvider = AWSCognitoCredentialsProvider(regionType:.SomeRegion,
                                                                identityPoolId:""my identity pool ..."")
        let configuration = AWSServiceConfiguration(region:.SomeRegion, credentialsProvider:credentialsProvider)
        AWSServiceManager.default().defaultServiceConfiguration = configuration

        self.transferManager = AWSS3TransferManager.default()
    }

    func uploadMedia(mediaData: Data, mediaID: String) {

        let uploadingFileURL = URL(fileURLWithPath: NSTemporaryDirectory()).appendingPathComponent(mediaID)
        do {
            try mediaData.write(to: uploadingFileURL, options: Data.WritingOptions.atomic)
        } catch let error as NSError {
            print(""Could not save \(error), \(error.userInfo)"")
        }

        let uploadRequest = AWSS3TransferManagerUploadRequest()!

        uploadRequest.bucket = ""myBucket""
        uploadRequest.key = mediaID + "".jpg""
        uploadRequest.body = uploadingFileURL
        uploadRequest.contentType = ""image/jpeg""

        transferManager.upload(uploadRequest).continueWith(executor: AWSExecutor.mainThread(), block: { (task:AWSTask&lt;AnyObject&gt;) -&gt; Any? in

            if let error = task.error as NSError? {
                if error.domain == AWSS3TransferManagerErrorDomain, let code = AWSS3TransferManagerErrorType(rawValue: error.code) {
                    switch code {
                    case .cancelled, .paused:
                        break
                    default:
                        print(""Error uploading: \(uploadRequest.key!) Error: \(error)"")
                    }
                } else {
                    print(""Error uploading: \(uploadRequest.key!) Error: \(error)"")
                }
                return nil
            }

            let uploadOutput = task.result
            print(""Upload complete for: \(uploadRequest.key!)"")
            return nil
        })
    }

}
</code></pre>
",1,CC BY-SA 3.0,48595543.0,2018-02-03T08:41:29.410,2.0,598.0,2018-02-03T08:56:41.707,2018-02-04T00:02:24.970,iOS image upload to AWS S3 Bucket very slow,<ios><swift><xcode><amazon-s3><image-uploading>,1.0,4.0,,,
95633921,11276003,0,"when you open rc.local you will see a big warning telling you that it is deprecated, and that the prefered way to do it now is : systemd --> see my answer. :)",2019-01-28T22:59:47.173,1833961,CC BY-SA 4.0,True,1,,:),0,"when you open rc.local you will see a big warning telling you that it is deprecated, and that the prefered way to do it now is : systemd --> see my answer. :)",False,False,,,,,,,,,,,,,,,,,,,,
95727802,3890289,3,The `ec2metadata` tool is deprecated. Now you query the 'magic' URL at http://169.254.169.254/latest/meta-data/ - hit it with cURL and it gives you magic endpoints you can use to get various bits of data. In this case `curl http://169.254.169.254/latest/meta-data/instance-id` gets your your instance ID,2019-01-31T11:51:18.113,144876,CC BY-SA 4.0,True,1,tool,.,0,The `ec2metadata` tool is deprecated.,False,True,3890289.0,3883315.0,2010-10-08T12:05:29.343,35.0,"<p>You can use a combination of the <a href=""http://developer.amazonwebservices.com/connect/entry.jspa?externalID=1825"" rel=""noreferrer"">AWS metadata tool</a> (to retrieve your instance ID) and the <a href=""http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/index.html?Using_Tags.html"" rel=""noreferrer"">new Tag API</a> to retrieve the tags for the current instance.</p>
",5,CC BY-SA 2.5,3883315.0,2010-10-07T15:37:14.137,96.0,95027.0,2019-04-17T00:17:39.253,2020-07-29T23:35:54.673,Query EC2 tags from within instance,<amazon-web-services><amazon-ec2>,14.0,0.0,,30.0,
96108393,8304724,3,jets3t is an old deprecated library. Instead use the aws-java-sdk.,2019-02-12T20:49:39.753,1787434,CC BY-SA 4.0,True,1,,.,0,jets3t is an old deprecated library.,False,False,8304724.0,8303011.0,2011-11-29T01:28:20.313,3.0,"<p>Use the jets3t library. Its a lot more easier and robust than the AWS sdk. Using this library you can call, s3service.getObjectDetails(). This will check and retrieve only the details of the object (not the contents) of the object. It will throw a 404 if the object is missing. So you can catch that exception and deal with it in your app.</p>

<p>But in order for this to work, you will need to have ListBucket access for the user on that bucket. Just GetObject  access will not work. The reason being, Amazon will prevent you from checking for the presence of the key if you dont have ListBucket access. Just knowing whether a key is present or not, will also suffice for malicious users in some cases. Hence unless they have ListBucket access they will not be able to do so.</p>
",3,CC BY-SA 3.0,8303011.0,2011-11-28T22:04:59.063,86.0,127044.0,2016-04-15T20:43:32.403,2020-07-16T06:51:07.697,How to check if a specified key exists in a given S3 bucket using Java,<java><amazon-web-services><amazon-s3><aws-sdk>,16.0,2.0,,17.0,
96108393,8304724,3,jets3t is an old deprecated library. Instead use the aws-java-sdk.,2019-02-12T20:49:39.753,1787434,CC BY-SA 4.0,True,1,,.,0,jets3t is an old deprecated library.,False,False,8304724.0,8303011.0,2011-11-29T01:28:20.313,3.0,"<p>Use the jets3t library. Its a lot more easier and robust than the AWS sdk. Using this library you can call, s3service.getObjectDetails(). This will check and retrieve only the details of the object (not the contents) of the object. It will throw a 404 if the object is missing. So you can catch that exception and deal with it in your app.</p>

<p>But in order for this to work, you will need to have ListBucket access for the user on that bucket. Just GetObject  access will not work. The reason being, Amazon will prevent you from checking for the presence of the key if you dont have ListBucket access. Just knowing whether a key is present or not, will also suffice for malicious users in some cases. Hence unless they have ListBucket access they will not be able to do so.</p>
",3,CC BY-SA 3.0,8303011.0,2011-11-28T22:04:59.063,86.0,127044.0,2016-04-15T20:43:32.403,2020-07-16T06:51:07.697,How to check if a specified key exists in a given S3 bucket using Java,<java><amazon-web-services><amazon-s3><aws-sdk>,16.0,2.0,,17.0,
96108675,54657874,0,"Wait. Dynamic inventory is deprecated??  

https://docs.ansible.com/ansible/latest/user_guide/intro_dynamic_inventory.html",2019-02-12T21:00:49.910,5188833,CC BY-SA 4.0,True,1,inventory," 

",0,"Dynamic inventory is deprecated??  

",False,False,54657874.0,54657344.0,2019-02-12T20:03:12.553,1.0,"<p><code>tag_env_prod</code>, <code>tag_env_stage</code> are group names in your example.</p>

<p>You can use:</p>

<pre><code>when: ('tag_env_stage' in group_names)
</code></pre>

<p>But this is a bit ugly. I'd recommend to use modern Ansible version with support of inventory plugins (instead of legacy dynamic inventories). If your inventory is generated with <code>aws_ec2</code> plugin, you have direct access to <code>tags</code> variable. And so you can use:</p>

<pre><code>when: tags['env'] == 'stage'
</code></pre>
",2,CC BY-SA 4.0,54657344.0,2019-02-12T19:26:05.480,1.0,685.0,2019-02-12T19:40:47.087,2019-02-12T21:11:26.130,Ansible -- Using Conditionals from Ec2 Tags in Dynamic Inventory,<amazon-ec2><ansible><ansible-inventory>,2.0,0.0,,1.0,
96109723,14930504,0,"Wait! As noted, this answer is now deprecated - see the much simpler answer from Dmitry Shevkoplyas. It worked perfectly for me.",2019-02-12T21:41:19.407,204842,CC BY-SA 4.0,True,1,answer,.,0,"As noted, this answer is now deprecated - see the much simpler answer from Dmitry Shevkoplyas.",False,False,,,,,,,,,,,,,,,,,,,,
96158733,40396529,0,`AWSS3TransferManager` is deprecated. Can you please update the answer with `AWSS3TransferUtility`,2019-02-14T07:35:09.720,4608334,CC BY-SA 4.0,True,1,,.,0,`AWSS3TransferManager` is deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
96784816,55012698,0,"I think this is right on track, except you probably also don't want to use the deprecated `context.succeed()` here, since that's mixing async/await and callbacks.  Return a response or a promise, since you're inside an async function (the handler), yes?",2019-03-06T04:01:47.713,1695906,CC BY-SA 4.0,True,1,, ,1,"I think this is right on track, except you probably also don't want to use the deprecated `context.succeed()` here, since that's mixing async/await and callbacks.  ",False,False,,,,,,,,,,,,,,,,,,,,
97069351,33962417,0,"On March 13, 2019, official [UIAutomator doc on AWS Device Farm](https://docs.aws.amazon.com/devicefarm/latest/developerguide/test-types-android-uiautomator.html) still obsolete since it points to upload a jar file.",2019-03-14T14:30:10.313,472681,CC BY-SA 4.0,True,1,,.,0,"On March 13, 2019, official [UIAutomator doc on AWS Device Farm](https://docs.aws.amazon.com/devicefarm/latest/developerguide/test-types-android-uiautomator.html) still obsolete since it points to upload a jar file.",False,False,33962417.0,33655499.0,2015-11-27T17:37:15.377,2.0,"<p>You'll still be able to execute these tests using AWS Device Farm without converting them to a JAR.</p>

<p>On March 12, 2015, Google announced <a href=""https://plus.google.com/+AndroidDevelopers/posts/WCWANrPkRxg"" rel=""nofollow"">uiautomator 2.0</a>. Without going into too much detail, the significant change made in this new version is that these tests are based on Android instrumentation (generated as APK files) instead of the previously used system of uiautomator 1.0 (generated as JAR files).</p>

<p>I would have to examine the gradle/build configuration further, but my guess is that you're using the new Android testing libraries and thus, using uiautomator 2.0. When you build a project such as this, it will generate two APK files, one for your application and another for your instrumentation tests.</p>

<p>When using AWS Device Farm, you'll want to take these two APK files and upload them using the <strong>INSTRUMENTATION</strong> test type. This test type works for all Instrumentation based frameworks/tools such as Espresso, uiautomator 2.0, and Robotium. The <strong>UIAUTOMATOR</strong> test type is specifically for older uiautomator 1.0 projects that still build and use JAR files for their test packages.</p>
",1,CC BY-SA 3.0,33655499.0,2015-11-11T16:46:23.557,4.0,584.0,,2016-07-12T10:53:12.027,Generate UIAutomator Test JAR for AWS Device Farm from Android Studio Gradle build,<android><amazon-web-services><gradle><jar><aws-device-farm>,1.0,0.0,,,
97084127,55161680,0,Hello. It's the now deprecated 'literal' I was after.,2019-03-14T21:44:10.683,2013689,CC BY-SA 4.0,True,1,,.,0,It's the now deprecated 'literal' I was after.,False,False,55161680.0,53868742.0,2019-03-14T11:44:37.837,0.0,"<p>Here i am assuming that you may have more than one slots in same intent/utterance. </p>

<p>Alexa request converts all the slot value in array, so you can access the slot value using below code.(here ""date"" is my slot name)</p>

<pre><code>       if (intentRequest.Intent.Slots.TryGetValue(""date"", out var dateSlot))
            {
                if (!string.IsNullOrEmpty(dateSlot.Value))
                {
                   //Slot value
                }
            }
</code></pre>
",1,CC BY-SA 4.0,53868742.0,2018-12-20T12:29:46.387,0.0,345.0,2018-12-20T13:16:51.420,2019-03-14T11:44:37.837,Retrieve uttered intent and slot type from Alexa request in c# service,<c#><amazon-web-services><aws-lambda><alexa><alexa-skills-kit>,1.0,1.0,,,
97298705,42285049,0,"I'm getting another behaviour different from my earlier comment. Previously, I was getting permission denied (via CloudFront) if I didn't set objects as public on S3. Now... after few hours, when testing with few more objects, it works even if I don't set them as public. I'm afraid info in the docs is outdated, hope the behaviour will be consistent.",2019-03-21T15:54:53.573,188470,CC BY-SA 4.0,True,1,,.,0,"I'm afraid info in the docs is outdated, hope the behaviour will be consistent.",False,False,42285049.0,42251745.0,2017-02-16T21:31:23.907,55.0,"<p>To assist with your question, I recreated the situation via:</p>

<ul>
<li>Created an <strong>Amazon S3 bucket</strong> with no Bucket Policy</li>
<li>Uploaded <strong>public.jpg</strong> and make it public via ""Make Public""</li>
<li>Uploaded <strong>private.jpg</strong> and kept it private</li>
<li>Created an Amazon CloudFront <strong>web distribution</strong>:

<ul>
<li><strong>Origin Domain Name:</strong> Selected my S3 bucket from the list</li>
<li><strong>Restrict Bucket Access:</strong> Yes</li>
<li><strong>Origin Access Identity:</strong> Create a New Identity</li>
<li><strong>Grant Read Permissions on Bucket:</strong> Yes, Update Bucket Policy</li>
</ul></li>
</ul>

<p>I checked the bucket, and CloudFront had added a Bucket Policy similar to yours.</p>

<p>The distribution was marked as <code>In Progress</code> for a while. Once it said <code>Enabled</code>, I accessed the files via the <code>xxx.cloudfront.net</code> URL:</p>

<ul>
<li><code>xxx.cloudfront.net/public.jpg</code> <strong>redirected</strong> me to the S3 URL <code>http://bucketname.s3.amazonaws.com/public.jpg</code>. Yes, I could see the file, but it should not use a redirect.</li>
<li><code>xxx.cloudfront.net/private.jpg</code> <strong>redirected</strong> me also, but I then received <code>Access Denied</code> because it is a private file in S3.</li>
</ul>

<p>I then did some <a href=""https://acloud.guru/forums/aws-certified-developer-associate/discussion/-KcC5f1fw3l4-tclsXje/cloudfront_redirecting_to_orig"" rel=""noreferrer"">research</a> and found that this is quite a common occurrence. Some people use a workaround by pointing their CloudFront distribution to the <strong>static hosted website URL</strong>, but this has the disadvantage that it will not work with the Origin Access Identity and I also suspect it won't receive the 'free S3 traffic to the edge' discount.</p>

<p>So, I waited overnight, tested it this morning and <strong>everything is working fine</strong>.</p>

<p><strong>Bottom line:</strong> Even if it says <code>ENABLED</code>, things might take several hours (eg overnight) to get themselves right. It will then work as documented.</p>
",9,CC BY-SA 3.0,42251745.0,2017-02-15T14:13:20.513,39.0,28143.0,2017-02-15T15:04:42.207,2020-05-13T07:33:27.593,AWS CloudFront access denied to S3 bucket,<amazon-web-services><amazon-s3><amazon-cloudfront>,5.0,2.0,,10.0,
97509151,55394283,2,"That's exactly what where I am right now. While this is certainly possible, it's just not the standard way of handling dependencies in the maven world. It really should be in the repo; having an old, outdated version in the repo and not mentioning anything about maven in the documentation is just lazy IMO.",2019-03-28T09:42:51.590,1047469,CC BY-SA 4.0,True,1,,.,1,"It really should be in the repo; having an old, outdated version in the repo and not mentioning anything about maven in the documentation is just lazy IMO.",False,False,55394283.0,55394008.0,2019-03-28T09:35:58.067,1.0,"<p>2.0.7 is not available in public Maven repos e.g. Maven Central. </p>

<p>I'd download it from <a href=""https://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html"" rel=""nofollow noreferrer"">Using Athena with the JDBC Driver</a> page and install locally with <a href=""https://maven.apache.org/guides/mini/guide-3rd-party-jars-local.html"" rel=""nofollow noreferrer""><code>mvn install:install-file</code></a>.</p>
",3,CC BY-SA 4.0,55394008.0,2019-03-28T09:21:37.193,2.0,810.0,2019-03-28T11:42:04.703,2019-03-28T11:42:04.703,Athena JDBC Driver 2.0.7 in maven repo?,<maven><amazon-athena>,2.0,1.0,,,
97526479,28719447,1,It's now 2019. I'd like to refer you to https://aws.amazon.com/premiumsupport/knowledge-center/cron-job-elastic-beanstalk/. Seems AWS has deprecated the `cron.yaml` approach for some reason.,2019-03-28T16:51:28.197,3330613,CC BY-SA 4.0,True,1,,.,0,Seems AWS has deprecated the `cron.yaml` approach for some reason.,False,False,,,,,,,,,,,,,,,,,,,,
97530620,29580279,0,"This answer, although acceptable, wasted my time uninstalling python 3.4 to install 3.6, since the version of python3 will not always be python34, as assumed by the answer. In fact, python 3.4 is being deprecated March 2019 by Django, being replaced by 3.6.",2019-03-28T18:59:49.517,3044062,CC BY-SA 4.0,True,1,python,.,0,"In fact, python 3.4 is being deprecated March 2019 by Django, being replaced by 3.6.",False,False,,,,,,,,,,,,,,,,,,,,
97548481,55414444,0,"This is a most-up-to-date article, also by Yan Cui, where he states it's around 5 minutes again. I think the link I sent you is outdated I think, so I am sorry for that.
https://hackernoon.com/im-afraid-you-re-thinking-about-aws-lambda-cold-starts-all-wrong-7d907f278a4f. I will need to run the benchmarks again to check, but the 5 minutes thing is much more reasonable to me. Have you tested it yourself?",2019-03-29T09:57:21.543,10950867,CC BY-SA 4.0,True,1,,"
",0,"I think the link I sent you is outdated I think, so I am sorry for that.
",False,False,55414444.0,55412624.0,2019-03-29T09:39:04.420,0.0,"<p>You will only pay for init if you spend more than 10s on it. In that case your init process will restart and you will start to pay for it.</p>

<p>But what you should know if that once your function has been warmed, you will not initialised it again (till around 45min of inactivity). You will then pay only the execution time.</p>
",10,CC BY-SA 4.0,55412624.0,2019-03-29T07:42:19.953,3.0,2120.0,,2019-03-31T08:35:41.430,Does AWS Lambda charge for the time spent initializing code?,<amazon-web-services><aws-lambda><aws-billing>,5.0,0.0,,1.0,
97873507,27538483,1,The `InternalUtils` class is deprecated (use [ItemUtils](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/document/ItemUtils.html) instead).,2019-04-09T08:26:54.373,639919,CC BY-SA 4.0,True,1,class,.,0,The `InternalUtils` class is deprecated (use [ItemUtils](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/document/ItemUtils.html) instead).,False,False,27538483.0,27500749.0,2014-12-18T02:50:07.320,38.0,"<p>Finally figured out by looking at how <a href=""https://github.com/aws/aws-sdk-java/blob/c19606a6a5bfcc0e8cc7d33dde5e4f20425ca33e/aws-java-sdk-dynamodb/src/main/java/com/amazonaws/services/dynamodbv2/document/Item.java#L908"">AWS parses the JSON</a></p>

<p>Basically, this is the code:</p>

<pre><code>    Item item = new Item().withJSON(""document"", jsonStr);
    Map&lt;String,AttributeValue&gt; attributes = InternalUtils.toAttributeValues(item);
    return attributes.get(""document"").getM();
</code></pre>

<p>Very neat.</p>
",5,CC BY-SA 3.0,27500749.0,2014-12-16T08:53:34.717,9.0,20987.0,,2018-07-13T20:48:34.900,DynamoDB - Object to AttributeValue,<java><amazon-dynamodb>,3.0,3.0,,4.0,
98149903,55732829,0,Please provide an explanation as to why this is a solution to OP's problem. Code only answers are discouraged on SO as they do not help OP or future visitors of the site.,2019-04-17T20:59:01.997,8468264,CC BY-SA 4.0,True,1,answers,.,1,only answers are discouraged on SO as they do not help OP or future visitors of the site.,False,False,,,,,,,,,,,,,,,,,,,,
98221405,50942386,1,After posting my previous comment I opened a support ticket and the AWS support representative came across this same StackOverflow post and confirmed that none of the methods answers on this page will work. The scripts have been deprecated in favor of their internal ECS agent. They recommended I explore ECS as it provides more control/options for this.,2019-04-20T14:31:21.390,4620962,CC BY-SA 4.0,True,1,scripts,.,0,The scripts have been deprecated in favor of their internal ECS agent.,False,False,50942386.0,28698061.0,2018-06-20T07:16:37.013,1.0,"<p>Not sure about single docker container deployment i.e v1 since i haven't tried that yet but they are definitely supported in v2 i.e multi container deployment. You can adda container hostname by putting below in <code>Dockerrun.aws.json</code> - </p>

<pre><code>  ""hostname"": ""this-is-my-container-hostname""
</code></pre>
",3,CC BY-SA 4.0,28698061.0,2015-02-24T14:13:14.807,7.0,1660.0,,2018-06-20T07:16:37.013,Setting Docker container hostname on Elastic Beanstalk,<amazon-web-services><docker><amazon-elastic-beanstalk>,4.0,0.0,,1.0,
98409825,48314242,1,I'm mildly confused why this keeps getting upvotes (4 in the last month) if it's now obsolete.,2019-04-26T21:15:02.757,194586,CC BY-SA 4.0,True,1,I,.,0,I'm mildly confused why this keeps getting upvotes (4 in the last month) if it's now obsolete.,False,False,,,,,,,,,,,,,,,,,,,,
98514322,47498650,0,This answer is outdated.,2019-05-01T01:17:44.353,1137672,CC BY-SA 4.0,True,1,answer,.,0,This answer is outdated.,False,False,,,,,,,,,,,,,,,,,,,,
98605992,55977524,0,"of course, I see `Flag --show-all has been deprecated, will be removed in an upcoming release` :)",2019-05-03T22:27:05.580,1223975,CC BY-SA 4.0,True,1,,:),0,"of course, I see `Flag --show-all has been deprecated, will be removed in an upcoming release` :)",False,False,55977524.0,55977244.0,2019-05-03T21:53:59.020,1.0,"<p>I have hardly ever seen anyone pulling all logs from entire clusters, because you usually either need logs to manually search for certain issues or follow (<code>-f</code>) a routine, or collect audit information, or stream all logs to a log sink to have them prepared for monitoring (e.g. prometheus).</p>

<p>However, if there's a need to fetch all logs, using the <code>--tail</code> option is not what you're looking for (<code>tail</code> only shows the last number of lines of a certain log source and avoids spilling the entire log history of a single log source to your terminal).</p>

<p>For kubernetes, you can write a simple script in a language of your choice (bash, Python, whatever) to <code>kubectl get all --show-all --all-namespaces</code> and iterate over the pods to run <code>kubectl -n &lt;namespace&gt; logs &lt;pod&gt;</code>; but be aware that there might be multiple containers in a pod with individual logs each, and also logs on the cluster nodes themselves, state changes in the deployments, extra meta information that changes, volume provisioning, and heaps more.</p>

<p>That's probably the reason why it's quite uncommon to pull all logs from an entire cluster and thus there's no easy (shortcut) way to do so.</p>
",3,CC BY-SA 4.0,55977244.0,2019-05-03T21:20:43.443,5.0,4643.0,,2020-08-01T13:32:20.167,How to tail all logs in a kubernetes cluster,<kubernetes><kubectl><amazon-eks><aws-eks><eks>,6.0,3.0,,,
98612788,36271458,0,"Path style will be deprecated in the future, https://www.reddit.com/r/aws/comments/bk9mfm/s3_path_style_being_deprecated_on_sep_30_2020/ do you have another solution?",2019-05-04T10:41:58.643,5004165,CC BY-SA 4.0,True,1,,?,0,"Path style will be deprecated in the future, https://www.reddit.com/r/aws/comments/bk9mfm/s3_path_style_being_deprecated_on_sep_30_2020/ do you have another solution?",False,False,36271458.0,29652193.0,2016-03-28T21:25:24.413,6.0,"<p>The S3 client allows you to set an option to configure this:</p>

<pre><code>    // Configure the S3 client to generate urls with valid SSL certificates. With this setting we will get urls
    // like https://s3.amazonaws.com/bucket_name/... but if this is not set we will get urls like
    // https://bucket_name.s3.amazonaws.com/ which will cause the browser to complain about invalid SSL
    // certificates.
    s3Client.setS3ClientOptions(new S3ClientOptions().withPathStyleAccess(true));
</code></pre>
",3,CC BY-SA 3.0,29652193.0,2015-04-15T13:54:22.293,5.0,3368.0,2015-11-18T17:38:21.270,2016-03-28T21:25:24.413,Generate Presigned URL with format: s3.amazonaws.com/bucket.name,<amazon-s3>,2.0,1.0,,,
98949285,44169507,0,"This answer is incorrect or at least outdated. Chalice creates a Proxy Mapping which means the IntegrationResponse is not applicable, and the instructions and screenshot above are completely off-base.",2019-05-16T04:08:27.587,196032,CC BY-SA 4.0,True,1,,.,0,This answer is incorrect or at least outdated.,False,False,44169507.0,44149971.0,2017-05-24T22:30:51.897,-1.0,"<p>Make sure the <a href=""https://docs.aws.amazon.com/apigateway/api-reference/resource/integration-response/#contentHandling"" rel=""nofollow noreferrer""><code>ContentHandling</code> attribute of the IntegrationResponse</a> is set to <code>CONVERT_TO_BINARY</code>. In the AWS console, navigate to the IntegrationResponse page for your binary method(s) and choose ""Convert to binary (if needed)"".</p>

<p><a href=""https://i.stack.imgur.com/Yc1GT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yc1GT.png"" alt=""Integration Response ContentHandling dropdown""></a>
If you're comfortable with the CLI, you could also use a command like the following.</p>

<pre><code>aws apigateway update-integration-response --rest-api-id foobar 
    --resource-id barfoo --http-method GET --status-code 200 
    --patch-operations op='replace',path='/contentHandling',value='CONVERT_TO_BINARY'
</code></pre>
",2,CC BY-SA 3.0,44149971.0,2017-05-24T05:49:08.007,1.0,956.0,,2019-06-26T02:28:32.213,AWS Chalice Return an Image File from S3,<python><amazon-s3><aws-api-gateway><chalice>,2.0,1.0,,1.0,
99100577,56178702,0,@EricSnyder Yes I think it is one of the least documented services by AWS. All the examples I find are very shallow and/or a very outdated version. The AWS quick start sets up a nice full environment using almost all of the features of the user pools and federated identities. The auto config of spring security 5 is what really sealed the deal on choosing Cognito.,2019-05-21T14:47:56.970,3558975,CC BY-SA 4.0,True,1,,.,0,All the examples I find are very shallow and/or a very outdated version.,False,False,56178702.0,56175857.0,2019-05-17T02:14:00.753,1.0,"<p>I would recommend using a custom attribute since you mentioned non-AWS services. Creating an attribute named customer:role with the value of ROLE_USER, ROLE_ADMIN and so on. </p>

<p><a href=""https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-attributes.html#user-pool-settings-custom-attributes"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-attributes.html#user-pool-settings-custom-attributes</a></p>

<p>Once they are authenticated in the app you can control their permissions from decoding the custom attribute. This tutorial is more for permissions with AWS services in a SAAS multi-tenant approach. However, I found it extremely useful to learn from. </p>

<p><a href=""https://aws-quickstart.s3.amazonaws.com/saas-identity-cognito/doc/saas-identity-and-isolation-with-cognito-on-the-aws-cloud.pdf"" rel=""nofollow noreferrer"">https://aws-quickstart.s3.amazonaws.com/saas-identity-cognito/doc/saas-identity-and-isolation-with-cognito-on-the-aws-cloud.pdf</a></p>
",6,CC BY-SA 4.0,56175857.0,2019-05-16T20:05:16.153,1.0,134.0,,2019-05-17T02:14:00.753,Using AWS Cognito for desktop authentication,<amazon-web-services><amazon-cognito>,1.0,0.0,,,
99681929,38811343,0,"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html  says ""You no longer have to randomize prefix naming for performance.""        But it's unclear from the documentation how S3 does indexing (hashing? b-trees?)  and whether it can efficiently list objects matching a prefix.   The following outdated documentation offers some hints: https://aws.amazon.com/blogs/aws/amazon-s3-performance-tips-tricks-seattle-hiring-event/",2019-06-11T18:52:36.723,4876331,CC BY-SA 4.0,True,1,,https://aws.amazon.com/blogs/aws/amazon-s3-performance-tips-tricks-seattle-hiring-event/,0,The following outdated documentation offers some hints: https://aws.amazon.com/blogs/aws/amazon-s3-performance-tips-tricks-seattle-hiring-event/,False,False,38811343.0,3980968.0,2016-08-07T05:56:28.543,10.0,"<ul>
<li>There is no limit on objects per bucket.</li>
<li>There is a limit of 100 buckets per account  (you need to request amazon if you need more).</li>
<li>There is no performance drop even if you store millions of objects in a
single bucket.</li>
</ul>

<p>From docs,</p>

<blockquote>
  <p>There is no limit to the number of objects that can be stored in a
  bucket and no difference in performance whether you use many buckets
  or just a few. You can store all of your objects in a single bucket,
  or you can organize them across several buckets.</p>
</blockquote>

<p><em>as of Aug 2016</em></p>
",2,CC BY-SA 3.0,3980968.0,2010-10-20T18:14:47.213,84.0,112302.0,,2020-09-02T21:17:26.667,S3 limit to objects in a bucket,<amazon-s3><amazon-web-services>,7.0,3.0,,10.0,
99919890,41368277,0,"First of all, GREAT answer. There's a lot of confusing and outdated information out there; this is simple to follow and still works in 2019. Second, would it be possible to expand your answer to include nginx for HTTPS/SSL or at least link to another answer that provides the steps to do this for the sake of completeness? And not like you'd need it, but I'd even award a bounty on your answer if you expand your answer to include HTTPS steps (not just a link).",2019-06-19T21:57:23.340,6702203,CC BY-SA 4.0,True,1,,.,0,There's a lot of confusing and outdated information out there; this is simple to follow and still works in 2019.,False,False,,,,,,,,,,,,,,,,,,,,
100039925,12044713,0,"Note that in March 2019, Amazon announced that the so-called ""path styled"" API is deprecated, and support will be dropped in September 30th, 2020. https://forums.aws.amazon.com/ann.jspa?annID=6776 and https://news.ycombinator.com/item?id=19821406",2019-06-24T16:23:26.157,783510,CC BY-SA 4.0,True,1,,.,0,"Note that in March 2019, Amazon announced that the so-called ""path styled"" API is deprecated, and support will be dropped in September 30th, 2020.",False,False,12044713.0,12044598.0,2012-08-20T20:33:42.880,1.0,"<p>Also, apart from giving the permissions , are you sure that your resultant url for image is</p>

<p><code>https://s3.amazonaws.com/&lt;bucketname&gt;/img/background.png</code> </p>

<p>if not, you might want to make it this way. also you should have img folder inside bucket and yours files should be present in that folder.</p>
",3,CC BY-SA 3.0,12044598.0,2012-08-20T20:24:24.647,0.0,2636.0,,2012-08-20T20:33:42.880,S3 not reading relative paths?,<python><django><amazon-s3>,2.0,0.0,,,
100049287,46436392,0,"@Brannon - the cloud is moving very fast. A question that is 18 months old is out of date (usually). Creating a new question is a good idea as documentation changes and features come and go. Plus, few will see this comment, but many will see a new question.",2019-06-25T00:04:13.847,8016720,CC BY-SA 4.0,True,1,question,.,0,A question that is 18 months old is outdated (usually).,False,False,46436392.0,38084303.0,2017-09-26T21:51:10.167,3.0,"<p>You can have private and public DNS names that are the same name. For DNS queries from the public Internet only the public name will resolve. For queries from your VPC (after you setup your VPC DHCP Options Sets to point to your Route 53), the private name will resolve. We use this all the time on our EC2 instances so that they can talk to each other using private IP addresses.</p>

<p>After you setup your private zones in Route 53, look at this document to setup DNS resolution in your VPC:</p>

<p><a href=""http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_DHCP_Options.html"" rel=""nofollow noreferrer"">DHCP Options Sets</a></p>

<p>Note: You do not have to duplicate all your public zone records. Just create records for your private zone that you require. From your VPC if you do not have a private record then the public record will be returned.</p>
",7,CC BY-SA 3.0,38084303.0,2016-06-28T19:03:42.787,13.0,6452.0,,2020-01-08T04:14:27.120,AWS Route53: Private and public hosted zones under the same domain,<amazon-web-services><dns><amazon-route53>,3.0,5.0,,1.0,
100375909,56840453,0,"I agree that the kernel doc says what you have described here but since the AWS docs say otherwise and because the kernel doc itself says it is outdated, I have to trust the AWS docs. There was [another stackoverflow question](https://stackoverflow.com/questions/44764135/aws-ecs-task-memory-hard-and-soft-limits) asking something similar about the soft/hard limit.",2019-07-06T18:20:14.293,7289454,CC BY-SA 4.0,True,1,,.,0,"I agree that the kernel doc says what you have described here but since the AWS docs say otherwise and because the kernel doc itself says it is outdated, I have to trust the AWS docs.",False,False,56840453.0,56795256.0,2019-07-01T18:08:46.850,1.0,"<p>What about the case where you have multiple containers in a single task. You might want to reserve some amount of memory for one container. Also you might have a case where different containers require a lot of memory at the same time and you don't want any of them to take too much memory and stall out the rest of the containers. </p>
",11,CC BY-SA 4.0,56795256.0,2019-06-27T16:26:26.857,1.0,1043.0,2019-07-02T12:45:05.110,2019-07-02T12:45:05.110,What does memoryReservation actually do on ECS with Fargate?,<amazon-ecs><aws-fargate>,2.0,1.0,,,
100592997,16816623,2,This has [since been deprecated](https://github.com/jschneier/django-storages/pull/553),2019-07-15T00:17:39.597,284164,CC BY-SA 4.0,True,1,,storages,0,This has [since been deprecated](https://github.com/jschneier/django-storages,False,False,16816623.0,16805232.0,2013-05-29T14:28:00.667,6.0,"<p>Set <code>AWS_PRELOAD_METADATA</code> to <code>True</code> in your settings so it pre-loads all files on s3 before syncing and only syncs the ones that are not already there (or have changed).</p>
",2,CC BY-SA 3.0,16805232.0,2013-05-29T03:55:41.273,12.0,4813.0,2013-07-26T18:14:55.583,2014-04-10T15:56:42.063,Faster alternative to manage.py collectstatic (w/ s3boto storage backend) to sync static files to s3?,<django><amazon-s3><boto><django-staticfiles>,2.0,1.0,,6.0,
100643439,49920562,0,the sqs plugin seems out of date there !,2019-07-16T13:56:22.917,4230397,CC BY-SA 4.0,True,1,,!,0,the sqs plugin seems outdated there !,False,False,49920562.0,45585748.0,2018-04-19T11:58:36.660,3.0,"<p>You can use a combination of SNS Notifications for new artifacts in the S3 bucket <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a> and the Jenkins AWS SQS plugin to trigger a build (<a href=""https://github.com/jenkinsci/aws-sqs-plugin"" rel=""nofollow noreferrer"">https://github.com/jenkinsci/aws-sqs-plugin</a>)</p>

<p>A little bit of manual configuration is required in terms of the AWS SQS plugin, but it should work.</p>

<p><code>S3 Upload</code> > <code>SNS Notification</code> > <code>Publish to SQS</code> > <code>Trigger Jenkins Build</code></p>

<p><em>Ideally it would be straight to Jenkins like so: <code>S3 Upload</code> > <code>SNS Notification</code> > <code>Publish to Jenkins HTTP Endpoint</code> > <code>Trigger Jenkins Build</code></em></p>

<p>Hope this helps</p>
",1,CC BY-SA 3.0,45585748.0,2017-08-09T08:44:24.667,6.0,2853.0,,2019-10-16T05:14:09.110,Trigger Jenkins job when a S3 file is updated,<jenkins><amazon-s3><deployment><jenkins-plugins>,3.0,2.0,,1.0,
100655727,55990039,0,The repository is outdated.,2019-07-16T20:56:59.793,1947482,CC BY-SA 4.0,True,1,repository,.,0,The repository is outdated.,False,False,55990039.0,26743056.0,2019-05-05T08:11:51.403,0.0,"<p>I think the solution is here , that may help
  <a href=""http://technomile.github.io/wordpress/setup.html"" rel=""nofollow noreferrer"">http://technomile.github.io/wordpress/setup.html</a></p>
",1,CC BY-SA 4.0,26743056.0,2014-11-04T19:09:11.030,1.0,601.0,,2019-05-05T08:11:51.403,Heroku dynos not sharing the file system,<node.js><heroku><amazon-s3>,2.0,0.0,,,
100722342,57100450,1,`mysql_connect` was deprecated in 5.5 and removed in PHP 7.0 in favor of using `mysqli_connect`. In this particular case Luke was asking about PHP 7.,2019-07-18T18:27:16.823,864538,CC BY-SA 4.0,True,1,mysql_connect,.,0,`mysql_connect` was deprecated in 5.5 and removed in PHP 7.0 in favor of using `mysqli_connect`.,False,False,57100450.0,57100171.0,2019-07-18T18:13:14.627,-1.0,"<p>Use mysql_connect($servername, $username, $password) instead.</p>
",1,CC BY-SA 4.0,57100171.0,2019-07-18T17:52:56.077,0.0,38.0,2019-07-18T23:14:52.887,2019-07-18T23:14:52.887,Connecting to AWS RDS database using beanstalk and PHP,<php><mysql><amazon-web-services><amazon-rds>,2.0,0.0,,,
100897279,52009439,1,"yes, unfortunately we've to upgrade using amzaon 2 instead. the amazon 1 was obsolete, there'll be a lot of package error / required to use systemd",2019-07-25T04:26:03.520,3763032,CC BY-SA 4.0,True,1,the,systemd,0,"the amazon 1 was obsolete, there'll be a lot of package error / required to use systemd",False,False,,,,,,,,,,,,,,,,,,,,
100976710,44402449,11,"This answer is most likely obsolete. Now, as mentioned by a few people here, you can use either the [Tag Manager](https://docs.aws.amazon.com/awsconsolehelpdocs/latest/gsg/find-resources-to-tag.html) or the _'Monthly cost by service'_ in [Cost Explorer](https://aws.amazon.com/aws-cost-management/aws-cost-explorer/).",2019-07-28T01:46:18.687,1764746,CC BY-SA 4.0,True,1,answer,.,0,This answer is most likely obsolete.,False,False,,,,,,,,,,,,,,,,,,,,
101040928,57270718,0,"I tired that too, but got this.
```
warning: sysvinit-tools-2.87-6.dsf.el6.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY
error: Failed dependencies:
 sysvinit-tools < 2.88-8 conflicts with (installed) util-linux-2.23.2-33.28.amzn1.x86_64
 sysvinit-tools is obsoleted by (installed) sysvinit-2.87-6.dsf.15.amzn1.x86_64
```",2019-07-30T12:17:26.737,1190184,CC BY-SA 4.0,True,1,,),0,"util-linux-2.23.2-33.28.amzn1.x86_64
 sysvinit-tools is obsoleted by (installed)",False,False,,,,,,,,,,,,,,,,,,,,
101065152,42271406,0,"Nice, 2019 here on arch, I had installed GD extension with `yay -S php-gd` but that didn't setup the php.ini/conf.d/gd.ini like it should have. Your tip worked. The AUR probably needs its build file updated to do that.3 minutes later... oh that package has been flagged as out of date, looks like `php70-gd` is the _one_.",2019-07-31T07:06:50.010,292408,CC BY-SA 4.0,True,1,,.,0,"oh that package has been flagged as outdated, looks like `php70-gd` is the _one_.",False,False,,,,,,,,,,,,,,,,,,,,
101159634,57331723,0,"yeah it looks quite outdated. Could you tell me what *zip -r9
../../../../$ZIP_FILE * should be replaced with? Is it asking me to specify the directory to zip file or create a directory with zip file in it?",2019-08-03T02:39:55.563,6067119,CC BY-SA 4.0,True,1,it,.,0,it looks quite outdated.,False,False,,,,,,,,,,,,,,,,,,,,
101239808,55407707,0,"I suggest upgrading to the Glue catalog, it automatically integrates with Athena, and the Athena catalog is deprecated. I concur with the comment from @Martin about the wide open policy, not safe. Also, ""Resource"" can be a list of buckets which will save you having so many statements in there. Generally for policies the data source `aws_iam_policy_document` works well as it is HCL syntax, and then you can use the `json` output of it as the `policy` property in an `aws_iam_role_policy` resource. Then it is easy to pass in a list of buckets as a variable without jsonencoding and other weirdness",2019-08-06T13:53:00.283,1335793,CC BY-SA 4.0,True,1,,.,0,"I suggest upgrading to the Glue catalog, it automatically integrates with Athena, and the Athena catalog is deprecated.",False,False,55407707.0,55129035.0,2019-03-28T22:20:11.627,0.0,"<p>I had many things wrong in my Terraform code. To start with:</p>

<ol>
<li>The <code>S3</code> bucket argument in the <a href=""https://www.terraform.io/docs/providers/aws/r/athena_database.html"" rel=""nofollow noreferrer""><code>aws_athena_database</code> code</a> refers to the bucket for query output <em>not</em> the data the table should be built from.</li>
<li>I had set up my <code>aws_glue_crawler</code> to write to a Glue database rather than an Athena db. Indeed, as Martin suggested above, once correctly set up, Athena was able to see the tables in the Glue db.</li>
<li><p>I did not have the correct policies attached to my crawler. Initially, the only policy attached to the crawler role was </p>

<pre><code>resource ""aws_iam_role_policy_attachment"" ""crawler_attach"" {
    policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole""
    role = ""${aws_iam_role.crawler_iam_role.name}""
} 
</code></pre>

<p>after setting a second policy that explicitly allowed all <code>S3</code> access to all of the buckets I wanted to crawl and attaching that policy to the same crawler role, the crawler ran and updated tables successfully.</p></li>
</ol>

<p>The second policy:</p>

<pre><code>resource ""aws_iam_policy"" ""crawler_bucket_policy"" {
    name = ""crawler_bucket_policy""
    path = ""/""
    description = ""Gives crawler access to buckets""
    policy = &lt;&lt;EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""Stmt1553807998309"",
      ""Action"": ""*"",
      ""Effect"": ""Allow"",
      ""Resource"": ""*""
    },
    {
      ""Sid"": ""Stmt1553808056033"",
      ""Action"": ""s3:*"",
      ""Effect"": ""Allow"",
      ""Resource"": ""arn:aws:s3:::bucket0""
    },
    {
      ""Sid"": ""Stmt1553808078743"",
      ""Action"": ""s3:*"",
      ""Effect"": ""Allow"",
      ""Resource"": ""arn:aws:s3:::bucket1""
    },
    {
      ""Sid"": ""Stmt1553808099644"",
      ""Action"": ""s3:*"",
      ""Effect"": ""Allow"",
      ""Resource"": ""arn:aws:s3:::bucket2""
    },
    {
      ""Sid"": ""Stmt1553808114975"",
      ""Action"": ""s3:*"",
      ""Effect"": ""Allow"",
      ""Resource"": ""arn:aws:s3:::bucket3""
    },
    {
      ""Sid"": ""Stmt1553808128211"",
      ""Action"": ""s3:*"",
      ""Effect"": ""Allow"",
      ""Resource"": ""arn:aws:s3:::bucket4""
    }
  ]
}
EOF
}
</code></pre>

<p>I'm confident that I can get away from hardcoding the bucket names in this policy but I don't yet know how to do that.</p>
",2,CC BY-SA 4.0,55129035.0,2019-03-12T19:08:02.597,9.0,1641.0,2020-02-26T22:01:55.310,2020-06-11T22:37:39.470,Terraform AWS Athena to use Glue catalog as db,<amazon-web-services><terraform><aws-glue><terraform-provider-aws><aws-glue-data-catalog>,2.0,3.0,,1.0,
101311177,38995889,0,"@ScottStensland that report has obviously been deprecated since I posted this answer 3 years ago. I don't work for Amazon. If you have a complaint about the Amazon billing reports I suggest you file that complaint on their site, not here.",2019-08-08T15:05:15.600,13070,CC BY-SA 4.0,True,1,report,.,0,that report has obviously been deprecated since I posted this answer 3 years ago.,False,False,38995889.0,38993458.0,2016-08-17T11:52:42.777,1.0,"<p>Go into your AWS account billing settings and enabled <a href=""http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html"" rel=""nofollow"">detailed billing reports</a>. Once you've followed the instructions on that page you will have detailed billing reports that AWS is delivering to an S3 bucket in your account. There will be several different reports delivered to that S3 bucket, all in CSV format.</p>
",3,CC BY-SA 3.0,38993458.0,2016-08-17T09:59:04.457,3.0,1758.0,,2016-08-17T11:52:42.777,How to get the Monthly Cost of aws ec2 instance using CLI tools,<amazon-web-services><amazon-ec2><amazon>,2.0,0.0,,,
101646390,38313021,0,"s3n is still in used and it is not deprecated, please refer this link https://github.com/databricks/spark-redshift#authenticating-to-s3-and-redshift.",2019-08-21T15:05:13.083,6248616,CC BY-SA 4.0,True,1,,.,1,"s3n is still in used and it is not deprecated, please refer this link https://github.com/databricks/spark-redshift#authenticating-to-s3-and-redshift.",False,True,38313021.0,38307640.0,2016-07-11T17:41:18.890,-1.0,"<p>I think the <code>s3n://</code> URL style has been deprecated and/or removed. </p>

<p>Try defining your keys as <code>""fs.s3.awsAccessKeyId""</code>.</p>
",2,CC BY-SA 3.0,38307640.0,2016-07-11T13:00:24.740,2.0,11117.0,,2019-08-20T14:34:32.357,Spark Redshift with Python,<apache-spark><amazon-redshift><databricks>,6.0,0.0,,3.0,
101730155,44710339,0,"Minot FYI : This answer is outdated (e.g. at the moment of writing this, lambda supports 15 minutes runtime)",2019-08-24T15:00:05.783,3340994,CC BY-SA 4.0,True,1,answer,),0,"This answer is outdated (e.g. at the moment of writing this, lambda supports 15 minutes runtime)",False,False,44710339.0,44709406.0,2017-06-22T22:33:34.197,2.0,"<p><strong>Pros</strong></p>

<ol>
<li><p>Invoke Lambda Functions only during event triggering <code>VS</code> keeping Instance(s) idle for a reasonable amount of time.</p></li>
<li><p>Pay only for what you are going to use <code>VS</code> pay for the running idle instance.</p></li>
<li><p>Easily Integrate with other popular AWS Services e.g API Gateway,S3 ,SNS,CloudTrail etc.</p></li>
<li><p>IAM Policies are configurable to each lambda function.</p></li>
<li><p>Scalable depending on the rate of invocations.</p></li>
</ol>

<p><strong>Cons</strong></p>

<ol>
<li><p>Only 5 minute with finite RAM is supported as of now ,so not for heavy and complex processing.</p></li>
<li><p>Only popular programming language support.</p></li>
</ol>

<p><strong>Conclusion</strong></p>

<p>It depends on your use-case as to whether use lambda or not !</p>
",1,CC BY-SA 3.0,44709406.0,2017-06-22T21:08:03.973,0.0,1455.0,2018-06-01T11:25:43.827,2018-06-01T11:25:43.827,When and when not to use aws lambda functions,<amazon-web-services><aws-lambda><serverless-architecture>,1.0,0.0,2017-06-23T01:17:10.627,,
101752758,27765049,0,"Answer is correct see Docs for set statement_timeout https://docs.aws.amazon.com/redshift/latest/dg/r_statement_timeout.html Note that `max_execution_time` is deprecated, it says use `query_execution_time`. It also says that neither of those include planning, queuing wait time, only execution time. The `statement_timeout` is the only one that does include the queue wait time. I don't see it as feasible to run (for 5 mins) `SET statement_timeout TO 300000` for every user session but if you query the `stv_recents` view you'll see that's exactly what the AWS system user rdsdb does.",2019-08-26T02:40:23.547,1335793,CC BY-SA 4.0,True,1,,.,0,"Answer is correct see Docs for set statement_timeout https://docs.aws.amazon.com/redshift/latest/dg/r_statement_timeout.html Note that `max_execution_time` is deprecated, it says use `query_execution_time`.",False,False,27765049.0,27701210.0,2015-01-04T11:29:00.280,1.0,"<p>You can control the amount of time that query spends waiting in queue indirectly by specifying statement_timeout configuration parameter on session or whole cluster level in addition to max_execution_time parameter on WLM level. If both WLM timeout (max_execution_time) and statement_timeout are specified, the shorter timeout is used. In this case the maximum time that query will be able to wait in the queue is ""statement_timeout"" minus ""max_execution_time"". </p>
",2,2015-01-04T11:29:00.280,27701210.0,2014-12-30T07:21:40.340,2.0,2695.0,,2015-01-04T11:29:00.280,How to set wait timeout for a queue in Amazon redshift?,<amazon-redshift>,2.0,1.0,,1.0,
101843755,57681862,0,context.fail is a deprecated right? Only callback is supported now. Why do we need them both?,2019-08-28T20:54:18.963,9525416,CC BY-SA 4.0,True,1,,?,0,context.fail is a deprecated right?,False,False,57681862.0,57680063.0,2019-08-27T20:31:41.930,0.0,"<p>Use your due diligence to find what all error you want to stop lambda function add try catch so will print log error for audit and loging purpose.</p>

<pre><code>}).catch((err) =&gt; {
   console.log(err);
   callback({ statusCode: 500});
   context.fail()
   process.exit(1); //&lt;--------
   })
</code></pre>

<ul>
<li><a href=""https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-handler.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-handler.html</a></li>
</ul>
",1,CC BY-SA 4.0,57680063.0,2019-08-27T18:04:10.397,1.0,655.0,2019-08-28T20:51:57.927,2019-08-28T20:51:57.927,When to use process.exit in aws lambda?,<amazon-web-services><aws-lambda>,1.0,0.0,,,
101871512,57411850,0,"`zipOutStream.closeEntry();` is not needed when there is a `zipOutStream.close();` right after it. Further, if `closeQuietly(byteArrOutputStream);` does what the name suggests, it’s obsolete as well, as `ByteArrayOutputStream` doesn’t need to be closed (and if it did, the close operation was again misplaced, being after the `toByteArray()` call). And `zipOutStream.close()` did already call `close()` on the underlying `ByteArrayOutputStream` anyway…",2019-08-29T16:59:56.683,2711488,CC BY-SA 4.0,True,1,,.,1,";` does what the name suggests, it’s obsolete as well, as `ByteArrayOutputStream` doesn’t need to be closed (and if it did, the close operation was again misplaced, being after the `toByteArray()` call).",False,False,57411850.0,57409645.0,2019-08-08T11:46:05.803,1.0,"<p>I think you should close your zip output stream to have everything written to the underlying byte array stream <em>before</em> you extract the byte array... Ie reorder the lines to:</p>

<pre><code>// close streams
zipOutStream.closeEntry();
zipOutStream.close();
byte[] streamBytes = byteArrOutputStream.toByteArray();
closeQuietly(byteArrOutputStream);
</code></pre>
",3,CC BY-SA 4.0,57409645.0,2019-08-08T09:42:29.957,0.0,159.0,2019-08-08T10:26:24.910,2019-08-08T11:46:05.803,Unable to open S3 zip file downloaded using spring-boot,<java><spring-boot><amazon-s3>,1.0,0.0,,,
101893832,28356197,0,I think this answer is now out of date: https://aws.amazon.com/about-aws/whats-new/2018/06/aws-storage-gateway-adds-smb-support-to-store-objects-in-amazon-s3/,2019-08-30T12:37:16.133,782911,CC BY-SA 4.0,True,1,,https://aws.amazon.com/about-aws/whats-new/2018/06/aws-storage-gateway-adds-smb-support-to-store-objects-in-amazon-s3/,0,I think this answer is now outdated: https://aws.amazon.com/about-aws/whats-new/2018/06/aws-storage-gateway-adds-smb-support-to-store-objects-in-amazon-s3/,False,False,28356197.0,28355537.0,2015-02-05T23:46:54.243,1.0,"<p>The short answer is <em>you can't</em>. S3 is Object storage; as opposed to EBS, which is block storage. It is for a reason... S3 has ""eventual consistency"", which means that you can't read the data immediately after write.</p>

<p>If you understand the limitations, there are good cheap products from S3 Browser and Cloudberry that will allow you to map S3 bucket to your computer. You can then use UNC the way you use any other drive</p>
",2,CC BY-SA 3.0,28355537.0,2015-02-05T22:49:16.617,0.0,3142.0,,2015-02-05T23:46:54.243,Amazon S3 and UNC paths,<amazon-web-services><amazon-s3><unc>,1.0,0.0,,,
101947658,6404747,0,"The answer is outdated, this is [now possible](https://aws.amazon.com/articles/overlay-multicast-in-amazon-virtual-private-cloud/).",2019-09-02T10:50:37.593,1236613,CC BY-SA 4.0,True,1,,.,0,"The answer is outdated, this is [now possible](https://aws.amazon.com/articles/overlay-multicast-in-amazon-virtual-private-cloud/).",False,False,,,,,,,,,,,,,,,,,,,,
101984394,57762933,0,"Hi, you're right. 
It seems like RLlib defaults to using it's conv vision model. 
Note that the RLlib that you're running uses a deprecated 
version of [visionnet_v1.py](https://github.com/ray-project/ray/blob/master/rllib/models/tf/visionnet_v1.py)(line 84)
which produces the new error. 
I don't see the error message in the newer  
[visionnet_v2.py](https://github.com/ray-project/ray/blob/master/rllib/models/tf/visionnet_v2.py)",2019-09-03T15:20:18.283,11427968,CC BY-SA 4.0,True,1,,"
",0,"Note that the RLlib that you're running uses a deprecated 
version of [visionnet_v1.py](https://github.com/ray-project/ray/blob/master/rllib/models/tf/visionnet_v1.py)(line 84)
",False,False,,,,,,,,,,,,,,,,,,,,
102022144,25111078,0,"Running this code today causes the following warning:

""(node:25440) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.""",2019-09-04T19:12:01.023,75694,CC BY-SA 4.0,True,1,Buffer,.,0,DeprecationWarning: Buffer() is deprecated due to security and usability issues.,False,False,,,,,,,,,,,,,,,,,,,,
102022995,25111078,0,@theycallmemorty This answer was from well before it was deprecated. I've updated it now for modern versions of node.,2019-09-04T19:48:32.917,2050455,CC BY-SA 4.0,True,1,,.,0,This answer was from well before it was deprecated.,False,False,,,,,,,,,,,,,,,,,,,,
102026242,42477724,1,"Is this still meant to work, because it appears to be totally ignored by my deployment? Possibly outdated, or should this work?",2019-09-04T23:18:00.197,3674263,CC BY-SA 4.0,True,1,,?,0,"Possibly outdated, or should this work?",False,False,42477724.0,28369235.0,2017-02-27T04:13:47.400,28.0,"<p>Taking <code>pip</code> as an example, the default AWS environment provides usually an old version. Currently it is a <code>6.1.1</code> on a machine I use, while <code>pip</code> repeats at each call that <code>9.0.1</code> is available.</p>

<p>Dependencies sometimes require recent versions of <code>pip</code>. One way to have it available is to rely on <code>pip</code> itself, as the <code>yum</code> sources provided by AWS are slower to upgrade (due to the sheer impact that would cause...).</p>

<p>Different AWS services have different solutions. The question is about Beanstalk. Assuming deployment based on <code>eb</code> provided by AWS, it is possible to execute commands in the target container:</p>

<ul>
<li>Create a <code>.ebextensions/upgrade_pip.config</code> file.</li>
<li>Insert the command to execute.</li>
</ul>

<p>To upgrade <code>pip</code>, a command like this does the job:</p>

<pre><code>commands:
  pip_upgrade:
    command: /opt/python/run/venv/bin/pip install --upgrade pip
    ignoreErrors: false
</code></pre>

<p>Note that the file name for <code>.ebextensions/upgrade_pip.config</code> defines the order of execution. If it needs to run earlier than any other script in <code>.ebextensions</code>, a prefix like <code>01_upgrade...</code> is necessary.</p>
",6,CC BY-SA 3.0,28369235.0,2015-02-06T15:33:37.447,13.0,3122.0,2017-02-27T04:16:39.550,2020-06-10T09:24:12.860,Can I update Amazon's old versions of pip and setuptools?,<python><amazon-web-services><pip><amazon-elastic-beanstalk><setuptools>,2.0,0.0,,6.0,
102070941,57818492,1,"`invokeAsync` has been deprecated. You should always use `invoke` and set the `InvocationType` attribute to either `RequestResponse` or `Event` which will trigger a Lambda function synchronously or asynchronously, respectively.",2019-09-06T10:36:06.177,10950867,CC BY-SA 4.0,True,1,invokeAsync,.,0,`invokeAsync` has been deprecated.,False,False,57818492.0,57814997.0,2019-09-06T08:35:08.630,1.0,"<p>instead of using two lambdas, a simpler approach is would be to restructure the code of your lambda such that it does the DynamoDB update as its last step. So, your code will look as follows (pseudo code):</p>

<pre><code>do something
do something
...
update DynamoDB
return the result
</code></pre>

<p>Actually, at the same point in your code where you wanted to invoke the second lambda (via ""SNS/SQS, streams, etc."") you can instead do the update of DynamoDB directly. </p>

<h2>side note</h2>

<p>In general, if you need one lambda to invoke another lambda, the simplest way to do that is to use AWS' Lambda API which allows you to make a call to a lambda without the need to setup SNS or a stream or any other service for that matter.</p>

<p>Examining the Javascript API as an example (the APIs in the other languages are quite similar) you can see that it offers an <a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Lambda.html#invoke-property"" rel=""nofollow noreferrer""><code>invoke()</code></a> method and an <a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Lambda.html#invokeAsync-property"" rel=""nofollow noreferrer""><code>invokeAsync()</code></a> method. The former allows you to send a request to lambda function and get a response back. the latter just sends the request without waiting for a response. </p>
",2,CC BY-SA 4.0,57814997.0,2019-09-06T02:28:00.547,0.0,23.0,,2019-09-06T08:35:08.630,How can I design my database to prevent my database to update if an error is produced later in the program?,<database><amazon-dynamodb>,1.0,1.0,,,
102087181,57827507,0,"I've found that issuing an ABANDON during a rolling update appears to trigger the ASG to simply try again a few times.   Even when it finally gives up things don't seem to work correctly as I finally saw this in the CloudFormation logs.  That was 30 minutes ago, so it's still not timing out despite saying it will this time. ```Rolling update initiated. Terminating 2 obsolete instance(s) in batches of 2. Waiting on resource signals with a timeout of PT10M when new instances are added to the autoscaling group.```",2019-09-06T21:06:49.440,2989193,CC BY-SA 4.0,True,1,,.,0,Terminating 2 obsolete instance(s) in batches of 2.,False,False,57827507.0,57824145.0,2019-09-06T19:21:59.883,0.0,"<p>Considering your use case, you could remove the <code>AutoScalingReplacingUpdate</code> property from the ASG. As far as I know, <code>AutoScalingReplacingUpdate</code> and <code>AutoScalingRollingUpdate</code> are usually mutually exclusive. This might explain why the PT10M is not taken into account. </p>

<p>Also, the PauseTime is a the upper time limit for newly started instance to trigger <code>SUCCESS</code> signal. I would maybe give some leeway, maybe a minute or two, for the <code>ABANDON</code> lifecycle event to occur. </p>
",1,CC BY-SA 4.0,57824145.0,2019-09-06T14:42:32.780,1.0,492.0,2019-09-06T15:06:11.313,2019-09-06T21:04:57.203,Why is AWS Cloudformation UpdatePolicy PauseTime not being used during an update,<amazon-web-services><amazon-ec2><amazon-cloudformation><autoscaling>,2.0,0.0,,,
102234676,57859064,0,"That webpage is quite old (and outdated) and its about `GitHub Pull Request and Webhook` but I am using codeCommit as source provider. Where I have three option for `reference type` namely `branch`, `tag`, `commit id`.",2019-09-12T13:07:04.230,5167801,CC BY-SA 4.0,True,1,,`,0,That webpage is quite old (and outdated) and its about `GitHub Pull Request and Webhook`,False,False,57859064.0,57854853.0,2019-09-09T18:02:12.960,0.0,"<p>CodePipeline is best suited to release automation and when using CodePipeline it's recommended to have a single ""release"" branch which changes that are ready for release are merged into. </p>

<p>Having a single ""master release"" branch will ensure you don't run into problems like accidentally deploying an old change in one branch over the top of a new change in a different branch, and therefore reverting the newer change.</p>

<p>If you want to run validation steps before merging into your release branch then a good option is to use CodeBuild in combination with CodePipeline. For example, <a href=""https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html"" rel=""nofollow noreferrer"">you can configure CodeBuild to automatically run on pull requests</a>.</p>

<p>You can also re-use the same CodeBuild configuration in CodePipeline to run the same build and testing steps as part of your release process after the pull request is merged.</p>
",2,CC BY-SA 4.0,57854853.0,2019-09-09T13:17:23.150,0.0,487.0,,2019-09-09T18:02:12.960,How to allow AWS CodeCommit to trigger pipeline from EVERY branch?,<amazon-web-services><aws-codepipeline><aws-codebuild><aws-codecommit>,1.0,0.0,,,
102346436,57921045,0,"Ok, it is missing in the german translation of the documentation ... thanks, I was simply reading an outdated documentation",2019-09-17T05:54:09.860,491605,CC BY-SA 4.0,True,1,,documentation,0,"thanks, I was simply reading an outdated documentation",False,False,,,,,,,,,,,,,,,,,,,,
102433045,58009499,2,"In addition to what @Adiii said, you can access that specific Dockerfile at https://github.com/aws/aws-codebuild-docker-images/blob/master/ubuntu/unsupported_images/nodejs/10.1.0/Dockerfile. They have been moved to ""unsupported_images"" directory, since the base image used is EOL (Ubuntu 14.04). Also that node version has a relatively outdated minor version.",2019-09-19T15:35:42.240,8137419,CC BY-SA 4.0,True,1,,.,0,Also that node version has a relatively outdated minor version.,False,False,58009499.0,58004426.0,2019-09-19T10:58:27.767,2.0,"<p>Here you can find the <code>aws/codebuild/nodejs:10.1.0</code>.</p>

<blockquote>
  <p>Release updated standard 2.0 image @minethai minethai released this on
  Jun 26 · 15 commits to master since this release</p>
</blockquote>

<p>You can download the zip folder form <a href=""https://github.com/aws/aws-codebuild-docker-images/archive/1.11.0.zip"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Changes:</p>

<p><strong>Updated minor versions for node8, node10, powershell, and gradle 5</strong></p>

<pre><code># Copyright 2017-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Amazon Software License (the ""License""). You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#    http://aws.amazon.com/asl/
#
# or in the ""license"" file accompanying this file.
# This file is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or implied.
# See the License for the specific language governing permissions and limitations under the License.
#

FROM ubuntu:14.04.5

ENV DOCKER_BUCKET=""download.docker.com"" \
    DOCKER_VERSION=""17.09.0-ce"" \
    DOCKER_CHANNEL=""stable"" \
    DOCKER_SHA256=""a9e90a73c3cdfbf238f148e1ec0eaff5eb181f92f35bdd938fd7dab18e1c4647"" \
    DIND_COMMIT=""3b5fac462d21ca164b3778647420016315289034"" \
    DOCKER_COMPOSE_VERSION=""1.21.2"" \
    GITVERSION_VERSION=""3.6.5""

# Install git, SSH, and other utilities
RUN set -ex \
    &amp;&amp; echo 'Acquire::CompressionTypes::Order:: ""gz"";' &gt; /etc/apt/apt.conf.d/99use-gzip-compression \
    &amp;&amp; apt-get update \
    &amp;&amp; apt install -y apt-transport-https \
    &amp;&amp; apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF \
    &amp;&amp; echo ""deb https://download.mono-project.com/repo/ubuntu stable-trusty main"" | tee /etc/apt/sources.list.d/mono-official-stable.list \
    &amp;&amp; apt-get update \
    &amp;&amp; apt-get install software-properties-common -y --no-install-recommends \
    &amp;&amp; apt-add-repository ppa:git-core/ppa \
    &amp;&amp; apt-get update \
    &amp;&amp; apt-get install git=1:2.* -y --no-install-recommends \
    &amp;&amp; git version \
    &amp;&amp; apt-get install -y --no-install-recommends openssh-client=1:6.6* \
    &amp;&amp; mkdir ~/.ssh \
    &amp;&amp; touch ~/.ssh/known_hosts \
    &amp;&amp; ssh-keyscan -t rsa,dsa -H github.com &gt;&gt; ~/.ssh/known_hosts \
    &amp;&amp; ssh-keyscan -t rsa,dsa -H bitbucket.org &gt;&gt; ~/.ssh/known_hosts \
    &amp;&amp; chmod 600 ~/.ssh/known_hosts \
    &amp;&amp; apt-get install -y --no-install-recommends \
       wget=1.15-* python=2.7.* python2.7-dev=2.7.* fakeroot=1.20-* ca-certificates \
       tar=1.27.* gzip=1.6-* zip=3.0-* autoconf=2.69-* automake=1:1.14.* \
       bzip2=1.0.* file=1:5.14-* g++=4:4.8.* gcc=4:4.8.* imagemagick=8:6.7.* \
       libbz2-dev=1.0.* libc6-dev=2.19-* libcurl4-openssl-dev=7.35.* libdb-dev=1:5.3.* \
       libevent-dev=2.0.* libffi-dev=3.1~* libgeoip-dev=1.6.* libglib2.0-dev=2.40.* \
       libjpeg-dev=8c-* libkrb5-dev=1.12+* liblzma-dev=5.1.* \
       libmagickcore-dev=8:6.7.* libmagickwand-dev=8:6.7.* libmysqlclient-dev=5.5.* \
       libncurses5-dev=5.9+* libpng12-dev=1.2.* libpq-dev=9.3.* libreadline-dev=6.3-* \
       libsqlite3-dev=3.8.* libssl-dev=1.0.* libtool=2.4.* libwebp-dev=0.4.* \
       libxml2-dev=2.9.* libxslt1-dev=1.1.* libyaml-dev=0.1.* make=3.81-* \
       patch=2.7.* xz-utils=5.1.* zlib1g-dev=1:1.2.* unzip=6.0-* curl=7.35.* \
       e2fsprogs=1.42.* iptables=1.4.* xfsprogs=3.1.* xz-utils=5.1.* \
       mono-devel less=458-* groff=1.22.* liberror-perl=0.17-* \
       asciidoc=8.6.* build-essential=11.* bzr=2.6.* cvs=2:1.12.* cvsps=2.1-* docbook-xml=4.5-* docbook-xsl=1.78.* dpkg-dev=1.17.* \
       libdbd-sqlite3-perl=1.40-* libdbi-perl=1.630-* libdpkg-perl=1.17.* libhttp-date-perl=6.02-* \
       libio-pty-perl=1:1.08-* libserf-1-1=1.3.* libsvn-perl=1.8.* libsvn1=1.8.* libtcl8.6=8.6.* libtimedate-perl=2.3000-* \
       libunistring0=0.9.* libxml2-utils=2.9.* libyaml-perl=0.84-* python-bzrlib=2.6.* python-configobj=4.7.* \
       sgml-base=1.26+* sgml-data=2.0.* subversion=1.8.* tcl=8.6.* tcl8.6=8.6.* xml-core=0.13+* xmlto=0.0.* xsltproc=1.1.* \
    &amp;&amp; rm -rf /var/lib/apt/lists/* \
    &amp;&amp; apt-get clean

# Download and set up GitVersion
RUN set -ex \
    &amp;&amp; wget ""https://github.com/GitTools/GitVersion/releases/download/v${GITVERSION_VERSION}/GitVersion_${GITVERSION_VERSION}.zip"" -O /tmp/GitVersion_${GITVERSION_VERSION}.zip \
    &amp;&amp; mkdir -p /usr/local/GitVersion_${GITVERSION_VERSION} \
    &amp;&amp; unzip /tmp/GitVersion_${GITVERSION_VERSION}.zip -d /usr/local/GitVersion_${GITVERSION_VERSION} \
    &amp;&amp; rm /tmp/GitVersion_${GITVERSION_VERSION}.zip \
    &amp;&amp; echo ""mono /usr/local/GitVersion_${GITVERSION_VERSION}/GitVersion.exe \$@"" &gt;&gt; /usr/local/bin/gitversion \
    &amp;&amp; chmod +x /usr/local/bin/gitversion

# Install Docker
RUN set -ex \
    &amp;&amp; curl -fSL ""https://${DOCKER_BUCKET}/linux/static/${DOCKER_CHANNEL}/x86_64/docker-${DOCKER_VERSION}.tgz"" -o docker.tgz \
    &amp;&amp; echo ""${DOCKER_SHA256} *docker.tgz"" | sha256sum -c - \
    &amp;&amp; tar --extract --file docker.tgz --strip-components 1  --directory /usr/local/bin/ \
    &amp;&amp; rm docker.tgz \
    &amp;&amp; docker -v \
# set up subuid/subgid so that ""--userns-remap=default"" works out-of-the-box
    &amp;&amp; addgroup dockremap \
    &amp;&amp; useradd -g dockremap dockremap \
    &amp;&amp; echo 'dockremap:165536:65536' &gt;&gt; /etc/subuid \
    &amp;&amp; echo 'dockremap:165536:65536' &gt;&gt; /etc/subgid \
    &amp;&amp; wget ""https://raw.githubusercontent.com/docker/docker/${DIND_COMMIT}/hack/dind"" -O /usr/local/bin/dind \
    &amp;&amp; curl -L https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-Linux-x86_64 &gt; /usr/local/bin/docker-compose \
    &amp;&amp; chmod +x /usr/local/bin/dind /usr/local/bin/docker-compose \
# Ensure docker-compose works
    &amp;&amp; docker-compose version

# Install dependencies by all python images equivalent to buildpack-deps:jessie
# on the public repos.

RUN set -ex \
    &amp;&amp; wget ""https://bootstrap.pypa.io/2.6/get-pip.py"" -O /tmp/get-pip.py \
    &amp;&amp; python /tmp/get-pip.py \
    &amp;&amp; pip install awscli==1.* \
    &amp;&amp; rm -fr /var/lib/apt/lists/* /tmp/* /var/tmp/*

VOLUME /var/lib/docker

COPY dockerd-entrypoint.sh /usr/local/bin/

ENV NODE_VERSION=""10.1.0""

# gpg keys listed at https://github.com/nodejs/node#release-team
RUN set -ex \
    &amp;&amp; for key in \
      94AE36675C464D64BAFA68DD7434390BDBE9B9C5 \
      B9AE9905FFD7803F25714661B63B535A4C206CA9 \
      77984A986EBC2AA786BC0F66B01FBB92821C587A \
      56730D5401028683275BD23C23EFEFE93C4CFFFE \
      71DCFD284A79C3B38668286BC97EC7A07EDE3FC1 \
      FD3A5288F042B6850C66B31F09FE44734EB7990E \
      8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600 \
      C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8 \
      DD8F2338BAE7501E3DD5AC78C273792F7D83545D \
      9554F04D7259F04124DE6B476D5A82AC7E37093B \
      93C7E9E91B49E432C2F75674B0A78B0A6C481CF6 \
      114F43EE0176B71C7BC219DD50A3051F888C628D \
      7937DFD2AB06298B2293C3187D33FF9D0246406D \
    ; do \
      gpg --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys ""$key"" || \
      gpg --keyserver hkp://ipv4.pool.sks-keyservers.net --recv-keys ""$key"" || \
      gpg --keyserver hkp://pgp.mit.edu:80 --recv-keys ""$key"" ; \
    done

RUN set -ex \
    &amp;&amp; wget ""https://nodejs.org/download/release/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.gz"" -O node-v$NODE_VERSION-linux-x64.tar.gz \
    &amp;&amp; wget ""https://nodejs.org/download/release/v$NODE_VERSION/SHASUMS256.txt.asc"" -O SHASUMS256.txt.asc \
    &amp;&amp; gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \
    &amp;&amp; grep "" node-v$NODE_VERSION-linux-x64.tar.gz\$"" SHASUMS256.txt | sha256sum -c - \
        &amp;&amp; tar -xzf ""node-v$NODE_VERSION-linux-x64.tar.gz"" -C /usr/local --strip-components=1 \
        &amp;&amp; rm ""node-v$NODE_VERSION-linux-x64.tar.gz"" SHASUMS256.txt.asc SHASUMS256.txt \
        &amp;&amp; ln -s /usr/local/bin/node /usr/local/bin/nodejs \
        &amp;&amp; rm -fr /var/lib/apt/lists/* /tmp/* /var/tmp/*

RUN npm set unsafe-perm true

CMD [ ""node"" ]
</code></pre>
",2,CC BY-SA 4.0,58004426.0,2019-09-19T05:46:15.147,1.0,462.0,,2019-09-19T10:58:27.767,Does anyone know where can I find DockerFIle for aws/codebuild/nodejs:10.1.0?,<amazon-web-services><dockerfile><open-source><aws-codebuild><docker-image>,1.0,0.0,,,
102539975,48495770,0,"So if this is being deprecated, what do we use now?",2019-09-24T02:31:00.367,9005758,CC BY-SA 4.0,True,1,,?,0,"So if this is being deprecated, what do we use now?",False,False,,,,,,,,,,,,,,,,,,,,
103014103,58315333,0,"I am seeing this issue when I add flush_size in amazon_es outplut plugin.

 Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:main, :exception=>""Java::JavaLang::IllegalStateException"", :message=>""Unable to configure plugins: (ConfigurationError) The setting `flush_size` in plugin `amazon_es` is obsolete and is no longer available. This setting is no longer available as we now try to restrict bulk requests to sane sizes. See the 'Batch Sizes' section of the docs. If you think you still need to restrict payloads based on the number, not size, of events.",2019-10-10T17:08:47.387,6311073,CC BY-SA 4.0,True,1,,.,1,amazon_es` is obsolete and is no longer available.,False,False,58315333.0,58313552.0,2019-10-10T04:07:46.130,0.0,"<p>aws limit size to 10MB, then maybe you sholud be change your conf Logstash output .. flush_size to 10</p>

<pre><code>vim /etc/logstash/conf.d/output
output {
    elasticsearch {
      hosts =&gt; [""XXX.XX.XX.XX""]
      index =&gt; ""logstash-%{+YYYY.MM.dd}""
      flush_size =&gt; 10
      template =&gt; ""/etc/logstash/template.json""
    }
}
</code></pre>
",1,CC BY-SA 4.0,58313552.0,2019-10-09T23:38:20.757,0.0,452.0,,2020-04-21T21:35:23.167,logstash keeps retrying infinitely when failed to upload to Elasticsearch cluster due to bulk upload error,<elasticsearch><logging><amazon-s3><logstash>,2.0,4.0,,,
103191393,44449068,0,"Buffer() is deprecated due to security issues, use Buffer.from instead: Buffer.from(str, 'utf-8')",2019-10-17T04:11:47.060,1701683,CC BY-SA 4.0,True,1,Buffer,),0,"Buffer() is deprecated due to security issues, use Buffer.from instead: Buffer.from(str, 'utf-8')",False,False,,,,,,,,,,,,,,,,,,,,
103224938,49240210,0,This answer is out of date.,2019-10-18T01:44:39.843,2327879,CC BY-SA 4.0,True,1,answer,.,0,This answer is outdated.,False,False,,,,,,,,,,,,,,,,,,,,
103249680,40589048,1,It should be noted that this answer is now substantially out of date with the launch of AWS Fargate in early 2018 (which runs ECS): https://aws.amazon.com/fargate/,2019-10-18T18:21:06.873,2738164,CC BY-SA 4.0,True,1,,https://aws.amazon.com/fargate/,0,It should be noted that this answer is now substantially outdated with the launch of AWS Fargate in early 2018 (which runs ECS): https://aws.amazon.com/fargate/,False,False,,,,,,,,,,,,,,,,,,,,
103316306,49988778,1,"@nitinr708 My solution is likely out of date (e.g., `pandas` should now be available with Python shell Glue jobs). The basic approach is to loop over all of your csv files, read each into a dataframe, and then write to parquet. Pandas DFs, Glue DynamicFrames, and PySpark DFs are your options. Each has a different API for reading/writing DFs. These links should be helpful-- Glue: https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-glue-context.html. PySpark: https://stackoverflow.com/a/45873742/5504459. Pandas: https://stackoverflow.com/a/37703861/5504459",2019-10-21T19:48:50.430,5504459,CC BY-SA 4.0,True,1,solution,.,0,"My solution is likely outdated (e.g., `pandas` should now be available with Python shell Glue jobs).",False,False,49988778.0,49986235.0,2018-04-23T19:40:09.580,0.0,"<p>Sounds like in your step 1 you are crawling the individual csv file (e.g some-bucket/container-path/file.csv), but if you instead set your crawler to look at a path level instead of a file level (e.g some-bucket/container-path/) and all your csv files are uniform then the crawler should only create a single external table instead of an external table per file and you’ll be able to extract the data from all of the files at once.</p>
",3,CC BY-SA 3.0,49986235.0,2018-04-23T16:54:04.187,11.0,12554.0,2018-05-15T23:50:10.490,2020-01-30T13:22:26.470,How to Convert Many CSV files to Parquet using AWS Glue,<amazon-s3><parquet><amazon-athena><aws-glue>,5.0,0.0,,4.0,
103349458,58505922,0,Do you have any plugins installed that may be out of date ? Have they been out of date previously ? Has an old version of WP been running previously that might have installed a backdoor ? The point still stands though: Start fresh with a new uncompromised instance.,2019-10-22T19:17:38.003,2695716,CC BY-SA 4.0,True,2,,?,0,Do you have any plugins installed that may be outdated ?,False,False,,,,,,,,,,,,,,,,,,,,
103491271,50455363,3,"Option number 1 shouts this message:
    /var/runtime/botocore/vendored/requests/api.py:67: DeprecationWarning: You are using the get() function from 'botocore.vendored.requests'.  This is not a public API in botocore and will be removed in the future. Additionally, this version of requests is out of date.  We recommend you install the requests package, 'import requests' directly, and use the requests.get() function instead.",2019-10-28T10:09:10.637,1716488,CC BY-SA 4.0,True,1,version, ,0,"Additionally, this version of requests is outdated.  ",False,False,,,,,,,,,,,,,,,,,,,,
103496581,11793702,0,**This answer is outdated** There are 2 types of queues now. Use FIFO to get Exactly-Once Processing: A message is delivered once and remains available until a consumer processes and deletes it. Duplicates are not introduced into the queue. https://aws.amazon.com/sqs/features/,2019-10-28T13:27:19.377,1737158,CC BY-SA 4.0,True,1,,outdated,0,This answer is outdated,False,False,11793702.0,10061843.0,2012-08-03T10:14:14.477,11.0,"<p>Be careful with using SQS for cronjobs, as they don't guarantee that only ""one job is seen by only one machine"". They guarantee that ""at least one"" will got the message.</p>

<p>From: <a href=""http://aws.amazon.com/sqs/faqs/#How_many_times_will_I_receive_each_message"" rel=""noreferrer"">http://aws.amazon.com/sqs/faqs/#How_many_times_will_I_receive_each_message</a></p>

<blockquote>
  <p><strong>Q: How many times will I receive each message?</strong></p>
  
  <p>Amazon SQS is engineered to provide “at least once” delivery of all messages in its queues. Although most of the time each message will be delivered to your application exactly once, you should design your system so that processing a message more than once does not create any errors or inconsistencies.</p>
</blockquote>

<p>So far I can think about the solution where you have one instance with Gearman Job Server instance installed: <a href=""http://gearman.org/"" rel=""noreferrer"">http://gearman.org/</a>. On the same machine you configure cron jobs that are producing command to execute your cronjob task in background. Then one of your web servers (workers) will start executing this task, it guarantees that only one will take it. It doesn't matter how many workers you have (especially when you are using auto scaling).</p>

<p>The problems with this solution are:</p>

<ul>
<li>Gearman server is single point of failure, unless you configure it with distributed storage, for example using memcached or some database</li>
<li>Then using multiple Gearman servers you have to select one that creates task via cronjob, so again we are back to the same problem. But if you can live with this kind of single point of failure using Gearman looks like quite good solution. Especially that you don't need big instance for that (micro instance in our case is enough). </li>
</ul>
",3,CC BY-SA 3.0,10061843.0,2012-04-08T09:37:42.737,112.0,32216.0,2015-09-10T21:23:58.740,2020-04-24T13:46:42.727,"How to convert Linux cron jobs to ""the Amazon way""?",<amazon-ec2><cron><scheduled-tasks><lamp><amazon-swf>,13.0,1.0,,56.0,
103528712,54209019,0,"Thanks A.Khan ;-) The blog post is a bit outdated now, best to just go to the developer portal (linked above) and follow the readme.",2019-10-29T14:02:56.037,646840,CC BY-SA 4.0,True,1,post,.,0,"The blog post is a bit outdated now, best to just go to the developer portal (linked above) and follow the readme.",False,False,54209019.0,54206514.0,2019-01-16T01:00:13.583,3.0,"<p>AWS API Gateway can only make documentation exportable but won't render a UI app. There is no endpoint that you can use in the gateway to render the documentation. You may wish to use third party tools or <a href=""https://github.com/awslabs/aws-api-gateway-developer-portal"" rel=""nofollow noreferrer"">AWS API Gateway Developer Portal</a> to allow users browse API documentation. You may find this <a href=""https://aws.amazon.com/blogs/compute/generate-your-own-api-gateway-developer-portal/"" rel=""nofollow noreferrer"">blog</a> helpful.</p>
",2,CC BY-SA 4.0,54206514.0,2019-01-15T20:42:33.420,12.0,5622.0,2019-01-16T00:21:21.567,2019-12-23T10:20:16.457,How to access AWS API Gateway documentation using Swagger UI,<amazon-web-services><aws-sdk><aws-api-gateway>,2.0,3.0,,1.0,
103916323,55376108,0,Same problem here. Running these commands also didn't help. Bitnami docs seems to be out of date and now my instance is messed up ¬¬,2019-11-12T13:16:29.300,2389475,CC BY-SA 4.0,True,1,,¬¬,0,Bitnami docs seems to be outdated and now my instance is messed up ¬¬,False,False,55376108.0,55363731.0,2019-03-27T11:28:25.633,0.0,"<p>Bitnami Engineer here,</p>

<p>Probably there is one folder inside the htdocs one that doesn't have the correct permissions. Can you run these commands modify the permissions of all the folders and files inside the htdocs folder?</p>

<pre><code>sudo chown -R bitnami:daemon /opt/bitnami/apps/wordpress/htdocs
sudo chmod -R g+w /opt/bitnami/apps/wordpress/htdocs
sudo chmod 640 /opt/bitnami/apps/wordpress/htdocs/wp-config.php
</code></pre>

<p>Thanks</p>
",3,CC BY-SA 4.0,55363731.0,2019-03-26T18:10:31.247,-1.0,136.0,,2020-02-04T11:01:33.403,My bitnami wordpress hosted on AWS asking for ftp details to update or install plugins,<wordpress><amazon-web-services><plugins><permissions><bitnami>,2.0,0.0,,,
104243326,58996384,0,"Hi noam, thanks! The `valueMap(true)` has been deprecated, but thanks to your answer I could find in the documentation the new way to do it, which is `valueMap().with(WithOptions.tokens)` as referenced [here](http://tinkerpop.apache.org/docs/3.4.0/upgrade/) in the `Modulation of valueMap()` section. Do you know anything about the `Bonus question`?",2019-11-22T14:43:22.963,1808082,CC BY-SA 4.0,True,1,valueMap(true,answer,0,"The `valueMap(true)` has been deprecated, but thanks to your answer",False,False,,,,,,,,,,,,,,,,,,,,
104243752,58996384,0,"Thanks, I forget it was deprecated, I will edit my answer, also you can use ElementMap step.",2019-11-22T14:55:01.990,5990248,CC BY-SA 4.0,True,1,,.,0,"Thanks, I forget it was deprecated, I will edit my answer, also you can use ElementMap step.",False,False,,,,,,,,,,,,,,,,,,,,
104251966,33277346,0,"Second link is also out of date, this functionality is built in to Spark 2.x",2019-11-22T19:48:44.297,2930025,CC BY-SA 4.0,True,1,,2.x,0,"Second link is also outdated, this functionality is built in to Spark 2.x",False,False,33277346.0,33270827.0,2015-10-22T09:10:49.493,2.0,"<p>If you look into the Spark <a href=""https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame"" rel=""nofollow"">Dataframe API</a>, and the <a href=""https://github.com/databricks/spark-csv"" rel=""nofollow"">Spark-CSV package</a>, this will achieve the majority of what you're trying to do - reading in the CSV file into a dataframe, then writing the dataframe out as parquet will get you most of the way there.</p>

<p>You'll still need to do some steps on parsing the timestamp and using the results to partition the data.</p>
",2,CC BY-SA 3.0,33270827.0,2015-10-21T23:15:37.477,0.0,5173.0,2015-10-21T23:21:44.497,2018-01-24T18:15:19.840,Convert csv.gz files into Parquet using Spark,<scala><hadoop><amazon-s3><apache-spark><parquet>,3.0,1.0,,,
104317233,40577276,0,"@KayaToast, thank you. I've updated the answer. This one was posted 3 years ago and was outdated",2019-11-25T17:30:09.683,1476885,CC BY-SA 4.0,True,1,,outdated,0,This one was posted 3 years ago and was outdated,False,False,,,,,,,,,,,,,,,,,,,,
104322230,48213414,0,This answer is outdated,2019-11-25T20:44:01.750,2495341,CC BY-SA 4.0,True,1,,outdated,0,This answer is outdated,False,False,,,,,,,,,,,,,,,,,,,,
104343967,48900262,0,Is this 'Events' section deprecated? I don't see it in the latest Lambda cloudformation documentation. Maybe use Lambda permission instead?,2019-11-26T13:26:37.873,6302803,CC BY-SA 4.0,True,1,,?,0,Is this 'Events' section deprecated?,False,False,48900262.0,48898995.0,2018-02-21T07:27:17.740,4.0,"<p>You can use events to set up the trigger. </p>

<pre><code>  lambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      Handler: 
      ------
      Events:
        SNS1:
          Type: SNS
          Properties:
            Topic:
              Ref: SNSTopic1
  SNSTopic1:
    Type: 'AWS::SNS::Topic'
</code></pre>

<p>Ref: Ref: <a href=""https://docs.aws.amazon.com/lambda/latest/dg/serverless_app.html#serverless_app_resources"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/serverless_app.html#serverless_app_resources</a></p>
",6,CC BY-SA 3.0,48898995.0,2018-02-21T05:54:37.787,2.0,6012.0,2019-03-01T04:46:23.720,2020-04-20T14:40:06.547,Triggering a lambda from SNS using cloud-formation?,<amazon-web-services><aws-lambda><amazon-cloudformation><amazon-sns>,3.0,0.0,,1.0,
104454870,59109528,0,"@matesio I agree that Glacier isn't right for this use case, but your statement that you *""first need to upload to S3, then with life cycle policy move to Glacier""* is based on outdated information and is no longer correct.  S3 has supported the ability to upload objects directly to the Glacier storage class since November, 2018: https://aws.amazon.com/blogs/architecture/amazon-s3-amazon-s3-glacier-launch-announcements-for-archival-workloads/",2019-11-29T23:21:08.690,1695906,CC BY-SA 4.0,True,1,, ,1,"I agree that Glacier isn't right for this use case, but your statement that you *""first need to upload to S3, then with life cycle policy move to Glacier""* is based on outdated information and is no longer correct.  ",False,False,59109528.0,59108126.0,2019-11-29T18:23:40.997,1.0,"<p><strong>AWS Glacier</strong> is incorrect as you can't directly put objects to the Glacier, to do so you first need to upload to S3, then with life cycle policy move to Glacier. So it will definitely increase the cost.</p>

<p><strong>EFS</strong> is wrong, as it has a very slow read/ write.</p>

<p><strong>S3</strong> standard has somehow equivalent pricing and speed but Infrequent access is the best option in this case. </p>

<p>In real scenario it will be a good idea to place Cloudfront in front of S3 bucket as files are static and you can save cost on read operations by caching the. </p>
",7,CC BY-SA 4.0,59108126.0,2019-11-29T16:14:46.260,0.0,38.0,,2019-12-01T13:25:56.507,Which AWS S3 retrieval model would be cost-effecient?,<amazon-s3>,2.0,0.0,,,
104553326,27241309,0,That library looks to be outdated.,2019-12-03T20:13:43.203,1128978,CC BY-SA 4.0,True,1,,.,0,That library looks to be outdated.,False,False,,,,,,,,,,,,,,,,,,,,
104724041,59247491,0,"Thank you, I have seen it, it is really aweful. The CodePloy pipe example repository is not working. First - it does not zip the file, it does not pic up the s3 bucket name, the Pipe version is outdated and so on and so forth.",2019-12-09T22:05:17.420,2005680,CC BY-SA 4.0,True,1,,.,1,"First - it does not zip the file, it does not pic up the s3 bucket name, the Pipe version is outdated and so on and so forth.",False,False,59247491.0,59188210.0,2019-12-09T11:02:49.940,0.0,"<p>There is the <a href=""https://confluence.atlassian.com/bitbucket/deploy-to-aws-with-codedeploy-976773337.html"" rel=""nofollow noreferrer"">Deploy to AWS with CodeDeploy</a> deployment guide from Bitbucket. You should be able to use this guide to start deploying your code using CodeDeploy.</p>
",1,CC BY-SA 4.0,59188210.0,2019-12-05T04:19:00.207,1.0,64.0,,2019-12-09T11:02:49.940,Deploy to AWS CodeDeploy using Bitbucket Pipes,<amazon-web-services><bitbucket><bitbucket-pipelines>,1.0,0.0,,,
104796668,59293992,0,"You *can* call `context.succeed()` or `context.fail()` as they're still supported, but it's generally discouraged as you should ideally return a promise from an async Lambda handler function.",2019-12-12T00:48:48.080,271415,CC BY-SA 4.0,True,1,,.,0,"You *can* call `context.succeed()` or `context.fail()` as they're still supported, but it's generally discouraged as you should ideally return a promise from an async Lambda handler function.",False,False,,,,,,,,,,,,,,,,,,,,
104797754,22212017,1,"@N.Nonkovic yes, that's true (now).  This answer is over 5 years old and was last updated 3 years ago. There is new material I need to add.  My prior comment about needing Internet access for NTP sync is also out of date, now, since 169.254.169.123 is now [an AWS-provided NTP server in every VPC](https://aws.amazon.com/blogs/aws/keeping-time-with-amazon-time-sync-service/).",2019-12-12T02:21:31.270,1695906,CC BY-SA 4.0,True,1,comment,.,0,"My prior comment about needing Internet access for NTP sync is also outdated, now, since 169.254.169.123 is now [an AWS-provided NTP server in every VPC](https://aws.amazon.com/blogs/aws/keeping-time-with-amazon-time-sync-service/).",False,False,,,,,,,,,,,,,,,,,,,,
104867658,59311952,0,"Since it's an old question, it's rather outdated for me. After some additional investigation, I've found that for many usage cases (when you have like hundreds-thousands of few KB items and never expect millions of them), you will never get out of a single partition. The last automatically means that there is no sense in considering uniform distribution of a partition key. Moreover, to have possibility to get items by some range, I would use the same constant value for all keys in the system (completely ignore partition key) like { partition: 1, sort: timestamp }",2019-12-14T11:39:42.373,1862998,CC BY-SA 4.0,True,1,it,.,0,"Since it's an old question, it's rather outdated for me.",False,False,,,,,,,,,,,,,,,,,,,,
104926395,41623101,0,The doc link and quoted text are outdated. Updated version: https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html,2019-12-17T01:04:57.553,152142,CC BY-SA 4.0,True,1,link,.,0,The doc link and quoted text are outdated.,False,False,,,,,,,,,,,,,,,,,,,,
104950057,50367472,0,"Natalie -not really, That answer looks at the s3n connector which was already deprecated in spark 2.7 and depends on the jets3t library. the S3A one uses the aws SDK",2019-12-17T17:08:49.137,2261274,CC BY-SA 4.0,True,1,,.,0,"Natalie -not really, That answer looks at the s3n connector which was already deprecated in spark 2.7 and depends on the jets3t library.",False,False,,,,,,,,,,,,,,,,,,,,
104998218,37072998,0,Use uuid as node-uuid is deprecated now.,2019-12-19T07:29:42.140,4394926,CC BY-SA 4.0,True,1,,.,0,Use uuid as node-uuid is deprecated now.,False,False,37072998.0,37072341.0,2016-05-06T12:54:37.720,62.0,"<p>DynamoDB doesn't provide this out of the box.  You can generate something in your application such as UUIDs that ""should"" be unique enough for most systems.</p>

<p>I noticed you were using Node.js (I removed your tag).  Here is a library that provides UUID functionality: <a href=""https://github.com/broofa/node-uuid"" rel=""noreferrer"">node-uuid</a></p>

<p>Example from README</p>

<pre><code>var uuid = require('node-uuid');
var uuid1 = uuid.v1();
var uuid2 = uuid.v1({node:[0x01,0x23,0x45,0x67,0x89,0xab]});
var uuid3 = uuid.v1({node:[0, 0, 0, 0, 0, 0]})
var uuid4 = uuid.v4();
var uuid5 = uuid.v4();
</code></pre>
",3,CC BY-SA 3.0,37072341.0,2016-05-06T12:22:48.100,74.0,69041.0,2019-12-03T00:16:50.950,2020-07-17T18:51:59.837,How to use auto increment for primary key id in dynamodb,<amazon-web-services><amazon-dynamodb>,4.0,2.0,,15.0,
105106162,35692394,1,"Hi, it's now 2019 and I can't seem to find any documentation on `context.fail()` -- has it been deprecated or removed? Following the link in this answer doesn't mention `fail()` anywhere",2019-12-24T00:43:35.450,5188833,CC BY-SA 4.0,True,1,it,?,1,and I can't seem to find any documentation on `context.fail()` -- has it been deprecated or removed?,False,False,,,,,,,,,,,,,,,,,,,,
105135475,59473251,0,"The `pdf` source looks helpful. But by looking at the date created of that document, it looks like it is outdated `October 2013`. Does it still work or is there any new version that I can look at?",2019-12-25T14:25:32.213,4626254,CC BY-SA 4.0,True,1,,.,0,"But by looking at the date created of that document, it looks like it is outdated `October 2013`.",False,False,59473251.0,59464305.0,2019-12-24T20:45:04.933,1.0,"<p>I would recommend looking at <strong>Scenario 3</strong> in the following AWS document:</p>

<p><strong>Setting Up Multiuser Environments in the AWS Cloud
(for Classroom Training and Research)</strong> </p>

<p>It references a ""students"" environment, however it should suite an interview-candidate testing needs.</p>

<blockquote>
  <p>The “Separate AWS Account for Each User” scenario with optional consolidated billing provides an excellent
  environment for users who need a completely separate account environment, such as researchers or graduate students.
  It is similar to the “Limited User Access to AWS Management Console” scenario, except that each IAM user is created in
  a separate AWS account, eliminating the risk of users affecting each other’s services.
  As an example, consider a research lab with 10 graduate students. The administrator creates one paying AWS account,
  10 linked student AWS accounts, and 1 restricted IAM user per linked account. The administrator provisions separate
  AWS accounts for each user and links the accounts to the paying AWS account. Within each account, the administrator
  creates an IAM user and applies access control policies. Users receive access to an IAM user within their AWS account.
  They can log into the AWS Management Console to launch and access different AWS services, subject to the access
  control policy applied to their account. Students don’t see resources provisioned by other students.
  One key advantage of this scenario is the ability for a student to continue using the account after the completion of the
  course. For example, if students use AWS resources as part of a startup course, they can continue to use what they have
  built on AWS after the semester is over.</p>
</blockquote>

<p><a href=""https://d1.awsstatic.com/whitepapers/aws-setting-up-multiuser-environments-education.pdf"" rel=""nofollow noreferrer"">https://d1.awsstatic.com/whitepapers/aws-setting-up-multiuser-environments-education.pdf</a></p>
",1,CC BY-SA 4.0,59464305.0,2019-12-24T05:41:04.097,0.0,110.0,2019-12-25T07:22:13.470,2019-12-25T07:22:13.470,How to create a dynamic AWS environment and teardown?,<amazon-web-services><boto3><amazon-iam><aws-userpools>,3.0,0.0,,,
105166429,59496377,0,"Since this solution is deprecated in java, I am still looking for a correction in the solution presented in the question?",2019-12-27T05:42:00.010,10840020,CC BY-SA 4.0,True,1,,?,0,"Since this solution is deprecated in java, I am still looking for a correction in the solution presented in the question?",False,False,59496377.0,59487701.0,2019-12-27T05:29:12.447,0.0,"<p>This solution works</p>

<pre><code>private fun uploadToS3() {
        val credentials = BasicAWSCredentials(awsAccessKey, awsSecretKey)
        val transferManager = TransferManager(credentials)
        val upload = transferManager.upload(awsBucketName, filename, File(filePath))
        val uploadResult = upload.waitForUploadResult()

        while (!upload.isDone){
            val bytesTransferred = upload.progress.bytesTransferred
            println(bytesTransferred.toString())
            Thread.sleep(200)
        }
    }
</code></pre>
",1,CC BY-SA 4.0,59487701.0,2019-12-26T11:20:44.300,1.0,130.0,,2019-12-27T05:29:12.447,Uploading audio files on s3 from kotlin app,<android><kotlin><amazon-s3>,1.0,0.0,,,
105315176,59572123,0,"Thanks, i was thinking the same. I am trying the try/catch block to find the info if it is available. Thanks for heads up about the deprecated images.",2020-01-03T06:22:23.020,1945171,CC BY-SA 4.0,True,1,,.,0,Thanks for heads up about the deprecated images.,False,False,59572123.0,59559826.0,2020-01-03T01:23:31.583,1.0,"<p>The problem is being caused by <strong>deprecated images</strong>.</p>

<p>For example, Windows images are deprecated when a newer version is made available. While the image object can be retrieved, it does not actually contain any information (name, tags, etc). Thus, it is not possible to display any information about it.</p>

<p>You will need to add a <code>try</code> statement to catch such situations and skip-over the deprecated image.</p>

<pre class=""lang-py prettyprint-override""><code>import boto3

ec2_resource = boto3.resource('ec2')

# Display AMI information for all instances
for instance in ec2_resource.instances.all():
    image = instance.image

    # Handle situation where image has been deprecated
    try:
        tags = image.description
    except:
        continue

    if image.tags is None:
        description_tag = ''
    else:
        description_tag = [tag['Value'] for tag in image.tags if tag['Name'] == 'Description'][0]

    print(image.description, image.image_type, image.name, image.platform, description_tag) 
</code></pre>
",1,CC BY-SA 4.0,59559826.0,2020-01-02T07:37:16.313,-1.0,776.0,2020-01-03T01:23:46.243,2020-01-03T01:23:46.243,How to print ec2 Instance Image tags using boto3?,<python-3.x><amazon-web-services><boto3><amazon-ami>,2.0,1.0,,,
105616038,57531718,0,NOTE: adminSetUserSettings has been deprecated. Use SetUserMFAPreference instead.,2020-01-14T12:18:38.237,228406,CC BY-SA 4.0,True,1,adminSetUserSettings,.,0,adminSetUserSettings has been deprecated.,False,False,57531718.0,50497388.0,2019-08-16T22:16:58.903,0.0,"<p>Actually you need to change user's settings, not preferences.</p>

<p>to remove MFA
        var cognitoidentityserviceprovider = new AWS.CognitoIdentityServiceProvider();</p>

<pre><code>    var params = {
        UserPoolId: poolData.UserPoolId,
        Username: userid, /* required */
        MFAOptions: [ /* required */
        ]
    };
    cognitoidentityserviceprovider.adminSetUserSettings(params, function(err, data) {
        if (err) reject(err);       // an error occurred
        else     resolve(data);     // successful response
    });
</code></pre>

<p>To Add/Change MFA:</p>

<pre><code>    var cognitoidentityserviceprovider = new AWS.CognitoIdentityServiceProvider();

    var params = {
        UserPoolId: poolData.UserPoolId,
        Username: userid, /* required */
        MFAOptions: [ /* required */
            {
                AttributeName: 'phone_number',
                DeliveryMedium: 'SMS'
            }
        ]
    };
    cognitoidentityserviceprovider.adminSetUserSettings(params, function(err, data) {
        if (err) reject(err);       // an error occurred
        else     resolve(data);     // successful response
    });
</code></pre>
",2,CC BY-SA 4.0,50497388.0,2018-05-23T21:00:46.707,10.0,2675.0,,2020-02-11T14:34:06.647,AWS Cognito - reset user MFA,<amazon-web-services><amazon-cognito><cognito><multi-factor-authentication>,4.0,1.0,,,
105680378,56534776,0,"You should inline a [mre] of this in here rather than linking to GitHub. As is, your answer is basically a link only answer which are discouraged here.",2020-01-16T11:05:06.307,2291321,CC BY-SA 4.0,True,1,,.,0,"As is, your answer is basically a link only answer which are discouraged here.",False,False,,,,,,,,,,,,,,,,,,,,
105830491,57189565,1,"As of 2019, this should now be the accepted answer. All the rest are outdated or slow.",2020-01-21T20:09:50.927,321866,CC BY-SA 4.0,True,1,rest,.,0,All the rest are outdated or slow.,False,False,57189565.0,2862617.0,2019-07-24T18:57:31.997,4.0,"<p>You can easily get the total count and the history if you go to the s3 console ""Management"" tab and then click on ""Metrics""... <a href=""https://i.stack.imgur.com/Fh6Pv.png"" rel=""nofollow noreferrer"">Screen shot of the tab</a></p>
",2,CC BY-SA 4.0,2862617.0,2010-05-19T03:15:49.457,158.0,125706.0,2010-05-19T17:07:37.787,2020-09-01T19:36:01.693,How can I tell how many objects I've stored in an S3 bucket?,<file><count><amazon-s3><amazon-web-services>,29.0,4.0,,38.0,
105865981,59836839,0,"Nice link. Is AppAuth your repo? I am not much into Android and I delved into AWS iOS SDK for some time, but seems they are making it obsolete in favor of Amplify. Basically I want to use anything that's more generic / pure without relying on 3rd party SDKs. I wonder if AppAuth is free. I came across Okta and Auth0 but they are not.",2020-01-22T20:13:24.340,1506363,CC BY-SA 4.0,True,1,,.,1,"I am not much into Android and I delved into AWS iOS SDK for some time, but seems they are making it obsolete in favor of Amplify.",False,False,59836839.0,59829219.0,2020-01-21T08:23:24.570,1.0,"<p>The direction I always take is a standards based approach rather than an AWS specific one, since your goal is to build great UIs and APIs with the best future options.</p>

<p>My below tutorial based blog links do not cover Amplify but I think they will be relevant to you. There can be quite a learning curve with this tech.</p>

<ul>
<li><p>Here is <a href=""https://github.com/gary-archer/authguidance.apisample.serverless/blob/master/src/framework-api-oauth/src/security/oauthAuthenticator.ts"" rel=""nofollow noreferrer"">some code of mine</a> to validate Cognito tokens in an API.</p></li>
<li><p>I have working demo Web and Mobile UIs that use Cognito tokens to call a cloud hosted version of the API - you can quickly run the UIs from <a href=""https://authguidance.com/home/code-samples-quickstart"" rel=""nofollow noreferrer"">this page</a>.</p></li>
</ul>

<p>The blog's index page has a number of step by step guides, such as explaining the Serverless API and <a href=""https://authguidance.com/2018/12/11/serverless-api-overview"" rel=""nofollow noreferrer"">running it on your PC</a>, though some of the posts are long and detailed.</p>

<p>If you find any of this useful then feel free to post any follow up questions.</p>
",9,CC BY-SA 4.0,59829219.0,2020-01-20T18:43:24.420,1.0,93.0,2020-01-29T21:17:13.837,2020-01-29T21:17:13.837,How to secure AWS REST back end for mobile access,<ios><oauth-2.0><aws-lambda><amazon-cognito><aws-amplify>,2.0,1.0,,1.0,
105968325,20700719,1,This is an outdated answer. You no longer need to stop/start or attach/detach.,2020-01-26T20:39:23.130,4102515,CC BY-SA 4.0,True,1,,.,0,This is an outdated answer.,False,False,20700719.0,20700299.0,2013-12-20T09:49:19.007,15.0,"<p>You can't change the EBS size and IOPS once it's attached to EC2 and it's a running instance. You'll need to stop it to make any changes.</p>

<p>First, shutdown your instance.</p>

<p>In the snapshots menu option - select the volume you want to change and make a snapshot. Once the snapshot is created, you right click on it and select ""Create Volume"" then select your size and type.</p>

<p>On your old Volume select it, right click and select the Detach option. Then find the snapshot you created and attach it.</p>

<p>Then start your instance again.</p>
",3,CC BY-SA 3.0,20700299.0,2013-12-20T09:28:43.650,13.0,9310.0,2013-12-20T09:41:35.583,2019-11-29T16:52:23.640,How to configure an IOPS type EBS after attachment on Amazon AWS?,<amazon-web-services><amazon-ec2><provisioned-iops>,3.0,0.0,,,
106021216,47355092,2,Is there a way to avoid the rather heavy (and riddled with outdated dependencies) Hadoop?,2020-01-28T14:17:51.513,539599,CC BY-SA 4.0,True,1,,?,0,Is there a way to avoid the rather heavy (and riddled with outdated dependencies) Hadoop?,False,False,,,,,,,,,,,,,,,,,,,,
106291696,60063922,1,"First thing I would say is that ENABLE_S3_SIGV4_SYSTEM_PROPERTY appears to be deprecated, so new code should not be written to use it. You'd have to try this to see what impact it has on the S3 pre-signed URL function.",2020-02-06T15:29:19.313,271415,CC BY-SA 4.0,True,1,,.,1,"First thing I would say is that ENABLE_S3_SIGV4_SYSTEM_PROPERTY appears to be deprecated, so new code should not be written to use it.",False,False,60063922.0,60058511.0,2020-02-04T18:57:52.493,1.0,"<p>All regions support signature v4. Some (older) regions also support signature v2. I would recommend that you use signature v4, always. </p>

<p>Here's a Java example of configuring signature v4:</p>

<pre><code>ClientConfiguration clientConfiguration = new ClientConfiguration();
clientConfiguration.setSignerOverride(""AWSS3V4SignerType"");

AmazonS3Client s3 = new AmazonS3Client(
    new ProfileCredentialsProvider(), clientConfiguration);

GeneratePresignedUrlRequest request = new GeneratePresignedUrlRequest(
    myBucket, myKey, HttpMethod.PUT);
URL puturl = s3.generatePresignedUrl(request);
</code></pre>
",10,CC BY-SA 4.0,60058511.0,2020-02-04T13:29:39.277,0.0,441.0,2020-02-04T13:34:52.773,2020-02-04T18:57:52.493,V4 Signing for aws S3 using aws java sdk,<amazon-web-services><amazon-s3><signing>,1.0,2.0,,,
106411102,60063922,0,"I have tried out that. Actually i am getting this error ""Attempting to re-send the request to s3_url with AWS V4 authentication. To avoid this warning in the future, please use region-specific endpoint to access buckets located in regions that require V4 signing"". Once i have set the ENABLE_S3_SIGV4_SYSTEM_PROPERTY to true, this warning disappears. And everything i am doing using aws java sdk 1.x version. Now do i need to use any other techniques to enable v4 signing? I found this solution in aws upgrade documentation. But i found it is deprecated. Is there any alternative solution using sdk?",2020-02-11T06:42:53.940,10237300,CC BY-SA 4.0,True,1,,.,0,But i found it is deprecated.,False,False,60063922.0,60058511.0,2020-02-04T18:57:52.493,1.0,"<p>All regions support signature v4. Some (older) regions also support signature v2. I would recommend that you use signature v4, always. </p>

<p>Here's a Java example of configuring signature v4:</p>

<pre><code>ClientConfiguration clientConfiguration = new ClientConfiguration();
clientConfiguration.setSignerOverride(""AWSS3V4SignerType"");

AmazonS3Client s3 = new AmazonS3Client(
    new ProfileCredentialsProvider(), clientConfiguration);

GeneratePresignedUrlRequest request = new GeneratePresignedUrlRequest(
    myBucket, myKey, HttpMethod.PUT);
URL puturl = s3.generatePresignedUrl(request);
</code></pre>
",10,CC BY-SA 4.0,60058511.0,2020-02-04T13:29:39.277,0.0,441.0,2020-02-04T13:34:52.773,2020-02-04T18:57:52.493,V4 Signing for aws S3 using aws java sdk,<amazon-web-services><amazon-s3><signing>,1.0,2.0,,,
106429437,43070006,0,"I also found this old (maybe outdated) AWS blog from 2012-06-05 saying ""Unlike a hard bounce, a soft bounce is not a permanent failure or rejection (e.g., mailbox full). Many email systems (including Amazon SES) will automatically try to resend a message that has generated a soft bounce over a period of time until the message either delivers or the system will no longer retry the delivery and in turn generates a hard bounce.""  https://aws.amazon.com/blogs/messaging-and-targeting/email-definitions-bounces/",2020-02-11T16:15:33.977,470749,CC BY-SA 4.0,True,1,,.,1,"I also found this old (maybe outdated) AWS blog from 2012-06-05 saying ""Unlike a hard bounce, a soft bounce is not a permanent failure or rejection (e.g., mailbox full).",False,False,43070006.0,35926627.0,2017-03-28T12:46:35.580,4.0,"<p>A similar question was asked long back on the Amazon Forum, the answer available there is : </p>

<blockquote>
  <p>When attempting to deliver an email, Amazon SES will continue making delivery attempts until receiving a successful response, or until 12 hours elapse. If the receiving ISP returns a temporary error code (e.g., a 4xx SMTP code), then SES will keep trying. There is no limit to the number of attempts; however, SES will apply exponential backoff between retries for up to 12 hours.</p>
</blockquote>

<p>You can check the <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=110939"" rel=""nofollow noreferrer"">post here</a></p>
",3,CC BY-SA 3.0,35926627.0,2016-03-10T20:29:44.250,4.0,917.0,2016-12-19T08:49:36.203,2019-03-27T17:10:16.067,Amazon SES Soft Bounce,<amazon-web-services><amazon-ses>,3.0,0.0,,1.0,
106448753,60182652,4,Code-only answers are discouraged. Please provide an explanation why and how your answer solves the problem.,2020-02-12T08:07:25.940,1075332,CC BY-SA 4.0,True,1,answers,.,0,Code-only answers are discouraged.,False,False,,,,,,,,,,,,,,,,,,,,
106479221,60193372,1,"Hadoop.mapred properties are deprecated, by the way, and you can put those in the xml files rather than code",2020-02-13T02:34:57.550,2308683,CC BY-SA 4.0,True,1,properties,code,0,"Hadoop.mapred properties are deprecated, by the way, and you can put those in the xml files rather than code",False,False,60193372.0,60172792.0,2020-02-12T16:51:41.403,0.0,"<p>I am posting what I ended up doing to fix the issue for anyone who might see the same exception:</p>

<p>I added <code>hadoop-aws</code> to <code>HADOOP_OPTIONAL_TOOLS</code> in  hadoop-env.sh. I also removed all configurations in spark for <code>s3a</code> except the access/secret and everything worked. My code before the changes:</p>

<pre><code># Setup the Spark Process
conf = SparkConf() \
       .setAppName(app_name) \
       .set(""spark.hadoop.mapred.output.compress"", ""true"") \
       .set(""spark.hadoop.mapred.output.compression.codec"", ""true"") \
       .set(""spark.hadoop.mapred.output.compression.codec"", ""org.apache.hadoop.io.compress.GzipCodec"") \
       .set(""spark.hadoop.mapred.output.compression.`type"", ""BLOCK"") \
       .set(""spark.speculation"", ""false"")\
       .set(""fs.s3a.aws.credentials.provider"", ""org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider"")\
       .set(""com.amazonaws.services.s3.enableV4"", ""true"")

# Some other configs

spark_context._jsc.hadoopConfiguration().set(
            ""fs.s3a.impl"", ""org.apache.hadoop.fs.s3a.S3AFileSystem""
)

spark_context._jsc.hadoopConfiguration().set(
            ""fs.s3a.access.key"", s3_key
)

spark_context._jsc.hadoopConfiguration().set(
            ""fs.s3a.secret.key"", s3_secret
)

spark_context._jsc.hadoopConfiguration().set(
            ""fs.s3a.multipart.size"", ""104857600""
)
</code></pre>

<p>And after:</p>

<pre><code># Setup the Spark Process
conf = SparkConf() \
       .setAppName(app_name) \
       .set(""spark.hadoop.mapred.output.compress"", ""true"") \
       .set(""spark.hadoop.mapred.output.compression.codec"", ""true"") \
       .set(""spark.hadoop.mapred.output.compression.codec"", ""org.apache.hadoop.io.compress.GzipCodec"") \
       .set(""spark.hadoop.mapred.output.compression.`type"", ""BLOCK"") \
       .set(""spark.speculation"", ""false"")

# Some other configs

spark_context._jsc.hadoopConfiguration().set(
            ""fs.s3a.access.key"", s3_key
)

spark_context._jsc.hadoopConfiguration().set(
            ""fs.s3a.secret.key"", s3_secret
)
</code></pre>

<p>That probably means that it was a class path issue. The <code>hadoop-aws</code> wasn't getting added to the class path and so under the covers it was defaulting to some other implementation of <code>S3AFileSystem.java</code>. Hadoop and spark are a huge pain in this area because there are so many different places and ways to load things and java is particular about the order as well because if it doesn't happen in the right order, it will just go with whatever was loaded last. Hope this helps others facing the same issue.</p>
",3,CC BY-SA 4.0,60172792.0,2020-02-11T15:53:28.133,3.0,705.0,,2020-02-12T16:51:41.403,"Reading data from S3 using pyspark throws java.lang.NumberFormatException: For input string: ""100M""",<apache-spark><hadoop><amazon-s3><pyspark>,2.0,2.0,,,
106609197,2141499,0,"This answer is out of date and/or inaccurate.  Folders are very much a concept on S3, such as the easily observed ""Create folder"" button in the console.  While the intention of the answer is good (in that folders aren't generally useful constructs alone), no help is provided if you have a legitimate need to create a folder with no initial content, such as cases where users may be uploading content to specially defined folders through the console.",2020-02-17T21:13:38.297,2738164,CC BY-SA 4.0,True,1,answer, ,0,This answer is outdated and/or inaccurate.  ,False,False,,,,,,,,,,,,,,,,,,,,
106717371,52855543,1,"`*** ValueError: DataFrame constructor not properly called!`

Answer may be deprecated. `body` is a bytes object, which I had to convert into a utf-8 specified string encoding and then a StringIO object as shown here:
https://stackoverflow.com/questions/47379476/how-to-convert-bytes-data-into-a-python-pandas-dataframe, but that just set the entire string as a column name with empty values.",2020-02-20T21:28:24.263,3121548,CC BY-SA 4.0,True,1,Answer,.,0,"`

Answer may be deprecated.",False,False,52855543.0,52855221.0,2018-10-17T13:03:08.600,7.0,"<p>The <code>boto3</code> API does not support reading multiple objects at once.  What you can do is retrieve all objects with a specified prefix and load each of the returned objects with a loop.  To do this you can use the <code>filter()</code> method and set the <code>Prefix</code> parameter to the prefix of the objects you want to load.  Below I've made this simple change to your code that will let you get all the objects with the prefix ""files/splittedfiles/Code-345678"" that you can read by looping through those objects where you can load each file into a DataFrame:</p>

<pre><code>s3 = boto3.resource('s3')
bucket = s3.Bucket('test-bucket')
prefix_objs = bucket.objects.filter(Prefix=""files/splittedfiles/Code-345678"")
for obj in prefix_objs:
    key = obj.key
    body = obj.get()['Body'].read()
</code></pre>

<p>If you have multiple prefixes you are going to want to evaluate you can take the above and turn it into a function where the prefix is a parameter then combine the results together.  The function could like something like this:</p>

<pre><code>import pandas as pd

def read_prefix_to_df(prefix):
    s3 = boto3.resource('s3')
    bucket = s3.Bucket('test-bucket')
    prefix_objs = bucket.objects.filter(Prefix=prefix)
    prefix_df = []
    for obj in prefix_objs:
        key = obj.key
        body = obj.get()['Body'].read()
        df = pd.DataFrame(body)
        prefix_df.append(df)
    return pd.concat(prefix_df)
</code></pre>

<p>Then you can iteratively apply this function to each prefix and combine the results in the end.</p>
",1,CC BY-SA 4.0,52855221.0,2018-10-17T12:47:04.473,3.0,9673.0,2018-10-17T13:47:15.020,2020-08-14T15:36:39.047,Reading multiple csv files from S3 bucket with boto3,<python><csv><amazon-s3><boto3>,3.0,0.0,,3.0,
106736985,60333506,0,"Hey David, thank you for the response. My understanding of window is that they are Akka actors. If a sensor is dead, the `sensorId` gets obsolete. So if we don't have a purging strategy for windows, these obsolete actors will be consuming the main memory.",2020-02-21T13:31:28.807,1609570,CC BY-SA 4.0,True,2,sensor,.,1,"If a sensor is dead, the `sensorId` gets obsolete.",True,True,,,,,,,,,,,,,,,,,,,,
106765844,60175675,0,I like that solution but `ResourceServerProperties` is deprecated in spring...,2020-02-22T18:08:20.230,8316249,CC BY-SA 4.0,True,1,ResourceServerProperties,...,0,`ResourceServerProperties` is deprecated in spring...,False,False,60175675.0,48327369.0,2020-02-11T18:58:23.920,1.0,"<p>We can create Spring Boot resource server, keeping Cognito as Identity Provider.</p>

<p><a href=""https://i.stack.imgur.com/9pCh3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9pCh3.png"" alt=""Spring boot Resource Server""></a></p>

<p><strong>Dependency:</strong></p>

<pre><code>    &lt;!--  Spring Security--&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-oauth2-resource-server&lt;/artifactId&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.security.oauth.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-security-oauth2-autoconfigure&lt;/artifactId&gt;
        &lt;version&gt;2.0.1.RELEASE&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p><strong>Spring Security Configuration:</strong></p>

<pre><code>EnableWebSecurity
@EnableGlobalMethodSecurity(prePostEnabled = true)
public class OAuth2ResourceServerSecurityConfiguration extends ResourceServerConfigurerAdapter {

  private final ResourceServerProperties resource;

  public OAuth2ResourceServerSecurityConfiguration(ResourceServerProperties resource) {
    this.resource = resource;
  }

  @Override
  public void configure(HttpSecurity http) throws Exception {

    http.cors();

    http.csrf().disable();

    http.authorizeRequests()
        .antMatchers(""/api/public/**"").permitAll()
        .antMatchers(""/actuator/health"").permitAll()
        .anyRequest().authenticated();
  }


  // Note: Cognito Converter
  @Bean
  public TokenStore jwkTokenStore() {
    return new JwkTokenStore(
        Collections.singletonList(resource.getJwk().getKeySetUri()),
        new CognitoAccessTokenConverter(),
        null);
  }
}
</code></pre>

<p><strong>Cognito Access Token Converter:</strong></p>

<p>Here we are converting the Cognito claims to Spring Security consumable format.</p>

<pre><code>@Component
public class CognitoAccessTokenConverter extends JwtAccessTokenConverter {

  // Note: This the core part.
  private static final String COGNITO_GROUPS = ""cognito:groups"";
  private static final String SPRING_AUTHORITIES = ""authorities"";
  private static final String COGNITO_USERNAME = ""username"";
  private static final String SPRING_USER_NAME = ""user_name"";

  @SuppressWarnings(""unchecked"")
  @Override
  public OAuth2Authentication extractAuthentication(Map&lt;String, ?&gt; claims) {

    if (claims.containsKey(COGNITO_GROUPS))
      ((Map&lt;String, Object&gt;) claims).put(SPRING_AUTHORITIES, claims.get(COGNITO_GROUPS));
    if (claims.containsKey(COGNITO_USERNAME))
      ((Map&lt;String, Object&gt;) claims).put(SPRING_USER_NAME, claims.get(COGNITO_USERNAME));
    return super.extractAuthentication(claims);
  }
}
</code></pre>

<p><strong>application.properties</strong></p>

<pre><code>server:
  port: 8081
security:
  oauth2:
    resource:
      userInfoUri: https://&lt;cognito&gt;.auth.eu-west-1.amazoncognito.com/oauth2/userInfo
      tokenInfoUri: https://&lt;cognito&gt;.auth.eu-west-1.amazoncognito.com/oauth2/token
      jwk:
        key-set-uri: https://cognito-idp.&lt;region&gt;.amazonaws.com/&lt;user-pool-id&gt;/.well-known/jwks.json
    client:
      clientId: &lt;client-id&gt;
</code></pre>

<p>For complete article, refer: <a href=""https://medium.com/@arjunsk/resource-server-with-cognito-b7fbfbee0155"" rel=""nofollow noreferrer"">Integrate Spring Boot Resource Server with Cognito Identity Provider</a> </p>
",6,CC BY-SA 4.0,48327369.0,2018-01-18T17:45:44.193,11.0,11633.0,2019-07-15T10:31:37.240,2020-02-11T18:58:23.920,Amazon Cognito Oauth2 with Spring Security,<java><spring-security><oauth-2.0><amazon-cognito>,2.0,0.0,,7.0,
106766445,60175675,0,"Interesting, I didn't thought about that option altough I planned on using API-GW. Will look at that one. 
However, doesn't solve the deprecated issue :)",2020-02-22T18:42:00.637,8316249,CC BY-SA 4.0,True,1,,:),1,"However, doesn't solve the deprecated issue :)",False,False,60175675.0,48327369.0,2020-02-11T18:58:23.920,1.0,"<p>We can create Spring Boot resource server, keeping Cognito as Identity Provider.</p>

<p><a href=""https://i.stack.imgur.com/9pCh3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9pCh3.png"" alt=""Spring boot Resource Server""></a></p>

<p><strong>Dependency:</strong></p>

<pre><code>    &lt;!--  Spring Security--&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-oauth2-resource-server&lt;/artifactId&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.security.oauth.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-security-oauth2-autoconfigure&lt;/artifactId&gt;
        &lt;version&gt;2.0.1.RELEASE&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p><strong>Spring Security Configuration:</strong></p>

<pre><code>EnableWebSecurity
@EnableGlobalMethodSecurity(prePostEnabled = true)
public class OAuth2ResourceServerSecurityConfiguration extends ResourceServerConfigurerAdapter {

  private final ResourceServerProperties resource;

  public OAuth2ResourceServerSecurityConfiguration(ResourceServerProperties resource) {
    this.resource = resource;
  }

  @Override
  public void configure(HttpSecurity http) throws Exception {

    http.cors();

    http.csrf().disable();

    http.authorizeRequests()
        .antMatchers(""/api/public/**"").permitAll()
        .antMatchers(""/actuator/health"").permitAll()
        .anyRequest().authenticated();
  }


  // Note: Cognito Converter
  @Bean
  public TokenStore jwkTokenStore() {
    return new JwkTokenStore(
        Collections.singletonList(resource.getJwk().getKeySetUri()),
        new CognitoAccessTokenConverter(),
        null);
  }
}
</code></pre>

<p><strong>Cognito Access Token Converter:</strong></p>

<p>Here we are converting the Cognito claims to Spring Security consumable format.</p>

<pre><code>@Component
public class CognitoAccessTokenConverter extends JwtAccessTokenConverter {

  // Note: This the core part.
  private static final String COGNITO_GROUPS = ""cognito:groups"";
  private static final String SPRING_AUTHORITIES = ""authorities"";
  private static final String COGNITO_USERNAME = ""username"";
  private static final String SPRING_USER_NAME = ""user_name"";

  @SuppressWarnings(""unchecked"")
  @Override
  public OAuth2Authentication extractAuthentication(Map&lt;String, ?&gt; claims) {

    if (claims.containsKey(COGNITO_GROUPS))
      ((Map&lt;String, Object&gt;) claims).put(SPRING_AUTHORITIES, claims.get(COGNITO_GROUPS));
    if (claims.containsKey(COGNITO_USERNAME))
      ((Map&lt;String, Object&gt;) claims).put(SPRING_USER_NAME, claims.get(COGNITO_USERNAME));
    return super.extractAuthentication(claims);
  }
}
</code></pre>

<p><strong>application.properties</strong></p>

<pre><code>server:
  port: 8081
security:
  oauth2:
    resource:
      userInfoUri: https://&lt;cognito&gt;.auth.eu-west-1.amazoncognito.com/oauth2/userInfo
      tokenInfoUri: https://&lt;cognito&gt;.auth.eu-west-1.amazoncognito.com/oauth2/token
      jwk:
        key-set-uri: https://cognito-idp.&lt;region&gt;.amazonaws.com/&lt;user-pool-id&gt;/.well-known/jwks.json
    client:
      clientId: &lt;client-id&gt;
</code></pre>

<p>For complete article, refer: <a href=""https://medium.com/@arjunsk/resource-server-with-cognito-b7fbfbee0155"" rel=""nofollow noreferrer"">Integrate Spring Boot Resource Server with Cognito Identity Provider</a> </p>
",6,CC BY-SA 4.0,48327369.0,2018-01-18T17:45:44.193,11.0,11633.0,2019-07-15T10:31:37.240,2020-02-11T18:58:23.920,Amazon Cognito Oauth2 with Spring Security,<java><spring-security><oauth-2.0><amazon-cognito>,2.0,0.0,,7.0,
107088029,60522951,0,"Thank you. This solves my problem. The only thing is, DynamoDBMapperConfig constructor is deprecated so I used this syntax.
dynamoDBMapper.save(geo, SaveBehavior.UPDATE_SKIP_NULL_ATTRIBUTES.config()); I have one more doubt, is there any way to load certain attributes while retrieving data from DB using DynamDBMapper?",2020-03-04T18:16:33.357,3619377,CC BY-SA 4.0,True,1,,deprecated,0,"The only thing is, DynamoDBMapperConfig constructor is deprecated",False,False,60522951.0,60508596.0,2020-03-04T09:35:13.840,1.0,"<p>Use the <code>UPDATE_SKIP_NULL_ATTRIBUTES</code> SaveBehavior</p>

<p>More details on: <a href=""https://aws.amazon.com/blogs/developer/using-the-savebehavior-configuration-for-the-dynamodbmapper/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/developer/using-the-savebehavior-configuration-for-the-dynamodbmapper/</a></p>

<p>Add the SaveBehavior to your <code>save</code> operation and keep fields other than <code>id</code> and <code>address</code> null:</p>

<pre><code>mapper.save(user, new DynamoDBMapperConfig(SaveBehavior.UPDATE_SKIP_NULL_ATTRIBUTES));
</code></pre>
",1,CC BY-SA 4.0,60508596.0,2020-03-03T13:42:58.157,1.0,257.0,,2020-03-04T09:35:13.840,Update specific attributes with DynamoDBMapper in java,<amazon-dynamodb><aws-sdk><amazon-dynamodb-local>,1.0,0.0,,,
107302954,53715318,0,The link seems to be outdated?,2020-03-12T08:36:06.973,133747,CC BY-SA 4.0,True,1,,?,0,The link seems to be outdated?,False,False,53715318.0,53713253.0,2018-12-10T23:37:46.620,0.0,"<p>Turns out the wildcard <code>*</code> is a catch all i.e. it will match unlimited trailing url parameters. An important thing to keep in mind is that <code>topicfilter</code> and <code>topic</code> are different and are different resources for different MQTT actions as outlined <a href=""https://docs.aws.amazon.com/iot/latest/developerguide/action-resources.html"" rel=""nofollow noreferrer"">here</a>. </p>
",1,CC BY-SA 4.0,53713253.0,2018-12-10T20:36:11.790,1.0,652.0,,2018-12-10T23:37:46.620,AWS IoT Core topic filter wildcards,<amazon-web-services><iot><aws-iot>,1.0,0.0,,,
107418725,60710879,1,"From AWS documentation, the `-i` option is obsolete.
it's mentioned `The --instance_type option is obsolete. It's replaced by the newer and more powerful --instance-types option.`

https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb3-create.html",2020-03-16T21:07:20.560,6906901,CC BY-SA 4.0,True,2,option,"
",0,"From AWS documentation, the `-i` option is obsolete.
",False,False,60710879.0,60687297.0,2020-03-16T17:52:04.273,1.0,"<p>I believe the option you'd want to use is -i or --instance-type. The -it option is for instance types which expects a list of instance types rather than just one. Most likely what happened was that the ebcli did not know what to make of only one instance type when it expected something like <code>eb create demov2 -it ""t2.micro, t3.micro, t3a.large""</code></p>
",1,CC BY-SA 4.0,60687297.0,2020-03-14T21:04:09.177,2.0,30.0,,2020-03-16T17:52:04.273,AWS Elasticbeanstalk CLI ignores instace type option,<amazon-web-services><amazon-ec2><amazon-elastic-beanstalk><ebcli>,1.0,0.0,,,
107585245,58160710,0,please be aware that NODE_PATH is deprecated... it is strongly advised to not use it!,2020-03-23T05:02:46.017,2346420,CC BY-SA 4.0,True,1,,...,0,please be aware that NODE_PATH is deprecated...,False,False,,,,,,,,,,,,,,,,,,,,
107615805,55282492,2,"For others looking, I had the same problem and the suggested methods are now deprecated moreover Android now uses content providers making it tricky to access the underlying file, if interested I documented my solution [here](https://gutier.io/post/android-upload-file-to-aws-s3-bucket-with-retrofit2/)",2020-03-24T04:59:03.410,2948807,CC BY-SA 4.0,True,1,,moreover,0,"For others looking, I had the same problem and the suggested methods are now deprecated moreover",False,False,55282492.0,55162161.0,2019-03-21T14:16:48.777,2.0,"<p>solution was to include multipart and expect Single response not Call :</p>

<pre><code>  @Multipart
    @PUT
    fun uploadAsset(
        @Header(CONTENT_TYPE) contentType: String,
        @Url uploadUrl: String,
        @Part file: MultipartBody.Part
    ): Single&lt;ResponseBody&gt;
</code></pre>

<p>where contentType is passed in </p>

<pre><code>val requestFile = RequestBody.create(MediaType.parse(contentType), file)
val body = MultipartBody.Part.createFormData(mediaType, task.file_name, requestFile)
assetService.uploadAsset(contentType, task.upload_url, body)
</code></pre>
",2,CC BY-SA 4.0,55162161.0,2019-03-14T12:07:42.227,2.0,1036.0,2019-03-14T12:15:00.477,2019-03-21T14:16:48.777,PUT upload file to AWS S3 presigned url Retrofit2 Android,<android><amazon-s3><kotlin><retrofit><retrofit2>,2.0,0.0,,,
107855245,38555806,0,"Where did this table of info come from? A reference link would be helpful. Some of it is out of date and there are more instance versions and types now, e.g. m5",2020-04-01T02:46:56.710,1335793,CC BY-SA 4.0,True,1,Some,m5,0,"Some of it is outdated and there are more instance versions and types now, e.g. m5",False,False,,,,,,,,,,,,,,,,,,,,
107971331,57137451,0,I just saw that AWS added new features to ECS and its ELB integration as Lasse pointed out correctly in the accepted answer. This makes my answer completly outdated. Just adding this note here to avoid confusion among readers,2020-04-04T15:24:34.500,10473469,CC BY-SA 4.0,True,1,,.,0,This makes my answer completly outdated.,False,False,,,,,,,,,,,,,,,,,,,,
108289590,55069330,0,"Great, thanks for the feedback. I ditched the date= prefix but had to reimport all my data, using dt= now. I've got daily (dt=yyyy-mm-dd/) partitions as i didn't really see the value in splitting into /y=/m=/d=/h=, but that may bite me later. I will run a scheduled lamda to prep for the next month that's easy enough. I have a CTAS table which gets queried much more often throughout the day on top of the daily partitioned one so that is immediately out of date. Any suggestions on how to keep that up to date (shortest 5 min intervals)? Are you on the og-aws slack or anything per chance?",2020-04-14T16:28:38.423,769237,CC BY-SA 4.0,True,1,that,.,0,so that is immediately outdated.,False,False,,,,,,,,,,,,,,,,,,,,
108318772,61223756,0,Do you know how can I resolve this? a3n is deprecated for this 3.0.0 version...,2020-04-15T13:03:21.483,8515356,CC BY-SA 4.0,True,1,a3n,...,0,a3n is deprecated for this 3.0.0 version...,False,False,,,,,,,,,,,,,,,,,,,,
108414191,17092058,0,"I was running around trying to figure out why I couldn't get this to work and I realized that this answer is outdated (2013)

AWS has updated to ""signature version 4"". The method is similar but has more information and steps. A zip with code shows how to do it like the answer here, only more up to date: https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-examples-using-sdks.html",2020-04-18T02:19:02.053,3293816,CC BY-SA 4.0,True,1,,"

",0,"and I realized that this answer is outdated (2013)

",False,False,17092058.0,16917768.0,2013-06-13T16:16:47.833,6.0,"<p>As no-one answering seems to have done it, I spent the time working it out based on guidance from Steve's answer:</p>

<p>In answer to this question ""is there any good examples of doing the authentication into S3 request using HTTPWebRequest, so I can duplicate what I do with Cloud Files?"", here is how to generate the auth header manually:</p>

<pre><code>string today = String.Format(""{0:ddd,' 'dd' 'MMM' 'yyyy' 'HH':'mm':'ss' 'zz00}"", DateTime.Now);

string stringToSign = ""PUT\n"" +
    ""\n"" +
    file.SelectSingleNode(""content_type"").InnerText + ""\n"" +
    ""\n"" +
    ""x-amz-date:"" + today + ""\n"" +
    ""/"" + strBucketName + ""/"" + strKey;

Encoding ae = new UTF8Encoding();
HMACSHA1 signature = new HMACSHA1(ae.GetBytes(AWSSecret));
string encodedCanonical = Convert.ToBase64String(signature.ComputeHash(ae.GetBytes(stringToSign)));

string authHeader = ""AWS "" + AWSKey + "":"" + encodedCanonical;

string uriS3 = ""https://"" + strBucketName + "".s3.amazonaws.com/"" + strKey;
var reqS3 = (HttpWebRequest)WebRequest.Create(uriS3);
reqS3.Headers.Add(""Authorization"", authHeader);
reqS3.Headers.Add(""x-amz-date"", today);
reqS3.ContentType = file.SelectSingleNode(""content_type"").InnerText;
reqS3.ContentLength = Convert.ToInt32(file.SelectSingleNode(""bytes"").InnerText);
reqS3.Method = ""PUT"";
</code></pre>

<p>Note the added <code>x-amz-date</code> header as HTTPWebRequest sends the date in a different format to what AWS is expecting.</p>

<p>From there it was just a case of repeating what I was already doing.</p>
",4,CC BY-SA 3.0,16917768.0,2013-06-04T12:24:30.023,7.0,3097.0,2013-06-04T14:40:21.040,2013-11-14T18:47:15.493,Upload to S3 from HTTPWebResponse.GetResponseStream() in c#,<c#><amazon-web-services><httpwebrequest>,5.0,0.0,,,
108478905,55632214,0,This is outdated in 2020... It has not been updated for 2+ years,2020-04-20T10:35:49.313,3421066,CC BY-SA 4.0,True,1,This,...,0,This is outdated in 2020...,False,False,55632214.0,55628932.0,2019-04-11T12:11:44.907,0.0,"<p>I found a solution in <a href=""https://github.com/awslabs/dynamodb-data-mapper-js/tree/master/packages/dynamodb-data-marshaller"" rel=""nofollow noreferrer"">DynamoDB Data Marshaller</a>.</p>

<pre><code>import { Handler, Context, Callback } from 'aws-lambda'
import { AttributeMap } from ""aws-sdk/clients/dynamodb"";
import { DataMapper } from '@aws/dynamodb-data-mapper'
import { unmarshallItem, Schema } from '@aws/dynamodb-data-marshaller'

const someSchema: Schema = {
    id: {type: 'String', keyType: ""HASH""},
    date_time: {type: 'Date', keyType: ""RANGE""},
    some_property: {type: 'String'},
    another_property: {type: 'String'},
};

const handler: Handler = async (event: any, context: Context, callback: Callback) =&gt; {

  event.Records.forEach((record) =&gt; {
    let dynamoDB: AttributeMap = record.dynamodb.NewImage
    let someObject = unmarshallItem(someSchema, dynamoDB)
    console.log('DynamoDB Object: %j', someObject)

  });
  return callback(null, `Successfully processed ${event.Records.length} records.`);
}

export { handler }
</code></pre>
",1,CC BY-SA 4.0,55628932.0,2019-04-11T09:19:10.647,0.0,1624.0,2019-04-12T17:33:14.847,2019-04-12T17:33:14.847,How to map a DynamoDB Streams object to a Javascript object?,<node.js><typescript><aws-lambda><amazon-dynamodb><amazon-dynamodb-streams>,2.0,0.0,,,
108568178,61351243,0,"Indeed you are correct, I was way out of date. With update, it works as expected.",2020-04-22T17:20:45.347,6831054,CC BY-SA 4.0,True,1,you,.,0,"Indeed you are correct, I was way outdated.",False,False,61351243.0,61349955.0,2020-04-21T19:06:25.493,0.0,"<p>This works fine for me with both the awscli and boto3. For example:</p>

<pre><code>import boto3

client = boto3.client('ec2')

subnets = client.describe_subnets()

for subnet in subnets['Subnets']:
    print(subnet['AvailabilityZone'], subnet['AvailabilityZoneId'])
</code></pre>

<p>Output is:</p>

<pre><code>us-east-1b use1-az2
us-east-1e use1-az3
us-east-1d use1-az6
...
</code></pre>

<p>I think your installation of awscli and boto3 may be out of date.</p>
",1,CC BY-SA 4.0,61349955.0,2020-04-21T17:55:31.790,0.0,38.0,,2020-04-22T01:59:00.633,AWS CLI or boto3: Trying to get the availability-zone id?,<amazon-web-services><command-line-interface><boto3><zone><availability>,2.0,0.0,,,
108791591,61500809,0,yeah thanks error reolve but new error is here (DeprecationWarning: Deep requiring like `const uuidv1 = require('uuid/v1');` is deprecated as of uuid@7.x),2020-04-29T11:42:17.987,13004406,CC BY-SA 4.0,True,1,,),0,(DeprecationWarning: Deep requiring like `const uuidv1 = require('uuid/v1');` is deprecated as of uuid@7.x),False,False,61500809.0,61500545.0,2020-04-29T11:32:26.417,1.0,"<p>Since aws-sdk library supports promise, its not necessary to use <code>es6-promisify</code> library. Using node.js <code>async/await</code> we shall achieve the same use case. </p>

<pre><code>'use strict';

const uuidV1 = require('uuid/v1');
const AWS = require('aws-sdk');
const dynamo = new AWS.DynamoDB.DocumentClient();

module.exports.saveBookingToDatabase = async function(Arrival_city, Departure_city, Flight_type, Phone_number){
    console.log('saveBookingToDatabase');

    const item = {};
    item.bookingId = uuidV1();
    item.arrivalCity = Arrival_city;
    item.departureCity = Departure_city;
    item.classType = Flight_type;
    item.phone = Phone_number;

    const params = {
        TableName: 'airstallion',
        Item: item
    };

    try {
        let result = await dynamo.put(params)
        console.log(`Saving ticket ${JSON.stringify(item)}`);
    return item;  
    } catch(e) {
        throw (e)
    }
}
</code></pre>
",4,CC BY-SA 4.0,61500545.0,2020-04-29T11:17:45.227,0.0,75.0,2020-04-29T14:06:42.820,2020-04-29T14:06:42.820,Promisify is not a function,<node.js><amazon-web-services><amazon-dynamodb><amazon-lex>,1.0,0.0,,,
108791758,61500809,0,"The way uuid/v1 library required is deprecated in uuid@7.x library, update your module import as `import { v1 as uuidv1 } from 'uuid';` For more info checkout this [link](https://github.com/uuidjs/uuid/blob/master/README_js.md#deep-requires-now-deprecated)",2020-04-29T11:48:05.630,3087716,CC BY-SA 4.0,True,1,,),0,"The way uuid/v1 library required is deprecated in uuid@7.x library, update your module import as `import { v1 as uuidv1 } from 'uuid';` For more info checkout this [link](https://github.com/uuidjs/uuid/blob/master/README_js.md#deep-requires-now-deprecated)",False,False,61500809.0,61500545.0,2020-04-29T11:32:26.417,1.0,"<p>Since aws-sdk library supports promise, its not necessary to use <code>es6-promisify</code> library. Using node.js <code>async/await</code> we shall achieve the same use case. </p>

<pre><code>'use strict';

const uuidV1 = require('uuid/v1');
const AWS = require('aws-sdk');
const dynamo = new AWS.DynamoDB.DocumentClient();

module.exports.saveBookingToDatabase = async function(Arrival_city, Departure_city, Flight_type, Phone_number){
    console.log('saveBookingToDatabase');

    const item = {};
    item.bookingId = uuidV1();
    item.arrivalCity = Arrival_city;
    item.departureCity = Departure_city;
    item.classType = Flight_type;
    item.phone = Phone_number;

    const params = {
        TableName: 'airstallion',
        Item: item
    };

    try {
        let result = await dynamo.put(params)
        console.log(`Saving ticket ${JSON.stringify(item)}`);
    return item;  
    } catch(e) {
        throw (e)
    }
}
</code></pre>
",4,CC BY-SA 4.0,61500545.0,2020-04-29T11:17:45.227,0.0,75.0,2020-04-29T14:06:42.820,2020-04-29T14:06:42.820,Promisify is not a function,<node.js><amazon-web-services><amazon-dynamodb><amazon-lex>,1.0,0.0,,,
108840352,61527478,0,"When i give `prefix` like this `prefix = ""/folder1/newfiles""`, it is showing up below `warning` 
`Warning: Interpolation-only expressions are deprecated

  on s3.tf line 24, in resource ""local_file"" ""foo"":
  24:     count    = ""${length(data.aws_s3_bucket_objects.my_objects.keys)}""`",2020-04-30T17:26:35.747,13081977,CC BY-SA 4.0,True,1,expressions,    ,0,"Interpolation-only expressions are deprecated

  on s3.tf line 24, in resource ""local_file"" ""foo"":
  24:     ",False,False,,,,,,,,,,,,,,,,,,,,
108845453,61531372,0,This video suggest that you can (but is outdated): https://youtu.be/iU4Q4Tv822Q,2020-04-30T20:18:56.113,13084623,CC BY-SA 4.0,True,1,,https://youtu.be/iU4Q4Tv822Q,0,This video suggest that you can (but is outdated): https://youtu.be/iU4Q4Tv822Q,False,False,61531372.0,61530690.0,2020-04-30T19:37:02.740,1.0,"<p>No.</p>

<p>AWS Dynamo is not a relational DB...it does not understand SQL.</p>
",3,CC BY-SA 4.0,61530690.0,2020-04-30T18:51:34.160,-2.0,26.0,2020-04-30T22:16:32.683,2020-04-30T22:16:32.683,Creating an Oracle Db in AWS,<amazon-web-services><amazon-dynamodb><oracle-sqldeveloper><amazon-rds>,1.0,3.0,2020-04-30T21:15:46.130,,
108846111,58184592,1,they're the ones in the documentation I linked to. copying them here only ensures that stack overflow posts end up full of out of date facts,2020-04-30T20:43:04.023,2261274,CC BY-SA 4.0,True,1,,facts,0,copying them here only ensures that stack overflow posts end up full of outdated facts,False,False,58184592.0,58143957.0,2019-10-01T12:15:46.720,2.0,"<p>You are using the wrong property names for the s3a connector.</p>

<p>see <a href=""https://hadoop.apache.org/docs/current3/hadoop-aws/tools/hadoop-aws/#Authentication_properties"" rel=""nofollow noreferrer"">https://hadoop.apache.org/docs/current3/hadoop-aws/tools/hadoop-aws/#Authentication_properties</a></p>
",2,CC BY-SA 4.0,58143957.0,2019-09-28T06:34:37.887,0.0,3033.0,,2019-10-01T21:42:08.147,Unable to load AWS credentials from any provider in the chain - error - when trying to load model from S3,<amazon-web-services><apache-spark><amazon-s3><pyspark><apache-spark-mllib>,2.0,1.0,,,
108999022,61619800,0,"Hi Abhinaya, I'm doing that already. However, I'm not using `cfnresponse` because of compatibility issues with `boto3` (they cannot share the same version of `botocore`). Instead, I used the piece of code in your link for Python 3 (the second starting from the end), just replacing `botocore.vendored.requests` by the `requests` library (since the former is deprecated and removed in the latest versions).",2020-05-05T18:15:30.773,2377885,CC BY-SA 4.0,True,1,,.,0,"Instead, I used the piece of code in your link for Python 3 (the second starting from the end), just replacing `botocore.vendored.requests` by the `requests` library (since the former is deprecated and removed in the latest versions).",False,False,61619800.0,61619575.0,2020-05-05T17:58:17.273,1.0,"<p>The issue is with your lambda function. You have to send back the SUCCESS or FAILURE signals back to the CFN. Since your lambda function is nots sending any signals, its waiting for Timeout (2 hours) and the Cloudformation gets failed</p>

<pre><code>1.The custom resource provider processes the AWS CloudFormation request and 
  returns a response of SUCCESS or FAILED to the pre-signed URL. AWS 
  CloudFormation waits and listens for a response in the pre-signed URL location. 

2.After getting a SUCCESS response, AWS CloudFormation proceeds with the stack 
  operation. If a FAILURE or no response is returned, the operation fails.

</code></pre>

<p>Please use cfnresponse module in your lambda function to send the SUCCESS/FAILURE signals back to your Cloudformation</p>

<p>For more details:
<a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html</a></p>
",1,CC BY-SA 4.0,61619575.0,2020-05-05T17:45:34.403,0.0,100.0,,2020-05-06T10:10:09.033,Custom resource not running properly on deployment,<aws-lambda><amazon-cloudformation><serverless-framework><aws-cloudformation-custom-resource>,2.0,0.0,,,
109032638,61623888,1,"True, title says no EMR. This was originally posted on the AWS EMR forum saying he did want EMR. Wasn't clear which was the typo. I get the impression OP is new to EMR but familiar with Spark, and admittedly when I was new to EMR, I was confused by (often outdated) advice online that was unfamiliar with EMRFS (instead advising now-very-silly things like using s3-dist-cp on startup to copy S3 data to HDFS on EBS, things like that). But agree if EMR is not used for whatever reason, EFS is a good option.",2020-05-06T15:54:45.713,6376831,CC BY-SA 4.0,True,1,,.,0,"I get the impression OP is new to EMR but familiar with Spark, and admittedly when I was new to EMR, I was confused by (often outdated) advice online that was unfamiliar with EMRFS (instead advising now-very-silly things like using s3-dist-cp on startup to copy S3 data to HDFS on EBS, things like that).",False,True,61623888.0,61587195.0,2020-05-05T22:08:16.747,0.0,"<p>I think you may have an X-Y problem here. You almost certainly do not want to have a remote HDFS filesystem on EMR.</p>

<p>EMR provides two HDFS-compatible filesystems to Hadoop and Spark natively:</p>

<p>1) A transient filesystem, accessed via hdfs://. This is primarily for scratch/temporary data. It lasts as long as the cluster does, and is backed by EBS.</p>

<p>2) A persistent filesystem, accessed via s3://. This is referred to as EMRFS in the documentation. It is backed by S3.</p>

<p>So for example if you're in Spark and you're used to doing something like spark.read.parquet(""hdfs://mydata/somepartition/"").doWork().write.parquet(""hdfs://mynewdata/somepartition/"")</p>

<p>you now just do 
spark.read.parquet(""s3://mybucket/mydata/somepartition/"").doWork().write.parquet(""s3://mybucket/mynewdata/somepartition/"") </p>

<p>and everything just works. s3:// is optimized by the EMR folks for speed since they know your EMR cluster shares a datacenter with the S3 data.</p>

<p>EFS, per Shubham Jain's answer, will probably cause problems with EMR, since you would be running effectively a second HDFS backend aside from the transient one provided with EMR. I suppose you could, but it would be a little strange. On your EMR cluster you would have to have NameNodes for EMR's HDFS, (referred to in EMR as core nodes), and separate NameNodes for the EFS-backed HDFS (which, I guess, would have to run as EMR task nodes?). EFS would be slower than the EBS-backed HDFS for transient data and more expensive than S3 for the permanent data.</p>

<p>If you don't want to use EMRFS for some reason (I have no idea why), you would probably be best off rolling your own cluster and not using EMR, because at that point you're looking to customize how HDFS is installed, and the point of EMR is to do that for you.</p>
",2,CC BY-SA 4.0,61587195.0,2020-05-04T07:32:55.007,1.0,76.0,,2020-05-05T22:08:16.747,Run HDFS cluster on AWS without EMR,<amazon-web-services><hadoop><amazon-ec2><hdfs><google-cloud-dataproc>,2.0,2.0,,,
109073082,61536665,0,"The last commit on GitHub for the [powerapi-akka](https://github.com/abourdon/powerapi-akka) project is dated 2016, and that is a statement that the project is deprecated. It advises to consider [powerapi.org](http://powerapi.org/) that is mentioned in the original question and explained as inapplicable to a virtual environment in the answer.",2020-05-07T16:37:32.353,11602913,CC BY-SA 4.0,True,1,,.,0,"The last commit on GitHub for the [powerapi-akka](https://github.com/abourdon/powerapi-akka) project is dated 2016, and that is a statement that the project is deprecated.",False,False,,,,,,,,,,,,,,,,,,,,
109188290,41115168,0,AWSS3TransferManager is deprecated,2020-05-11T11:53:18.910,6894614,CC BY-SA 4.0,True,1,,deprecated,0,AWSS3TransferManager is deprecated,False,False,41115168.0,41114991.0,2016-12-13T06:51:45.047,2.0,"<p>Okay I found the solution to my problem, apparently I just didn't follow through enough.</p>

<p>Here is my new revised uploadImage function</p>

<pre><code>func uploadImage(filename:String){
        print(""AWS Upload Image Attempt..."")
        //defining bucket and upload file name
        let S3BucketName: String = ""distribution-tech-mobile-live""
        let filepath = ""\(AppDelegate.appDelegate.applicationDocumentsDirectory())/\(filename)""
        let imageURL = URL(fileURLWithPath: filepath)
        let S3UploadKeyName = filename //TODO: Change this later

        let uploadRequest = AWSS3TransferManagerUploadRequest()
        uploadRequest?.bucket = S3BucketName
        uploadRequest?.key = S3UploadKeyName
        uploadRequest?.contentType = ""image/jpeg""
        uploadRequest?.body = imageURL
        uploadRequest?.serverSideEncryption = AWSS3ServerSideEncryption.awsKms
        uploadRequest?.uploadProgress = { (bytesSent, totalBytesSent, totalBytesExpectedToSend) -&gt; Void in
            DispatchQueue.main.async(execute: {
                self.amountUploaded = totalBytesSent // To show the updating data status in label.
                self.fileSize = totalBytesExpectedToSend
                print(""\(totalBytesSent)/\(totalBytesExpectedToSend)"")
            })
        }

        let transferManager = AWSS3TransferManager.default()
        transferManager?.upload(uploadRequest).continue(with: AWSExecutor.mainThread(), withSuccessBlock: { (taskk: AWSTask) -&gt; Any? in
            if taskk.error != nil {
                // Error.
                print(""error"")
            } else {
                // Do something with your result.
                print(""something with result when its done"")
            }
            return nil
        })

    }
</code></pre>

<p>This has spot for when result is done and during the upload progress and makes a lot more sense.</p>
",4,CC BY-SA 3.0,41114991.0,2016-12-13T06:37:36.783,2.0,2109.0,2017-05-23T12:16:29.860,2016-12-13T06:51:45.047,Show Upload Progress for Image Upload to Amazon S3 using Swift 3 and Amazon SDK,<swift><file-upload><amazon-s3><swift3><ios10>,1.0,2.0,,1.0,
109210333,61740781,0,I would say don't use deprecated SDK's :),2020-05-11T23:53:28.513,7859515,CC BY-SA 4.0,True,1,,:),1,I would say don't use deprecated SDK's :),False,False,61740781.0,61719005.0,2020-05-11T22:52:18.423,2.0,"<p>There are several SDK's that can be used with Cognito.</p>

<p>You can use the AWS js SDK to make low level API calls but you will lose the benefits that the Cognito Client Side SDKS provide like session management, caching of tokens and SRP calculations.
<a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityServiceProvider.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityServiceProvider.html</a></p>

<p>You can also use the Cognito Identity SDK which Amplify uses under the hood:
<a href=""https://www.npmjs.com/package/amazon-cognito-identity-js"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/amazon-cognito-identity-js</a></p>

<p>The Auth SDK if you are going to be integrating with the OAuth endpoints, which is now deprecated but can still be used or referenced, Amplify also uses or has similar functionality.
<a href=""https://github.com/amazon-archives/amazon-cognito-auth-js"" rel=""nofollow noreferrer"">https://github.com/amazon-archives/amazon-cognito-auth-js</a></p>

<p>And then finally Amplify which is the go to, feature rich client side SDK which for the majority of usecases should be the SDK of choice in my opinion.</p>
",1,CC BY-SA 4.0,61719005.0,2020-05-10T21:31:01.810,0.0,48.0,,2020-05-11T22:52:18.423,Is AWS Amplify the only way to make SDK calls to Cognito?,<amazon-web-services><aws-api-gateway><amazon-cognito>,1.0,1.0,,,
109316782,61783736,0,"That referenced documentation page says the --output-s3-region argument is deprecated and SSM ""automatically"" figures out what region to write to.  I tried it anyway and it did not work.",2020-05-14T17:06:39.513,13536294,CC BY-SA 4.0,True,1,, ,0,"That referenced documentation page says the --output-s3-region argument is deprecated and SSM ""automatically"" figures out what region to write to.  ",False,False,61783736.0,61783211.0,2020-05-13T19:58:21.337,0.0,"<p>Use argument --output-s3-region</p>

<p><a href=""https://docs.aws.amazon.com/cli/latest/reference/ssm/send-command.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cli/latest/reference/ssm/send-command.html</a></p>
",1,CC BY-SA 4.0,61783211.0,2020-05-13T19:24:35.700,0.0,50.0,2020-05-14T00:09:22.913,2020-05-14T00:09:22.913,ssm-agent trying to write to wrong region,<amazon-web-services><amazon-s3><ssm>,1.0,2.0,,,
109488871,32429489,0,"2020 aproach? PhantomJS is deprecated, ANgular Universal exists... help please",2020-05-20T00:30:13.627,2163398,CC BY-SA 4.0,True,1,,please,0,"PhantomJS is deprecated, ANgular Universal exists... help please",False,False,,,,,,,,,,,,,,,,,,,,
109716907,62024178,0,The latest version of Neptune seems to have a better error message. I can also reproduce this behavior using TinkerGraph. The only time there is no exception is if the number is matched exactly before being tested as a String. In general overloading the type of a property is something that should probably be discouraged.,2020-05-27T01:24:12.247,5442034,CC BY-SA 4.0,True,1,,.,0,In general overloading the type of a property is something that should probably be discouraged.,False,False,,,,,,,,,,,,,,,,,,,,
109776705,56019729,1,It says that ```'AWSS3TransferManager' is deprecated: Use `AWSS3TransferUtility` for upload and download operations.```. Can you update this answer for the latest S3 SDK?,2020-05-28T15:11:14.003,1198077,CC BY-SA 4.0,True,1,,.,0,It says that ```'AWSS3TransferManager' is deprecated: Use `AWSS3TransferUtility` for upload and download operations.,False,False,56019729.0,56018179.0,2019-05-07T09:39:43.927,2.0,"<p>Hey Zubair just use below UIImage extension I made it will directly work in UIImage object </p>

<pre><code> extension UIImage {

    //MARK: Uploading image function with S3 server...
    //MARK: ==========================================
    func uploadImageToS3(uploadFolderName: String = """",
                         compressionRatio : CGFloat = 1,
                         success : @escaping (Bool, String) -&gt; Void,
                         progress : @escaping (CGFloat) -&gt; Void,
                         failure : @escaping (Error) -&gt; Void) {

        let name = ""\(Int(Date().timeIntervalSince1970)).jpeg""
        let path = NSTemporaryDirectory().stringByAppendingPathComponent(path: name)

        //MARK: Compressing image before making upload request...
        guard let data = UIImageJPEGRepresentation(self, compressionRatio) else {
            let err = NSError(domain: ""Error while compressing the image."", code : 01, userInfo : nil)
            failure(err)
            return
        }

        //MARK: Making upload request after image compression is done...
        guard let uploadRequest = AWSS3TransferManagerUploadRequest() else {

            let err = NSError(domain: ""There is a problem while making the uploading request."", code : 02, userInfo : nil)
            failure(err)
            return
        }
        uploadRequest.bucket = ""\(S3Details.bucketName)/\(S3Details.BUCKET_DIRECTORY)\(uploadFolderName.isEmpty ? """" : ""/\(uploadFolderName)"")""
        uploadRequest.acl    = AWSS3ObjectCannedACL.publicRead
        uploadRequest.key    = name

        try? data.write(to: URL(fileURLWithPath : path), options : .atomic)
        uploadRequest.body  = URL(fileURLWithPath: path)

        uploadRequest.uploadProgress = {(
            bytesSent : Int64,
            totalBytesSent : Int64,
            _ totalBytesExpectedToSend : Int64) -&gt; Void in

            progress((CGFloat(totalBytesSent)/CGFloat(totalBytesExpectedToSend)))
                    print((CGFloat(totalBytesSent)/CGFloat(totalBytesExpectedToSend)))
        }

        AWSS3TransferManager.default().upload(uploadRequest).continueWith(executor: AWSExecutor.default()) { (task) -&gt; Void in

            //MARK: That will remove image from temporary storage (NSTemporaryDirectory())...
            try? FileManager.default.removeItem(atPath : path)
            if let err = task.error {

                failure(err)
            } else {

                let imageURL = ""\(S3Details.s3Url)\(S3Details.bucketName)/\(S3Details.BUCKET_DIRECTORY)\(uploadFolderName.isEmpty ? """" : ""/\(uploadFolderName)"")/\(name)""
                //                printDebug(imageURL)
                success(true, imageURL)
            }
        }
    }
  }
</code></pre>
",3,CC BY-SA 4.0,56018179.0,2019-05-07T08:09:22.733,3.0,547.0,2019-05-07T08:29:07.990,2019-05-07T09:39:43.927,How to upload to S3 bucket taking a photo with the iOS Camera,<ios><swift><amazon-s3><uiimageview>,2.0,0.0,,1.0,
109812334,62076034,0,"Hi @John, you've been my AWS saviour for more than a year now, thanks for that. I was thinking through the similar lines of your second last paragraph which will submit a final step to self terminate the EMR, but no luck finding a set of step arguments/parameters for the same. Also checked your other answer but the isIdle Boolean seems to be deprecated. Please let me know if there's a way to crack this.",2020-05-29T14:31:11.487,5536733,CC BY-SA 4.0,True,1,,.,0,Also checked your other answer but the isIdle Boolean seems to be deprecated.,False,False,62076034.0,62072351.0,2020-05-28T23:12:00.887,1.0,"<p>I agree with your research. The optimal situation would be to set <code>KeepJobFlowAliveWhenNoSteps</code> to <code>FALSE</code> to have the cluster self-terminate.</p>

<p>I do notice that the <a href=""https://docs.aws.amazon.com/emr/latest/APIReference/API_RunJobFlow.html"" rel=""nofollow noreferrer"">RunJobFlow</a> documentation says:</p>

<blockquote>
  <p>If the <code>KeepJobFlowAliveWhenNoSteps</code> parameter is set to <code>TRUE</code>, the cluster transitions to the <code>WAITING</code> state rather than shutting down after the steps have completed.</p>
</blockquote>

<p>Therefore, the Lambda function could check whether the cluster is in the <code>WAITING</code> state and, if so, shutdown the cluster. However, this would take repeated checking.</p>

<p>It <em>might</em> be possible to submit a final step that calls the EMR API to shutdown the cluster. This means that the cluster is effectively calling for its own termination as a final step. (I haven't tried this concept, but it would be a clean way of performing the shutdown without having to repeatedly check the status.)</p>

<p>There is also a similar discussion about shutting down idle clusters on this Question: <a href=""https://stackoverflow.com/q/49806543/174777"">How to terminate AWS EMR Cluster automatically after some time</a></p>
",4,CC BY-SA 4.0,62072351.0,2020-05-28T18:49:22.520,0.0,112.0,2020-05-28T23:06:30.600,2020-05-28T23:12:00.887,How to terminate an EMR Cluster from Lambda when all Steps are COMPLETE?,<python-3.x><amazon-web-services><aws-lambda><amazon-emr><aws-step-functions>,1.0,0.0,,,
109853737,62076034,0,[Monitor Metrics with CloudWatch - Amazon EMR](https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html) makes several references to the `isIdle` metric. I couldn't see any indication that it is deprecated.,2020-05-31T05:57:15.290,174777,CC BY-SA 4.0,True,1,,.,1,I couldn't see any indication that it is deprecated.,False,False,62076034.0,62072351.0,2020-05-28T23:12:00.887,1.0,"<p>I agree with your research. The optimal situation would be to set <code>KeepJobFlowAliveWhenNoSteps</code> to <code>FALSE</code> to have the cluster self-terminate.</p>

<p>I do notice that the <a href=""https://docs.aws.amazon.com/emr/latest/APIReference/API_RunJobFlow.html"" rel=""nofollow noreferrer"">RunJobFlow</a> documentation says:</p>

<blockquote>
  <p>If the <code>KeepJobFlowAliveWhenNoSteps</code> parameter is set to <code>TRUE</code>, the cluster transitions to the <code>WAITING</code> state rather than shutting down after the steps have completed.</p>
</blockquote>

<p>Therefore, the Lambda function could check whether the cluster is in the <code>WAITING</code> state and, if so, shutdown the cluster. However, this would take repeated checking.</p>

<p>It <em>might</em> be possible to submit a final step that calls the EMR API to shutdown the cluster. This means that the cluster is effectively calling for its own termination as a final step. (I haven't tried this concept, but it would be a clean way of performing the shutdown without having to repeatedly check the status.)</p>

<p>There is also a similar discussion about shutting down idle clusters on this Question: <a href=""https://stackoverflow.com/q/49806543/174777"">How to terminate AWS EMR Cluster automatically after some time</a></p>
",4,CC BY-SA 4.0,62072351.0,2020-05-28T18:49:22.520,0.0,112.0,2020-05-28T23:06:30.600,2020-05-28T23:12:00.887,How to terminate an EMR Cluster from Lambda when all Steps are COMPLETE?,<python-3.x><amazon-web-services><aws-lambda><amazon-emr><aws-step-functions>,1.0,0.0,,,
110109648,62216661,0,Thanks for the hint @Kloar. It seems this is somewhat outdated. This post https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/ sounds like Lambda can now be controlled better. My guess is that this might not have found its way into terraform yet. So the workaround suggested here might still be the best option for the now.,2020-06-08T09:13:35.513,8195190,CC BY-SA 4.0,True,1,It,.,0,It seems this is somewhat outdated.,False,False,,,,,,,,,,,,,,,,,,,,
110159882,62277602,1,return the `id` and `label` and not just the properties. you can also use the deprecated `valueMap(true)` instead,2020-06-09T16:11:36.657,5990248,CC BY-SA 4.0,True,1,,instead,0,you can also use the deprecated `valueMap(true)` instead,False,False,62277602.0,62276357.0,2020-06-09T07:51:10.340,5.0,"<p>In this case you should use <code>project</code>:</p>

<pre><code>g.V().has('name', 'josh').
  project('properties', 'out', 'in').
    by(valueMap().
        with(WithOptions.tokens)).
    by(out().values('name').fold()).
    by(__.in().values('name').fold())
</code></pre>

<p>example: <a href=""https://gremlify.com/c8nm1j16033g"" rel=""noreferrer"">https://gremlify.com/c8nm1j16033g</a></p>
",2,CC BY-SA 4.0,62276357.0,2020-06-09T06:29:04.170,2.0,46.0,,2020-06-09T07:51:10.340,"Get a vertex with its label, properties and in/out vertices and their properties?",<graph><gremlin><tinkerpop3><amazon-neptune>,1.0,0.0,,1.0,
110175732,62294868,0,thanks.  i believe the new SDK has deprecated all these classes. i get class not found. error: package software.amazon.awscdk.services.ssm does not exist,2020-06-10T04:15:34.720,13488243,CC BY-SA 4.0,True,1,,.,0,i believe the new SDK has deprecated all these classes.,False,False,62294868.0,62294751.0,2020-06-10T02:29:39.793,0.0,"<p>I think GitHub may be of help. I searched for <a href=""https://github.com/search?q=SsmClient+getParameter+language%3Ajava&amp;type=Code"" rel=""nofollow noreferrer"">SsmClient getParameter language:java</a> and some of the results seem promising.</p>

<p><a href=""https://github.com/efenderbosch/discord-pogo-bot/blob/bd4118e13a022dc67bd9e112ce3e794799f34c53/src/main/java/net/fender/discord/DiscordConfiguration.java"" rel=""nofollow noreferrer"">This one for example:</a></p>

<pre><code>    public static String getDiscordToken(SsmClient ssmClient) {
        GetParameterRequest request = GetParameterRequest.builder().
                name(""/discord/token"").
                withDecryption(Boolean.TRUE).
                build();
        GetParameterResponse response = ssmClient.getParameter(request);
        return response.parameter().value();
    }
</code></pre>
",2,CC BY-SA 4.0,62294751.0,2020-06-10T02:14:04.380,0.0,87.0,,2020-06-10T08:03:36.320,looking for a sample code to read parameter value from aws parameter store,<java><amazon-web-services><amazon-iam>,2.0,3.0,,,
110246324,37499226,0,"the Action ""lambda:InvokeAsync"" is deprecated, you just need the ""lambda:InvokeFunction""",2020-06-12T00:19:37.523,1000075,CC BY-SA 4.0,True,1,,"""",0,"the Action ""lambda:InvokeAsync"" is deprecated, you just need the ""lambda:InvokeFunction""",False,False,,,,,,,,,,,,,,,,,,,,
110342473,60118032,0,see this a bit outdated article: https://medium.com/slido-dev-blog/aws-glue-python-shell-jobs-first-impressions-d26582123829,2020-06-15T13:58:43.610,3891053,CC BY-SA 4.0,True,1,,https://medium.com/slido-dev-blog/aws-glue-python-shell-jobs-first-impressions-d26582123829,0,see this a bit outdated article: https://medium.com/slido-dev-blog/aws-glue-python-shell-jobs-first-impressions-d26582123829,False,False,60118032.0,60117544.0,2020-02-07T17:06:46.017,0.0,"<blockquote>
  <p>which are the best/typical use cases for each of them? Some document
  says python shell job is suitable for simple jobs whereas spark for
  more complicated jobs, is that correct?</p>
</blockquote>

<p>AWS Glue is quick development facility/service for ETL jobs, given by AWS.
IMHO it is very quick development if you know what needs to be done in your etl pipeline.</p>

<ul>
<li><p>Glue has components like Discover, Develop, Deploy.
In Discover... <em>automatic crawling (run or schedule a crawler multiple times)</em> is the important feature which differentiates with other tools I observed.</p></li>
<li><p>Glue has seems like integration feature to connect to AWS eco system services (where as spark you need to do it)</p></li>
</ul>

<p>Typical use case of AWS Glue could be...<br>
1) Load data from Dataware houses.<br>
2) Build a data lake on amazon s3 .  </p>

<p><a href=""https://www.slideshare.net/AmazonWebServices/abd315serverless-etl-with-aws-glue"" rel=""nofollow noreferrer"">See this presentation of AWS for more insight.</a></p>

<p>Custom Spark Job also can do the same thing, but it needs to be developed from the scratch. and it doesnt have in built automatic crawling kind of feature.</p>

<p>But if you develop a spark job for etl you have fine grained control to implement complicated jobs. </p>

<p>Both glue, spark has same goal for ETL. AFAIK, Glue is for simple jobs such as loading from source to destination. Where as Spark job can do wide variety of transformations in a controlled way. </p>

<blockquote>
  <p><strong>Conclusion :</strong>  For simple use cases of ETL (which can be done with out much development experience ) go with Glue. For customized ETL
  which has many dependencies/transformations go with spark job.</p>
</blockquote>
",5,CC BY-SA 4.0,60117544.0,2020-02-07T16:34:29.610,1.0,1123.0,2020-02-08T05:45:30.500,2020-02-08T05:45:30.500,What are the best use cases for aws glue python shell jobs vs. spark jobs?,<amazon-web-services><apache-spark><aws-glue>,1.0,0.0,,,
110474963,62461548,0,"I don't know if flash is deprecated (never interested in it), but yes, this widget uses flash. Do you mind to change the widget to add to your site before spending other time on it?",2020-06-19T09:43:34.953,4690946,CC BY-SA 4.0,True,1,,.,1,"I don't know if flash is deprecated (never interested in it), but yes, this widget uses flash.",False,False,,,,,,,,,,,,,,,,,,,,
110475181,62461548,0,"I mean go to amazon website and choose another widget to add to your website, a widget which doesn't use flash; if flash is really deprecated I think amazon already developed other widgets will not going to break in 6 months.",2020-06-19T09:51:10.663,4690946,CC BY-SA 4.0,True,1,,deprecated,1,"I mean go to amazon website and choose another widget to add to your website, a widget which doesn't use flash; if flash is really deprecated",False,False,,,,,,,,,,,,,,,,,,,,
110475284,62461548,0,"Yes I'm able to display the prime day ads, but I was having trouble with this one and I think some other ones that display a specific product. I'll have to post details on those in the morning though. And about flash being deprecated, I just know that chrome tells me it's no longer supporting flash starting at the end of 2020. I think it told me that because I had some flash plugin that I installed to use this really old website. I don't have the plugin anymore, but my firefox has a flash plugin apparently",2020-06-19T09:55:25.133,6331353,CC BY-SA 4.0,True,1,,.,1,"And about flash being deprecated, I just know that chrome tells me it's no longer supporting flash starting at the end of 2020.",False,False,,,,,,,,,,,,,,,,,,,,
110574192,62524780,0,So the problem was that the user data file was outdated...for whatever reason it was using an old user data file in the instance. Is there a way to refresh the user data file per instance restart?,2020-06-23T00:01:27.953,13689608,CC BY-SA 4.0,True,1,,.,0,So the problem was that the user data file was outdated...for whatever reason it was using an old user data file in the instance.,False,True,62524780.0,62524748.0,2020-06-22T22:57:13.433,1.0,"<p>To troubleshoot <code>UserData</code> issues, the best thing to do is to login to an instance,
and inspect one of <code>UserData</code> log files.</p>
<p>Most impotently <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html"" rel=""nofollow noreferrer"">/var/log/cloud-init-output.log</a>:</p>
<blockquote>
<p>The cloud-init output log file (/var/log/cloud-init-output.log) <strong>captures console output</strong> so it is <strong>easy to debug</strong> your scripts following a launch if the instance does not behave the way you intended.</p>
</blockquote>
<p>Also your <code>UserData</code> script will be located in <code>/var/lib/cloud/instances/&lt;instance-id&gt;/</code>. Thus, once you are in the instance you can manually try to run it and <strong>fix/debug while in the instance</strong>.</p>
",11,CC BY-SA 4.0,62524748.0,2020-06-22T22:53:01.040,1.0,69.0,2020-06-23T11:36:06.957,2020-06-23T11:36:06.957,AWS EC2 User Data script isnt working as expected,<amazon-web-services><amazon-s3><amazon-ec2>,2.0,0.0,,,
110593746,48559373,0,Just an update: `AvroParquetReader.builder(path)` is deprecated an will be removed in version 2.0.0,2020-06-23T14:03:46.257,1530948,CC BY-SA 4.0,True,1,,deprecated,0,Just an update: `AvroParquetReader.builder(path)` is deprecated,False,False,48559373.0,46831273.0,2018-02-01T09:30:38.073,5.0,"<pre><code>String SCHEMA_TEMPLATE = ""{"" +
                        ""\""type\"": \""record\"",\n"" +
                        ""    \""name\"": \""schema\"",\n"" +
                        ""    \""fields\"": [\n"" +
                        ""        {\""name\"": \""timeStamp\"", \""type\"": \""string\""},\n"" +
                        ""        {\""name\"": \""temperature\"", \""type\"": \""double\""},\n"" +
                        ""        {\""name\"": \""pressure\"", \""type\"": \""double\""}\n"" +
                        ""    ]"" +
                        ""}"";
String PATH_SCHEMA = ""s3a"";
Path internalPath = new Path(PATH_SCHEMA, bucketName, folderName);
Schema schema = new Schema.Parser().parse(SCHEMA_TEMPLATE);
Configuration configuration = new Configuration();
AvroReadSupport.setRequestedProjection(configuration, schema);
ParquetReader&lt;GenericRecord&gt; = AvroParquetReader.GenericRecord&gt;builder(internalPath).withConf(configuration).build();
GenericRecord genericRecord = parquetReader.read();

while(genericRecord != null) {
        Map&lt;String, String&gt; valuesMap = new HashMap&lt;&gt;();
        genericRecord.getSchema().getFields().forEach(field -&gt; valuesMap.put(field.name(), genericRecord.get(field.name()).toString()));

        genericRecord = parquetReader.read();
}
</code></pre>

<p>Gradle dependencies</p>

<pre><code>    compile 'com.amazonaws:aws-java-sdk:1.11.213'
    compile 'org.apache.parquet:parquet-avro:1.9.0'
    compile 'org.apache.parquet:parquet-hadoop:1.9.0'
    compile 'org.apache.hadoop:hadoop-common:2.8.1'
    compile 'org.apache.hadoop:hadoop-aws:2.8.1'
    compile 'org.apache.hadoop:hadoop-client:2.8.1'
</code></pre>
",3,CC BY-SA 3.0,46831273.0,2017-10-19T13:38:24.900,12.0,6246.0,,2018-12-14T23:31:13.963,Read parquet data from AWS s3 bucket,<java><amazon-web-services><amazon-s3><parquet>,1.0,0.0,,1.0,
110612301,62547230,0,"I think the aws tutorial is outdated? they're using amplify push --y instead, i tried the amplify push --simple but still fails, should i just remove the aws-exports.js on gitignore? @Kevin Gelpes",2020-06-24T03:55:07.183,5042771,CC BY-SA 4.0,True,1,,?,0,I think the aws tutorial is outdated?,False,False,62547230.0,62547071.0,2020-06-24T03:40:12.313,1.0,"<p>For security reasons aws-exports.js is added to git ignore by default on amplify projects. This file is generated when you run the <code>amplify push</code> command.</p>
<pre><code>backend:
  phases:
    # IMPORTANT - Please verify your build commands
    build:
      commands:
        - '# Execute Amplify CLI with the helper script'
        - amplifyPush --simple
</code></pre>
<p>Make sure you have <code>amplifyPush --simple</code> on your Build Settings -&gt; amplify.yml file so you generate aws-exports on your amplify builds</p>
",1,CC BY-SA 4.0,62547071.0,2020-06-24T03:20:00.970,1.0,173.0,,2020-06-24T03:40:12.313,Why does AWS-Amplify build fails when Auth feature is added,<reactjs><amazon-web-services><aws-amplify>,1.0,3.0,,,
110656808,37153571,0,"Link is deprecated, could you update please?",2020-06-25T10:21:12.737,202037,CC BY-SA 4.0,True,1,,?,0,"Link is deprecated, could you update please?",False,False,,,,,,,,,,,,,,,,,,,,
110690272,61995539,1,"Thanks for this. It's extremely frustrating that Amazon's ""latest"" docs refer to outdated information",2020-06-26T09:50:37.850,682673,CC BY-SA 4.0,True,1,,information,0,"It's extremely frustrating that Amazon's ""latest"" docs refer to outdated information",False,False,61995539.0,61201006.0,2020-05-25T04:00:01.657,3.0,"<p>On Amazon Linux 2 the folder is:</p>

<pre><code>/opt/elasticbeanstalk/config/private/logtasks/bundle
</code></pre>

<p>The <code>.ebextensions/mydaemon-logfiles.config</code> should be:</p>

<pre><code>files: 
  ""/opt/elasticbeanstalk/config/private/logtasks/bundle/mydaemon-logs.conf"":
    mode: ""000644""
    owner: root
    group: root
    content: |
       /var/mydaemon/deeperlogs/*.log

container_commands:
  append_deeperlogs_to_applogs:
    command: echo -e ""\n/var/log/eb-docker/containers/eb-current-app/deeperlogs/*"" &gt;&gt; /opt/elasticbeanstalk/config/private/logtasks/bundle/applogs
</code></pre>

<p>The <code>mydaemon-logfiles.config</code> also adds <code>deeperlogs</code> into <code>applogs</code> file. Without it <code>deeperlogs</code> will not be included in the download log zip bundle. Which is intresting, because the folder will be in the correct location, i.e., <code>/var/log/eb-docker/containers/eb-current-app/deeperlogs/</code>. But without being explicitly listed in <code>applogs</code>, it will be skipped when zip bundle is being generated. </p>

<p>I tested it with single docker environment (3.0.1). </p>

<p>The full log bundle successful contained <code>deeperlogs</code> with correct log data:</p>

<p><a href=""https://i.stack.imgur.com/Vxzlu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vxzlu.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/1LYnw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1LYnw.png"" alt=""enter image description here""></a></p>

<p>Hope that this will help. I haven't found any references for that. AWS documentaiton does not document this, as it is mostly based on Amazon Linux 1, not Amazon Linux 2. </p>
",8,CC BY-SA 4.0,61201006.0,2020-04-14T05:04:58.603,9.0,177.0,2020-04-14T05:12:25.023,2020-08-06T07:40:43.660,Elastic Beanstalk: log task customization on Amazon Linux 2 platforms,<linux><amazon-web-services><docker><amazon-elastic-beanstalk>,2.0,0.0,,1.0,
110824652,52682996,0,"@DeveloperDave Just updated those links. Keep in mind, this answer was written in 2018. So it might be slightly outdated. I linked the links back to v1 of the documentation. However since then v2 has been released. The answer was completely valid at time of writing it. And all of the content in the answer is self contained (not unnecessarily to outside links). So it doesn't violate any Stack Overflow guidelines about only using links in answers.",2020-07-01T01:40:53.640,894067,CC BY-SA 4.0,True,1,it,.,0,So it might be slightly outdated.,False,False,,,,,,,,,,,,,,,,,,,,
110943196,43583352,0,get-login is deprecated. I added an updated answer here: https://stackoverflow.com/questions/41379808/authorization-token-has-expired-issue-aws-cli-on-macos-sierra/62736623#62736623,2020-07-05T03:51:36.753,8808193,CC BY-SA 4.0,True,1,login,.,0,get-login is deprecated.,False,False,43583352.0,41379808.0,2017-04-24T08:36:54.003,45.0,"<p>Neither of solutions above worked for my but I found that when I set region in ecr login command it worked.</p>

<p><code>aws ecr get-login --region us-west-2</code></p>
",3,CC BY-SA 3.0,41379808.0,2016-12-29T12:32:22.153,45.0,29513.0,,2020-07-05T03:49:16.013,`Authorization Token has expired` issue AWS-CLI on MacOS Sierra,<amazon-web-services><docker><aws-cli><macos-sierra>,22.0,1.0,,7.0,
111574648,52982922,0,"this is not working, may be due AmazonS3Client, because it says deprecated, and what about cache as  @Asif Saeed mentioned, that it will call it again as url changing",2020-07-26T03:33:26.103,8245963,CC BY-SA 4.0,True,1,,changing,1,"this is not working, may be due AmazonS3Client, because it says deprecated, and what about cache as  @Asif Saeed mentioned, that it will call it again as url changing",False,False,,,,,,,,,,,,,,,,,,,,
111717597,35061033,0,"as of 2020 get-login is deprecated, use get-login-password instead. https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html",2020-07-30T15:59:35.400,4546901,CC BY-SA 4.0,True,1,login,.,0,"as of 2020 get-login is deprecated, use get-login-password instead.",False,False,35061033.0,34689445.0,2016-01-28T12:04:45.247,112.0,"<p>if you run <code>$(aws ecr get-login --region us-east-1)</code> it will be all done for you</p>
",9,CC BY-SA 3.0,34689445.0,2016-01-09T03:38:41.623,179.0,83844.0,2020-02-04T12:51:17.837,2020-07-23T14:27:42.267,"Can't push image to Amazon ECR - fails with ""no basic auth credentials""",<amazon-web-services><docker><aws-ecr>,40.0,10.0,,35.0,
111756365,10676950,0,"running MySQL 8.0.17. Apart from replacing ```key_buffer``` with ```key_buffer_size``` as Ivan suggests, I had to remove the ```query_cache_size``` and ```query_cache_limit``` lines. MySQL docs say
""The query cache is deprecated as of MySQL 5.7.20, and is removed in MySQL 8.0. """,2020-07-31T21:12:09.370,4163443,CC BY-SA 4.0,True,1,,"""",0,"MySQL docs say
""The query cache is deprecated as of MySQL 5.7.20, and is removed in MySQL 8.0. """,False,False,,,,,,,,,,,,,,,,,,,,
111849266,63247775,1,I did not need to do this. My Browser was outdated and the HTML5 audio did not support that version. I updated the browser and it works perfectly fine. But thank you for this solution as well. I will look into this as well and try to explore it more. Might come handy in future. Thanks,2020-08-04T17:45:47.330,13299635,CC BY-SA 4.0,True,1,Browser,.,1,My Browser was outdated and the HTML5 audio did not support that version.,False,False,63247775.0,63236398.0,2020-08-04T13:19:18.180,0.0,"<p>This is related to the backend. You need to provide 'contentType' as audio. The implementation is different based on the programming language you are using for the backend. Please check the sample code in PHP.</p>
<pre><code> $s3-&gt;create_object($bucket, $file_name, array(
                    'fileUpload' =&gt; $resized_image,
                    'contentType' =&gt; $_FILES['image']['type'],
                    'acl' =&gt; AmazonS3::ACL_PUBLIC
            ));
</code></pre>
<p>The above issue is not limited to audio files, it's for PDF, Images as well. Follow are some helpful reference</p>
<p><a href=""https://www.digitalocean.com/community/questions/how-to-view-pdf-s-stored-in-spaces-without-them-being-downloaded"" rel=""nofollow noreferrer"">How to view pdf's stored in spaces without them being downloaded</a></p>
<p><a href=""https://stackoverflow.com/questions/4751360/amazon-s3-image-downloading-instead-of-displaying-in-browser"">amazon s3 - image downloading instead of displaying in the browser</a></p>
",1,CC BY-SA 4.0,63236398.0,2020-08-03T20:18:28.843,0.0,26.0,2020-08-04T17:43:48.720,2020-08-04T17:46:21.153,Audio URL get downloaded instead of playing in the browser - DigitalOcean Spaces/Buckets,<html><url><amazon-s3><html5-audio><digital-ocean>,2.0,1.0,,,
111893971,63275335,0,"Hi.  I tried /home/webapp as well and that didn't work.  Should it?  In any case I am not able to add the .ebextensions folder to the root of my spring boot war package as is needed.  I Googled around for it and there are many different solutions, but they all seem to be outdated.  None worked for me.  Do you have any knowledge on that? I am using Maven",2020-08-06T06:06:30.027,1304505,CC BY-SA 4.0,True,1,, ,0,"and there are many different solutions, but they all seem to be outdated.  ",False,False,63275335.0,63275250.0,2020-08-06T00:43:56.577,0.0,"<p>Yes, this is permission issues. Your app runs under <code>webapp</code> user, while <code>/var/log</code> is own by root. Thus you can't write to it.</p>
<p>The proper way of adding your log files to be recognized by EB is through config files.</p>
<p>Specifically, assuming Amazon Linux 2, you can create <code>.ebextensions/mylogfiles.config</code> with the content of:</p>
<pre><code>files: 
  &quot;/opt/elasticbeanstalk/config/private/logtasks/bundle/myapplogs.conf&quot;:
    mode: &quot;000644&quot;
    owner: root
    group: root
    content: |
       /var/app/current/log/*.log
</code></pre>
<p>Obviously, <code>/var/app/current/log/*.log</code> would point to location where your app stores its log files. <code>/var/app/current</code> is the home folder of your app.</p>
",8,CC BY-SA 4.0,63275250.0,2020-08-06T00:31:46.233,2.0,59.0,2020-08-06T16:26:57.797,2020-08-06T16:26:57.797,Spring Boot on AWS Elastic Beanstalk and logging to a file,<amazon-web-services><spring-boot><amazon-ec2><amazon-elastic-beanstalk>,1.0,0.0,,,
111903000,63276981,0,"OK, to address these points:
1. Nugets were pretty out of date, scrub move on my part. This did not resolve the issue however
2. No middleware
3. I don't actually have a startup.cs as I am not running .NET Core (please correct me if I have misunderstood this entirely)
4. I'll let you know the results of this shortly
5. Link in Question",2020-08-06T11:44:24.747,4259466,CC BY-SA 4.0,True,1,Nugets,.,0,"Nugets were pretty outdated, scrub move on my part.",False,False,,,,,,,,,,,,,,,,,,,,
112433710,63562739,0,"@shreyaskar, The links I have shared with you have the reasons as well. It could have due to missing AWS credentials not configured using AWS CLI configure command, due to outdated Python Modules like certifi, requests, urllib3 or pyopenssl, due to outdated version of AWS CLI, due to missing environment variables in Windows related to HTTP/HTTPS proxy, due to CA certificate issues related to content or location or due to Fiddler running.",2020-08-25T15:29:15.263,11758843,CC BY-SA 4.0,True,1,,.,1,"missing AWS credentials not configured using AWS CLI configure command, due to outdated Python Modules like certifi, requests, urllib3 or pyopenssl, due to outdated version of AWS CLI, due to missing environment variables in Windows related to HTTP/HTTPS proxy, due to CA certificate issues related to content or location or due to Fiddler running.",False,False,63562739.0,63557500.0,2020-08-24T14:09:48.990,0.0,"<p>This question has already been answered on Stack Overflow before.</p>
<p>Try out the following solutions:</p>
<ul>
<li><a href=""https://stackoverflow.com/a/54773707/11758843"">Reset AWS Credentials using AWS Configure</a></li>
<li><a href=""https://stackoverflow.com/a/40517581/11758843"">Issue Due to Fiddler</a></li>
<li><a href=""https://stackoverflow.com/a/48874444/11758843"">Reset HTTP/HTTPS Proxy Related Environment Variables</a></li>
<li><a href=""https://stackoverflow.com/a/53746691/11758843"">Reinstall and Upgrade AWS CLI</a></li>
<li><a href=""https://stackoverflow.com/a/55117069/11758843"">Using AWS_CA_BUNDLE Environment Variable</a></li>
<li><a href=""https://stackoverflow.com/a/62939329/11758843"">Moving CA Certificate PEM File in the Right Folder</a></li>
<li><a href=""https://stackoverflow.com/a/27847883/11758843"">Verifying CA Certificate</a></li>
<li><a href=""https://stackoverflow.com/a/48134650/11758843"">Install certifi Python Module</a></li>
<li><a href=""https://stackoverflow.com/a/48810442/11758843"">Install pyopenssl Python Module</a></li>
<li><a href=""https://stackoverflow.com/a/52051009/11758843"">Adding Trusted Root CA Details</a></li>
<li><a href=""https://stackoverflow.com/a/44390144/11758843"">Adding Trusted Host</a></li>
<li><a href=""https://stackoverflow.com/a/55996714/11758843"">Fixing the Version of requests and urllib3 Python Modules</a></li>
<li><a href=""https://stackoverflow.com/a/31915123/11758843"">Fixing CA Certificate Content and Location</a></li>
</ul>
<p>Note: There is a solution related to disabling the SSL verification but that is not recommended.</p>
",5,CC BY-SA 4.0,63557500.0,2020-08-24T08:40:14.813,0.0,34.0,,2020-08-24T14:09:48.990,botocore.exceptions.SSLError: SSL validation failed on WIndows,<python><amazon-web-services><ssl><boto3>,1.0,0.0,,,
112580441,63663332,0,Normally i would agree. But in this instance - I disagree.  Pointing to the definitive source for the procedure is better than re-producing it here in case AWS make changes to the process and it would leave this answer out of date.  I put that answer here to make clear the feature exists now for future reference.,2020-08-31T05:31:07.820,6317620,CC BY-SA 4.0,True,1,, ,0,AWS make changes to the process and it would leave this answer outdated.  ,False,False,63663332.0,59576932.0,2020-08-31T00:19:50.087,0.0,"<p>The ability to do this is now available since August 2020.</p>
<p><a href=""https://docs.aws.amazon.com/glue/latest/dg/resuming-workflow.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/glue/latest/dg/resuming-workflow.html</a></p>
",2,CC BY-SA 4.0,59576932.0,2020-01-03T10:28:24.363,0.0,296.0,,2020-08-31T00:19:50.087,How to run/re-run subset of jobs AWS Glue workflow?,<amazon-web-services><aws-glue>,1.0,1.0,,,
112679866,22210077,0,I want to apply this solution to my app but request module is deprecated I want to migrate the code using axios is ther anyone could help me?,2020-09-03T10:00:17.157,9043355,CC BY-SA 4.0,True,1,,deprecated,0,I want to apply this solution to my app but request module is deprecated,False,False,,,,,,,,,,,,,,,,,,,,
